## 23 мая 2024 - MeTTa / Hyperon альфа-релиз - Алексей Потапов — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/ayUw1f98iUY/hqdefault.jpg)](https://youtu.be/ayUw1f98iUY)


Основные моменты:

Алексей Потапов, ведущий ученый и руководитель разработки искусственного интеллекта в Singularity.net, рассказывает о последних достижениях в разработке Общего искусственного интеллекта (AGI) проекта Singularity.net, в частности о языке программирования Мета.
Мета - это R&D-платформа для создания различных когнитивных архитектур и моделей, нацеленная в конечном итоге на разработку AGI. Недавно вышел альфа-релиз Мета, и Алексей рассказывает об основных особенностях языка:

 - Синтаксис в виде s-выражений, похожий на Lisp
 - Основан на символьных вычислениях и унификации выражений
 - Поддерживает недетерминизм и частичное определение функций
 - Имеет систему типов с концепцией "атомов" для метапрограммирования
 - Интегрируется с Python и позволяет использовать внешние библиотеки
 
Мета рассматривается как язык для представления знаний и логических рассуждений, в том числе в связке с вероятностными моделями и большими языковыми моделями (LLM). Алексей демонстрирует примеры использования Мета для решения простых задач.
Одна из ключевых особенностей Мета - возможность интеграции символьных и субсимвольных (нейросетевых) вычислений. Мета позволяет встраивать вызовы нейросетевых моделей, в том числе LLM, в символьные программы.
Алексей также рассказывает о библиотеке Metamoto, которая обеспечивает интеграцию LLM с Мета, позволяя использовать LLM в качестве компонентов символьных программ на Мета.
Обсуждаются вопросы интерпретируемости и прозрачности систем, построенных на основе LLM и Мета. Отмечается, что полная интерпретируемость недостижима, но можно ослабить проблему, используя комбинацию символьных и субсимвольных методов.
Затрагиваются вопросы интеграции Мета с внешними базами знаний, такими как Wikidata, DBpedia, SUMO. Отмечается, что это технически возможно, но требует дополнительной проработки.
В заключение Алексей упоминает о других компонентах платформы Singularity.net, в частности о распределенном Atom Space для хранения больших баз знаний, и о планах использования Мета для композиции сервисов искусственного интеллекта на платформе.



S00 [00:00:02]  : Коллеги, всем добрый вечер. У нас в сообществе есть несколько добрых традиций, и одна из добрых традиций раз в год примерно приглашать Алексея Потапова, ведущего ученого и руководителя разработки искусственного интеллекта и общего искусственного интеллекта в компании Singularity.net с просьбой рассказать его о последних достижениях в разработке Общего искусственного интеллекта проекта Singularity.net, под брендом OpenCock, сейчас с брендом Hyperon, и включая язык программирования метта. В этом году весной вышел новый релиз этих технологий, и Алексей сегодня нам расскажет об этом, а также о его понимании, куда мы движемся с общим искусственным интеллектом. Алексей, пожалуйста. 

S01 [00:00:53]  : Здравствуйте, спасибо за приглашение. Сразу приношу извинения, что мой доклад, в общем-то, не совсем про AGI будет. Вот, потому что у нас действительно был альфа-релиз OpenCock гиперона. В прошлом году я делал доклад по прогрессу в этом направлении, и в частности в языке мета. Соответственно, с тех пор прошло какое-то время, и язык дозрел до того, чтобы мы его официально зарелизили, к этому было приурочено создание туториалов на веб-сайте с песочницей, где можно без установки гиперона потыкать язык. Думаю, что мало кто помнит, и, может быть, не все были в прошлом году, поэтому я, наверное, начну с повторного обзора языка короткого. И потом немножко скажу, какие практические применения он начинает находить. Хотя, конечно, основные планы у нас — это двигаться с его помощью в сторону AGI. Сейчас мы, наконец, возвращаемся к фокусировке именно на разработке AGI-архитектуры. поскольку OpenCock Hyperon — это R&D-платформа, то есть фреймворк для создания разных моделей и когнитивных архитектур. Например, команда, точнее говоря, часть людей из NARS, которые разрабатывали OpenNARS, портировали его на мету и получилось гораздо компактнее, чем их предыдущая реализация. То есть сама платформа, она предназначена для разработки и экспериментирования с различными когнитивными архитектурами и другими подходами к AGI, но помимо платформы, естественно, у нас есть и собственное видение и планы на дальнейшую R&D, Research and Development, о которых я, к сожалению, сегодня не расскажу, но, надеюсь, через год мы точно поговорим, собственно, об конкретно AGI и нашем подходе к нему. А сейчас, пользуясь случаем, хочу порекламировать язык мету. Хотя сразу замечу, что альфа-релиз, он более или менее устаканившийся синтаксис содержит и какие-то базовые элементы, но к производительности и масштабированию мы вот только сейчас тоже приступаем, поэтому Это, наверное, будет иметь интерес только для тех, кому данный подход близок и кого это в целом заинтересует, но сразу многого ждать от использования меты, наверное, не стоит. Соответственно, Так. Экран шарится. 

S00 [00:05:00]  : Антон. Да, все видно, все видно. С мета сайтом. 

S01 [00:05:09]  : Так, а я вот тут между вкладками переключаюсь. Переключение идет? 

S00 [00:05:15]  : Да, мета, ланг, деф, лока, хост, 51, 73, видно. 

S01 [00:05:20]  : Хорошо. Ну, соответственно, есть сайт, который находится в общем доступе, но там он сейчас поднят на не очень мощном сервачке. У нас, соответственно, песочница работает на стороне сервера, и поэтому там не очень много выделяется потоков для этого дела и ресурсов. Вот есть идеи, планы перенести песочницу, ну не песочницу, но сделать вариант, когда интерпретатор мета будет компилироваться в WebAssembler, чтобы запускался на стороне клиента, тогда можно будет большему количеству пользователей играть с этим делом, но в этом есть ряд ограничений и недостатков, в частности, Нельзя будет какие-то дополнительные библиотеки так использовать, и в частности интероперабельность с Python, ну и плюс на сайте развернута актуальная версия, которая поддерживает все функции, которые будут работать, если вы будете устанавливать язык вручную. А с WebAssembler такое может не вполне получиться. Поэтому я в основном буду демонстрировать на локальной версии, потому что она будет работать шустрее. Соответственно, пока у нас есть ряд базовых туториалов, мы планируем добавлять, может быть не очень спеша, уже более advanced материалы по использованию зависимых типов для представления знаний, по использованию каких-то дополнительных библиотек. по реализации рассуждений, связки с нейросетками, LLM-ками в частности. Но для старта, чтобы получить представление о языке, этих начальных туториалов должно хватать. Соответственно, я напомню, что на текущий момент синтаксис может поменяться. Есть идея сделать более хаскеловский синтаксис, но в связи с ограничением ресурсов мы исходно выбрали Синтексис лисповский, поскольку он наиболее легко парсится и ближе всего к внутреннему представлению программы. Соответственно, на текущий момент синтексис у нас в виде с-выражений. Можно писать практически все, что угодно. Это все, что угодно. просто символьные выражения, которые, с одной стороны, представляют собой, собственно, саму программу, с другой стороны, могут трактоваться и как знания, к которым можно обращаться. В общем-то весь код, который пишется, он добавляется в то, что в OpenCog'е с незапамятных времен называется атомспейсом. Фактически исходный атомспейс — это гиперэлементографовое хранилище знаний. Но фактически оно изоморфно разным представлением. Есть реализация атомспейсов, где это действительно метаграф, есть, где это просто лес деревьев. В принципе, эти представления изоморфны. Вопрос лишь в эффективности. различного рода query в них. Но сейчас я в это не углубляюсь. Просто любые выражения, которые мы пишем, добавляются в некий контейнер в метте. Мы можем некоторые выражения сразу выполнять. они будут выполняться в контексте того контейнера, который, собственно, участвует в интерпретации. Соответственно, если выражения состоят из просто символов, которые никак дополнительно не определены, то эти символы и выражения над ними интерпретируются, собственно, в самих себя. Если есть какие-то операции, для которых либо через нативный код, либо в библиотеках или в самом спейсе, в самом этом контейнере даны какие-то определения, то эти выражения будут, соответственно, интерпретироваться и редюситься к какому-то значению. Соответственно, основной способ задания редукции выражения является равенство. Эти равенства, они таким же образом записываются, как выражение или подграфы метаграфов хранилища, но когда мы пытаемся выполнить некоторое выражение, которое содержит символы, соответствующие записанным равенством, то это выражение может, соответственно, эвалюироваться в какой-то результат. В этом туториале представлены примеры, которые наиболее близки и понятны программистам на традиционных языках, то есть так в традиционных языках задаются функции, как это на самом деле происходит в мете, я говорил в прошлом году и сейчас немного напомню чуть позже, но в целом сразу можно сказать, что есть некоторые различия, в частности у нас выражение, для которого определяется равенство, может включать какие-то прочие символы, и это не обязательно функция от переменных. И в этом случае это работает как частично определенная функция. То есть когда мы находим данное определение в нашей базе, Знаний, то происходит подстановка, и выражение к чему-то редуцируется. Если мы, соответственно, в базе знаний ничего не находим, то, как и было выше, наше выражение остается, как есть, нередуцированным. Ну, мы, соответственно, также можем сказать, что выражения начинают редуцироваться по мере добавления в space, то есть если у нас есть исходная попытка вычислить или интерпретировать выражение, которое никак еще не определено, оно остается как есть. Если же мы уже что-то добавили в space, то А следующий вызов будет приводить к редукции этого выражения к какому-то другому результату. Все происходит динамически, и это интенсионально, потому что предполагается, что когнитивная архитектура или какая-то протеиджай-система, которая будет работать на основе этого языка, она может содержать какие-то символы, символьные выражения, которые она в какой-то момент может не иметь возможности вычислить, но по ходу своей работы тем или иным способом, или из внешних данных, или в результате рассуждений она может приходить к каким-то новым равенствам, к новым правилам редукции, и в дальнейшем уже эти выражения будут способны превратить во что-то другое. Ну, например, человеку никто не мешает записать значок интеграла, скажем, от гиперболического тангенса, но если человек не знает правил интегрирования данной функции, он это символное выражение так и оставит, а когда узнает, соответственно, сможет вычислить. В равной мере мы можем вводить в функции переменные, И это выглядит, в принципе, более или менее так же, как и в традиционных языках программирования. Соответственно, данная функция просто составляет выражение, дублирующее переданный ей на вход аргумент. Мы в равной степени можем писать вообще абсолютно всё, что угодно в качестве аргументов, то есть у нас нет какого-то жёсткого ограничения на то, что переменная — это должна быть одинокая переменная, а не должна быть обёрнута в какое-то другое выражение. Соответственно, это уже ближе к паттерн-матчингу в функциональных языках программирования, но реализовано немножко по-другому, и чуть попозже я об этом скажу. Соответственно, если мы запускаем такую функцию на таком выражении, то оказывается возможным сопоставить данное выражение с записанным равенством левой ее частью, и в результате происходит преобразование переданного аргумента в некое новое выражение. Собственно, это является основой для будущей трансформации метаграфов как основу каких-то когнитивных действия и рассуждений. Таким образом, мы можем задавать, ну, заниматься деконструкцией выражений в стиле функциональных языков типа Хаскила и ему подобных. И это работает, в принципе, аналогичным образом. У нас есть возможность делать вещи похитрее. Например, мы можем передавать некое выражение, в котором могут содержаться и повторяющиеся переменные. Соответственно, там, где образец может быть сопоставлен передаваемым значением происходит редукция, в противном случае выражение остается нередуцированным. Это, собственно, основа философии метты, что если у нас что-то не определено, то оно просто-напросто не редуцируется и остается в том исходном символьном виде, в котором оно было задано. Соответственно, У нас нет такой четкой границы между, например, определением функций и определением конструкторов типов, как в Хаскеле. Потому что, по сути дела, структурно это одно и то же. Просто для одного определены равенства, а для другого не определены. Ну вот чем, скажем, конс, как конструктор типа списка отличается от кудора как функции. Ну, собственно, тем, что для кудора есть какое-то определенное равенство, а для конса нет. В общем-то, практически вот и вся разница. Еще одна особенность мета — это недетерминизм. Мы можем задавать столько равенств, сколько хотим, и, соответственно, вычисление некоторого выражения, для которого определено несколько равенств, будет приводить к множественным результатам. В определенных случаях мы, соответственно, будем иметь Ну, скажем так, если в Haskell у нас может быть набор кейсов для функций, которые являются фактически взаимоисключающими и последовательными, то в мете это действительно недетерминированный вызов функции или конструктора типа, в зависимости от того, как он определен. И результат является невзаимно исключающим. Например, если у нас есть два определения для функции, и мы вызываем функцию на значение аргумента, который подпадает только под одно из этих определений, мы получим единственный результат. Если у нас значение, которое подпадает под оба определения, мы получаем два результата. Это имеет свои неудобства для традиционного программирования, но эти неудобства могут покрываться другими синтаксическими и функциональными возможностями. которые специально предназначены для того, чтобы выполнять какие-то вещи последовательно, ну, например, там, выполнять сопоставление с образцом, образец за образцом в некотором порядке. А так у нас на самом деле то, что мы посылаем в Atomspace, оно не вполне гарантированно имеет какой-то порядок, и мы, в принципе, даже не всегда можем сказать, положили ли мы какие-то выражения в одном порядке или в другом. Поэтому здесь нет, в принципе, то есть AtomSpace в нашем случае — это такой multi-set, потому что В нём может лежать несколько элементов идентичных, но при этом это не упорядоченный контейнер. Хотя, в принципе, в зависимости от реализации, какие-то из атомспейсов могут поддерживать и порядок, но это их личное дело. Соответственно, так же, как мы можем в определении функций, если это считать вообще определением функций, писать в принципе все, что угодно, так же и вызов мы можем делать любым образом. Скажем, у нас есть что-то, что мы можем назвать функцией браузер, и мы ее можем поместить внутрь любого другого выражения, соответственно, Она будет редьющиться внутри этого выражения и будет конструироваться результат. который, соответственно, удовлетворяет результатам матчинга под выражение с равенствами. То есть мы видим, что здесь появляется, что Том из образов Майк, а Боб, соответственно, брат Сэма. То есть в этом выражении у нас результат интерпретации BrowserX. Да, кстати, обратите внимание, что мы можем передавать при вызове функций использовать также переменные. Собственно, это тоже одно из центральных отличий мета от других функциональных языков. Ну и здесь показывается, что результат, условно говоря, вычисления данного выражения, он также распространяется на подстановке для x в над выражении. Соответственно, по большому счету, у нас происходит не столько подстановка значений переменных, это, конечно, очень сложно для компиляции, над которой мы сейчас начали работать, но, тем не менее, очень удобно во многих других аспектах. В общем, происходит не просто подстановка значений переменных на основе передаваемых значений аргументов, а происходит фактически унификация выражений, которые мы хотим оценивать, с выражениями в определенных равенствах. Ну и, собственно, понятно, что, как и в обычных языках программирования, результат такой подстановки, если мы... Скажем, здесь вызвали функцию, сматчили левую часть с данным выражением, получили какой-то результат. Этот результат дальше продолжает оцениваться, то есть происходит чейнинг оценки выражений. Из хоть чуть-чуть относящегося к искусственному интеллекту, если это все собрать вместе, то недетерминизм с вызовом функций с некоторыми переменными можно использовать для решения простых задач. Простых в смысле размерности. Например... Да, это очень простой пример. Я, наверное... про тот пример, который, я думаю, он будет в другом месте. Но не суть. То есть, да, здесь немножко другое демонстрируется. Здесь демонстрируется то, что мы недетерминированные результаты можем без проблем передавать в детерминированные функции. То есть у нас есть недетерминированная функция bin и, например, детерминированная функция triple, Если мы передаем результат BIN в triple, то у нас, соответственно, получается два результата. Три единички и три нолика. Если мы, допустим, дважды вызываем BIN, то получается, соответственно, комбинация всех результатов. Таким образом, мы можем делать некоторую комбинаторику, создавать. На текущий момент это делается генерацией всех результатов, но сразу замечу, что вот этот механизм медитерминизма — это своего рода заглушка для полноценного inference в языке, потому что если мы посмотрим на, скажем, вероятностные языки программирования, которые тоже были одним из мотивационных оснований для дизайна мета, то там также пишется стахастический недетерминированный код со случайными выборами. Но дальше там происходит сэмплирование значений случайных переменных с последующим отсевом по заданным условиям. И вероятностное программирование является очень мощным средством для создания генеративных моделей. К сожалению, оно не смогло развиться до каких-то прям очень мощных приложений. Все, в общем-то, перешло на генеративные модели гораздо более узких классов, но зато которые гораздо легче масштабировать. Я имею в виду там стабильную диффузию в генерации изображений или большие языковые модели в языке, но в целом Это очень родственные вещи, когда мы говорим про вероятностное программирование. Ну так вот, одна из причин, почему вероятностное программирование не отмасштабировалось, потому что там использовались достаточно простые алгоритмы сэмплирования типа семейства Монта-Карло-Марков-Чейнс, когда, собственно, сэмплировались сначала случайные значения, и происходила прогонка всей программы до тех пор, пока хотя бы один раз не соблюдалось наложенное условие. После этого брался след выполнения этой программы и, точнее говоря, сделанные в нем случайные выборы, и в них вносились изменения в стиле маркерский цепочек. То есть находилось сначала хотя бы одно подходящее решение, а потом оно просто немножко похоже на мутации в генетических алгоритмах модифицировалось. То есть фактически, ну это какой ни на есть, но все-таки inference Хотя там можно придумывать кучу всего разного. Мы в свое время экспериментировали и с генетическими алгоритмами над следами выполнения вероятностных программ. Как мы, так и другие люди занимались амортизированным инференсом с использованием нейросеток над вероятностными программами. А также можно делать много чего другого, например, в стиле рассуждений, когда мы берем вот это вот условие, которое мы хотим проверить, потому что без условия это достаточно скучно, это unconditioned inference, а нас всегда интересует все-таки какой-то условный байосовский вывод, потому что к нему, в общем-то, сходятся практически все формы рассуждений. В итоге, если у нас есть какое-то условие, которое мы в конце должны удовлетворить после результатов сэмплирования, то, вообще говоря, мы можем это условие заранее попытаться проанализировать и протолкнуть ближе к случайным выборам, чтобы не вслепую что-то сэмплировать, а сразу знать какие-то значения. Например, если мы берем и в качестве такой очень как это сказать, тестовые генеративные модельки, просто генерируем все битовые строки заданной длины, а в качестве условия у нас заключается в том, чтобы все элементы этой строки были единичками, то, в общем-то, большинство движков вероятностного программирования будут вслепую перебирать, Понятно, что им будет очень сложно найти этот единственный результат, а по большому счету мы можем просто из структуры самого условия сразу вывести, что, в общем-то, нам сэмплировать-то ничего и не надо. То есть, повторюсь, вероятностное программирование было тоже одним из... мотивационных оснований для создания метты, и, в частности, у нас в планах исходно как было, так и остается срастить генеративное вероятностное программирование с логическим выводом. Вот, и недетерминизм здесь — это, в принципе, такая заглушка на текущий момент, хотя, в общем-то, опять же, повторюсь, Гиперон как R&D-платформа допускает и другие модели вычислений. Но я немножко отвлёкся на всё-таки что-то, относящееся хоть немного к AJAI. Возвращаясь к самому языку, соответственно, тут без проблем можно задавать рекурсию. в структуре очень похожей на то, что можно встретить в Лиспе, схеме, Хаскеле и прочих языках. И, собственно, когда мы с рекурсию скрещиваем с недетерминизмом, то возникают уже интересные вещи. В частности, вот я как раз хотел показать пример решения задачи о сумме подмножеств на основе такой стахастической рекурсии, или недетерминированной в данном случае рекурсии, то есть когда у нас есть какие-то случайные функции, возвращающие 0 к 1, и мы фактически порождаем все решения для некоторой задачи. Повторюсь, на данный момент будут тупо перебираться, даже без всяких эвристик или метаевристик, или рассуждений, или еще чего-то. То есть это на текущий момент именно заглушка. Нас интересовало, чтобы это по форме было компактно, удобно. походила на какие-то другие языки программирования. Ну и в данном случае у нас просто процедурка генерации бинарного листа. Списка просто генерирует все возможные списки. заданной длины, ну, точнее говоря, длины, соответствующей списку заданному в качестве условия задач. Ну и в данном случае мы ищем подмножество заданного множества, сумма у которого будет равна 20. Ну и, соответственно, оно находит, что 3 плюс 17 действительно 20. Выглядит все это достаточно компактно. Нам единственное надо посчитать скалярное произведение сгенерированного бинарного списка с, соответственно, заданным списком. И на основе этого мы проверяем вот это то самое условие, про которое я говорил, которое может встречаться немножко в другом контексте, когда мы делаем вероятностный, условный вероятностный вывод. Здесь это не условный вероятностный вывод, а скорее недетерминированный вывод для решения какой-то задачи. Но, тем не менее, по структуре это все очень и очень похоже. Это выглядит так, как будто мы в принципе и не пишем алгоритмы решения, а пишем просто, создавляем описание задачи, а интерпретатор, язык нам эту задачу за нас как бы решает. Вот. Это, в принципе, одно из удобств, которые присутствуют и в логических языках типа Prologo, и в вероятностных языках типа WebPPL, там, Church, Venture, Anglican и прочих. Как на самом деле все это… А, давайте, наверное, все-таки здесь… У меня локально немножко не отображается. в песочнице расцветка, поэтому переключусь на официальный сайт, где она отражается. Все это, в принципе, можно использовать для разных видов реализации логических рассуждений. Здесь в туториалах ничего сложного нет, у нас, понятное дело, есть Свои люди, которые долгое время раньше работали над классическим опенкогом и реализовывали, скажем, вероятностные логические сети на них, где гораздо более навороченные системы рассуждений присутствуют. Их также портируют на метту сейчас. Ну, собственно, я, наверное, в прошлый раз тоже немножко рассказывал. В классическом OpenCog там, соответственно, был Atomspace как метаграф знаний, был движок, запросов к этому метаграфу, поверх которого, точнее говоря, с ним был срочен язык Atomis, который фактически вырос из языка запросов в недостаточно полноценный язык программирования. У него такой задачи не было. но достаточно сложный, чтобы уже называться просто языком запросов. Но, к сожалению, его не хватало, чтобы делать логические рассуждения, и поэтому там был разработан дополнительный компонент, который назывался Unified Rule Engine, унифицированная машина правил. И он был очень прекрасен в том смысле, что он действительно позволял задавать разные наборы правил и при этом превращался или в нечеткую логику, или в обычную логику, или в вероятностные логические сети, которые Ну, там плюс-минус можно считать чем-то средним между расширенной и нечеткой логикой и байосовским выводом. Или во что-нибудь еще. Соответственно, в нашем случае необходимости дополнительно разрабатывать Unified Rule Engine, такой нет. Мы можем прямо на мете сразу писать разные наборы правил, но здесь в этих туториалах этого нет. Ну а к тому, что вот этот очень простенький примерчик, он, естественно, далеко не все, что можно пытаться с этим делать. Соответственно, Опять же, это можно делать в очень разных стилях, и мы планируем в продвинутых туториалах обсудить, какие варианты, какие идиоматические варианты к разработке системы вывода в мете могут присутствовать. То есть один из подходов — это, например, задавать равенство которые редюсят предикаты к значениям true и false или к каким-то расширенным значениям истинности. Другой вариант может быть представлять это в виде неравенств, а в виде просто каких-то выражений, которые не редьюсятся сами по себе, а непосредственно хранятся в базе знаний, и которые можно дальше будет экварировать, но про экварирование я чуть позже скажу. Соответственно, здесь используется функциональное представление, поскольку весь этот туториал, он отталкивается от более таких, как я сказал, традиционных языков программирования для большей понятности для широкой аудитории. Но здесь сразу видно, что мы можем За счет того, что мы можем оценивать выражения с переменными, мы можем сразу делать такой, условно говоря, логический вывод. Вот у нас, условно говоря, база знаний, которая также помещается в пространство программы. Вот у нас импликация, записанная в функциональном виде, что x — это лягушка, если она квакает и ест мук. Там есть, на самом деле, опять же, некоторые аспекты, связанные с тем, как мы интерпретируем логику, насколько мы там закон исключенного третьего включаем в рассмотрение, насколько мы конструктивную логику рассматриваем, но, опять же, к самому языку это имеет костное отношение, скорее, это уже имеет отношение к тому, какую семантику мы хотим привнести за счет того или иного способа написания программ. А язык на самом деле даже не про это, он про более базовые вещи, к которым я, наконец-то, очень скоро перейду. Соответственно, в этом примере мы задаем базовые факты, мы задаем некое правило в функциональной форме, и мы можем вполне себе, передавая просто переменные на месте аргументов получить, что Фриц – это лягушка, потому что она квакает и ест мух. А Сэм – это вообще-то не лягушка, потому что он не ест мух. Там будет интереснее, если мы какие-то из равенств просто уберем, но там поведение мета не на 100% регламентировано, и разные версии все-таки пока работают немножко по-разному, поэтому язык пока не до конца специфицирован. Собственно, как это на самом деле все работает? Все основано на базовой функции — это запросы к графу знаний. То есть в данном случае мы берем очень простой граф, состоящий из одного выражения. Опять же, трактовать это как, скажем, просто аннотированную связь между Бобом и Энн. Или трактовать это как дерево в лесе, или трактовать это как связь в гиперграфии, которая безымянной и объединяет три элемента. Это вопрос слегка вкуса и вопрос внутреннего представления для атомспейса. Повторюсь, тут могут быть разные изоморфные представления. Можно их к теории категории приписать и к естественным преобразованиям между ними. Но, в общем, мы не будем тут переусложнять. Соответственно, есть некоторое выражение, помещённое в пространство программы. Мы, соответственно, можем его смачить и получить какой-то... Давайте сленгевы переключусь обратно на... локальную версию, чтобы она всё-таки чуть поживее работала. Вот. Оно либо может смачиться, либо может не смачиться. Соответственно, функция match, она берёт первый паттерн и для всех вариантов его матчинга составляет второй паттерн. Ну, собственно, варианты матчинга становятся интересными. Тогда у нас в первом паттерне, который мы кверим, появляются переменные. Ну, соответственно, здесь мы очевидным образом сматчили parent Tom Bob с parent X Bob и parent Pam Bob с parent X Bob. Соответственно, в качестве X получили Tom и Pam. Ну, это можно пропустить. Дальше. Собственно, теперь как это все работает. То есть когда мы делаем определение некоторой функции и пытаемся ее оценить, на самом деле это вовсе не функция, это просто такое же выражение, которое кладется в пространство программы, Но попытка оценить это выражение приводит к тому, что интерпретатор составляет запрос вот такого вида. То есть фактически весь процесс интерпретации идет на квырях, на запросах к контейнеру, будь то гиперграф или какое-нибудь другое представление. Фактически мы вот можем вместо того, чтобы просто говорить, давайте оценим это выражение, напрямую составить вот такой запрос, ну и посмотреть на результат. В данном случае этот паттерн непосредственно матчится с выражением в пространстве программы, Ну и в качестве результата выступает соответствующее выражение. Этот паттерн не матчится, этот паттерн тоже матчится. Здесь x матчится с a и выдается такой же результат. мы можем использовать этот матч в явном виде внутри других функций и смешивать вычисление обычных выражений с вызовом подобного матчинга. То есть что в результате происходит? Тут, наверное, Прямо в виде кода это не приведено. То есть когда мы делаем подобного рода запрос, попытку проинтерпретировать выражение, у нас составляется фактически query к нашему контейнеру. И для результата матчинга происходит абсолютно такая же попытка составить такую же квырю. И происходит дальнейшее вычисление, дальнейшее конструирование запросов к графу знаний до тех пор, пока эти запросы что-то возвращают. Собственно, это основа всего процесса интерпретации. Причем query у нас, в отличие от подавляющего большинства баз данных, включая классический OpenCog и его AtomSpace, на самом деле являются не запросами pattern matching, а запросами унификации, потому что у нас переменные могут встречаться как на стороне query на стороне запроса, так и на стороне контента AtomSpace, поэтому происходит не просто обычный запрос pattern matching, В обычных базах данных у нас, соответственно, переменные будут находиться только на стороне запроса. В обычных языках программирования функциональных у нас переменные находятся только на стороне выражений. В нашем случае переменные находятся и там, и там, и происходит попытка унифицировать два выражения. То равенство, которое мы сконструировали при попытке проинтерпретировать некое выражение, и те выражения, в данном случае с равенствами, которые хранятся в нашем пространстве программы. Опять же там, естественно, поскольку это все-таки предполагается, что некий контейнер со знаниями, что это фактически база знаний, то происходит индексация этих выражений. Они ищутся, естественно, не линейно, а на основе либо хэшей, либо каких-то других хитрых приемов. в зависимости от имплементации конкретного спейса. То есть мы здесь сращиваем программирование и запросы к графовым базам знаний фактически в расширенном виде. И делаем это на основе того, что превращаем запросы в унификацию. Собственно, почему, как я вот упоминал, нам не требуется Unified Rule Engine, именно потому что сам движок метты выполняет унификацию. Это то, что не делал Atomspace в OpenCode и то, что пришлось докодить дополнительно в Unified Rule Engine. Собственно, процесс унификации. Ну и дальше, если у нас первый запрос вернул какие-то результаты, мы для этих результатов, как я уже сказал, конструируем следующие, как мы их называем, equality queries, то есть запросы на основе равенств, и получаем там последующие результаты. Поскольку запросы к базе знаний в норме возвращают много результатов, то абсолютно естественно, что если у нас интерпретация выражений происходит на основе этих запросов, то мы ожидаем заранее, что результатов, как правило, будет много. Поэтому язык получается исходно недетерминированный. И это, опять же, хорошо с точки зрения того, что мы Можем объединять функциональное программирование с вероятностным программированием, с запросами к базам знаний, а благодаря унификации еще и с логикой. То есть язык получается такой очень мультипарадигмальный, но Хочу заметить, что все-таки это нельзя рассматривать как язык общего назначения, потому что вот эта вот особенность языка того, что программы хранятся как есть в базе, к которой идет обращение через запросы, оно все-таки предполагает, что это не обычные программы, писать обычный функциональный код на мете можно, но не очень понятно, зачем. А это все-таки какие-то знания и правила рассуждения над ними и так далее. Еще раз, Подчеркну, что при этом у нас и сами знания, и какие-то как... функциональные, такие, я не берусь их назвать императивными, потому что они, опять же, лежат в доступном для анализа виде в пространстве программы. Мы, в общем-то, можем закверить, я не знаю, был тут примерчик такой или нет, мы можем закверить, собственно, определение функции, которую мы положили в программу. Ну где-то это было, но не так важно. Поэтому мы можем анализировать, трансформировать те функции, которые в нашем пространстве программы лежат. Поэтому даже если они выглядят императивно, ну или хотя бы функционально, это все-таки не совсем императивные, сабсимвольные какие-то вещи. Это всё-таки процедурные знания. И это является, наверное, таким водоразделом между тем, где стоит этот язык использовать и где не стоит. Ну, я не знаю, стоит ли тут сильно далеко углубляться в другие особенности меты на более сложных примерах. Наверное, все равно в рамках созвона все это будет не очень понятно и будет сложно покрыть. Всегда можно обратиться к этим туториалам, почитать их, поанализировать и разобраться, если кого-то это вдруг заинтересует. Здесь как раз представлен пример того, о чем я говорил, что можно какие-то знания представлять сугубо в функциональном виде, как у нас было с frog от x равно end green от x. it's fly от x, а можно просто писать это с использованием каких-то кастомных символов в виде неравенств, таких чистых выражений, а вот уже интерпретировать эти выражения на каких-то правилах. Например, мы можем вывести b, если a из А следует Б, и у нас есть в базе запись о факте А. скажем так, процедурное знание, а вот это вот уже превращается совсем в чисто декларативное знание. И опять же, тут, конечно, это делается на уровне совершенно базовой логики, но в целом это может Использоваться как угодно. То есть мета, на самом деле, это все-таки не про логику, это про произвольные символьные вычисления. Просто когда вы пытаетесь описать какой-то язык, вы можете его семантику пытаться описать в виде Ну, в общем, это тоже на каком-то языке, и всегда все равно все сводится к тому, что есть какой-то базовый язык, в терминах которого вы пытаетесь описать свои языки. Вот с метой была попытка как раз сделать такую базу, которую уже дальше не имеет смысла куда-то редуцировать, описывать в каких-то других терминах. То есть здесь не идет речь про логику в чистом виде или про что-то еще. Здесь просто есть символиные выражения, И есть правила их преобразования. И, соответственно, любую систему можно попытаться описать в таком виде. Там есть разные тонкости. И я думаю, что это очень важный момент, и я думаю, что это очень важный момент. о многопоточности, о параллельных вычислениях, то понятно, что язык надо расширять. Опять же, не детерминизм. В данном случае это просто заглушка. Вместо недетерминизма можно, например, какую-то формальную модель параллельных вычислений пытаться задать вместо того, чтобы пытаться туда впикнуть inference. Поэтому язык еще продолжает развиваться, и тут есть некоторые вещи, которые следует еще доделывать. Может быть это как раз самая сложная интересная вещь, но вот тут в туториале в принципе описывается, как в конечном итоге по шагам происходит унификация запросов, к чему они приводят, и, в общем-то, процесс интерпретации и получения промежуточных результатов представлен достаточно подробно. В языке есть запросы составные, также как и джойны в базах знаний. В языке есть система типов. И это, в принципе, отдельная история. Достаточно Ну, сложная, немножко неоднозначная. Суть в том, что... Ещё одной дополнительной мотивацией при дизайне языка были языки с зависимыми типами. Есть такая штука, как изоморфизм Кари Горварда, которая соединяет логику и программирование. Фактически данный изоморфизм гласит, что всё то, что можно выразить в виде императивного программного кода, можно также выразить и в виде логических каких-то конструкций выражений. И этот абстрактный изоморфизм, он принимает очень конкретную форму в теории типов. В теории зависимых типов тип трактуется как логическое утверждение, а программа или функция, которая type-чекается, трактуется как экземпляр этого типа, что доказывает, что тип населенный в рамках теории типов. Соответственно, населенный тип — это истина. То есть, ну я не знаю, тут у нас нет примеров, это будет в более продвинутых туториалах. Но хитрость в том, что вывод и проверку типов в зависимых типах можно также представить просто как манипуляцию с символьными выражениями на основе правил, которые также можно задавать равенствами. И возникает вопрос, нужны ли типы вообще. И вопрос достаточно любопытный, потому что если взять, скажем, лямбда-куб, то бестиповое лямбдоисчисление Оно, в общем-то, не сильно отличается от исчисления конструкции, где в качестве типов можно задавать абсолютно всё, что угодно. Собственно, в исчислении конструкции проблема проверки не просто вывода типов, а даже просто проверки того, что выражение правильно исчислимо, оно алгоритмически неразрешимо, так же, как неразрешима проблема останова. И чем примечательны зависимые типы? Это тем, что в них хотя бы проблема type-чекинга имеет эффективное решение. Поэтому языки с зависимыми типами очень широко используются в системах помощи при доказательстве теорем. С одной стороны, а с другой стороны, это достаточно мощная система типов, которая, во-первых, помогает или позволяет представлять нетривиальные знания. С одной стороны, а с другой стороны, позволяет все-таки делать проверку корректности программ. Поэтому если мы ограничиваемся, скажем, зависимыми типами, то разница между тем, чтобы все символы, описывающие типизацию, делать просто кастомными символами, которые интерпретируются абсолютно так же, все остальное, или же вводить все-таки типы как некий регламентированный элемент языка. Вот последнее имеет то преимущество, что можно проводить статическую проверку типов достаточно эффективным образом, и с точки зрения практики программирования это полезно. В итоге мы все-таки решились ввести типы как именно базовый элемент языка. по вот этим вот двум соображениям, но до сих пор остаются сомнения, не надо ли все эти типы все-таки реализовать на самой бестиповой мете, более базовой, или все-таки тащить их за собой, как что-то дополнительное. Но, тем не менее, на текущий момент у нас градуальные зависимые типы, то есть мы вполне можем работать с нетипизированными выражениями, что до этого все время и было. Но использование типов вполне себе допустимо и даже приветствуется для разных целей. И в целом оно в таком виде, наверное, и останется, хотя имплементация может перекочевать из опорного языка в саму мету. Потому что реально, на самом деле, опять же, когда мы занимались дизайном меты, там было очевидно, что type checking в зависимых типах делается абсолютно на таком же сопоставлении с образцом. И вот прям грех не использовать единый механизм прямо для всего, если он для всего подходит. Но при этом есть свои подводные камни, естественно. Я, соответственно, не буду особо сильно углубляться в систему типов, потому что хотя она с одной стороны полезна, С другой стороны, она действительно является, ну так, слегка дополнительной и опциональной. Единственное, что вот прямо... Без чего очень сложно обойтись в языке, это без использования специальных типов, которые регламентируют порядок вычислений, а именно у нас имеется такое понятие, как метатип, и оно описывает, то является ли некий элемент С которым мы работаем традиционно, мы их называем атомами. Является ли он символом, выражением? Раньше это называлось ноды и линки, но мы перешли на немножко другую терминологию, переменной или grounded символом. Про grounded символы я пока не говорил. Это отдельный тоже важный компонент, но он является важным, но тоже дополнительным элементом языка к паттерн-матчингу, к вариантам, к унификации и к символьным вычислениям. Соответственно, то, о чем я сейчас просто хочу упомянуть, это то, что у нас есть понятие такого типа, как Atom. Если мы в определении функции указываем, что аргументом, типом аргумента является Atom, то этот аргумент не редуцируется при передаче в функцию, то есть это способ немножко корявый в итоге, потому что смешение типов и метатипов не всегда удобно, но тем не менее сейчас это устоявшийся способ. определить, случается ли у нас call by value или call by name фактически, ну или, как я уже сказал, редуцируем ли мы выражение до того, как вызываем pattern matching на саму функцию. Поскольку это все делается через pattern matching, то как бы у нас нет никакой проблемы. Как я показывал, мы можем там передавать в качестве аргументов функции, все что угодно, и функция дальше сама будет с этим разбираться. Ну, точнее говоря, не функция будет разбираться, а pattern matching будет разбираться, смачивается он там с определением, ну, с равенством для данной функции или нет. Так, давайте я на вариант с подсветкой переключусь. И это, на самом деле, оказывается очень такой занятный механизм для очень разных вещей. Например, if у нас фактически не является специальной функцией, как во многих языках программирования, является обычной функцией, но при этом такой, что у нее аргументы определены как… типы аргументов определены как атомы. И два равенства для этой функции вот таковы. То есть, если у нас Первый аргумент, поскольку он не является атомом, а является просто bool, он редюсится к какому-то выражению. Дальше происходит pattern matching. Мы составляем equality query, которая может быть сматчена или с этим выражением, или с этим выражением. Но хитрость в том, что поскольку два других аргумента имеют тип Atom, они не вычисляются перед паттерн-матчингом с равенствами для, в данном случае, MyIf. Соответственно, здесь у нас получается false, с этим true с false не матчится, у нас происходит matching с этим выражением и loop у нас не редуцирует, не происходит попытка редукции вот этого бесконечного loop, просто они передаются, ну даже не передаются, формируются query такого вида в итоге, то есть my if true loop ok == result. Составляется такая query. Так, тут не true, у нас получается false. Она матчится вот с этим выражением, и в качестве резалта появляется else, а else у нас ok. Соответственно, в результате делается попытка уже редукции такого выражения, точнее говоря, не редукции, а pattern matching. Этот pattern matching уже срабатывает с этой записью в пространстве программы. Ну и там у нас self пишется, это space, к которому делается query. Это, соответственно, выражение... Это выражение у нас унифицируется с этим выражением и в качестве результата просто появляется OK. Да, появляется. То есть в обычных языках программирования if не удается реализовать как обычную функцию, просто потому что у этой функции не очень хочется вычислять аргументы всегда. хочется сделать ее именно ветвлением, то есть если true, то вычисляем одно, если false, то вычисляем другое. С вот этим вот механизмом определения типа аргумента как атом, у нас получается задать их как обычную функцию. В некоторых языках за счет ленивых вычислений это может делаться автоматически, но там есть свои хитрости и подводные камни. Соответственно, тип Atom очень широко может использоваться для метапрограммирования Это не просто трюк для определения порядка вычислений, а именно, зачастую, указание на то, что функция хочет получить само выражение, которое ей передают без редукции, чтобы дальше с ним что-то сделать. задания каких-то правил вывода каких-то DSL в мете, когда мы берем выражение и применяем уже к нему какой-то анализ с матчами или с чем-то еще. Поэтому в первую очередь этот тип Atom задумывался именно для метапрограммирования внутри метты. Но оказался полезным и для других вещей. То есть, например, ну даже банальный квот — это не функция даже, это просто конструктор Да, это просто конструктор, который говорит, что принимаемый им аргумент имеет тип Atom. И в результате мы таким образом получаем квотирование выражений. В Lisp, опять же, квот приходится вводить как очень специальную функцию. То есть, ну, фактически даже не функцию, а какую-то встроенную операцию, потому что для нее правила оказываются другими, нежели чем для других функций. Вот этот механизм с заданием метатипа оказывается таким, что можно его применить вот для того, чтобы реализовать if или реализовать последовательный end, который не оценивает заранее оба своих аргумента, а... Передактируется. Так, где тут сэндбокс какой-нибудь? Ну вот, сэндбокс. А вот end можно свой написать. Не уверен, так или нет. Допустим, если end true y, то y. Если end false y, то false. То есть он не будет оценивать y сразу, он, как обычный sequential end, сначала оценит свой первый аргумент, и только если он true, он будет оценивать второй аргумент в этом случае и просто вернет его для дальнейшей оценки. Вот, то есть получается, что вместо того, чтобы вводить какой-то искусственный, ну не искусственный, а выделенный для этих целей квот, который все будет врапать, и чтобы там что-то дальше сделать, надо занврапать, вот, можно сделать это через тип. Ну вообще тип и штука в целом хорошая для... регламентации поведения программ для задания семантики, DSL или еще чего-то. Ну и для паттерн-матчинга, соответственно, тоже Удобно использовать этот тип для разных целей. Но иногда на самом деле сам матч принимает паттерн, который не редуцируется. То есть у матча первый аргумент тоже определен как атом. Но если мы захотим выполнить его оценку до передачи в паттерн матчинг, то это можно сделать другими средствами. Соответственно, я дальше углубляться не буду. Скажу еще немного про связку мета с питоном, потому что это имеет определенное значение для текущего состояния области R&D в AI, потому что очень многое делается на питоне. Собственно, у метты хороший враппер питоновский, и можно делать как запуск кода метты из питона, Причем API достаточно подробный, можно и парсить, и запускать программы по командам, по строкам, или добавлять непосредственно атомы в пространство программное. Вот, также и есть возможность наоборот встройки питоновских объектов, говоря широко, в метту. То есть, зачастую мы хотим из программы для символьных вычислений вызывать все-таки какие-нибудь… какой-то нативный код для разных целей. Это могут быть и нейросетки, это может быть просто взаимодействие с каким-то оборудованием или внешним миром. Собственно, одна из мотиваций в разработке метты была как раз, ну по сравнению с атомизмом, была как раз попытка сделать более нативную и тесную интеграцию с субсимвольными вычислениями. То есть сделать субсимвольные вычисления объектами первого класса в языке, в символьном языке. Поэтому, вот как я говорил, у нас есть фактически четыре метатипа для атомов. Это символы, выражения, переменные и grounded. Я не знаю, честно говоря, как по-русски их лучше называть, там, приземлённые, ну, наверное, проще субсимвольные в данном случае. Вот, то есть, ну, это... термин, наверное, идет еще из проблемы символ-семантик граунинга. По-русски в этом контексте это переводится как семантическая привязка символов, но то, к чему они привязываются, какая-то grounded information, здесь хорошего перевода я, к сожалению, не знаю. Соответственно, идея в том, что у нас в целом в мете могут идти символьные вычисления над какими-то субсимвольными сущностями. И когда мы еще с OpenCock Classic работали, мы, например, делали Экспериментировали с Visual Question Answering, с ответом на визуальные вопросы, на основе связки нейросеток, более конкретно модульных сетей с символьными представлениями. То есть мы как раз на основе вероятностных логических сетей PLN, Делали некий, ну не очень сложный, но тем не менее ризнинг, рассуждения, которые комбинировали куски нейросеток, стоящих за этими символами. Например, слово «красный», за ним стояла некоторая головка Слой классификационный, над нейросеткой строящей какие-то признаки верхних уровней. И когда у нас появлялась, например, есть ли на этой картине, на этом изображении красная машина, то это транслировалось в запрос к Атомспейсу, который формировал в результате фрагменты нейросетки, комбинировал их выходы в логическое какое-то выражение. Для этого логического выражения оценивались значения истинности, которые там в плене четырехкомпонентные и непрерывные, а не дискретные. И все это можно было продифференцировать и результат донести до весов нейросетки. То есть через комбинацию значений истинности мы получали какой-то результирующий ответ. ну, логически вероятностный, который там был или правильный, или неправильный, соответственно, loss-функция там штрафовала его, и через выражение для вычислений Многие исследователи не знают, что такое нейросетка. Нейросетка – это нейросетка, которая демонстрирует Кое-как работало, но было очень неудобно все это сращивать в старом Оппенкоге, потому что вот эти вот субсимвольные объекты, grounded атомы, они не были объектами первого класса. В метте мы их сразу ввели как компоненты, которые являются полноправными членами этого языка, и, соответственно, Возвращаясь к интеграции с питоном, в питоне можно также вводить новые grounded атомы, которые будут доступны из самой метты. Это, конечно, очень примитивные примеры. Но, тем не менее, чтобы там было понятно, мы в Python можем написать какую-нибудь функцию и... Интересно, что-то я тут... вот оно только снизу происходит, там зарегистрировать Dupster как токен, который превращается в соответствующий атом, который врапает питоновскую функцию, и уже внутри самой меты ее использовать. Собственно, планов у нас, опять же, на этот счет очень много. Собственно, у нас есть grounded операции, например, арифметические, но никто нам не запрещает также для них писать какие-то символьные выражения. Например, мы можем просто записать, что a плюс b равно b плюс a. И, может быть, там имплементация плюса не всегда идеально будет удовлетворять законам арифметики. И, в принципе, есть целый большой обзор того, каким аксиомам не удовлетворяют имплементации вообще элементарнейших арифметических операций практически чуть ли не во всех языках программирования, компилируемых, по крайней мере. То есть там и a плюс b может быть не равно b плюс a, и даже a плюс 0 может быть не равно a. Например, если там используются какие-то специальные значения для бесконечностей, результатов деления на ноль каких-то ошибок и так далее. Но, тем не менее, у нас в мете вполне могут быть какие-то символьные знания об операции сложения, которая, тем не менее, сама может быть заимплементирована в нативном коде и вычисляться субсимвольно. И в целом у нас тоже много планов, например, на предмет как просто компиляции символьных определений для быстрых вычислений, так и более интересные вещи, такие как, я уже упоминал, амортизированный вывод и специализация именно Вычислений для недетерминированных функций, которые фактически превращаются... Ну, там разные варианты могут быть. Например, если у нас в виде недетерминированной функции задано решение какой-то задачи, которая имеет эффективное алгоритмическое решение, то можно сказать, что проекция Футамуру Турчина, вторая интерпретатора на данное выражение, должно приводить к тому, что эта неэффективная, недетерминированная функция превратится в эффективное выражение для непосредственного вычисления или решения задач эффективным алгоритмом. Либо же, если там не углубляться в сторону суперкомпиляции, то это может быть просто обучение какой-то нейросетки, которая будет делать амортизированный вывод. То есть у нас есть некоторая задача, мы ее описываем, декларативно, и у нас есть универсальный, а значит тупой как всё универсальный, движок вывода, который эту задачу решает неэффективно, но в результате тренировки нейросетки на примерах решения этой задачи, мы формируем рефлекс или, скажем так, скилл, который это делает эффективно. Это не обязательно решение абстрактных задач, это может быть хоть управление роботом, хоть агентом в Майнкрафте. когда мы можем декларативно описать, что для того, чтобы получить камень, надо срубить этот блок, для этого там надо нанести 100-500 ударов молотком, киркой или чем-нибудь еще. И мы не должны контролировать процесс реализации этого декларативного знания на практике сознательно. Метта фактически как символьный язык, она нами предполагается именно как сознательный уровень, но все остальное должно делаться эффективно специализированными алгоритмами, иначе Эйджай не построить. Эйджай — это не в том, чтобы был один универсальный метод решения всех задач, а в том, чтобы была возможность строить специализированные методы для конкретных случаев, будь то скиллы каких-то действий в там, физическом или виртуальном мире, или даже в том же ГО, ну, возвращаясь уже к примерам более абстрактного мышления, там, когда люди играют, ну, профессионалы играют в ГО, они могут вообще не думать и при этом играть лучше там любого любителя, вот, потому что У них это все, условно говоря, прокэшировано, прохэшировано, проспециализировано, и идет просто процесс, когда мы идем от данных сразу к оценкам. Ну и, собственно, абсолютно так же играет и AlphaGo или вслед за ним Musero. Это важная часть архитектуры AGI, иметь возможность формировать субсимвольные скиллы, которые будут встраиваться в символный ризнинг. Это немного отвлеченная философия и рукомахательство, то, что вот эти вот grounded атомы должны быть объектами первого рода в языке, и мы должны иметь возможность нативной интеграции символьных вычислений с вычислениями в нативном коде, это просто было положено в один из принципов дизайна данного языка, ну и в целом это просто-напросто достаточно удобно с точки зрения не то чтобы обычного программирования, но хотя бы Neural AI программирования, когда мы там нейросетками или каким-то другим императивным кодом решаем эффективно какие-то компоненты задачи, а поверх этого настраиваемся. Вот, соответственно, такой овервью языка получился, частично повторение того, что было год назад, частично, надеюсь, что-то нового. Ну и помимо этого, хотелось бы добавить, что кроме какого-то выкристаллизования в языке. более или менее стабильных структур того, как мы вообще на нем пишем. Естественно, альфа-релиз связан не только с самим языком мета, но и с написанием некоторого количества туториалов, чтобы люди Могли начать с ним понемножку знакомиться. Ну и с другими компонентами гиперона. и некоторыми моментами в использовании метты. И, собственно, к сожалению, у нас не все из того, что хотелось сделать, было сделано по разным причинам. Но, тем не менее, в принципе, уже появились какие-то интересные возможности по использованию языка. и какие-то дополнительные компоненты. В частности, один из важных компонентов – это реализация атомспейсов, которые допускают хранение очень больших баз знаний распределенным образом и их квирирование. Опять же, тут есть разные нестыковки и неувязки, но в целом на текущий момент есть реализация того, что называется Distributed Atom Space, который другой командой реализовывается. И этот AtomSpace является таким хорошим примером того, что мы в принципе не привязаны в нашем языке к конкретной реализации. Если AtomSpace удовлетворяет определенному API, то мы вместо пространства самой программы можем подставлять другой Space с таким же API. Мы в принципе можем врапать в Space, скажем, Neo4j. там SparkQL или еще что-нибудь, они будут ограничены по функциональности, потому что они не будут поддерживать унификационные query, но обычные query мы к ним посылать можем, поэтому если там кому-то очень хочется, например, в качестве бэкэнда иметь Neo4j, то, в принципе, это вполне реализуемо. на мете это будет выглядеть весьма аналогичным образом. В данном случае у нас где-то поднят сервер с очень большой генетической онтологией. Это, кстати, зря я это IP назвал, это IP, а не API, но неважно. Соответственно, мы абсолютно таким же образом можем посылать query к этому удаленному space, который там хранится ну, наверное, на нескольких машинах, я не знаю деталей. Вот. И эти query, в принципе, выглядят абсолютно так же, как если бы мы делали запросы к локальному space. В этом смысле нам, в принципе, мы агностичны относительно бэкэндов для наших Atom Spaces. Вот, но на текущий момент они не доимплементили кэширование, поэтому запрос выполняется достаточно долго или у них там что-то с сервером, но вот перед... Семинаром я, соответственно, сделал запрос, и вот он там обнаружил, что протеин О43264 транслируется в транскрипт вот такой-то. Что бы это ни значило, я небольшой специалист в биоинформатике. Но просто один из компонентов в этом случае уже гиперона, а не просто языкометы, это распределенный атом-спейс, который мы надеемся В ближайшем будущем будет не просто работать, но работать эффективно и поддерживать кверирование больших баз знаний. Хотя, честно говоря, он на текущий момент унификационные квери тоже не поддерживает. Но, тем не менее, обычные квери к нему можно слать. Другой компонент, который я бы хотел успеть упомянуть, который, ну, можно сказать, отдает дань текущей моде, с одной стороны. С другой стороны, действительно, это не просто мода, а текущая реальность. Вот. Это наша библиотечка, которая в работе LLM в метту. Она у нас называется metamoto. С учетом того, что нам по-любому надо было вызывать большие языковые модели из метты, и нас, в общем-то, не очень устраивала архитектура таких о фреймворках, как, например, LangChain, или Guidance, или других существующих. Мы решили все-таки сделать свою библиотечку. Опять же, там много дополнительных причин. Я, может, успею тоже о них пару слов в конце сказать. Вот. Но одна из причин заключается в том, что мета как раз и является таким символьным языком, который не надо придумывать уже для нас, потому что мы его разработали, он у нас есть. не надо дополнительно имплементировать, чтобы делать с LLM-ками все то, что делает, скажем, Guidance или LangChain, то есть в этих фреймворках придумывается кастомный ad-hoc такой символьный язык для составления промптов, ллмок и для чейнинга ллмок. В нашем случае мета работает из коробки, то есть мне коллеги, когда показали библиотечку guidance, я сказал, да это ж не надо даже имплементить на мете, нам надо просто взять, заврапать ллм в grounded функцию, и все, что они делают на гайденсе, ну, будет практически из коробки сразу же доступно. И, в общем-то, так и оказалось. Конечно, когда мы говорим о таких разросшихся фреймворках, как линкчейн, там основная их сила не в самом базовом механизме чейнинга запросов к ллнкам, а к инфраструктурной обвязке, потому что там реализована куча вещей и библиотека промптов очень большая для того, чтобы делать самые разные вещи. с LLM-ками, и интеграция с тем же SparkQL и другими базами сделана. И, например, для запросов к Wikidata или DBpedia тоже подготовлены, наработаны все эти промпты, которые представляют больше интерес, опять же, повторюсь, чем сам по себе фреймворк. И прочие тулы тоже подогнаны для использования из коробки retrieval argument generation, есть память векторная и так далее. Естественно, все это переимплеменчивать большого смысла нет. Благодаря нашей интеграции с Python мы можем компоненты лэнгчейна переиспользовать тоже практически из коробки в нашем фреймворке. Нам не обязательно в этом смысле с ними соревноваться. Но то, что дает для нас данная библиотека, это именно возможность интероперабельности с самой метой. Ну не знаю, просто какой-нибудь пример приведу. Скажем, есть некоторое сообщение пользователя. Допустим, мы вызываем чат GPT-агента. мы добавляем системное сообщение, которое вообще отсылает нас к агенту, написанному уже на мете, и этот агент Он говорит, что у него есть некоторая функция, которую нужно вызвать, если человек хочет вычислить какое-то выражение. Ну и говорит, что давайте мы возвратим... данное выражение в виде схемы или LISP, и в конечном итоге вызывает функцию, которая просто принимает это выражение и редюсит его. В итоге у нас оказывается, что... что... происходит возврат некоторого выражения, которое мы сразу же интерпретируем как меттовское выражение. В данном случае это сделано, опять же, для примера достаточно примитивно, но работоспособным образом. В данном случае это не chat.jpt вычисляет, а он возвращает выражение для вычисления, и мы его вычисляем в мете. И здесь есть разные примеры. там, например, того, что у нас есть набор некоторых фактов, есть некоторая такая, опять же, крайне упрощенная, но тем не менее, логика, которая работает внутри данного меттовского скрипта, вот, и мы там через чат GPT Условно говоря, понимаем смысл заданного вопроса. И, соответственно, ну тут вот промтик, опять же, не очень выверенный, но хоть какой-то, который говорит, что если у нас есть такие-то вопросы, то представляй их в виде таких-то выражений. И, соответственно, данный Агент, он говорит в чат GPT, вот был такой вопрос от пользователя, если можешь, то представь его в некотором выражении, в виде некоторого выражения для последующего вычисления в мете. И это, в общем-то, работает. То есть, опять же, у нас, ну, если мы возьмем какой-нибудь лэнгчейн, то там отдельная интеграция с какими-то базами знаний. Про какие-то логические рассуждения там, ну, в принципе, речь вроде как Раньше не шла и, может быть, до сих пор не идет. В Metamoto у нас это все вместе, это все единое представление. Там сама программа — это тоже часть графа знаний. Внутри наших агентов мы можем использовать Или заранее какой-то имеющийся там спейс, или помещать факты или какие-то правила тоже внутрь самого спейса. Делать, опять же, в данном случае крайне упрощенный, но некоторый ризнинг над ним, и, соответственно, все это интероперейтится в единой среде. Ну и, в принципе, там, опять же, у ребят, которые NARS портировали, у них была NARS GPT, тоже в свое время реализация. Ну и оказалось, что она, опять же, переносится в этот фреймворк на мете очень легко, и там можно кучу кастомных вещей выкинуть. Получается, что и NARS хорошо переносится на мету, и NARS GPT хорошо переносится, потому что, в принципе, вот это вот... ну, единое представление всего и вся в метаграфах, и логика вычислений через унификацию на основе query к этому метаграфу, она, оказывается, для этих вещей очень удобна. То есть это именно язык символьных вычислений, и внутрь него мы можем встраивать все, что угодно. То, что я сейчас показываю, не есть какой-то такой ультимативный конечный продукт для решения чего-то, но просто именно как фреймворк для создания подобных продуктов, он работает весьма-весьма хорошо и удобно, как мне кажется. То есть именно как… язык символьных выражений, куда можно встраивать все, что угодно, и делать какие угодно символьные вычисления. Конечно, это именно фреймворк, потому что все равно это придется откуда-то брать. Либо делать какие-то общие фреймворки обучения. Там у нас есть люди, которые паттерн-майнингом занимаются. или алгоритмической химии, то есть попытка выучить символьные выражения и их преобразование на основе данных, либо кодить это ручками, либо тренировать LLM на то, чтобы они продуцировали соответствующие символьные выражения или еще как-то эти проблемы решать. То есть это не AGI из коробки, естественно, но именно как фреймворк, который все эти вещи позволяет так или иначе исследовать, вроде как это работает неплохо. Последний момент, который я хотел успеть отметить, почему мы все-таки стали и этот фреймворк связки LLM'ок с меттой делать, хотя мы считаем, что это абсолютно недостаточный шаг для AGI, и сами LLM'ки, вообще говоря, тот еще компоненты IGI, в лучшем случае они работают именно как лингвистический скилл, в худшем случае они вообще в таком виде в конечную архитектуру IGI не войдут, но тем не менее, как текущий инструмент, Они очень и очень полезны и поучительны во многих отношениях. Ну и в целом они просто реально широко используются на практике. И у нас на платформе Singularity.net, я не знаю, насколько про нее рассказывалось, какие-то вещи на предыдущих каких-то семинарах. Но есть сервисы, которые представляют собой и LLM, и Knowledge Graph. И есть одна из проблем — это связка этих сервисов друг с другом. И то, что поначалу было указано анонс про AI DSL, это как раз оттуда, то есть когда у нас есть сервисы искусственного интеллекта, которые мы хотим автоматически комбинировать и композировать, то есть когда мы берем выходы от некоторых сервисов и Используем их как входы к другим сервисам, чтобы какие-то пайплайны организовывать. Для этого тоже мы планируем использовать метту, и мы к этому уже мы практически подобрались, и это был бы еще один пример приложения мета, но здесь у нас со стороны платформы и SDK платформы не со стороны разработки мета, были некоторые заминки, поэтому Сегодня я это не могу продемонстрировать, но в целом идея в том, что из мета будут как раз вызываться не просто чат GPT по его API, а будут вызываться разные LLM-ки. на платформе на нашей, вот, у них будет спецификация на мете, и, соответственно, это будут такие декларативные знания, которые будут позволять делать композицию сервисов, в частности, внутри метамод, например, автоматически обнаруживать подходящие сервисы на платформе для того, чтобы делать подобного рода комбинации. Так, ну, на этом я закончу. Тем более, что время, наверное, уже на исходе. Спасибо. 

S00 [01:51:21]  : Алексей, огромное спасибо. К сожалению, у нас очень мало времени на вопросы. Точнее, ответы. Вопросы и ответы. Правда, на часть из них вы по ходу дела ответили. У нас там довольно бурное обсуждение было на Ютьюбе. Несколько вопросов. Один я не успел записать. Скажите, по поводу последней истории про интеграцию метты Как вы видите решение вопроса о пресловутой объяснимости, прозрачности или интерпретируемости искусственного интеллекта, построенного в результате интеграции элелемок и семантической технологии на основе графов знаний? в том числе с использованием вашего фреймворка. Я правильно понял, что мы можем рассматривать ее, обучив, грубо говоря, ЛЛМ-ку с помощью приводимых вами промтов выражать свои мысли и логику в МЕТе. задавать ей вопросы, чтобы, ну, грубо говоря, если там Ильямкова делает какой-то вывод, просить ее структурировать логику принятия решения в терминах метты. 

S01 [01:52:44]  : Ну, смотрите, тут это разговор на отдельный семинар. Вот. Можно немного ослабить проблему плохой интерпретируемости, прозрачности. галлюцинации у ЛЛМок с использованием нейросимвольной интеграции. Это можно делать чуть-чуть разными методами. Можно, например, ЛЛМки ограничить только задачей семантического парсинга, а все остальное делать на основе символьных рассуждений и явных знаний. Насколько это в принципе будет продуктивно работать, вопрос неоднозначный, но можно делать на символной стороне только верификацию того, что сказала LM на основе там знания из Wikidata, DBpedia, пользовательских баз знаний или еще чего-то. Там есть SUMO с Upper Merged Anthology. Но это тоже все даже не полумеры, а как бы Ну, небольшой довесок, то есть нельзя получить от LLM-ок интерпретируемый гарантированный результат. Это инстинкт, это рефлекс, который просто мэпит входы на выходы. Ну, не очень просто, но как бы… У него внутри себя нет какой-то консистентности, внутренних проверок и так далее. Но можно идти дальше и пытаться модифицировать архитектуры LLM по-разному, начиная от Как вы сказали, пытаясь заставить их проипродуцировать какие-то выражения в более структурированном виде на ЛЛМ. Как вы сказали, пытаясь заставить их припродуцировать какие-то выражения в более структурированном виде на ЛЛМ. там языке, удобном для рассуждений и символьных вычислений. Можно идти еще дальше и пытаться строить просто символьные LLM-ки, а не нейросимвольные. И вот тут мы приходим на самом деле к тому, что Полностью интерпретируемый Эйджай вы никогда не получите, потому что даже если все будет абсолютно символьное и основанное на концептах известных человеку, то сложность будет настолько высока, всего этого дела, что там верификацию вы не сделаете. Это работает в лучшем случае и то с натяжкой в математике. А если мы говорим про какое-то real-world приложение, то в конечном итоге это в полной мере работать не будет в принципе. То есть эту проблему можно, да, пытаться ослабить, так или иначе можно уменьшить негативные эффекты от использования фит-форварда решений и пытаться в явном виде делать моделирование мира, чтобы LLM-ка не постфактум придумывала объяснения тоже в фит-форвард манере. bottom-up там от данных к результату тому, что она сама спродуцировала своим неизвестным образом, вот, а именно пыталась построить, ну, промоделировать ситуацию и действительно Понимать — это значит иметь возможность спроецировать решение на, скажем так, модель мира. Но, опять же, это очень длинный разговор, если мы решим углубиться, что же такое модель мира. То есть проблему, да, можно ослабить. Может быть, сильно ослабить, но полностью ее решить все равно не удастся. 

S00 [01:57:26]  : Спасибо, Алексей. Вы упомянули антологию, дебипедию, сумму и так далее. А скажите, сейчас хиперон их может в себя загружать, чтобы взаимодействовать с ними через метту? 

S01 [01:57:42]  : В принципе, да. Принципиальной проблемы здесь нет. Но есть ряд технических проблем, там DBpedia и Wikidata, наверное, большеваты, чтобы их пихать в пространство программы интерпретатора, просто оно для этого вообще не очень предназначено, оно все-таки больше про как бы общее правило, равенство и так далее. Поэтому в этом смысле и значения особого нет, потому что если они уже хранятся в некотором формате представления знаний, который подогнан под соответствующий язык запросов, то там, скажем, манификационных квырей к ним не будет. Поэтому гораздо проще заврапать их в Space, предоставив соответствующий API, и использовать из метты не грузя в, как это сказать, аутентичный Atom Space, а просто эмулировать работу Atom Space на тех бэкэндах, на которые они работают. Для сумо эта загрузка в Atomspace имеет гораздо большее значение, потому что в сумо там очень много общих знаний. В целом у нас люди грузили какие-то фрагменты сумо в спейсы, это работало, но там стало понятно, что само сумо по себе сильно недостаточно для решения тех задач, которые они рассматривали, потому что там очень многих знаний все-таки явных не хватает, к сожалению. 

S00 [01:59:48]  : Спасибо. Еще вопрос. Я помню, что в Атомспейсе и в Атомизе была возможность ассоциировать вероятности, точнее truth values, со стейтментами. А в примерах сегодняшних я не увидел того, что в МЕТе для правил, о которых мы говорим, Я не увидел возможности указывать вероятность. Это просто не было в примерах или этого нет в принципе? И как тогда быть в ситуации, когда у нас разные правила имеют разное распределение? вероятностные, ну и сразу же вопрос в догонку, значит раз уж проговорили про вероятности, значит у нас, если я правильно понимаю, унификация правил тоже не как различную вероятность и ту уверенность или конфиденц этих правил не учитывает, да? 

S01 [02:00:48]  : Смотрите, тут опять же достаточно длинный, может быть, разговор на этот счет, потому что между Классическим атомспейсом, атомизмом и метой есть расхождение. В опенкок классике, в атомспейсе было различие между атомами и вэльюсами. И values, они там не индексировались, они не были searchable, а атомы были прям основой представления знаний. У нас values отсутствуют, у нас присутствуют grounded атомы. Как бы абсолютно нет никаких проблем задавать true values в качестве выражений типа tv 0.1, 0.2, 0.3. То есть их можно задавать просто в виде атомов, и да, сегодня не было примеров такого кода, но в прошлый год назад вроде как были. То есть в этом особых проблем нет, ну в смысле вообще никаких проблем нет, но есть тем не менее ряд особенностей. Например, на мете можно описать какие-нибудь вероятностные зависимые типы на самой мете. как бы для них составить любые правила вывода, которые мы хотим, но это будет именно внутри метта работать. Вот я про типы говорил, что есть, в принципе, возможность сделать типы как просто кастомные символы, которые реализованы внутри самой меты, как библиотека или еще что-нибудь. А есть возможность типы реализовать внутри интерпретатора, чтобы он с ними быстро и эффективно обращался. То же самое, в принципе, и с теми values, которые были в OpenCock классике. То есть там были truth values, там были attention values. И если мы, скажем, attention values будем реализовывать в самой мете, то, может быть, они будут недостаточно эффективны для управления выводом, например. Где проходят границы между тем, что надо выражать в самой мете, а что надо, например, использовать как своего рода способ обогащения атомов, вопрос открытый. То есть никто, опять же, не мешает сделать свой спейс в котором вот эти true values будут пристыковываться прямо к атомам, вместо того, чтобы храниться в виде отдельных атомов и отдельных выражений. Но это решение достаточно частное, потому что кому-то нужны true values в стиле ПЛН, кому-то нужны трусвелюсы в стиле Нарса, кому-то вообще нужны байсовские правила вычисления вероятностей, сэмплирования и так далее. Поэтому мы как именно встроенные объекты эти трузвелюсы не вводили. Хотя в каких-то условиях, может быть, в этом был бы смысл. Когда мы наконец-то доберемся до управления выводом и до, собственно, движка вывода, как я говорил, Тот же недетерминизм, например, является частным случаем. Может быть, вот эти вероятностные значения, они окажутся тем, чем будет параметризовано движок вывода. И, скажем, если мы хотим делать сэмплирование, то движок вывода, который будет подставляться вместо этой заглушки, будет брать эти значения и оперировать с ними вот так, как ему нужно. Тогда это, может быть, будет чуть-чуть ближе к OpenCock классику. А может быть это все так и останется в виде просто кастомных экспрессионов в атомспейсе, и в этом есть куча своих преимуществ, потому что когда это лежит именно в виде атомов, там вот у нас truth value имеется такой, то мы можем взять и сделать ковырю, а какие у нас вообще уверенные знания хоть о чем-нибудь. В атомизе такой запрос построить было, в принципе, невозможно. Где компромисс между тем, что должно быть, условно говоря, на сознательном уровне, а что должно быть встроено в прошивку – это вопрос исследовательский. Он для нас пока открытый. 

S00 [02:06:22]  : Спасибо, Алексей. Понятно. Развилка понятна. И Вы упомянули Атомис. Я правильно понял, точнее Нарсиз. Я правильно понял, что Нарс мигрировал свою работу с Нарсиза на Метту? Или это просто экспериментальная была миграция? 

S01 [02:06:42]  : Я, честно говоря, не знаю, какой коллектив сейчас работает над OpenNars. Судя по появляющимся статьям, люди продолжают этим пользоваться. Те, часть людей, которые разрабатывали OpenNAS, они реализовали, сделали его порт на метту, но у них при этом Narciss, он, можно сказать, присутствует как DSL на метте. 

S00 [02:07:20]  : Вот, кстати, как они поступили вот с Truth Values, потому что Truth Values в Мете как раз являются частью языка, а как в этом парте они с этим поступили? Они сделали это сверху или они наоборот сделали это частью? 

S01 [02:07:39]  : Честно говоря, я настолько детально не смотрел, я видел у них реализацию самих представлений. То есть, это как подмножество выражений на метте, подмножество, которое используют символы, имеющие определенную интерпретацию в системе Нарса. То есть, это реально выглядит как DSL. А как именно они с Трузвелюсами поступили, я вот сейчас не могу сказать. 

S00 [02:08:12]  : Хорошо. Алексей, большое Вам спасибо. У нас нет уже времени на ещё вопросы, но было очень интересно и будем ждать Вас в следующий раз с новыми достижениями в развитии на пути к VGI. Спасибо. 

S01 [02:08:28]  : Надеюсь, уже будем говорить про когнитивную архитектуру в больших деталях. 

S00 [02:08:34]  : Это будет уже еще круче. Спасибо всем, кто участвовал и тем, кто нас слушал в прямом эфире. Всем до свидания. Алексей, спасибо и до свидания. 

S01 [02:08:44]  : До свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
