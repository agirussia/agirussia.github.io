## 20 июня 2024 - Нейронные сети для прогноза свойств нефтегазовых месторождений - Иван Приезжев — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/UMF_VEp4KuI/hqdefault.jpg)](https://youtu.be/UMF_VEp4KuI)



S01 [00:00:05]  : Коллеги, всем добрый вечер. На нашем семинаре разработчиков общего и сильного искусственного интеллекта мы говорим не только о фундаментальных проблемах построения общего и сильного искусственного интеллекта, но и о прикладных задачах, которые решают современные системы на основе искусственного интеллекта и машинного обучения. И сейчас как раз тот случай, мы во-первых поговорим о применение нейронных сетей в применительных задачах нефтегазовой геологии. Кроме того, как я понял из анонса доклада, мы сегодня также узнаем про популярные последние несколько месяцев сети Калмогорова. А еще мне лично этот доклад еще интересен тем, что я сам по начальной своей профессии геолог и геофизик, и тоже будет очень интересно послушать существующий уровень развития этой области с применением современных математических методов. Иван, пожалуйста. 

S00 [00:01:08]  : Спасибо, Антон. Добрый вечер. Меня зовут Иван Приезжев. Я немножко кратко расскажу о себе. Закончил Алматинский политехнический институт в Казахстане. Долгое время работал в компании Schlumberger около 20 лет, в том числе в Хьюстоне около 5 лет. Работал по разработке, занимался разработкой различных алгоритмов по интерпретации сейсморазведочных данных совместно с измерениями в скважинах. Занимался разработкой программного продукта Петрель долгое время. В 2016 году уволился из шлюмбирже и вот создал свою компанию в рамках Сколковского инновационного центра. И вот мы сейчас её развиваем. Мой доклад будет касаться именно применения нейронных сетей для прогноза от резервуара, свойств резервуара, нефтегазовых месторождений. расскажу про нейронные сети Калмогорова, которые мы применяем с 2017 года, расскажу сравнительно с классическими нейронными сетями, и также расскажу о наших последних разработках. и российские нейронные сети на основании ближайшего соседа. Начну с нейронных сетей Калмогорова. Нейронные сети Калмогорова, всем известно, что происходят из 13 проблемы Гильберта, который он сформулировал в начале XIX века, что невозможно представить трёхмерную функцию в виде суперпозиции двухмерной функции. Эту задачу решал Арнольд в 1957 году. Затем более общую теорему доказал Калмагоров, которая называется теорема представления непрерывных функций нескольких переменных в виде суперпозиции непрерывных функций одного переменного сложения. Теорема суперпозиции Калмогорова – это представление многомерной функции N в виде вложенной суммы функций одной переменной. И вот как раз о чем говорил Антон, что в конце апреля этого года вдруг все масс-медиа возникли, что возник новый убийца нейросетей Калмогоров Арнольд Нетвёк Канст. И было очень много публикаций во всех, в том числе в наших и в зарубежных. даже C-News, принципиально новая архитектура нейросетей, назвали их в честь советских академиков, и так далее и подобное. Сама теорема Калмогорова, если ее представить в виде нейронной сети, это как раз будет фактически двухслойная нейронная сеть с одним скрытым слоем. И где доказано, что в скрытом слое должно быть 2n присоединиться, где n это размерность входной функции, фактически число входов в нейронную сеть. Мы еще в 2018 году предложили нейронные сети на основании теоремы Калмогорова. Суть её какая, если представить сравнительно с нейронами классические, когда на вход подаётся какой-то вектор x, и каждый вход взвешивается своими отдельными коэффициентами, суммируется, применяется активационная функция, всем известный классический нейрон. который еще изобретен в начале 50-х годах, 60-х. Мы предложили использовать, что каждый вход будет преобразовываться своей функцией. И тогда, собственно, сам нейрон будет в виде суммы преобразованных функций от каждого входа. Проблема какая? Как определить эту функцию? В 2018 году у нас был патент подан и уже более 20 статей опубликовано. Функцию мы применяем, в отличие от КАН, Значит, где они применяют сплайновую каждую функцию, а мы здесь задаем в виде табличного задания функции, в виде таблицы. То есть это, как это, lookup table по-английски. И тогда выходы вот этой таблицы, количество swaps, это количество неизвестных фактически вот этой функции, чтобы определить ее, такую функцию. Двухслойная нейронная сеть, если два входа, то видно, что на каждый вход вместо одного коэффициента, которое в классической нейронной сети, здесь требуется целая таблица, то есть много переменных. И количество переменных растет очень стремительно. то есть возрастает на порядок по сравнению с классической нейронной сетью. Но все зависит, сколько столбцов в каждой такой таблице, которые можно задавать. В принципе, если в таблице только один столбец, это фактически переводит к классическому нейрону, но правда с линейной функцией. активационной. Мы применяем для обучения нейронных сетей гибридную схему, которая была опубликована еще в 2016 году в журнале Geophysics американском толстом. Смысл которой следующий, что сначала определяется геометрия нейронной сети, потом генерится множество вариантов нейронной сети со случайными коэффициентами, в данном случае функциями. Потом каждая нейронная сеть считается с заданным массивом обучения и получается ошибка. И потом на следующем этапе выбирается несколько заданное количество нейронных сетей, у которых ошибка минимальная. Допустим, если у нас было 100 вариантов нейронных сетей, то выбралось, допустим, нейронная сеть номер 3, 15, 21, 57. И дальше только выбранные нейронные сети участвуют в генерации нового поколения нейронных сетей. Классический генетический алгоритм предполагает два механизма формирования вот этих новых нейронных сетей. Это кроссово, это обмен, разбиение по парам нейронной сети и обмен коэффициентами случайным образом, также как обмен генами. в парах. И другой механизм – это мутация, когда к результату добавляется маленькое случайное число. Мы еще добавили один градиентный шаг, чтобы улучшить качество этого обучения. И таким образом происходит обучение в виде такого цикла. Почему мы добавили именно градиентный шаг, но стахастическая схема это фактически вот если приставить мишень, то выставляем и как-то более-менее равномерно попадаем в мишень и какие-то случаи могут давать нам глобальный минимум, то есть он лучше работает именно для поиска глобального минимума, в отличие, допустим, от градиентного, который всегда от первого приближения скатывается ближайший локальный минимум, который может и не быть глобальным. А градиентные шаги, когда мы нашли какой-то минимум, он ещё даёт возможность скатиться и улучшить улучшить качество поиска коэффициентов неизвестных как здесь показано также мы применяем регулизацию по Тихонову в данном случае классический функционал Тихонова это нужно подобрать функцию t от коэффициентов в нашем случае В теореме Калмогорова все эти функции должны быть монотонны. Требования монотонности можно выразить таким образом, чтобы коэффициенты были всегда монотонны, возрастающие или убывающие. Также выбирается некий оптимальный альфа. Это известная из публикации Тихонова вещь. Кроме этого, мы оцениваем точность прогнозов на основе многократных вариантов прогноза по частичным выборкам. Если у нас, допустим, есть некий набор скважинных данных, то мы делаем несколько вариантов прогноза, случайным образом выбирая некое количество скважинок заданных. И потом по этим многократным прогнозам можно вычислить средний прогноз, разброс прогнозов в виде стандартного отклонения. Также можно вычислить по 10, по 50, по 90 некие статистики. Я говорил о патенте, видеть патент как раз это построение нейронной сети на основании нейронов с функциональным преобразованием произвольного вида для входных сигналов. И здесь список публикаций, но не все, а вот где напрямую указано даже в названии слово Калмогоров. В том числе, видите, были публикации, в том числе за рубежом, на SPI, в Abu Dhabi, на английском языке. Много наших публикаций. Ну, это только публикации в журналах, не считая многих докладов на различных конференциях. Здесь я их просто не привёл, там ещё, наверное, порядка 20, то ли, упоминаний вот этих нейронных сетей. Ну, что можно сказать, что как, значит, американские учёные, как бы, как всегда, имеют такой этот хайп, какие-то возможности по раскрутке своих идей. У нас таких возможностей нету. И поэтому вот почему-то, несмотря на то, что даже всё это опубликовано, и только мы узнаём о колмогоровских нейронных сетях только от американских или английских учёных, пресловутых, так сказать. Немножко расскажу о наших задачах. Ну, кто не знает задач сейсморазведки, вот я здесь такой слайд привез. Это такая схема сейсморазведочных измерений, когда есть какой-то источник колебания, это может быть взрыв, это может быть какие-то специальные вибраторы и ставятся сейсмоприемники и вот отраженные сигналы попадают на сейсмоприемник и здесь мы получаем информацию об отраженных что ли слоях, о глубинном строении значит этой земли В том числе, можно определить, найти какие-то структуры и нефтегазовые залежи. Но это давно применяется и, в общем-то, просто как это все делается. И схема еще, допустим, у сейсморазведочных данных, вот если есть у нас волновая картина перехода вот этих отраженных волн, в данном случае после обработки, то наша задача заключается в том, что если есть у нас какие-то измерения в скважинах, допустим, и мы получили какой-то параметр, то нам нужно обучить нейронную сеть вблизи вот этой, допустим, скважины, как по сейсмическому полю можно прогнозировать вот эти измерения скважины. И потом распространить, в общем-то, этот прогноз дальше на все сейсмическое поле и получить, ну, как бы, 

S01 [00:15:33]  : Иван, извините пожалуйста, звук почему-то стал чуть-чуть хуже несколько минут. 

S00 [00:15:41]  : Вот так слышно? 

S01 [00:15:42]  : Да, так лучше. 

S00 [00:15:44]  : Тогда я немножко повернул, да. Дальше прогноз объектов с помощью нейронной сети Колногорова. То есть вот здесь вот схема, допустим, подается сейсмическая трасса и есть измерения в скважинах, и вот мы обучаем нейронную сеть на скважинах. В данном случае здесь приведена вход 84 параметров, это сейсмическая трасса по нескольким кубам сейсмическим. Мы применяем обычно, ну для нейронной сети Калмогорова не нужно много скрытых слоев, достаточно одного, ну максимум двух скрытых слоев, потому что у нейронной сети Калмогорова очень высокая степень свободы. И поэтому здесь достаточно одного скрытого слоя, чтобы получить достаточно хороший прогноз. И здесь показан пример размера разреза по сейсмическому клубу. Видно сейсмическое поле, показано в виде этих картинок. Это как происходят волны, допустим, отраженные волнами каких-то геологических объектов. Здесь приведены какие-то картажи измерения склажинок. Показаны 4 склажина, которые характеризуют изучаемую толщу от кровли до подошвы. После прогноза мы получаем прогноз, допустим, каких-то эффективных параметров, в данном случае прогноз полистости с помощью нейронной сети. Видно, как складно-скважинно показаны кривые именно полистости, а здесь на разрезе, это разрез по системическому кубу, показан прогноз вот этого параметра. Но в данном случае мы показываем пример именно сейсмической инверсии. Сейсмическая инверсия – это стандартная процедура, которая позволяет обратить сейсмическое поле в некие параметры, Решается обратная задача по волнонормовому уравнению, чтобы получить такие параметры, как акустический педаль, это произведение скорости, распространение волн на плотность горных пород. Скорости протойных волн, скорости поперечных волн. Протойные волны, когда колеблются вдоль распространения волн. Поперечные, когда поперек. Также плотность еще можно определить с помощью классической и технической инверсии. Которая вот до сих пор является основным таким инструментом определения В крайнем случае, эта сейсмическая инвестиция приведена из круглых параметров параметра пористости, то есть параметра резервуара. Это нейросетевой прогноз с помощью классических нейронных сетей, по тому же разрезы, видите, что разрешающая способность немножко повысилась за счёт того, что всё-таки нейронная сеть – это нелинейный оператор, а классическая сейсмическая инверсия – это всё-таки линейный оператор. И сейчас в отрасли очень большое нефтегазовой отрасли, очень большой интерес к нейронным сетям, к таким преобразованиям, к таким прогнозам, потому что они реально дают немножко лучший прогноз и позволяют сэкономить набрение и так далее и тому подобное. Нейронные сети Калмогорова дают еще лучший прогноз с более лучшим разрешением что ли вот здесь показан пример вот и можно посмотреть на графиках вот этот график показан четыре скважины вот и каждая панель это одна скважина и график черная графика это измерены значения измерено значение пористости, и красным – это как эта пористость прогнозируется с помощью сейсмических данных. Это как раз классическая сейсмическая инверсия. Нейросеть, видите, даёт за счёт своей нелинейности немножко лучший результат. Можете обратить внимание, красная кривая лучше аппроксимирует черную кривую за счет нелинейности этого оператора. И нейронные сети Калмогорова за счет повышенной своей адаптивности к входным данным и нелинейности Луч в степени свободы дает еще чуть лучше результат, но видно, что здесь получается достаточно хорошее приближение измеренных пористостей в этих четырех скважинах. Также мы опробовали на классических моделях. Это знаменитая модель MagMossia 2. Здесь показан разрез скоростей продольных волн. Это модельное значение. Мы искусственно сделали четыре скважины, где сняли с этого куба значение скоростей продольных волн. Дальше обучили нейронные сети Калмогорова и получили вот такую картинку, это уже прогнозный куб Мармоси с помощью нейронных сетей Калмогорова. Видите, конечно не идеально, но какие-то основные моменты мы можем определить, если иметь в виду, что мы фактически только брали вот эти 5 скважин частично как бы данные то в принципе результат достаточно хороший вот и сейчас я немножко хочу показать вот наш программный продукт в живом виде как он это выглядит значит вот здесь Этот программный продукт является российской разработкой, то есть российское программное обеспечение, и в принципе сейчас очень большой интерес к нему, потому что наше решение как раз включает такие нейросетевые прогнозные построения, которых нет в западных программных продуктах. Сейчас на рынке программного обеспечения сложилась такая сложная ситуация, Потому что, в общем-то, западные производители практически все ушли с рынка российского. Но, тем не менее, их софт до сих пор работает. Нефтяные компании перестали платить поддержку, перестали покупать новые программные продукты, и в принципе живут достаточно хорошо, потому что они решают, фактически сократили свои расходы на программное обеспечение почти до нуля, хотя пытаются, конечно, там разработать. И пробиться на этот рынок, в общем-то, только можно с тем, чего нет в западных программных продуктах. И вот как раз мы предлагаем сейчас нейросетевые прогнозные решения, но не только нейросетевые, там много используем других еще алгоритмов машинного обучения. Пытаемся пробиться на этот рынок усиленно, именно на этой волне. Сейчас я покажу. наш интерфейс. Видно разрезы сейсмического поля. Слева это карта на каком-то горизонте. И сейчас я покажу, как мы делаем этот прогноз. Прогноз у нас имеется. Прогноз сейсмических кубов. Это модуль прогноза. Здесь задается сейтмический куб, задается кровля подошва, но чтобы не весь куб выполнять прогноз, а только какую-то часть. И здесь задается учителя, но в данном случае у меня на этом примере Всего 4 скважины, хотя скважин может быть достаточно большое количество на некоторых. И выбирается параметр для прогноза. На данном случае прогноз по вистости. Дальше здесь, если между кровью и подошвой еще есть другие какие-то горизонты, то они тоже задаются. Вот. И выбирается алгоритм прогноза. Можно начать вот с линейной регрессии. Ну, фактически это аналог сейсмической инверсии. Запускаем. Ну, во-первых, можно предварительно посмотреть коллекционную таблицу, как параметры коррелируют в общем-то с нашим прогнозным этим полем. Вот и дальше запускаем собственно сам прогноз и получаем вот такую картинку это вот то что я говорил как раз черным это измеренно измеренно коричнесть и красным это полученный прогноз с помощью линейного оператора. Если дальше можем построить, использовать классическую нейронную сеть и вот она уже идет обучение нейронной сети вот видите как раз циклы идут и красная кривая это целебная функция как уменьшается зеленая кривая это концентрализация ну фактически как на скважинах прогноза кривая коррелирует в текущий момент вот и получаем результат прогноза который ну выглядит вот видите уже чуть получше нежели было с помощью линейной регрессии. Следующий – это комагоровские нейронные сети. Последний параметр, Defunction, это размер таблицы, вот это lookup table для функции. Три нейрона, всего мы используем в данном случае в качестве промежутка одного скрытого слоя. Если задавать через запятую, то можно здесь сразу несколько скрытых слоев задавать. Запускаем тоже. И вот видно, что цифровая функция уменьшается, а кризинквалификация сразу пошла гораздо выше, потому что в общем-то степень свободы в этой нейронной сети чуть-чуть повыше. И сейчас как раз идет обучение нейронной сети. В данном случае, конечно, это не такой большой объем данных, это специальный демонстрационный набор данных. Но, тем не менее, все это работает на моем лаптопе, то есть не где-то на каком-то сервере. В принципе, все это работает, и вот можно увидеть, вот получился этот прогноз уже с помощью нейронной сети Комагорова, и видно, что в принципе аппроксимация ног гораздо лучше получается. И сразу я здесь еще один алгоритм запущу, что это за алгоритм, я потом как раз иерархические нейронные сети запускаю и покажу, что принципе вот иерархические нейронные сети они смотрите вот здесь я практически точно результат точно прогнозируется извиняюсь да точно как бы это повторяет Здесь не всё это поле мне видно, поэтому здесь немножко промахнулся. Вот оно. Вот видите, что черный и красный кривая полностью совпадают, это потому что здесь применяется метод ближайшего соседа, который своего учителя точно находит. И соответственно результаты прогноза здесь в виде виртуальных кубов, сейчас я их покажу. Это исходный куб. И вот виртуальные кубы, которых физически нет, есть только алгоритм их вычисления. Поэтому сейчас, когда я нажимаю, как раз происходит вычисление, собственно, вот этого разреза. Это как раз прогноз по резкости с помощью линейного оператора. Дальше прогноз повистости с помощью классических нейронных сетей. Как раз разрез сейчас считается. Дальше разрез с помощью нейронных сетей Калмагорова. Показываю, как он выглядит. Видите, здесь немножко как бы разброс чуть повыше, но мы считаем, что разрешающая способность это чуть повыше. И прогноз с помощью алгоритма ближайшего соседа. Вот сейчас он как раз тоже выведется. Ближайшим соседом с помощью эротических нейронных сетей нашла новая разработка, которая имеет достаточно хорошие перспективы. Также мы можем прогнозировать, кроме кубов, еще и карты с помощью нейронных сетей. Я их для экономии не буду показывать, но просто немножко это покажу, что здесь достаточно большой В общем-то, выбор алгоритмов – это нейронные сети Коханина, но фактически метод ближайшего соседа, линейная регрессия, ICI-регрессия – это нелинейная регрессия, классические нейронные сети Random Forest и нейронные сети Калмогорова. также имеется. Результатом здесь получаются прогнозные карты, которые, в общем-то, самое важное в прогнозных построениях, когда прогнозируются карты эффективных толщин, это дает буровикам информацию, где закладывать новые скважины. На этом я еще немножко покажу Давайте это другой проект, что мы решаем не только такие задачи, вот именно прогнозы, но с помощью машинного обучения, в данном случае вот результат аналог муравьиных труб, то есть мы можем выполнять вот такие улучшения как бы сейсмического имиджа, вот здесь вот показаны как бы результаты прогноза не прогнозов, в общем-то, вот в юрских отложениях вот эта пара словы и отложения, как они вот хорошо это видны, и вот это вот, это месторождение Узень в Казахстане, которое ведет разработку еще со времен, с 50-х годов, вот, и здесь вот видно, что это достаточно хорошо разбурлено, здесь несколько, ну, Много тысяч скважин разбурено, достаточно хорошо разбурено, но тем не менее с помощью нейросетей мы смогли выделить какие-то перспективные участки, где можно еще заложить какие-то дополнительные скважины, в том числе на большой глубине, на границе юрских отложений фундамента. Ну вот, пожалуй, всё по поводу нашего программы продукта. Я показал только маленькую часть, конечно, вот этого продукта. Он включает в себя достаточно большое количество различных процедур и сейчас достаточно хорошо востребован на рынке. её используют несколько нефтяных компаний, сервисные компании, но потом я немножко ещё покажу об этом. И сейчас вот перейду к такой основной части, что ли. Преимущество неровной сети Калмогорова. В чём же её преимущество? Ну, во-первых, Имеется возможность контроля коэффициентов нейронной сети, которые располагаются в одной функции. Например, то, что я показывал, можно потребовать монотонность этой функции. Можно потребовать гладкость. Это можно задать прям непосредственно функции регулизации Тихонова. В классической нейронной сети неизвестные коэффициенты разбросаны в сети в самых разных нейронах. И здесь можно требовать только минимизации этих коэффициентов, что тоже хорошо стабилизирует нейронную сеть, но все знают, что во всех нейронных сетях применяется регулизация Тихонов. Я, кстати, это не люблю, когда говорят просто регулизация, потому что регулизации на Западе много ученых наших выехало, они внесли туда регулизацию по Тихонову. Тихонову слово выбросили, оставили слово регулизации. Теперь нам идут программные продукты, где выполняется некая регулизация там самая разная. Зайдешь даже в Википедию, там есть Допустим, регулизация по Тихону, когда квадрат с функцией минимизируется. И регулизация, допустим, Лассо, когда минимизируется по абсолютной величине. На самом деле и то, и другое это регулизация Тихонова. И Лассо придумано там это где-то в 90-х годах, а Тихонов опубликовал это еще начиная с 40-х годов. И там в том числе есть эта функция, приведена у него минимизация вот именно вот этих в абсолютной горячине. Так что это немножко неправильно. Количество столбцов функции, которые мы задаём, это тоже регулирует степень свободы нейронной сети. Если задать, допустим, всего три столбца, фактически это достаточно гладкая функция, приближающаяся, может быть, даже к этому, к сплайну. А если задать, допустим, столько столбцов, то, в принципе, там может быть достаточно хорошо адаптироваться, что ли, к входным данным. Каждая функция, тем не менее, количество неизвестных резко возрастает. Также нейронная сеть адаптируется к распределению входных параметров. Это позволяет использовать меньше скрытых слоев, меньше нейронов. Тем не менее, преимущества в виде отсутствия нейронной сети галлюцинации нет. Потом я подробно расскажу, почему я так думаю. Во-первых, мы видим это сплошь и рядом. что, в принципе, если у нас скважины находятся в какой-то одной части нашего разреза, а нам приходится прогнозировать в другой части разреза, где совсем другие сейсмические данные, тем не менее нейронная сеть даст какой-то прогноз, и фактически это можно считать некой галлюцинацией, что ли. Это неправильный прогноз. То есть на самом деле мы сталкиваемся с галлюцинациями сплошь и рядом. Также преимущества в виде возможности дообучения нейронной сети без эффекта катастрофического забывания тоже нет. То есть, если добавляется новая скважина, то нам требуется использовать весь обучающий массив, то есть все скважины, которые до этого обучали. а использовать только отдельную скважину для обучения, это проблематично, фактически это всё нарушает полностью эту нейронную сеть. Примерно то же самое, что и в других применениях таких нейронной сети. Хочу подчеркнуть, что то, что там написано в этих публикациях, которые там «убийца нейронных сетей» по поводу КАН, Калмогоров Арнольд, что там и галлюцинации пообеждены, и катастрофическое отзабывание пообеждено. На самом деле этого еще нет. Почему нейронная сеть является неустойчивой? Потому что если мы подаем какой-то сигнал на вход нейронной сети, то вот этот сигнал проходит практически через все нейроны. Причём неуправляемо, то есть может пройти достаточно сложным путём и где-то зацепиться, где-то исправить. Такой вот пучка информации, которая проходит от входа до выхода в нейронной сети нет. И фактически современные активационные функции, которые половинку только считают, по факту, почему я считаю, они лучше работают, чем, допустим, классические функции, которые работают и для минуса, и для плюса. активационные функции сегмоидные, то вот эта половинка позволяет сэкономить, что ли, не все нейроны будут активированы, что ли, и не все нейроны будут работать. Недостатки нейронной сети, исходя из нашего опыта, это, во-первых, сложность обучения. Это экспоненциальные требования к затратам на обучение в зависимости от количества обучающего массива, от размера обучающего массива и сложности геометрии нейронной сети. И поэтому требуются очень большие вычислительные затраты на обучение. Но все мы знаем, вот это GPT нейронные сети, вот генеративные сети, там применяются вот эти вот очень большие вычислительные мощности на основании вот этих видеокарт современных. и обучение происходит достаточно длительное время. Количество неизвестных там исчисляется миллиардами, по-моему. Эта сложность всегда зависит Наши задачи, в общем-то, не такие сложные, как чат GPT, чтобы обучить. Но, тем не менее, всё равно, если скважин, допустим, большое количество, больше 20, то уже, допустим, требуются достаточно большие вычислительные ресурсы. на обучение такой нейронной сети. Поэтому фактически с помощью нейросетевых технологий, классических нейросетевых технологий, использовать больше 25-50 скважин уже проблематично, 100 скважин это уже фактически предел. То есть там нужны тогда очень большие вычислительные затраты, какие-то сервера, какие-то видеокарты, что проблематично в наших условиях. Другая проблема, но я уже упомянул, это галлюцинация. Галлюцинация происходит в результате обобщения всех данных в процессе обучения и невозможность или сложность определения отсутствия нужных данных, используемых при обучении. То есть, если данные какие-то не были использованы, Если в запросе требуются какие-то данные, которые не были в обучающем множестве, то нейронная сеть все равно даст какой-то ответ, который будет фактически галлюцинацией. подтверждение вот таких вот вещей, множество примеров и так далее. Наверняка многие сталкивались с этими проблемами галлюцинации. У нас галлюцинация выражается в том, что прогноз происходит даже в том случае, если у нас, допустим, звучащее множество не описывает полностью сейсмическое поле, то есть они находятся в каких-то сложных других условиях. Допустим, вот сейчас мы работаем над одним проектом, это проект газового месторождения в Египте, и там есть, там такая ситуация, что над пластом имеется соляные купола и там где соляные купола сейсмические отражения просто не все проходят потому что там очень пологие такие края вот этого сейсмического тела и сейсмические волны отражаются в сторону и не приходят на сейсмоприемник нет отражений и там сейсмическое поле совсем другое нежели там в других участках где нету таких пологих участков. И вот там фактически можно получить галлюцинации, потому что сейсмическое поле совсем разное получается. Вот сложность для обучения. То есть при появлении новых данных для обучения нужно полностью повторить весь процесс обучения с использованием всех данных. Иначе эффект катастрофического забывания. Но всем это известно тоже, это такой момент. Любой сигнал, который подается на вход нейронной сети, проходит почти через все нейроны. И это делает нейронную сеть очень неустойчивой и сложной для переобучения. Представьте, что нам какой-то видеосигнал на глаз попал, и вот этот видеосигнал, чтобы принять какое-то решение, проходит через весь мозг. То есть это фактически очень неустойчивая нейронная сеть. И при этом неустойчивость можно проявить, если любой элемент, нейрон или даже один коэффициент нейронов будет нарушен, то нейронная сеть может полностью перестать работать. Это тоже известный факт, в общем-то, неустойчивость такая существует. А хотелось бы вот такую нейронную сеть, чтобы сигнал распространялся неким пучком, что ли. И я так предполагаю, что в нашей голове мозг примерно так же работает, что если подали какой-то сигнал на вход, то, в общем-то, активируются нейроны не все, которые там имеются, а какой-то пучок нейронов, что ли. идет вдоль сигнала, но в данном случае фактически один пучок, то есть какой-то путь определенный проходит, и мы получаем значение прогнозного параметра y. Хотелось бы такую нейронную сеть в идеале получить. И мы основывались на методах ближайшего соседа, я уже говорил. Это широко применяемый метод, который не требует этапа обучения, очень стабильный, имеет множество преимуществ. Например, прямое сравнение тестируемого объекта со всеми объектами обучающего множество. он основан на методе аналогии. Это метод аналогии фактически наш главный инструмент мышления. Когда приходит новая информация, новый видим объект или явление, мы всегда ищем некое подобие, чем мы встречались ранее. И этот метод аналогии фактически как раз вот это является реализацией метода ближайшего соседа. Дальше, легко определить точность прогноза по значению похожести определяемого объекта с объектами из обучающего множества. Можно установить некий порог похожести и определять, что определяемый объект не принадлежит обучающему множеству. То есть это новый объект, который мы раньше не видели и который может составить некую основу как дополнительное объект, который можно добавить к обучающим множество, как новая разметка, что ли. Но главный недостаток ближайших свет, это необходимость выполнять операции сравнения по всем объектам. Для этого нужно иметь доступный весь массив обучения. Этот процесс может занять достаточно большое количество времени. Допустим, если мы представим, что у нас имеется миллиард фраз на русском и соответствующая фраза ему на английском. Чтобы мы задаем фразу, чтобы выполнить перевод, надо выполнить миллиард сравнений вот этой входной фразы, чтобы получить вот это вот решение. перевода, что ли, именно на основе ближайших соседов. Поэтому необходимо, в общем-то, придумать некий метод ускорения ближайших соседов. Большое количество, большое множество всяких разных подходов мы использовали на основе построение кластерного поискового дерева на основе алгоритма классификации без учителя, только по описанию объекта, без учета разметки. То есть фактически мы берем вот эти объекты обучающие, где есть и описание, и разметка, и вот как бы это метка что ли, значение прогнозного параметра. Так вот мы классификацию делаем только по описанию В нашем случае это фактически сейсмический сигнал. Для этого мы используем алгоритм кластеризации без учителя Self-Organized Mapping Коханина с использованием вариантов проекции многомерного пространства Коханина 1D, 2D, 3D. Это мы предложили достаточно давно. Но в нашей отрасли используется коханин 1D, когда номер класса это фактически один индекс номер класса, а 1, 2, 3, 4, 5 номер класса идёт. Сам коханин предложил карту классов. Это фактически двухиндексная такая вот, здесь на рисунке видно. Но мы ещё решили, что не карту, а куб классов, когда трёхиндексная классификация происходит. что позволяет лучше использовать возможность построения поискового дерева. Весь массив обучения разбивается на заданное количество кластеров. Затем все объекты Дальше массив обучения разделяется на эти кластеры, в каждом кластере. Потом каждый кластер еще разделяется на новый уровень кластеров на следующем уровне. Потом каждый кластер следующего уровня еще на один уровень. Фактически строится дерево, кластерное дерево. До тех пор, пока, допустим, объектов в текущем кластере будет мало, тогда это формируется лист этого дерева. Затем как работает? Тестируемый объект сравнится нейронами первого уровня, затем нейронами второго уровня. по ветке победителя, и так далее, и так далее. Количество сравнений уменьшается кратно. То есть мы уже теперь не должны сравнивать, в общем-то, каждый нейрон со всеми, каждый объект со всеми объектами обучающего множества, а только сравниваем, вот именно, достаточно быстро и еще. Ну вот как раз я вот эту картинку уже показывал. то есть у нас выполняется разделение на кластеры и вот таким образом мы находим некий путь некий путь поиска объектов, которые находятся в листе вот этого последнего кластера и там уже выполняем прямое сравнение со всеми объектами в этом листе и получаем прогнозный параметр. Достаточно быстро получается и очень эффективный алгоритм, но имеет свои недостатки, конечно. имеет свои недостатки. Мы опробовали этот алгоритм на рукописных символах MNIST. В чате AGI Russia всегда говорили, не приходите с какими-то новыми алгоритмами, которые не опробованы на классических тестовых наборах. В данном случае тестовый набор включает 60 тысяч изображений символов 28 на 28 пикселей для обучения и 10 тысяч для изображений для тестирования. То есть если взять каждый объект из этого для тестирования и начинать сравнивать с этими объектами обучения, то мы написали такую программку, у нас на лаптопе мы потратили 80 минут и ошибка 369 символов, но там был очень простой алгоритм, просто простое сравнение попиксельное изображение из набора для тестирования с изображениями наборов для тренинга. После того, как мы сделали разделение на кластеры, здесь схема не совсем такая, что сделано в данном случае. 60 тысяч разделено на три кластера и так далее. И потом получилось какое-то дерево. Но даже вот это дерево, оно существенно сокращает время поиска. Это просто для показа, приблизительная схема. И тестовый набор, мы сделали 10 кластеров на каждом уровне и с глубиной дерева до 5. Прогноз по тестовому набору затратил уже 0,12 минуты, то есть ускорение 670 раз по сравнению с полным перебором, 81 минута. Но правда ошибка распознавания увеличилась немножко, там было 300, а здесь вот 564 символа. Здесь показаны символы, которые ошибочно были сделаны. И вот видно символ какой-то похожий, то ли на семёрку, то ли на единицу. символы в качестве ближайших соседей. Здесь пять ближайших соседей было сделано. И вот другие символы тоже можно будет. Шестёрка, вот она похожа на четвёрку, где-то шестёрка есть. Это прямое сравнение по пиксельным. Здесь, конечно, можно было придумать более сложный алгоритм, более сложную метрику, но здесь не ставилась какая-то задача, чтобы получить какое-то рекордное значение. прогноза, и никак мы не оптимизировали. Это самая простая, тупая реализация. Видно, как вот эти символы и ближайшие соседи из обучающего множество. В нашем случае, как мы работали, то есть у нас есть некая скважина, И вот точки это где измерено значение прогнозного параметра, та же пористость с помощью картарных измерений. И мы берем сейсмический сигнал вокруг этой точки и формируем некий обращающийся сигнал значения прогнозного свойства. И таких вот достаточно множество. Это когда одна скважина, то их может быть большое количество. И здесь почему мы раньше не могли применять вот этот метод, потому что много, большое количество скважин просто занимало неприемлемо большое время. А это вот просто другая картинка, аналогичная, что мы берем не просто один сигнал, а некое сейсмическое изображение вокруг точки прогноза. Вот здесь показано таким образом. И вот сам этот алгоритм, как это подготовка данных, потом обучающая выборка, формируется сейсмический отклик, прогнозный параметр, кластеринация построения дерева, поискового дерева кластерного. Вот, и собственно сам сейсмический прогноз. Видите, сейсмические прогнозные данные, они сразу идут вот здесь. И здесь вот этот пример, это вот разделение на кластеры. на 5 кластеров, каждый кластер еще делится на кластеры, и потом формируем, здесь вот это немножко другая картинка, вот сейсмический отклик, и видно прогнозный параметр, он не проходит через кластеризацию, он сразу прикреплен к каждому сигналу и приходит уже в листья, где похожие сигналы. Это вот разрез по месторождению Кронинген, это газовое месторождение около Голландии, которое недавно закрыли за счёт того, что там землетрясения техногенные всё время проходили. они открытые данные здесь порядка 300 скважин и вот сам пласт вот находится где-то вот здесь вот видите он достаточно раздроблен и раньше мы просто не могли даже нейросетевой прогноз по нему применить но в данном случае мы выполнили прогноз с помощью иерархических нейронных сетей методом ближайшего соседа получился результат достаточно хорошего качества, потому что скважины на скважины и прогнозный параметр достаточно хорошо, но с точностью до дискретизации как бы сейсмики, там 10 метров дискретов мы получили 5 метров дискретно, получили достаточно хорошее приближение. Черное это измеренное значение, в данном случае мы измерили плотность, и красное это прогноз с помощью эргономических нервных сетей. По всем данным мы получили коэффициент корреляции 0.89, ну что достаточно очень хорошей коэффициент корреляции считается. Сейчас мы предлагаем такие решения вот именно для горизонтального курения. Здесь показано сейсмическое поле и горизонтальный ствол с измеренной, с воспроизведенной полеистостью в данном случае. Здесь буровики, конечно, увидят, что Сначала скважина зашла в какой-то пласт. Здесь пористость начала повышаться. Они думали, что это как раз является этим пластом. Начали заворачивать уже горизонтальный ствол. Пошли горизонтально, потом вдруг раз, вышли из пласта. Видите, пористость уменьшилась. потом вдруг опять вошли. И все это непонятно. После того, как выполнил прогноз, сразу картина стала ясна, что здесь вот какой-то промежуточный слой, не главный, а главный слой вот как раз здесь. И дальше мы можем указать, куда дальше бурить, в каком направлении продолжать бурение, допустим, в этой скважине. Вот это вот один метод, этот метод ближайшего соседа, который был выполнен. Это в плане. И преимущество иерархических нейронных сетей – это возможность большим и громадным количеством скважин. Мы опробовали 3000 скважин уже на таких примерах. Сейчас очень много компаний имеет такую потребность, в общем-то, прогнозных построений, там очень много, большое количество скважин. Еще скважины, они, сейсмическое поле обычно измеряется как бы в единицах времени прихода отраженной сейсмической волны, фактически в микросекундах, от момента взрыва до момента первого всплеска, допустим, на сейсмоприемнике. А скважины бурятся в глубинном масштабе. Поэтому нужно сейсмическое поле привязывать к скважинам. Это тоже проблема в сейсморазведке, достаточно сложная. И всегда имеются какие-то ошибки в привязке скважин. И вот сами нейронные сети классические, они очень чувствительны к этим дефектам. То есть там нейронная сеть пытается аппроксимировать все, и если дефекты, то результат получается либо плохо аппроксимирован, либо получается какой-то большой шум что ли в прогнозных построениях. Метод ближайшего седа на основе иерархических нервных сетей, он нечувствительен к этим дефектам, и возможно плавное изменение прогноза при приближении к скважине тестируемого этого момента. Есть возможность анализа распределения прогнозного свойства в листьях деревьев. Это позволяет выполнить некое обобщение, или обнаружить отсутствие связи прогнозного параметра и сейзмики, либо обнаружить, что связь нашего обучающего массива, он недостаточно хорош для выполнения прогноза. Обобщение можно делать и классическими нейронными сетями, или можно просто, допустим, средне посчитать что-нибудь типа такого. Если текущий сигнал не похож на сейсмический сигнал вблизи скважин, то создается новый кластер с новой разметкой. Фактически это и является возможностью дообучения нейронной сети. Это позволяет избежать нейросетевой галлюцинации. Просто мы задаем некий порог на похожесть этих новых объектов с уже существующими. Если они больше этого порога, то этот объект просто создает новый кластер, новый объект со своей новой разметкой, объект номер один, допустим. Потом следующие объекты, которые будут похожи на него, они будут уже в этот кластер падать и уже будут с этой разметкой. И так далее. Фактически, как мы воспринимаем мир, то есть постепенно, познавая его, появляются какие-то новые знания, новые объекты, они всегда не похожи на те, которые мы уже ранее знали, то они создают новое качество знаний, что ли. И эргономические нейронные сети как раз могут описывать этот момент. И понятный алгоритм работы – это уже не чёрный ящик, это жадный алгоритм, нет размазывания информации по всей сети. Вы видели, Путин достаточно прямой и очень хороший результат работы. Я думаю, что иерархические нейронные сети… Обучение, ну это фактически вот кластеризация, построение как бы кластерного дерева, оно линейно зависит от обучающей выборки и сложности геометрии. Не требуются супер вычислительные комплексы, даже для громадных данных с миллиардами пар в обучающем массиве достаточно хорошей обыкновенной машины, здесь не надо кластерных каких-то супер вычислителей. важностей. Нет проблем с галлюцинацией, так как можно всегда определить, что запрашиваемые данные просто отсутствуют, если задать порог похожести объектов. Иерархическая нейронная сеть очень устойчива. Если любой элемент, нейрон или даже коэффициент нейрона будет нарушен, то нейронная сеть продолжит работать, но с обыванием неких ветвей памяти всего лишь. Вот легко, легкое для обучения, без повторения полного обучения. Эффект катастрофического забывания полностью отсутствует. Также нет эффекта катастрофического запоминания тоже, он полностью отсутствует в этой нейронной сети. Есть возможность анализа совместимости пар обучения, если в листах должны быть похожие объекты с похожей разметкой. Если разметка разная, то дальше описание объектов не полное для решения данной задачи. Есть возможность генерализации одинаковых объектов в листах. Возможно обучение на самом небольшом количестве данных, которое сразу можно даже сразу вовлекать в работу. Возможность обучения на неразмеченных данных, то есть объединение подобных объектов, ну что-то типа кластеризации обыкновенной. Причем, смотрите, вот когда мы создаем кластерное дерево обучающего, Первый уровень – это некие кластеры с большим обобщением всех объектов. Их тоже можно описать, что это некое обобщение знаний. Потом каждый уровень – это всё идёт. Так же, как у нас есть библиотека. или, допустим, какие-то классификации вот этих, ну, животных дарвинских, всегда имеется какой-то общий какой-то класс, потом каждый класс разбивается еще на подклассы, каждый подкласс еще на подклассы, то есть очень легкое объяснение вот этих вот кластерного дерева и эротических нейронных сетей. И, в общем-то, очень понятная как бы технология, ничего не нужно там вот там. придумывать, в отличие от нейронной сети, которая, я считаю, нейронная сеть, это такое, ну, немножко, применение, оно полезное, как бы, эта штука, но, тем не менее, то, что пытаются применить и строить вот эти громадные нейронные сети, Это вот неправильное применение. Там сейчас на хайпе всё это идёт, вот эти вот чаджи пяти и подобные как бы решения. Но на самом деле они всех страдают, то есть скрываются как бы вот эти недостатки, которые сейчас полезны уже. Это вот как раз галлюцинации, сложность до обучения. и тому подобное, то есть очень много. И то, что вся информация размазывается по всей нейронной сети, вот миллиардная нейронная сеть, представляете, много-много слоев, и вот сигнал идет по всем этим штукам. Ну, представим, что это в нашем мозге, в каких-то реальных условиях. То есть, чтобы такие нейронные сети существовали, это сложно. Там всегда какие-то пучки информации идут. Недостаток. Нужно хранить весь массив обучения. Ну, фактически, когда мы построили эту нейронную сеть иерархическую, то фактически вот если у нас миллиард обучающих множества, мы все должны хранить. Но на самом деле это не такая большая проблема сейчас, тем более сейчас появились и диски какие-то громадные, то есть миллиард даже обучающих множества это не такая большая информация, которую там нужно специально как-то хранить или какие-то специальные там хранилища делать. В принципе все знания человечества, там кто-то вычислил, не такое большое количество информации в общем-то, их тоже можно где-то записать и выполнять, сделать классификацию и выполнять как бы вот эти вот решения. И еще вот это, здесь вот я полазил по интернету, интернета, вот изображение мозга, вот смотрите, как оно выглядит, кусочки как бы этого, фрагменты мозга, то есть видно, что вот какие-то есть центральные как бы это части, потом идет разветвление, то есть фактически это что-то похоже вот на наши российские нейронные сети, которые идут от какого-то входа и потом разветвляются по областям каких-то описаний объектов и так далее и тому подобное. Я думаю, что совмещение метода ближайших соседей и кластерного анализа классификации без учителя возможно является реальной имитацией процесса мышления человека в процессе осознания окружающего мира. Так как метод аналогии при выделении похожих объектов и природных явлений есть важный инструмент познания. Ну вот, собственно, всё, что я хотел сегодня сказать. Спасибо, жду вопросов. 

S01 [01:08:53]  : Спасибо большое, Иван. Да, у меня вот есть ряд вопросов. Если у кого-то коллеги еще есть, можете подключаться. Давайте я начну. Вот у вас в начале, на 17 слайде, по-моему, было сказано, что 84 входных параметра сейсмической трассы. Вы можете пояснить? 

S00 [01:09:18]  : Ну, там, да, наша задача, мы берем фактически же это ну вот некий сигнал вот есть точка прогноза и вот вокруг нее вот это сигнал и вот это вот сигнал как раз вот может быть всего там это 20 самплов вверх 20 самплов вниз сейсмического сигнала это получается 41 сампл если два будет то это будет 84 то есть куба ну рядом или две трассы рядом Вот это вот я имел ввиду. 

S01 [01:09:51]  : А вопрос, а вы не пытались, допустим, вместо просто самплов, которые подаются на входе, брать какие-то расчетные параметры, допустим, мгновенный спектр сигнала, Гильберт преобразования, доминирующую частоту, то есть, какие-то производные вместо самплов? 

S00 [01:10:11]  : Да-да, конечно, у нас на входе можно подать множество кубов и можно задать какое-то окно, чтобы она вот эти самплы брала вверх-вниз, а можно задать, чтобы в одной точке, то есть и такая возможность тоже есть, но она тоже применяется нами, но все-таки преимущества, я всем рекомендую эти исходные смеськи и поля, хотя подается много кубов, вот сейчас вот мы как раз выполняли там это, делали пока консультацию, там подавалось около 10 кубов Тюменского нашего пользователя. Он 10 кубов подавал. Самых различных это исходное сейсмическое поле, это угловые суммы, это результаты сейсмической инверсии, акустический импеданс, VPVS, Density и какие-то еще параметры, атрибуты типа мгновенной частоты, и доминантные частоты. Вот все это подавалось, а нейронная сеть уже сама решает, насколько это полезно или не полезно. В данном случае как раз это наш метод от ближайшего свидетеля, он сам выбирает степень похожести. 

S01 [01:11:26]  : А скажите, а вот у вас не было такой проблемы, когда начинаешь увеличивать количество кубов, как вы сказали, все вместе брать, что возникает эффект переобучения и система начинает давать лучшую точность на тренировочных данных, но хуже на тестовых? 

S00 [01:11:44]  : Я показывал, что мы в принципе применяем для защиты переобучения Очень хорошая технологическая регуляция, которая не даёт переобучиться на этой системе. 

S01 [01:12:03]  : Я правильно понимаю, что те результаты, которые вы показывали, где сравнивали обычные нейронные сети, колмогоровские, иерархические, это вы показывали результаты сравнения на тестовых скважинах. То есть это не те скважины, на которых происходило обучение, правильно? 

S00 [01:12:25]  : В данном случае я на тех же скважинах показывал. Тестовые скважины у нас не один раз, а мы делаем многократные тестовые скважины. Я уже говорил, что для прогноза у нас есть некий набор скважин, и мы разбиваем их с перекрытием, что ли, выбираем из них какое-то количество скважин и делаем, допустим, 100 вариантов прогноза. а потом в каждом прогнозе неполное количество скважин. Это фактический вариант скрытых данных, если бы один путь сделать, то это как раз о чем вы говорите, что применяется в западных этих пакетах, а мы все-таки придумали более сложный вариант проверки, когда многократно делается. ну что-то типа вот идет, растут ноги, как выполняется там этот random forest что ли, когда разбивается выпорка на несколько частей. Один выход это тестовая скважина и мы можем потом получить как бы ну некое распределение, кроссплод, когда только на тех скважинах, которые в момент их получения не участвовали в облучении. И можно получить очень объективный параметр точности наших прогнозов. 

S01 [01:13:56]  : А вы делали какое-то исследование по поводу того, как для разных сетей, то есть для разных методов, про которые вы рассказывали, в среднем различается точность на тренировочных скважинах и на тестовых? 

S00 [01:14:13]  : Я показал уже как раз сравнение. Есть несколько вариантов. Это сейсмическая инверсия. Толстейская нейросеть и нейросеть в Колмогород. 

S01 [01:14:27]  : Можно вот здесь остановиться? Вот эти три варианта, это на тестовых скважинах проверка или это на тренировочных? 

S00 [01:14:38]  : Это на тренировочных, да. А вот вопрос, то есть если попытаться... Здесь всего четыре скважины, поэтому мы особо Я здесь не ставил показать точность. Вообще вопрос точности это отдельная презентация, там достаточно сложные вот эти подходы, алгоритмы. Если хотите, я могу даже живьем показать, как это мы делаем, оценку точности. Можно? 

S01 [01:15:09]  : Да, я просто уточню вопрос. Собственно, это был мой следующий вопрос, как вы делаете оценку точности. Пожалуйста, спасибо. 

S00 [01:15:19]  : Оценка точности – это важный такой момент. всех этих построений, потому что здесь очень большие цены. Если мы ошибку сделаем, то цена от ошибки будет достаточно высокая, потому что заложен Сквапор. Хорошо, если там какая-то дешёвость, важно, если в море, на глубокой воде, то это многие миллионы, даже до сотни миллионов долларов, получается, ошибка будет в стоимости. Поэтому здесь нужно быть аккуратным вот с такими оценками. Ну вот, давайте я вам покажу. пример прогноза и здесь вот каким образом здесь задаются сейсмические кубы здесь два куба задано и задаются в качестве тренинг дэйта заданы точки пластопересечения вот я сейчас на карте их это нанесу и покажу что это вот карты эффективных толщин ну допустим на фоне ну вот здесь вот, это точки пластопересечения, вот величина вот этого кругляшка, это значение, как бы, пропорциональное значению эффективных толщин. И вот мы делаем прогноз, вот сейчас скорость валидации, 100 вариантов прогноза делаем, и на каждом варианте прогноза 30 процентов скважин, ну фактически будем обучаться только на 70 процентов скважин, и это мы проделаем многократно, но давайте я сейчас вот прям запущу, вот идут итерации, то есть это варианты прогноза 1, 2, 3 и 100. может быть долгое время не будет хотя бы 10 вариантов 20 сделаем и вот можем потом определить То есть это многократные прогнозы. Здесь 30% на каждом варианте прогноза это слепые скважины фактически будут. И вот давайте сейчас хватит, чтобы времени не занимать. Прервусь. И получаем некие карты. Сейчас я покажу. Сейчас они рассчитаются. Получаются прогнозные карты. Во-первых, это некий средний прогноз. И давайте я немножко эти увеличу. 

S01 [01:18:00]  : Это карта в чем прогноз? В чем идет здесь? В чем измерение? И в каких единицах? 

S00 [01:18:07]  : в метрах эффективных толщин. Вот там где больше там как бы резервуар получше и там можно получить лучше как бы результаты. И вот видите здесь пробуренные скважины и вот они как хорошо совпадают. И потом есть допустим вариант вот это P10, P50, P90. P10 это оптимистический вариант. это медианный прогноз P50 и P90 это пессимистический прогноз, где вот это можно обуреть, но фактически показывает разброс прогнозов и одновременно еще у нас формируется некая вот такая таблица вот этих то есть таблица всех вот этих операций, которые мы произвели. То есть здесь в этой таблице скважина, каждая показана, и вот итерации, которые мы сделали. На каждой итерации вот показано вот это вот сампл на этой скважине участвовал, единица, 0 это не участвовал. И можно построить кроссплод. Вот кроссплод как раз является такой важный момент, чтобы оценить точность наших прогнозов. Здесь вот я по оси X это измерено аж эффективно, по оси Y это прогнозно. сам кроссплот будет выглядеть таким следующим образом вот этот разброс во время итерации коэффициент корреляции получился 0.9 еще можно допустим поставить этот фильтр только те скважины которые участвовали в обучении Вот, и видите, как циркуляция сразу вырос. И можно добавить теперь те скважины, которые не участвовали в обучении, вот самые важные. И вот они дают вот эти точки 0.84 достаточно приемлемый хороший этот результат. Здесь можно этот цвет немножко под более темный что ли сделать, чтобы было видно. Вот это является такой оценкой. Это на основании многократных прогнозов. То, что называется бустингом. В Стэнфордском университете по этим многократным прогнозам, когда они описывают дерево, решений то есть и вот это лес решений как раз вот в общем-то там фактически многократные такие прогнозы и каждый каждый вариант прогноза он является ну неким слабая оценка прогноза а в совокупности она усиливает что ли этот прогноз вот что-то типа такого получается у нас 

S01 [01:20:58]  : Ну, если я правильно понял, то количественную оценку точности вы считаете через коэффициент корреляции, и на тестовом наборе у вас получается выше 90, выше 0.9, а на тестовом выше 0.8. Как-то так, да, получается? 

S00 [01:21:19]  : Да-да-да. Понятно. Сочтительски достоверно это больше, чем 0.7. 

S01 [01:21:28]  : А есть какие-то оценки, допустим, по нейросетям? Если взять этот коэффициент корреляции на тестовых данных. Количество нейронной сети, иерархические и КНН. Есть понимание? Грубо говоря, нейронные сети 0,6, КНН 0,8, а иерархические 0,85. Какие-то такие оценки вы не делали? 

S00 [01:21:58]  : Я показывал, иерархические нейронные сети всегда дают коэффициент корреляции единицы на тех данных, которые участвовали в обучении. Которые не участвовали, там, конечно, это хуже будет. Мы каждый день делаем это все. 

S01 [01:22:18]  : Я правильно понимаю, именно с точки зрения, что иерархические нейронные сети дают большую корреляцию на тестовой, на тренировочной выборке, но хуже на тестовой, правильно? 

S00 [01:22:30]  : Иногда хуже, иногда лучше. То есть это зависит от качества данных. Когда лучше? Когда, если сейсмика сильно разнородная, вот сейчас это я говорю, что вот в Руснефте мы на этом египетском месторождении, там очень хорошие получаются результаты, потому что там очень плохая сейсмика. Она разнородная в пласте, то есть в одном месте она находится под солями, там фактически нету отражений. Плохие отражения. А где солей нету, там хорошее отражение. И вот там нейронная сеть вообще не работает, классическая. Потому что нейронная сеть пытается все аппроксимировать, под одну гребенку подогнать. А здесь вот этот иерархический всё-таки метод ближайшего соседа, он лучше работает, и там коррекционная корреляция, конечно, гораздо лучше, чем нейронные сети Калмогорова. Но в некоторых случаях, да, бывает, что и нейронные сети Калмогорова лучше дают. Все зависит от качества материалов. Понимаете, наши задачи, их нельзя там под гребенку подогнать, но вы сами это сейсмик знаете, что сейсмика очень разнородная, и мы не можем обучить, допустим, даже нейронную сеть на одном участке потом применить ее даже на соседний участок она будет уже другая там другую методику отстрела сделали другие другая аппаратура другая обработка то есть все другое и поэтому там это сложности вот такие вот имеются 

S01 [01:24:08]  : Спасибо. А скажите, пожалуйста, здесь вопрос задал один из наших участников семинара. Для каких еще задач вы видите применение сетей Кармагол, Калмагорова и иерархических сетей? 

S00 [01:24:26]  : Для всех задач, где применяется нейронная сеть, В общем-то, ответ простой. То есть нейронные сети Калмогорова ничем не отличаются от классических нейронных сетей, кроме того, что там немножко другая архитектура. Я говорил, какое отличие. А нейронные сети иерархические, это достаточно перспективная такая штука. И я могу даже предсказать, что может быть через несколько месяцев, может через год, Вдруг опять все масс-медиа, что американские или английские учёные открыли новую нейронную сеть, назовут её по-другому, нейронная сеть какая-нибудь другая. Будет что-то типа такого бума. 

S01 [01:25:15]  : А вот скажите, значит, такой вопрос. Вот вы говорили про аналогию того, как ведут себя в вашем понимании иерархические нейронные сети по сравнению с тем, как это гипотетически происходит у нас в голове. С одной стороны, да, ну и понятна эта аналогия. Но с другой стороны, вы же сказали, что проблемой иерархических нейронных сети является то, что нужно хранить всю обучающую выборку. Соответственно, вопрос – вот как тут быть с этой аналогией? Нужно ли нам в человеческом мозге для того, чтобы таким же образом мыслить, хранить где-то на самом деле всю обучающую выборку? И если приходится с собой еще обучающую выборку таскать, ее же тоже где-то надо хранить. 

S00 [01:26:07]  : Я говорил, что фактически в листах можно обобщать, каждый лист это по факту одинаковые объекты, похожие объекты с маленьким разбросом прогнозного параметра. их не надо хранить, их можно обобщить. Здесь в листьях можно и классическую нейронную сеть запустить для обобщения. То есть фактически вот эта иерархическая нейронная сеть это типичная глубокая нейронная сеть. Если помните, глубокая нейронная сеть это неиспользование На первых слоях не надо использовать как бы разметку, а просто как-то обобщить входные данные, что даёт преимущество в построении нейронной сети. А это типичная глубокая нейронная сеть на самом деле. 

S01 [01:27:01]  : Спасибо. Еще пару вопросов. Скажите, если это не секрет, то ваши работы, ваши заказчики, на сколько процентов это импортозамещение, на сколько процентов это работа на западные заказы? 

S00 [01:27:18]  : Нет, мы совсем на западные заказы не работаем. Я послал статью, кстати, с нейронными сетями Калмогорова в журнал Geophysics, и они написали мне такую рецензию, что здесь ничего нового нет. Потом, через два года, как раз бум пошёл КАН. Потом меня выкинули еще с проекта Кашаган, где напрямую сказали, что ребят, я не буду инвестировать в российских программистов. Пусть он идет своим ходом, я лучше программистов своих, американских, проинвестирую. Не будем мы покупать этот софт. Вот так прямо сказали и оттуда ушел. И сейчас я никаких с западными точно не буду никаких дел иметь. У меня эмбарго. 

S01 [01:28:11]  : Понятно. Но вы же назвали какие-то зарубежные объекты. Это получается работа наши выполняют, да? 

S00 [01:28:18]  : Да, это Руснефть. Она там работает в Египте. И в Казахстане у нас есть пользователи уже Сейчас мы работаем и пытаемся на Индию выйти, на Китай тоже. В принципе, дружескими странами мы работаем. 

S01 [01:28:36]  : Понятно. Здорово. Успехов вам. И последний вопрос, Иван. Скажите, пожалуйста, а слайды можно ваши получить? Или это тайна? 

S00 [01:28:46]  : Да нет, здесь никаких нет условий. 

S01 [01:28:49]  : То есть имеется ввиду, что мы просто можем к вашему докладу приложить слайды, если это возможно и не является секретом, то было бы здорово. 

S00 [01:29:00]  : Нет проблем, я пришлю вам ссылку на скачивание этих 

S01 [01:29:08]  : А если это ссылка публичная, тогда я могу просто и ссылку разместить. Коллеги, есть ли ещё какие-то вопросы к сегодняшнему докладчику? Нет. Тогда, Иван Иванович, спасибо Вам большое за доклад. Было очень интересно. И успехов Вашему проекту и Вашей работе как научной, так и бизнес-аспектам Вашей деятельности. 

S00 [01:29:43]  : Спасибо. Я в заключении тоже хочу сказать, что мне интересно иногда посматривать на этот чат от GE Russia. И там я черпаю какую-то информацию для себя полезную. Но вообще, чего не хватает, это вот, может быть, как раз вы могли бы на основании вот этого сообщества все-таки организовать какую-то площадку для публикаций, таких кратких электронных публикаций. Сейчас у нас очень не хватает вот куда даже опубликоваться с какими-то идеями. и который был бы достаточно может быть слабо без сложных как бы проходов то есть без рецензии слабое рецензирование какое-нибудь электронное вот и с возможностью может быть каких-то комментариев то есть электронная такая электронный журнал я думаю что Могли бы там и скинуться на вот такую работу, чтобы все, по-моему, заинтересованы получить вот такую площадку для публикаций. Я думаю, она будет очень востребована и может быть даже коммерчески выгодна. И вот я думаю, что как раз кто бы мог это сделать на основании как раз вот этого чата, вот этого сообщества. 

S01 [01:31:04]  : это было бы замечательно а вы а можно тогда вот эту тему чуть-чуть еще с вами обсудить а вы имеете ввиду что вы друзья я просто приведу пример вот допустим в В той сфере, в которой я сейчас работаю, машинный интеллект в общем, не конкретно применительно геологии, геофизики, в англоязычной сфере для таких слабо реферируемых публикаций используется архив. То есть, есть архифорг, куда можно без больших проблем залить статью. Единственное, что нужно, чтобы кто-то уже, кто в данном рубрикаторе как-то уже зарегистрирован как автор, чтобы он тебя рекомендовал. То есть, по сути, нужен только один рецензент, которого ты сам можешь найти. 

S00 [01:31:52]  : вот этот вариант он чем-то не устраивает или хочется иметь именно русскоязычное свое на свои хочется русскоязычное иметь все-таки там это понятно тот он его сложно читать нужно переводить а здесь вот все-таки такая это если есть искусственный интеллект вот именно по искусственному интеллекту такие не реферированные, что ли, эти журнальчики электронные. Ну, именно русскоязычность, конечно, главный момент. По-моему, вы даже когда-то обещали такое, я видел там в переписке, такую штуку сделать. 

S01 [01:32:25]  : Ну, значит, там много вопросов разных обсуждалось, в том числе и эта идея была, но, что называется, настоящих буйных мало. Поэтому как-то эта история заглохла. Ну, спасибо. Я вот тему себе помещу, подумаю, с кем можно её попытаться поразвивать. 

S00 [01:32:48]  : Я думаю, что это может быть даже коммерчески выгодно, что какую-то рекламку туда допустить на этот сайт. И оно было бы достаточно привлекательно. не только супернаучные, а какие-то более простые, чтобы публикации туда допускать для более широкой публики. 

S01 [01:33:13]  : Понятно. Хорошо. Спасибо. Будем думать. Будем на связи. Всего доброго, Иван Иванович. Спасибо вам за доклад и до свидания. Коллеги, всем спасибо, кто участвовал. До новых встреч. 

S00 [01:33:29]  : До свидания. 











https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
