## 28 марта 2024 - Обучение с подкреплением на импульсных нейронных сетях — Михаил Киселев — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/_Wu5cU-OFDk/hqdefault.jpg)](https://youtu.be/_Wu5cU-OFDk)



S00 [00:01:54]  : Всё, замечательно. Давайте начнём. Меня зовут Михаил Киселёв. Я в данном случае представляю АО «Лаборатория» Касперского, точнее, нашу группу нейроморфного машинного обучения. Это на самом деле презентация, которая должна была быть прочитана, озвучена на прекрасной конференции Open Talks AI, которая проходила в Тбилиси, которая там прошла в полулегальном режиме. Но вот это ее, как бы, в этой презентации более расширенная версия. И, ну, в принципе, она и предназначалась для этого, поэтому она еще, к тому же, на английском языке. Я буду говорить по-русски, разумеется, но сама презентация на английском, я думаю, не проблема, поскольку все как-то английский более-менее, наверное, знают. Итак, этот доклад будет посвящен, он называется «Нероморфный подход к обучению с подкреплением». Ну и, собственно, вот об этом я буду рассказывать. Но для начала я хотел бы уточнить, что, собственно, понимается под словом неромофный, поскольку под этим словом сейчас понимается все, что угодно. Вот в данном контексте, в данном докладе под неромофным будет пониматься… Неромофный состоит из двух принципов. Которое, в общем-то, ну, правда, слово нейроморф, поскольку это похоже на то, как это в голове, да, как это в мозгу происходит. Ну, во-первых, ну, это нейронная сеть, идти о нейронных сетях, и нейроны в ней обмениваются не числами, а некими атомарными событиями. Ну, у нас, допустим, в биологических нейронах им соответствует потенциал действия, спайки. Это раз. То есть, это не числа, а вот эти самые спайки поставляют информацию. Второй тезис – это то, что все нейроны никоим образом, явным образом не синхронизированы. То есть, они как-то принимают эти самые спайки, что-то с ними делают, отдают их кому-то дальше, и все. Нет никакого явного синхронизирующего сигнала, который приводил бы все это к общему знаменателю. В этом и то, и другое отличает от традиционных нейронных сетей, которые, во-первых, в основном обмениваются числами большинства подходов, и, во-вторых, то, что они синхронизированы, они слоистые, считают слой за слоем и на инференсе, и на обучении. Вот здесь такого нет. Это, опять же, похоже на мозг, где тоже нейроны, не видно, чтобы они хоть как-то были Синхронизированы. Ну, вернее, конечно, они отчасти синхронизируются, там есть всякие альфа, тета и так далее, ритмы, но это скорее следствие, а не причина. То есть, ну, разумеется, большие ансамбли независимых агентов, взаимодействующих сильно, они часто, так сказать, проявляют такую волновую активность, поэтому это не синхронизация, это скорее проявление этой самой коллективности. Ну и эта самая нейроморфность, будучи спроецирована на программно-аппаратные реалии, дает две вещи. В смысле модели-модели. Ей соответствуют импульсные нейронные сети. которые как раз вот этим требованием подчиняются. А в смысле железа это все соответствует и реализуется наиболее эффективно на не фон Неймановских вычитителях, состоящих из очень большого количества параллельно работающих простых процессоров. Каковы процессоры уже есть, но это как бы тема отдельного доклада. Теперь что такое reinforcement learning в обучении с подкреплением? Это многие знают, но тем не менее для фиксации терминов я это еще раз повторю. Классическая формулировка обучения с подкреплением состоит из следующих понятий. Во-первых, у нас имеется S, большое. У нас есть агент, обучающийся, и есть внешний мир. с которым она взаимодействует. Так вот, предполагает, что S – это в некотором обобщенном смысле набор возможных состояний внешнего мира. И агент получает информацию об этом самом внешнем мире, но в том числе и о себе, поскольку его телесная составляющая тоже есть часть этого мира, в виде некоторых числовых переменных, которые описывают это состояние мира. Агент может делать некие действия. Это множество возможных действий агента в этом мире, с которыми он взаимодействует. Эти действия как-то меняют состояние мира. И это изменение выражается вот этой функцией P, которая есть вероятность того, что если мир находит состояние S и агент делает действие A, то мир переходит в некое другое состояние S в трих. Эта функция неизвестна. Ну и самое главное, что задаёт собственно цель всего этого мероприятия, это функция вознаграждения R. Обычно ее рассматривают как функцию состояния мира. Есть некие состояния мира, которые являются хорошими, они оцениваются как эти положительные, эти большие R. Но иногда рассматривается зависимость R от тройки, от перехода. Откуда, с помощью какого действия, куда мы перешли. Эта функция тоже неизвестна, но предполагается, что в каждый момент времени этот эх информации о нем имеется, то есть агент знает, что такое хорошо, что такое плохо. Ну и поскольку мы хотим в этой задаче не только максимизировать текущее вознаграждение, но и будущее, то вот имеется некое число гамма меньше единицы, которое говорит нам, что насколько вознаграждение в будущем хуже, чем вознаграждение сейчас. И наша задача – это максимизировать кумулятивное вознаграждение, как вот в этой формулке написано. не только текущее состояние, но и следующее состояние. Для того, чтобы достигать этой самой максимизации, этого критерия, агент применяет политику. Политика – это некое правило, как по данному текущему состоянию выбрать какое-то действие. Политика в общенном смысле параметризуется неким набором параметров θ, и, собственно говоря, задача обучения с подкреплением, она в сущности в том, чтобы оптимизировать выбор этих самых параметров θ. Почему вообще встал вопрос о нейроморфном подходе к обучению с подкреплением, и что это может, по идее, дать, и в чём актуальность данной задачи? большим плюсом, одним из больших плюсов нейроморфного подхода, она вытекает из того, что поскольку у нас нету явной никакой синхронизации, то в общем гораздо меньше проблем с наращиванием, с масштабированием таких систем. Поскольку у нас нету никакой жесткой связи, то мы можем просто лепить, для ансамбля делать очень большую, что позволяет решить потенциально очень сложные задачи в реальном мире. Затем второе, вот сам по себе подход обучения с подкреплением, он такой наиболее реалистичный. Он похож на то, как люди учатся, как хотели бы мы, чтобы роботы доучивались, выполнять какие-то задачи. То есть это связано с тем, что обучение должно быть инкрементальным и постоянным. То есть не должно быть четкого разделения на какие-то обучающие тестовые примеры, ВЧ и так далее. И вот как раз парадигма асинхронности очень подходит под эту задачу. Также важно то, что если мы хотим взаимодействовать с внешним миром реальным, то внешний мир – это некая динамическая штука, в которой много разных процессов, идущих на разных временных интервалах с разными временными шкалами. И поэтому хотелось бы, чтобы система, которая обучается в этом мире и взаимодействуя с ним, тоже была бы… чтобы время было существенной компонентой всех этих процессов. Это не совсем так для обычных сетей, потому что там нет реального времени, в нашем понимании, там есть как бы такты выполнения этих самых слоев один за одним. Хотя, конечно, она решает издачи и временного прогноза, и временных рядов, но все-таки это не совсем... А импульсные нейроны живут в непрерывном времени, они как-то обмениваются спайками, причем процесс этого обмена тоже имеет временной аспект, потому что спайки распространяются от нейрона к нейрону не мгновенно, а за какое-то время, и это время является существенным элементом всех вычислительных процессов. процедур, и сам отдельный нейрон тоже некая динамическая система в сущности, простая. Поэтому привлекательно, мне кажется, использовать эту присущую импульсным сетям временную составляющую в их процессинге информационном для решения взаимодействия с сложными динамическими системами. Ну и, наконец, уже такая прикладная вещь, которая уже много раз была продемонстрирована. Мы хотим, чтобы агент был, грубо говоря, не был привязан к розетке, чтобы он был сам по себе, чтобы он автономный был, малоэнергопотребляющий. По многим причинам так получается, что как раз вот такие устройства, которые реализуют этот подход нейроморфный, они там демонстрируют Это уже известно. Там на 2-3 порядка меньше часто энергопотребления. Как же выглядит вообще обучение с подкреплением импульсном домене? Импульсный домен потому импульсный, что в нем нет чисел. В нем, в сущности говоря, вся информация представляется в виде потоков спайков. А каналов этого информации в нашем случае три. Во-первых, мы получаем информацию о внешнем мире в виде потоков спайков. Тут нарисована, скажем, девиз камеры, которая именно эти спайки посылает. Нейронная сеть что-то такое там делает внутри себя, вырабатывает какие-то управляющие команды, которые точно так же имеют вид спайковых потоков, подающих на какие-то актуаторы, эффекторы. Ну и более того, то, что она делает, оценивается специальными сигналами. Вот они тут в черном и красном изображены. Сигналное подкрепление и наказание, которые также имеют спайковый вид. То есть у нас все спайковое. Это создает проблему, потому что классическая задача rl формулируется в виде чисел, а тут у нас никаких чисел в сущности нет. Можно, допустим, сказать, что наша задача reward, которую мы хотим максимизировать, это разница между количеством спайков reward и punishment, можно так сказать. Но вот даже, например, описание мира – это тоже уже целая проблема, как-то его вычленить, потому что у нас нет текущего какого-то вектора переменных, которые нам говорят, что скорость такая-то, а давление такое-то. У нас есть потоки спайков, которые, в общем-то, ну, как из них, так сказать, понять, какое сейчас состояние мира – тоже отдельная проблема. Ну вот, собственно, эти проблемы, они выписаны вот здесь. С чем они связаны? Про то, что числа, это я уже сказал. Но имеются еще две такие специфические вещи, которые, кстати, очень реалистичны. Довольно часто такая ситуация, что оценка в какой бы то форме действий агента, она далеко не после каждого действия, допустим, ему предъявляется. Как правило, есть некая задача, которую надо решить, и пока он ее решил или не упустил, не угробил задачу, ничего не понятно. То есть только в конце, если он решил, он получает вознаграждение агента, если он не решил, он получает наказание. А пока он там еще не достиг этого состояния, ничего не понятно. Поэтому реворд для большинства состояний равен нулю, и очень сложно выбрать какую-то политику, потому что непонятно, у нас нет никаких градиентов, грубо говоря, мы не понимаем, что такое хорошо, что такое плохо, если мы рассмотрим мгновенную ситуацию. Ну и более того, в реальном мире эти самые сигналы и оценки могут запаздывать по отношению к этому целевому состоянию. Допустим, мы там, не знаю, что-нибудь съели ядовитое, а животное заболело только через два часа. И пойди догадайся, с чем это связано. Это самая оценка. Вот это все создает очень большие проблемы. Но тем не менее именно эти аспекты являются реалистичными, то есть надо как-то на них ответить, чтобы применять все это в реальном мире. Теперь в целом об учении. Вот мы говорим об учении с подкреплением. А как вообще осуществляется обучение импульсных нейронных сетей? Тут я перечислил основные направления, основные подходы к обучению импульсно-нейронных сетей. Во-первых, довольно часто применяемый подход основан на том, что если мы рассматриваем частотное кодирование сигналов, то есть мы говорим, что у нас какой-то сигнал кодируется спайком в виде их частоты, и мы берем большие промежутки времени, то в силу нелинейного порогового поведения нейрона импульсного мы получаем такую вещь, что он эмулирует обычный нейрон. Веса входов умножаются на частоты спайков по этому входу. Это дает общую стимуляцию по одному входу. Сумма этих стимуляций по всем входам дает в среднем частоту его генерации спайков. с помощью сегмоидной функции активации. Это показано, что с помощью достаточно больших сетей импульсных можно сколько угодно хорошо приблизить любую обычную традиционную нейронную сеть. Этот подход применяется, обучают обычные традиционные нейронные сети, а потом с помощью некоторых формальных приемов из них делают импульсные сети. Но это как бы не решение, потому что здесь мы не наблюдаем никаких особо преимуществ нейроморфности. То есть само СНМ не учится. Очень много было посвящено вниманию подходу, который называется машинно-жидким состоянием. Буквально очень кратко о ней скажу, потому что она не является целью нашего изложения. Машинно-жидким состоянием – это огромная хаотическая импульсная нейронная сеть, статическая, то есть сама она не обучается, в ней все веса статические, на нее подается какое-то воздействие, какие-то пространственновременные паттерны, она на них как-то специфически отвечает. И за счет того, что она очень большая, и мы меряем активность всех ее нейронов, у нас получается очень многомерное представление. Мы разворачиваем некую динамическую картину в некое очень многомерное мгновенное представление. А по глубоким причинам часто, если мы решаем задачу классификации, то такие очень многомерные представления являются линейной сепарабелью. Мы каким-то линейным классификатором, измеряя активность нейронов, как-то решаем задачу классификации. Но опять же тут сеть сама не учится, а учится простой линейный классификатор на дне стоящей. Если говорить об обучении собственной сети, то довольно часто применяется метод суррогатных градиентов. Обучение импульсно-нейронной сети теми же алгоритмами, что и обычных сетей, невозможно напрямую, потому что обычная сеть учится с помощью алгоритма обратного распространения ошибок, который является частным случаем алгоритма градиентного спуска. Градиентные спуски основаны на вычислении градиентов, на вычислении частных производных, а импульсная нейронная сеть – это по своей сути дискретная система, и там вообще говоря никаких градиентов в принципе нет, никаких частных производных там построить нельзя. Чтобы их строить, поменяется некая алхимия, о которой я сейчас не буду останавливаться, Опять же, это перенос логики обычных сетей на импульсные сети, сопровождающиеся потерей преимуществ, которые они с собой, по идее, должны нести. Поэтому мы это не будем рассматривать. Наконец, нативные способы обучения импульсных сетей. Есть два основных. Наиболее часто применяются локальные правила синаптической пластичности, потому что у импульсных нейронных сетей обучение тоже, как правило, имеет вид модификации весов связей в их нейронах. Но в отличие от подхода бэкпропа, модификация связей в этой парадигме делается локальными правилами. Это означает, что модификация веса связи между двумя нейронами зависит только от активности и свойств этих двух нейронов, больше ни от чего. Такое локальное обучение. Это очень хорошо вписывается в парадигму асинхронности, поскольку она не требует никакой слоистой синхронизации, ничего этого не требует. Ну и еще один метод применяется, это связанный с перестройкой. То есть мы в процессе обучения сети динамически меняем ее топологию, то есть буквально перекоммунитируем ее связи. Это менее распространенный метод, но он тоже применяется и приводит к некоторым положительным результатам. Теперь речь идет о том, как Reinforcement Learning, как обучение с подкреплением, какие подходы к его решению на данный момент существуют. Существует до больших классов подхода к этому КРЛ. Один класс методов называется Model-Free, независимо от моделей. Это более простые методы, которые часто используются. Они основаны на том, что мы напрямую методами фактически машинного обучения делаем какие-то модели. Но только эта модель напрямую применима в нашей политике. То есть мы, допустим, можем поставить опрос. Вот у нас есть какая-то историческая практика нашего агента. можно поставить вопрос, как выбрать наилучший параметр политики для данного состояния. То есть мы знаем, вот для данного состояния было лучше такой-то параметр, для таких-то параметров. Мы решаем машину обучения прямого мейвинга состояния на параметры политики. Другой подход – мы экспериментируем, у нас есть какая-то история поведения нашего агента, и мы смотрим, какие состояния ведут к какому кумулятивному реварду. Эта вещь используется для часто применяемой схемы Actor-critic, которая нам говорит, что, допустим, у нас есть состояние, мы выучим модель, как из состояния и действия получить ожидаемый кумулятивный реварт. И, соответственно, актор предлагает возможные действия для данного состояния, а критик выбирает из них наилучше. То есть это такие простые достаточно механизмы, которые... это работает, они эффективны, несложны, но у них имеется, конечно, несколько минусов. Во-первых, они требуют, чтобы у нас был достаточно часто оценка, чтобы punishment reward для большинства состояний был бы не равен нулю. Иначе эти модели, как мы понимаем, не могут быть построены. Чтобы у нас не было задержки этой оценки. Ну и чтобы у нас мы всегда обладали достаточной информацией о текущем состоянии мира. Но в большинстве реальных задач ни одно из этих трех условий не выполняется. И поэтому наиболее эффективными являются более сложные подходы, основанные на моделях, model-based RL, которые предполагают несколько разных уровней этого моделирования. Ну, во-первых, нам хотелось бы, поскольку у нас сигналы оценки бывают нечистые, редкие, а нам надо все-таки их получить часто, то нам хотелось бы построить систему производительных целей. То есть мы должны понимать, что основной ревард, который приходит из внешнего мира, он у нас связан с таким состоянием, но в зависимости от близости текущего состояния к этому целевому состоянию мы можем генерировать какие-то внутренние реварды внутри сети. То есть если мы знаем эти промежуточные цели, мы таким образом восполняем нехватку этих внешних оценочных сигналов. Затем очень важным, разумеется, для решения этой задачи является получение модели динамики внешнего мира. Потому что если мы знаем эту динамику, если мы ее как-то выучили, то мы можем оценивать те состояния, которые мы доселе не наблюдали, потому что мы знаем, к чему они приведут. Ну и наконец, по самый полный уровень этого подхода, это если мы строим полную модель не только динамики внешнего мира, но и воздействия на внешний мир действия агента. Тогда мы получаем все необходимое для планирования, И, соответственно, мы уже вполне обоснованно строим политику не только на основе текущего состояния, которому мы можем дать неполную информацию, но на основе динамики, на основе истории. То есть мы делаем уже полномасштабное планирование, которое нам позволяет достаточно уверенно достигать цели, получать максимальный реверт. Это все здорово, но это сложно. Такие вещи уже сделаны для обычных традиционных сетей, но в области импульсных сетей это пока еще является нерефонной проблемой. Хотя Model Free RL в импульсных сетях уже был продемонстрирован, в том числе и нашей группой, Model Based RL пока еще никем в полном объеме не продемонстрирован. Это открытая научная проблема. Ну и теперь, собственно, об элементной базе немного. Вот мы говорим импульсные нейронные сети. С чего они у нас состоят? Что, собственно, представляют себя нейроны, которые в них есть? Мы используем простую, но функционально достаточно богатую модель. который называется пороговый интегратор с утечкой и адаптивным порогом. Это чуть-чуть более сложная версия самого простого порогового интегратора с утечкой. Она отличается от него тем, что порог срабатывания у него тоже является переменным. Динамика такого нейрона описывается двумя уравнениями, которые здесь показаны в верхнем левом углу. Нейрон характеризуется двумя параметрами. Это текущим мембранным потенциалом, как и в случае обычного лифт-нейрона, и текущим порогом срабатывания, UCR. который тоже является переменной. Очень простая идея. Обычный лифт-нейрон работает очень просто, как конденсатор с параллельно к нему присоединенным сопротивлением. То есть на него поступают спайки, спайки повышают потенциал. потенциал стекает через сопротивление, если спайков много, потенциал дорастает до какого-то порога в уровне, тогда нейрон срабатывает, сам посылает спайк, а конденсатор разряжается, как бы самая базовая модель импульсного нейрона. У нас чуть посложнее с тем, что у нас порог этого срабатывания тоже вариабелен, то есть после каждого срабатывания он немножко повышается, а потом обратно релаксирует к базовому значению. Вот как здесь показано на рисунке. Эта простая дополнительная фича дает много плюсов. Например, обладает такой нейрон более гемостатическими свойствами. То есть, чем он более активен, тем сложнее дальше его сбить, потому что у него порог высокий. Такой нейрон более активно реагирует на начало стимуляции, то есть он акцентирует на динамике, на динамических стимулах. удобнее, чем обычный лифт-нейрон. Плюс наша практика показала, что надо ввести еще две фичи в этот нейрон, которые оказываются очень полезными, в то время очень просто реализуем. К тому же, этот нейрон нами выбран, потому что он, в принципе, допускает очень эффективную аппаратную реализацию. Все эти его компоненты, они очень-очень эффективно реализуются на очень простых вычислителях. Да, и вот, собственно, эти дополнительные фичи – это, во-первых, мы хотим, чтобы какие-то нейроны играли роль элементов памяти. Это можно достичь, если мы зациклим выход этого нейрона на себя через некую сильную возбуждающую связь. Тогда получится, что он по этой связи будет гулять спайки, которые будут сами себя возбуждать, и это может длиться какое-то время. Это может длиться либо долго, либо всегда. Либо это может длиться какое-то ограниченное время, потому что у нас постепенно растет порог у такого нейрона, и в один прекрасный момент очередного такого циклического спайка не хватает для возбуждения. То есть мы тем самым имеем контролируемую по времени ячейку памяти, что бывает часто полезно. И второе нововведение, тоже очень простое и очень хорошо реализуемое аппаратно – это так называемые вентильные синапсы. Ну, можно сказать, что это крайнее проявление сильных стимулирующих или сильных тормозных синапсов. То есть у нейрона имеется некий счетчик, счетчик активности, который бывает отрицательным и положительным. Когда он положительный, то нейрон – это обычный активный нейрон, который описывается этими формулами в левом верхнем углу. Если у него неположительное значение этого счётчика, то он просто неактивный. Он не реагирует ни на что. И эти открывающие и закрывающие синапсы действуют так, что они меняют этот счетчик в положительную и отрицательную сторону. Тем самым они могут абсолютно заблокировать нейрон или наоборот разблокировать. Это нужно для реализации всякого рода логики. И в то же время понятно, что аппаратно это все реализуется очень просто. Вот наша модель нейрона, она достаточно проста. Теперь о том, как выглядит наше обучение. Вот я говорил, что мы используем локальные локальный принцип обучения. Классический вариант такого обучения – это довольно известная модель STDP – Spatial Time Independent Plasticity. Но модель STDP, которая действительно наблюдается в природе, ее несколько раз измерили, она правда есть, но у нее есть много минусов. Ну, один минус, что там экспоненты фигурируют, а экспоненты – это вещь не очень хорошая. Ну, по крайней мере, там какие-то мультипликативные элементы. В общем, она не очень дружественна к вылезу, надо что-то более простое. Это раз. Ну, а два, там имеются еще дополнительные не очень хорошие вещи, связанные с тем, что она в себе содержит некую положительную обратную связь. Лучше я не буду сейчас на этом останавливаться, а скажу просто про наш… по наш вариант локального обучения. Поскольку мы... Да, во-первых, у нас их несколько. Но и несколько им несколько, потому что для разных типов сдачи обучения на самом деле нужны немножко разные модели пластичности. Одно дело – это unsupervised learning. Там нужна одна модель пластичности, другое дело в нашем случае, когда это у нас RL. То есть эта модель пластичности, которая показана на этом слайде, она заточена именно под RL. Это так называемая трехфакторная модель RSTDP, в смысле reward. Трехфакторная она потому, что изменение веса синапса происходит только в случае совмещения трех условий. Во-первых, у нейрона имеются синапсы обычные, которые, собственно говоря, приводят к изменению, приход спайка по которому приводит к изменению его потенциала. И имеются специальные синапсы. Они обозначены красным и черным. Синапсы приклепления и наказания. Вернее, в сущности, это один тип синапсов, просто у них вес положительный или отрицательный. которые играют некую специальную роль в этом обучении. А именно, вот как раз эти три фактора, которые дали название этому методу. Первое, чтобы изменился вес синапса, на нейрон должен прийти спайк на один из этих оценочных либо reward, либо punishment синапсов. При этом до этого, какое-то время до этого, нейрон должен сработать. И при этом меняются веса у тех синапсов, обычных пластичных синапсов, которые получили спайки незадолго до или после вот этого срабатывания. То есть, во-первых, должен на обычный синапс прийти спайк. Где-то около этого времени нейрон должен сработать. И потом он должен быть оценен приходом этого реворда или панишмент спайка. То есть, три фактора. Вот это как раз показано на этих графиках. Вот у нас синие линии – это обычные синусокластичные, красные – это у нас вознаграждения, синие – черные – наказания. Вот мы видим, что если у вас сам нейрон не срабатывает, то спайки эти приходят оценочные, но ничего не происходит. А вот в средней части мы видим, что пришел черный спайк наказания. До этого, за какое-то время, до некий периода пластичности синоптической, у нас сам нейрон сработал, и вот три синопса, которые получили спайки недалеко от этого срабатывания, они, соответственно, подавлены, потому что у нас наказание. Ну и точно так же, наоборот, то же самое происходит Тут мы видим много плюсов. Во-первых, у нас тут нет никаких экспонентов. Фактически вообще ничего не происходит с сложными вычислениями. Да, и величина этого подавления и потенцирования, она одинаковая. То есть, она либо есть и какая-то константная, либо ее нет вообще. То есть, мы видим, что это крайне простые операции. А мы помним, что наша задача – это как раз сделать все как можно более hardware-friendly. Таким образом, это наша базовая модель обучения. Теперь, как мы все это применяем, какие мы строим структуры для того, чтобы реализовывать это в сетях. И здесь тоже имеется существенное отличие от традиционных нейронных сетей. потому что архитектура импульса нейронных сетей строится по совсем другим принципам. Тут нет привычных слоев, свёрток, пулинга и всё это. Вернее, есть, но по-другому выглядит. Это скорее некие ансамбли нейронов, которыми у собой связаны в виде графа, который бывает сложным, циклическим. Архитектоника совсем другая, не как у обычных сетей. Допустим, простая сеть, которая решает задачу обучения с подкреплением в виде model-free, то есть без строения модели, то, о чем я говорил в левой части слайда про подходы к RL. Вот эта сеть сделана. Она выглядит вот так. У нас имеются обучающие нейроны. Они вот такими коричневыми квадратиками показаны. Это целая ансамбля нейронов. Они получают вход со стороны входных узлов, которые несут информацию о текущем состоянии внешнего мира в виде потоков спайков. Более того, они между собой соединены латеральными блокирующими связями, т.е. когда у нас какой-то нейрон или группа нейрон в разный разум активны, то они подавляют активность других нейронов, так называемый принцип winner-takes-all, т.е. у нас только одна группа может быть активна. Вначале у нас нейроны необычные, сначала они могут быть неактивны. Эти нейроны дают приказ на эффекторы. Они вырабатывают управляющие команды. Пока у нас ничего не обучено, у нас эти управляющие команды, они от фонаря, значит, они просто на бум, они просто некие случайные. Вот, собственно, у нас имеется источник посоновского шума, который, значит, когда у нас внешний сигнал не активен, и когда у нас нейроны не обучены, то, значит, у нас гейд работает, нам пропускается этот шум, этот шум стимулирует как-то случайным образом наши нейроны, и они делают какие-то действия. Эти действия, они могут быть правильные и неправильные, они оцениваются извне приходящими сигналами подкрепления и наказания. Как я сказал, это всё работает тогда, когда они чисты. Как и любые Model-free методы, они работают, когда мы постоянно знаем хорошо и плохо. Они, соответственно, приходят на нейроны гейт-рев и гейт-пун. которые посылают сигналы оценки на нейроны, которые вырабатывают эти команды. При этом как раз понятно, что наша трехфакторная модель обучения как раз достигает цели, потому что вот мы получили какое-то описание текущей ситуации, вот в ответ на него нейрон сработал, что-то он сказал, что-то сделать, допустим, это сделано хорошо. Замечательно. Если это сделано характерно, через какое-то время на него приходит сигнал вознаграждения, который усиливает именно эти синапсы, и дальше именно это описание внешнего мира приведет к волнению именно этого действия. что и требуется доказать. И наоборот, если мы сделали что-то плохое, то придет сигнал наказания, у нас эти синусы будут подавлены, и они уже не приведут к этому действию, потому что этого делать не надо. Понятно, что такая простая схема, нет причин, почему бы она не работала. Ну и действительно, вот, допустим, маленький пример, ну это пока что все достаточно игрушечные примеры, у нас пока все-таки такой proof-концепт, но вот, допустим, у нас требуется научить сеть держать шарик в соединении поля зрения. Сначала она не знает, чего они вообще хотят, сначала она как-то хаотически двигается, она не понимает, что надо ей держать шарик в центре этой рамки, но потом она начинает это понимать, как раз с помощью этих сигналов наказания и поощрения. Мы видим, что, во-первых, она уже справляется с этой целью. Во-вторых, у неё уменьшается количество хаотических действий, поскольку блокируется источник этого начального исследовательского поведения, этого посоновского шума. Ну и вот через очень небольшой период времени она понимает, что от неё хотят, и вполне нормально с этим справляется. Опять же, я говорю, что это все так хорошо, потому что мы постоянно знаем, у нас есть постоянный корректирующий сигнал. Если бы у нас их не было постоянно, то все было бы не так радужно. И вот дальше, собственно говоря, мы посмотрим, что можно сделать, если задача у нас более сложная. Так, ладно, остановлю бегание. 

S01 [00:39:32]  : Как следующий кадр сделать? Минуту. Все. 

S00 [00:39:38]  : Итак, мы теперь хотим заняться чем-то более серьезным и применимым в реальных задачах сложных, где, как я сказал, эти проблемы имеются. Требуются две вещи вначале, которые нам надо имплементировать, чтобы реализовать Model Based RL на импульсных сетях. Во-первых, нам надо сделать модуль, который бы нам создавал внутренние наказания на основе системы промежуточных целей. То есть мы должны научить сеть распознавать состояние, насколько оно близко или далеко от цели, чтобы понимать, движемся мы к цели или не движемся. для выработки внутренних аналогов подкрепления и наказания. И второе – нам надо создать модель динамики внешнего мира, чтобы иметь возможность как-то предсказывать, что будет дальше. Вот, собственно, на этих двух целях мы сейчас и остановились, мы их сейчас реализуем, и отчасти уже реализовали. Я примерно расскажу в остатке моего доклада, как это, в принципе, решается, на основе каких структур. К сожалению, структуры достаточно сложны, поэтому, в общем, я только, наверное, расскажу общую идею, потому что я даже... Вот это, допустим, структура, которая применяется для создания целей, для создания системы промежуточных целей. Я долго думал, как же её нарисовать, чтобы это было понятно, но, видимо, мои способности иллюстратора не очень хороши. Вот это лучшее, до чего я мог додуматься. Но, тем не менее, я примерно расскажу, как это работает. во-первых, эта сеть является модульной. каждый модуль – это некий уровень цели, уровень близости цели. допустим, самый левый модуль – это самые близкие к цели состояния. этот модуль распознает наиболее близкие к цели состояния. Второй модуль, который здесь правый, он уже создает состояние менее близкое, то есть менее хорошее. И так далее. Их могут быть несколько уровней. То есть наша цель сделать так, чтобы мы в каждый момент времени индицировали, на каком уровне мы сейчас находимся. И по изменению этого уровня мы могли бы генерировать внутренние сигналы оценки. Если мы перешли от уровня 2 к уровню 3, значит плохо, мы отдалялись от цели. Если наоборот, то все хорошо, мы к цели приблизились. Это нам дает возможность строить политики не на основе внешнего сигнала, который редок, а на основе чего-то внутреннего. Как же это работает? Кроме всего прочего, назовём каждый такой уровень колонкой. Я не имею в виду какие-то аналогии с критикальными колонками. Просто такой термин. Я не хочу, чтобы это рассматривалось как претензия на что-то нейрофизиологическое. Совсем нет. Просто удобно. У каждой колонки имеется ещё микроколонки. Они показаны как вертикальные столбцы этого прямоугольника цветных. Зачем они нужны? Затем, что если мы хотим… На самом деле, хорошие состояния бывают довольно разными. То есть мы хотим классифицировать состояния, находящиеся близко от реворда. Их может быть довольно много, они могут быть довольно непохожи за собой. Поэтому нам необходимы существенно разные классификаторы, классифицирующие существенно разные инстансы от этих хороших состояний. Это выполняют отдельные микроколонки в рамках одной колонки. Самый нижний уровень этих самых микроколонок, то что с сиреневым, это обучающиеся нейроны. Только они имеют пластичные синапсы. Сейчас, дайте я уточню, у меня же еще 10 минут, верно, Антон? 

S02 [00:44:01]  : Михаил, ну, в принципе, плюс-минус у нас не жестко, поэтому распоряжайтесь. 

S00 [00:44:08]  : Я думаю, я постараюсь в 10 минут уложиться, поскольку это последний слайд. Итак, они обучаются, они принимают на вход описание внешнего мира, которое снизу идёт. Сначала они все совершенно не обучены, и более того, возможно, их веса могут быть либо случайными, либо просто нулевыми. Допустим, мы только начали обучение, ничего не понятно, никто не обучен. Но, тем не менее, мы с помощью какого-то модуля отдельного, который нам начальное поведение генерирует, исследуется, когда случайно, как в предыдущем кадре, мы что-то такое делаем. И случайным образом получаем наказание и поощрение. Допустим, мы сейчас рассматриваем только поощрение. Нам удалось случайным образом что-то такое сделать хорошее. Нам это удалось, у нас пришел сигнал REWARD, вот он пришел у нас с левого верхнего угла. И этот самый REWARD, он заставил генерировать, он взбудил нейрон, который называется REF BIAS. Это нейрон памяти, то есть он как раз, это та самая ячейка памяти, которая начинает генерировать сигналы непрерывно. Она вот непрерывно начала генерировать сигналы. Эти сигналы, которые он генерирует, идут на нейроны обучающиеся, вот такие тонкие синие стрелочки – это возбуждающие связи, но не сильные, слабые возбуждающие связи, идущие на нейроны сиреневого квадратика. Нейроны средней квадратики на самом деле на себя получают какую-то стимуляцию. Причем эта стимуляция, допустим, у них веса рандомные, какая-то из них сильнее, у кого-то из этих нейронов какая-то слабее. Мы начинаем подавать стимуляцию от нейронов рефбирос. Мы это делаем до тех пор, пока какой-то из них не срабатывает. Срабатывает из них тот первый, у кого потенциал был самым большим. То есть, кто был наиболее близок к распознаванию этой самой ситуации, которая принесла нам успех. Замечательно. Понятно, что мы тем самым делаем зародыш распознавания данного класса околоцелевых состояний. Он генерирует спайк, такой наведенный, вынужденный спайк, который заставляет сработать нейрон, находящийся в серой зоне этого прямоугольника. А это VTA нейроны, которые связаны с собой сильными подавляющими связями. После того, как он сработал, он запрещает другим нейронам срабатывать. То есть, он сработал, другие нейроны уже не работают. И одновременно этот нейрон, который сработал, WTA нейрон, Wiener Teissol, он открывает гейт желтый нейрон. Желтый нейрон – это вентили, пропускающие оценку, пропускающие стимуляцию. Он открыл этот вентиль. И этот вентиль, и заодно он послал сигнал вознаграждения на нейрон обучающийся. При этом у нас сработал наш вышеизложенный трехфакторный метод обучения, метод пластичности. То есть у нас пришли какие-то спайки, описывающие мир. Сам нейрон сработал. Он сработал вынужденно, поскольку пока сам стоит, он этого делать не может. И он был оценен. Соответственно, те синапсы, которые получили стимуляцию от входа, они будут усилены. Но только у него, поскольку у нас VTA-сеть, то остальные нейроны не получат эту стимуляцию, у них будут закрытые гейты. Поэтому мы только тот нейрон, который был наиболее близок к распознаванию целевой ситуации, только его мы простимулируем, чтобы он дальше делал распознавание. Так происходит какое-то время. пока наконец нейроны сами не начинают распознавать эти целевые состояния. Сначала у них слабые случайные веса, но потом из-за этого процесса, когда мы их заставляем распознавать то, к чему они наиболее близки, они сами начинают их распознавать без внешней стимуляции. Тогда получается, это уже рабочий режим, когда он сам распознает какое-то состояние, он сам стимулирует нейрон WTA, а нейроны WTA посылают блокирующий импульс на REF BIAS. Поскольку мы сами перераспределяем, нам уже не нужны внешние помощники, и нам не нужен уже этот нейрон памяти RefBias, который нам эту симуляцию посылает, когда мы еще не могли ничего сделать. Он блокируется, он больше не будет нам мешать. Ну и еще, кроме всего прочего, мы посылаем спайк на нейрон or, который просто нейрон или. Он срабатывает, когда что-либо срабатывает из этой колонки. Который говорит, что мы распознали наше состояние. И он возбуждает нейрон secondary reward, который, с одной стороны, это выходной нейрон, стрелочка не погасла, но он куда-то отдает сигнал, что мы распознали целевое состояние, а с другой стороны, он является источником ревуарда, то есть он играет для следующей колонки такую же роль, как входной изначальный сигнал ревуарда внешний играл для первой колонки. Он фактически начинает обучать точно тому же самому следующую колонку. Следующая колонка начинает учиться распознавать, что было до того, как мы распознали это самое хорошее состояние. Вторая колонка будет распознавать следующие по хорошести. И этот процесс интерактивно повторяется. Таким образом, мы получаем научение распознавать все более и более отдаленные от целевого состояния состояния. Таким образом, мы получаем систему промежуточных целей. Ну, это все еще заполняется некой механикой, что мы, допустим, если мы распознали хорошее состояние, мы должны заблокировать распознавание плохих состояний, то есть мы уверены в этом. У нас такая ситуация, что у нас самые научные нейроны, они левые, а менее научные правые. Поэтому, если левый что-то говорит, значит, мы должны ему верить, мы должны других блокировать. Поэтому тянутся стрелочки боковые от левых колонок к более правым колонкам. Ну, в общем, основная мысль примерно такая. Я думаю, что там много нюансов, но где-то я её примерно, значит, наверное, сказал. Теперь касательно второго. Второй задачей – это задача нахождения динамики внешнего мира. Пока что мы решаем такое в несколько упрощенной обстановке. Мы считаем, что внешний мир – это конечный автомат. У него есть какие-то выраженные состояния, между которыми по какой-то логике имеются какие-то переходы. Эти переходы делаются какой-то вероятностью. Внешний мир – это недоторминированный конечный автомат, и мы должны научиться выучить вероятность его перехода между состояниями. Ну, в общем-то, подход примерно такой же. Я сейчас, наверное, не буду совсем подробно говорить. Точно так же у нас имеются колонки. Колонки отвечают за одно состояние, индикация одного состояния. Они получают на вход стимуляцию, которая говорит, что данный момент мир находится в таком-то состоянии. И нам надо отслеживать логику переходов. Каждая колонка включает в себя обучающие нейроны, которые связаны с пластичными связями с другими. состояниями, другими колонками. В данном случае мы выучиваем не распознавание состояния внешнего мира, а выучиваем переходы. То есть у нас фактически веса от этих нейронов обучающихся, они на рисунке А нарисованы на самых нижних слоях этих колоночек. соответственно, большие веса соответствуют вероятным переходам, маленькие веса отсутствующим или маловероятным переходам. Мы должны помнить только текущее состояние, но и предыдущее состояние. То есть, мы должны иметь две виды активности. Активный нейрон, который индицирует, что у нас сейчас, и мы должны помнить, а что было до этого. Поэтому тут находятся нейроны памяти, которые как раз опять же играют роль… Почему это полезно, это нововведение с этими индивидуальными связями, потому что они помнят какое-то время, что было до этого. Но они должны помнить именно предыдущее состояние, а не предпредыдущее, поэтому тут имеется некая логика блокирования. То есть, у нас была все время пара колонок, текущая колонка, предыдущая колонка. А предпредыдущая должна блокироваться. Вот тут есть некая логика, связанная с этими гейтами, с этими вентильными нейронами, которые нам сейчас блокируют. Совсем старые состояния, чтобы они нам не мешали, чтобы они лишних переходов нам не создавали. С помощью такой же логики и такого же подхода к обучению такая матрица этих переходов выучивается. Это следующий этап, который пока нами не реализован. Дальше она может быть использована для прогноза, что будет дальше, какое состояние дальше ожидать. делать какое-то планирование, но это пока нереализованный этап. Пока более-менее реализованный, но здесь две вещи, о которых я уже то, что сказал. Так, ну вот, наверное, да, я уложился практически. То есть, О чем, собственно говоря, мы сейчас говорили? Мы говорили о том, что получается, если классическое RL транслировать в мир спайков, где нет чисел. Там нет чисел, но зато есть времена. Там есть времена, там есть динамика, там есть возможность... Причем эта динамика может разворачиваться на разных временных скалах. Ну и вот, собственно говоря, что мы будем иметь, проекцируем RL на импульсный домен. Будем иметь совершенно другие структуры нейронов, которые отличаются от традиционных нейронных сетей. Будем иметь совершенно другие концепции, принципы обучения. Как я уже сказал, они совершенно не похожи на никакой бэкпроп. Но это даст возможность, ну вот плюсы я уже говорил. Энергоэкономичность, перманентное обучение. Я сейчас не затронул этот вопрос. У нас есть ответ на очень важную проблему катастрофического забывания. Как сделать так, чтобы у нас в процессе того, что мы что-то новое учим, мы не забывали старого. Это тоже достаточно естественно решается. Это проблема, но это тема отдельного разговора. Ну вот, короче говоря, и к тому же мы всегда держим в уме необходимость обеспечения эффективной реализации этого всего на крайне простых, но массивно параллельных много десятков тысяч вычислителей на одном чипе. Мы стараемся удовлетворить всем этим требованиям. Ну и вот мы находимся сейчас в пути, то есть это большой исследовательский проект, который будет длиться ещё долго, потому что там ещё много-много чего неизвестного. И, собственно говоря, еще раз хочу поблагодарить, во-первых, комьюнити, наш телеграм-сообщество AGI Russia, и хочу еще так же напомнить, что есть еще одно сообщество, которое, собственно, прямо заточено под тематику нероморфных всяких вещей, это NRME Airus. Так что, welcome. Всё, теперь, я думаю, можно задавать вопросы. 

S02 [00:56:48]  : Спасибо. Вопрос. На самом деле, самые интересные вопросы посвящены тому, про что вы сказали, то, что является темой отдельного разговора. Соответственно, у меня возникла пара вопросов. Может ли вы, не уходя в очередной час доклада, намекнуть Как именно вы видите решение проблемы пресловутого ваншот-ленинга? Если вы вообще видите эту проблему, потому что есть же такая точка зрения, что эта проблема выдуманная, что ее на самом деле не существует, что на самом деле то, что мы называем ваншот-ленингом – никакой ваншот-ленинг. Это вот один вопрос. Ну и сразу же, может быть, паровозиком решение проблемы быстрого переобучения, когда мы можем быстро чему-то относительно переучиться без необходимости поиска очередного большого терабайтного тренировочного корпуса. А с другой стороны, третья часть аспектного вопроса, что да, у нас есть проблема катастрофического забывания, и тоже, может быть, намекнуть, в какую область, в каком направлении вы двигаетесь, чтобы ее решить. Но с другой стороны, есть проблема катастрофического запоминания, когда, может быть, нам нужно что-то забыть или от каких-то навыков избавиться, а мы от них никак избавиться не можем. 

S00 [00:58:18]  : Понятно. Ну, очень кратко. Но то, что касается One-Shot Learning, я тоже здесь не вижу отдельной проблемы. One-Shot Learning — это просто очень большой шаг обучения. Вот мы хотим обучиться с первого раза, но мы тронули горячий чайник и больше этого не хотим делать. Соответственно, у нас просто очень сильное воздействие на синапсис, чтобы нам сильно подавить Это я очень обобщенно говорю. Если это оказывается не так, потом это дело размоется. Ваншотленинг, в моем понимании, это просто резкое, сильное изменение весов. Поэтому нет ничего такого специального. А вот катастрофическое забывание и катастрофическое запоминание – это действительно проблема. И оно требует действительно некоего специального решения. И более того, причем оно довольно-таки разное для разных видов обучения. Потому что вообще разные виды обучения ввлекут за собой разные структуры и разные законы пластичности, разные все. Ну, я имею в виду, допустим, unsupervised learning и, ну, скажем там, supervised или reinforcement learning. И ответы у нас довольно разные на то и на другое. Они заключаются в том, что, ну, допустим, если говорить вот сейчас о нашей теме, да, Supervised Learning и Reforced Learning, у нас есть такая вещь, как стабильность нейрона. То есть нейрон, ну, я как бы не всё рассказал о пластичности, чтобы не углубляться, да. На самом деле вот эта константа, на которой изменяются синапические веса при обучении, она не константа, она зависит от maturity нейрона. У нейрона, который хорошо обучен, она должна быть маленькая, грубо говоря, потому что если она будет большая, мы разрушим обученное состояние. Наоборот, у нейрона, который необычно, она должна быть большая. Соответственно, у нас имеются в сети нейроны как обученные, так и необученные. У нас большая сеть, мы можем себе позволить, мы масштабируем и всё такое. Если нейрон обучен, то надо, чтобы очень много актов чего-то неправильного он забыл. Конечно, он тоже забудет, потому что, действительно, у нас может просто измениться правило играть, тогда надо, чтобы он забыл. После этого процедуру мы контролируем. Не должно быть гистелезии, что мы долго-долго-долго учили, а потом раз и два, с моих примеров, нам всё разрушили. Вот такого быть не должно. И это как раз контролируется введением дополнительного элемента в состояние нейрона, который отвечает за его пластичность. Который тоже по некоторым законам меняется, они весьма просты. Ну, в общем, они дают возможность... У нас в сети имеется костяк обучившихся нейронов, и есть некий пул нейронов, которые еще могут чему-то учиться, по сравнению с пластичными. Более того, у нас предусматривается, я говорил, что у нас одно из видов обучения – это перестройка топологии сети. У нас нейрон, если он, допустим, совсем кем-то подавлен, он вообще может умереть и родиться в другом месте. Это наиболее характерно для unsupervised learning. Допустим, мы покрыли нашими кластерами всё возможное, у нас там 5 кластеров, а нейронов у нас там, не знаю, 30. Остальные нейроны просто всё время подавлены, на них ничего не попадает. Зачем они нужны? Они могут умереть и делать что-то другое, переползти куда-то в другое место. Это тоже один из аспектов, связанных с дообучением и социалистическим взаимодействием. То есть, ответы на это дело, они есть, и их несколько. Он не один, ответ на проблему социалистического взаимодействия. Ну, это так, если в очерчных чертах сказать. 

S02 [01:02:13]  : Спасибо. Было бы интересно в какой-то момент, когда эта тема у вас будет более проработана, и у вас будет время и желание и возможность продолжить эту тему. Но у нас сейчас еще есть вопросы. Есть два вопроса от Андрея Ершова. Первый вопрос. Зачем меняются синапсы, на которые пришли импульсы после активации нейрона? 

S00 [01:02:37]  : Зачем они это делают в обычных сетях? То есть это обучение. И в традиционных сетях, и в импульсных сетях обучение — это и есть модификация весов связи. И там, и там как бы в этом нет различий. Просто различие в том, как это делается. Но, по сути, это и там, и там так. То есть они для этого и... Это обучение. За этим и меняются. 

S03 [01:03:00]  : Михаил, вот в чем суть. Смотрите, те синапсы, которые получили импульсы до активации нейрона, меняются. Понятна причина. А вот почему меняются те синапсы? на которые поступили люди после активации нейронов. 

S00 [01:03:21]  : Да, это интересный вопрос. Это стоит рассмотреть, потому что это действительно важная вещь. Давайте сначала рассмотрим ансупервайзлинг. Допустим, мы хотим найти некие устойчивые комбинации входов, которые часто бывают вместе активны. Именно такая установка задачи, она справедствовательна для импульса сетей, то есть нам некие структуры найти, некие коррелирующие входы. Это часто делается с помощью стдп классического, который говорит, что если мы получили импульсы до того, как сами сработали, то такие синапсы усиливаются, а если после, то такие ослабляются. Но теперь давайте представим, что у нас этот кластер, составляющий эти коррелирующие входы, посылает спайки с неким согласованием, а так оно в реальности всегда и бывает. Тогда мы начинаем учиться, и получается, что веса, связанные с этими входами, делаются сильными. вот у нас, допустим, 10 входов, которые на этих классах составляют, мы с ними научились их распознавать, с ними связаны сильные веса. и вот представьте себе, что 7 из них получили уже спайки, а 3 еще не получили. поскольку у нас веса сильные, то мы на них сработаем, на этих семи, мы их усилим, но три, которые пришли после, мы их зачем-то ослабим. Зачем? Непонятно, зачем их ослабим, потому что они тоже входят в этот кластер. Более того, это приведет к нестабильному обучению. То есть, если у нас будет такой джиттер, а он всегда есть в приходе эспадников, то мы произвольным образом будем то усиливать, то ослаблять, то одно, то другое, ничего хорошего. А если мы чуть модифицируем закон, сделаем его симметричным, то есть мы говорим, что мы любые синапсы усиливаем, получили ли они немного до или немного после, то тогда все получается хорошо, мы устойчиво учимся и все замечательно. Еще в этом минус этого классического антисимметричного SDP. В этом случае гораздо полезнее симметричный SDP. И нечто похожее мы здесь тоже имеем и в случае supervised learning, и в случае reinforcement learning. Поэтому мы усиливаем и те спайки, и те синапсы, которые получили спайки после срабатывания. Я ответил? 

S02 [01:05:40]  : Спасибо. Михаил, спасибо. Еще здесь был вопрос от Андрея Ершова. Обучение с моделью. На колонке справа приходят более ранние состояния, чем на колонке слева? Вопросительный знак. 

S00 [01:05:57]  : Да, конечно, они более ранние приходят, но когда мы научаем колонку слева, которая самая близкая к цели, Тогда она начинает вырабатывать сигналы и для колонки справа. Пока колонка слева не научилась, для колонки справа сигналы вообще не вырабатываются. Там есть вентиль, который запрещает это делать. То есть, пока мы ее не научили, пока мы ее учим форсированно с помощью этого REF BIAS, у нас этот refbias блокирует получение дальнейшего вторичного ревварда следующей колонки. А вот когда мы уже сами, без всякого refbias, сами распознаем, тогда этого блокирования не происходит, и тогда уже начинают обучаться следующей колонке. То есть у нас обучение как бы идет слева-направо, пока она слева не научилась, правой учиться не будет. 

S02 [01:06:47]  : Спасибо. Еще вопрос, точнее комментарий от Виктора Артюхова. Виктор не понимает, почему данная схема должна работать. У меня вопрос. Каков объем экспериментальных работ именно по верификации этой модели на практике. Вы показывали какие-то симуляции. Это уже работа вашей модели? 

S00 [01:07:14]  : Да, это работа моей модели. Пока это такие игровые вещи. Ловли этих шариков в рамочках. Это, кстати, реальная вещь. Она еще два года назад была продемонстрирована. Это Atari Games всяческие, которые с пинг-понгом. Нет, пока это все на эмуляторах. Это не применяется пока в реальных задачах, но я надеюсь, что скоро будет, в течение года-двух. 

S02 [01:07:42]  : А можете как-то чуть-чуть более развернуто рассказать в каком сейчас у вас состоянии это на каких эмуляторах, то есть где грубо говоря вы догоняете, где у вас получается, где у вас не получается, где вы догоняете State of the Art, где вам очень далеко до него. Вот в таком аспекте. Грубо говоря, какие игры, какие AI-G модели. 

S00 [01:08:15]  : Понимаете, тут вот какая ситуация, тут довольно сложно сравнить, поскольку немножко разные условия. Скажем, вот в этих самых AI Gym, там во многих случаях реввард подается искусственно и всегда. То есть мы знаем примерно, что такое хорошо и плохо всегда. Я в этом смысле действую честно, то есть у меня реввард, шарик отбит, пинг-понгом реввард, шарик не отбит, punishment, все, у меня больше ничего нет. Что является более жесткой постановкой, чем стандартная постановка. 

S02 [01:08:50]  : То есть, всё-таки я уточню тогда. Глядите, с Понгантом там же ситуация какая, что если ты подкрепляешь за отбивание шарика, Это первый уровень сложности. Если ты начинаешь подкреплять за то, что шарик ударится на той стороне, это уже усложняет задачу. 

S00 [01:09:17]  : Пока что мы только отбиванием занимаемся, но зато у нас редкие сигналы подкрепления. То есть мы отбили – подкрепились, не отбили – не подкрепились. Расстояние до шарика мы не меряем. 

S02 [01:09:34]  : Вы подкрепляетесь только за отбивание, за разбивание препятствий на той стороне или за попадание на ту сторону противника вы не подкрепляете? 

S00 [01:09:45]  : Нет, не употребляюсь. Тривиальный пинг-понг – это хорошая задача именно с тем, что это редкие сигналы вознаграждения, это относительно нетривиальное правило, то есть он с ракеткой двигается медленно, нам надо заранее догадываться. Мы не можем его мгновенно перемещать, нам нужно сильно заранее думать, куда ее надо переместить. У нас есть еще ряд задач, связанных с отслеживанием объектов. 

S02 [01:10:14]  : А можно я все-таки уточню здесь? Алексей Удот, я вижу вашу руку. Сейчас вот я вот эту тему закончил снять. Смотрите, вот по ПОНГу, там же ситуация следующая. Если взять классический ПОНГ из реайджима, то там с редкостью подкреплений все вообще очень хорошо. Потому что там подкрепление вообще идет в конце. вот в конце вот этого раунда то есть если ты в конечном итоге я вот просто помню я сам с ним экспериментировал вот и там подкрепление очень-очень долго нужно ждать 

S00 [01:10:51]  : Ну, да, наверное. Я, честно говоря, не помню. Поскольку надо всё там спайковать, мне пришлось сделать собственный эмулятор, поэтому я уже чуть-чуть отдалился. Я уже не помню, как конкретно сделано в классическом Atari. Да, наверное. Я говорю, что почему пока импульсные сети проигрывают? Потому что не решена задача создания model-based RL. На самом деле, в таких случаях, если у нас нет model-based RL, то у нас ничего нет. Без него здесь невозможно. Потому что предприятие нередкое. Поскольку мы пока находимся в пути к этому, к созданию полноценного, и более того, никто пока в мире этого не продемонстрировал, То есть есть некие половинчатые конструкции, когда что-то делается на импульсовых сетях, а что-то там в какой-то механике на C++ программировано извне. Но это не то. Мы хотим делать все по-честному. 

S02 [01:11:47]  : Я позволю себе тогда просто дать некоторую рекомендацию, потому что на этом пути я тоже какое-то время со своим подходом альтернативным шел. И тоже начал со своей модельки, с отбиванием шарика. С подкреплением за отбивание шарика получалось всё замечательно. Когда начал подкреплять за попадание в ту сторону, всё стало значительно хуже. А потом, когда я эту модельку запустил на Atari, там всё стало совсем плохо, потому что всё сильно поздно происходит. И там как раз нужно учиться подкреплять за промежуточные шаги. Учиться предсказывать за то, что ты предсказываешь траекторию шарика. Подкреплять за то, что ты можешь управлять собственной рукой. То есть самоподкрепление за правильное предсказание и за правильное поведение. Если мы учим как некоторые промежуточные предварительные шаги, то дальше из этих макрошагов уже начинаем собирать более сложные когнитивные схемы с отбиванием шариков, попаданием и так далее. У меня еще последний вопрос на эту тему. Потому что я тоже с Понгом в своем симуляторе разные эксперименты ставил. В самом начале вы сказали, что у вас есть возможности кнута и пряника. Положительное подкрепление и отрицательное. Вы ставили какие-то эксперименты, даже на вашем симуляторе, как влияет то или иное подкрепление, либо комбинация их обоих на параметры обучения, на кривую обучения? 

S00 [01:13:25]  : Это сильно зависит от, собственно, контекста обучения. Скажем, если мы говорим о задаче наведения рамки, то там влияет очень хорошо и то, и другое, да, потому что информативно и то, и другое. Если мы говорим о пинг-понге, то почему ты отбил – это понятно, а вот почему ты не отбил – это непонятно. То есть, там эти сигналы сильно не равноценны по своей семантике. Поэтому это очень сильно зависит от конкретной сдачи рейла. Где-то полезно наличие панишмента, где-то нет. То есть, это очень вариабельно. Но у нас есть механизм передавать другого. 

S02 [01:13:55]  : Просто я помню, в конкретном опонге, если вводить отрицательное подкрепление, то там становилось хуже. То есть, грубо говоря, получилось, что если только пряником, подкреплять, то лучше, чем и пряником и кнутом, потому что кнут, очевидно, демотивирует проявлять инициативу. Окей. Следующий вопрос от Александра Удода. Алексей Удода, извиняюсь. Алексей, пожалуйста. 

S05 [01:14:21]  : Меня слышно? 

S02 [01:14:22]  : Да-да, пожалуйста. 

S05 [01:14:24]  : Михаил, спасибо за очень интересный доклад. Действительно, очень интересная архитектура. У меня вопрос по поводу моделей. У вас сейчас, как я понимаю, модели строятся архитектурно через использование нескольких состояний. То есть вы храните предыдущее состояние, смотрите разницу с текущим и таким образом вы делаете переход, ну как вы писали в докладе. То есть если переходить на физические термины, то вы, по сути, моделируете скорость. скорость движения некоторого объекта, ну в некотором фазовом, понятное дело, сложном пространстве. И, соответственно, продолжая физические аналогии, хотелось бы спросить, у вас есть в планах развивать вот эти вот более физико-подобные системы и, например, вводить еще один слой для вычисления того же ускорения, как это происходит в реальном физическом мире, и, к примеру, добавить туда линейность пространства, то есть чтобы была не просто марковская модель, а с пониманием того, что многие части этого пространства являются линейными и их не нужно, например, обсчитывать как уникальные пиксели, и что если шарик здесь, то он будет двигаться туда и туда и дальше, и что он не будет перепрыгивать, что там нет, например, градиента плотности или там еще чего-то. Это первый вопрос. 

S00 [01:15:48]  : Ну, на самом деле, мы же явно ничего не вводим, то есть мы физику руками не вводим. Если состояние ускорения в нем является существенным, и мы можем как-то его формализовать на терминах потоков спайков, это, соответственно, будет что такое? В сущности, это будет какой-то следующий слой, берущий разницу между предыдущими слоями. Замечательно. Если это будет информативно, то если это будет коррелировать с первичными или вторичными вознаграждениями и punishment, то прекрасно, мы будем это использовать. Если это не будет, то не будет. Как эти структуры будут синтезироваться? В какой-то степени, наверное, пока мы этим не занимались, это будет, ну, отчасти какой-то самоорганизация какого-то хаоса. Сейчас эти структуры, они ручками задаются такими линейными плоскими. Возможно, там будет нечто, ну, не Liquid State Machine, не тема хаотическая вещь, но какая-то более-менее обобщенная такая хаотическая сеть, с которой как-то будут на основе этой информативности, корреляций, будут сами синтезироваться какие-то структуры, наверное, такое и будет. Но об этом пока еще, ну, пока определенно говорить, это будет уже следующий этап. Но, по-любому, мы не хотели бы делать это руками. А что руками? Наступит программа, пишется, и все, вот ускорение. Нам надо, чтобы это все работало в общем случае. Но вот ответ будет какой-то, наверное, такой. 

S05 [01:17:26]  : Тогда вместо второго вопроса у меня просто комментарий к тому, что все равно некоторые параметры этой модели придется задавать вручную, самые основные, и у вас это и так задается вот через разницу в состояниях, то есть вы по сути и так архитектурно задаете некоторые параметры этой модели. И просто чем больше таких приоров будет введено в модель, тем быстрее модель будет сходиться. И, соответственно, при использовании того же ускорения, что является аналогом силы, можно будет искать причину следственной связи. То есть они же все через силу идут. 

S00 [01:18:01]  : Я бы не согласился. На самом деле, мы исходим не из того, что есть ускорение, скорость или что-то еще, а что есть просто некие состояния. В данном случае они дискретны, но они могут быть и более размыты, они обязательно дискретны. И между ними есть некие переходы. Это не обязательно переходы пространственные, что-то там шарик в одном месте. Но это не некое обобщенное описание состояния. Не имеется перехода, но это как бы совсем… Это всегда так. Если есть некое состояние, некая динамика, то это всегда будет так. А скорость, это ускорение или что-то еще, мы в таких ситуациях здесь не работаем. Поэтому мы не то чтобы ручками это сдаем. Конечно, приходится сдавать ручками что-то, но это скорее вещи, связанные с характерными временами. Допустим, что-то мы все-таки о сдаче знаем. Мы знаем, что примерно это не микросекунды, а миллисекунды или секунды. Какие-то размерности мы ручками задаем, но только самые общие. Ничего более подробного мы стараемся не задавать. 

S05 [01:18:59]  : Спасибо. 

S02 [01:19:01]  : Спасибо. Еще вопрос от Михаила Сараева. Добрый день. На последних слайдах колонка – это аналог колонки нейронов в коре горловного мозга. Есть ли примеры использования для обработки речи? 

S00 [01:19:16]  : Нет, я заранее открестился, я говорил, что это не модель колонки, что я не нейрофизиолог, я не стараюсь ничего такого моделировать. Возможно, какие-то аналогии и есть. Принципы организации процесса информации апоскоповского мозга – это импульсная нейронная сеть. Наверное, что-то похожее там, может быть, тоже есть. Никаких прямых аналогий. И эти слои, которые получаются, они получаются естественно. Это не то, что я моделировал гору. Нет. Но если вдруг нейрофизиологи обнаружат, что нечто похожее есть в коре, я буду счастлив. Но я этого не хотел и не утверждал. 

S02 [01:20:00]  : Хорошо. Спасибо. Коллеги, есть еще какие-то вопросы к Михаилу? Михаил, пожалуйста. 

S04 [01:20:14]  : Здравствуйте. Спасибо большое за конференцию, доклад. У меня два вопроса по обработке речи. Используется ли сейчас активно технологии? И еще один вопрос. 

S00 [01:20:28]  : Нет, про бродки речи пока мы это не используем никак. То есть, пока это более простые задачи такие, механистическо-роботообразного уровня, а не лингвистический. 

S04 [01:20:43]  : И никто другой тоже нигде не использует? 

S00 [01:20:49]  : Ну, были какие-то отдельные работы, но все-таки это, мне кажется, Очень далеко пока что. Зачем нужно представлять либо речь, либо текст в виде спайков, мне пока не ясно. Что это могло бы дать? Ну, конечно, да, речь, безусловно, динамический процесс, поэтому там время важно. Ну, наверное, и речь, да. Вот текст совсем непонятно, зачем там спайки. Вот пост совсем непонятно. А речь, ну, кстати, да, со звуком, между прочим, работа. Я вспомнил, не речь, а со звуком работают сплучатники, кстати. Там есть группа Демина, и там как раз импульсные сети применяются для анализа звуковых сцен, вот это и есть. Насчет текста просто не знаю, а насчет звука да, есть такое дело. 

S02 [01:21:40]  : Михаил, а почему, скажите, а почему вот по звуку да, а по речи нет звука, это речь, а по тексту нет текста, это та же самая последовательность? 

S00 [01:21:53]  : Конечно, последовательность, безусловно, но все-таки один из плюсов импульсной сети – это учет динамики. Можно сказать, что у всех есть динамика, но это очень символическая вещь. Представляете, в виде спайков. Ну, я не знаю. Может быть, я просто об этом не думал. Но мне пока не видно, где бы это могло быть полезно и оправданно. 

S02 [01:22:22]  : Спасибо. Михаил, пожалуйста. 

S04 [01:22:26]  : Спасибо большое. У меня третий вопрос. В целом, по архитектуре, про модули, вы тогда рассказывали, последние два слайда. У меня в архитектуре такой вопрос. Если, допустим, у нас программу, которую вы уже делали пинг-понгом, отбили шарик, не отбили, то у нас, допустим, достаточно будет двух модулей, чтобы это описать. Или их там будут десятки, сотни. Но два модуля, то есть отбили или не отбили. 

S00 [01:22:56]  : Немного. 4-5 модулей, если говорить практически об этой задаче. 

S04 [01:23:02]  : А можете прям несколько комментариев добавить? Почему не два? Просто отбили, не отбили? 

S00 [01:23:09]  : Нет-нет, там не то, что отбили или отбили, там надо узнать, какое состояние лучше, ближе к цели, а какое дальше, для того, чтобы сгенерировать. Харик еще далеко летит, еще далеко нам до реворда, до панишмента, но нам не надо представлять, куда бьется ракетка. Более сложный вариант, и когда у меня будет некая модель мира и модель моей действия, я, конечно, напишу некие прогностические моды, которые сейчас пока у меня нету. Поэтому я могу только сказать, вот я сейчас двинул ракетку и стала лучше или стала хуже. А как я могу это сказать, если у меня нету реворда? Я должен реворд генерировать, например, того, ближе состояние стало к целевому или дальше. И уровни этой близости – то, о чем я говорю. Сейчас этих уровней 4-5 нормально. Делать их 100 абсолютно бессмысленно. Делать их 2 слишком грубо. Ну, в этой задаче как бы так. В основном, где-то будет несколько таких уровней, очевидно. 

S02 [01:24:07]  : Да, это, собственно, то, к чему мы тоже пришли. Коллеги, еще есть какие-то вопросы к Михаилу? 

S04 [01:24:19]  : Можно я еще уточню? Пожалуйста. У нас внутри модуля получается очень много, может быть, состояний. У нас пять модулей, допустим, и в каждом модуле могут быть описаны состояния. В, по-моему. 

S00 [01:24:35]  : Я не понял, не расслышал. В каком модуле, в чьем состоянии? 

S04 [01:24:41]  : Когда мы, по сути дела, это событие, да, маркетские, мы описываем, вы говорили, да? И вот у нас есть модули, вы говорили, 5 модулей, да? 

S00 [01:24:54]  : Ну, 5 уровней хорошести состояния, если мы об этом сейчас говорим. 

S04 [01:24:59]  : А мы же не говорим, что у нас всего 5 хороших состояний? 

S00 [01:25:04]  : Нет, именно поэтому в мире колонки есть микроколонки, которые распознают существенно разные хорошие состояния данного уровня. 

S02 [01:25:24]  : Хорошо, спасибо. Хорошо. Коллеги, есть еще вопросы к Михаилу или отпустим его? Вопросов нет. Михаил, огромное вам спасибо за интересный доклад. Жалко, что не удалось на OpenTalk выступить. 

S00 [01:25:44]  : На этом чуть-чуть. У нас там была подпольная конференция. 

S02 [01:25:50]  : Понятно. Хорошо. Будем с нетерпением ждать новых достижений на более сложных и разнообразных эмуляторах, а может быть даже не эмуляторах. И успехов Вам. И всего доброго. И до новых встреч. Спасибо. Спасибо. Всем до свидания. 

S00 [01:26:10]  : Спасибо за внимание. Всего доброго. До свидания. 

S02 [01:26:15]  : Коллеги, напоминаю, у нас следующий семинар пойдёт уже по летнему времени. У нас семинары со следующего четверга будут начинаться уже в 18 по Москве, а не в 17. И следующий четверг Сергей Довгань будет рассказывать про АИКС и своё развитие идей АИКС, как универсальной индукции. Всем спасибо и до свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
