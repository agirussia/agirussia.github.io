## 18 июля 2024 - Обсуждение статьи "Понимание ситуации” Леопольда Ашенбреннера (ex Open AI) о наступлении AGI через 3 года - Игорь Пивоваров — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/65BVk12lqEU/hqdefault.jpg)](https://youtu.be/65BVk12lqEU)


Доклад, представленный Игорем Пивоваровым, касается статьи Леопольда Ашенбреннера о перспективах создания сильного искусственного интеллекта (AGI) и суперинтеллекта. Вот краткая суммаризация ключевых тезисов и обсуждений:

### Основные Тезисы Статьи

1. **Создание AGI через три года**:
   - Автор утверждает, что AGI будет создан через три года, основываясь на текущих трендах роста вычислительных мощностей, алгоритмической эффективности и потенциале моделей (например, через техники как "chain of thought").
   - Используется понятие "OOM" (orders of magnitude), показывающее экспоненциальный рост вычислительных мощностей.

2. **Суперинтеллект через год после AGI**:
   - После создания AGI, через год будет создан суперинтеллект, который будет способен решать более сложные задачи и обладать более широким кругозором.

3. **Автоматизация AI-исследований**:
   - Ключевым прорывом будет создание автоматизированных AI-исследователей, которые смогут самим улучшать модели и решать задачи, ранее не решенные человеком.

4. **Вычислительные мощности и энергетические требования**:
   - Для достижения AGI и суперинтеллекта необходимы колоссальные вычислительные ресурсы и энергетические мощности. Автор упоминает о строительстве огромных вычислительных кластеров и проблемах с энергопотреблением.

5. **Безопасность и совместимость ценностей**:
   - Основной проблемой является совместимость ценностей создаваемого AGI с человеческими ценностями (супераллайдмент). Автор подчеркивает, что создание AGI, не обладающего моральными качествами, может быть опасным.

6. **Национальный проект**:
   - Автор считает, что создание AGI должно быть проектом национального масштаба, с секретностью и государственным контролем, чтобы предотвратить доступ к технологиям потенциально враждебным странам.

### Обсуждения и Критика

1. **Сомнения в прогнозах**:
   - Некоторые участники семинара выражают сомнения по поводу прогнозов создания AGI через три года, указывая на нерешенные технические проблемы, такие как отсутствие lifelong learning и энергетическая эффективность.

2. **Политическая пропаганда**:
   - Критика политической пропаганды в статье, которая рассматривается как попытка ограничить доступ к технологиям AGI для других стран, что может спровоцировать гонку и сделать ее более бесконтрольной.

3. **Агентность и субъектность**:
   - Обсуждение необходимости агентности в моделях и отсутствия дискуссий о субъектности в публичном пространстве OpenAI. Участники подчеркивают, что агентность необходима для того, чтобы модели могли ставить цели и решать задачи самостоятельно.

4. **Эволюционно-исторический подход**:
   - Александра Булдачев подчеркивает, что мы стоим на пороге смены эволюционного уровня, и будущий интеллект будет отличаться от нашего представления о нем. Он предлагает видение глобальной системы, которая будет контролировать и обрабатывать данные.

5. **Критика бенчмарков**:
   - Участники отметили, что текущие бенчмарки для оценки моделей могут не отражать реальную сложность интеллекта, который мы пытаемся создать. Новые алгоритмические идеи необходимы для достижения настоящего AGI.

6. **Ограничения текущих моделей**:
   - Владимир Смолин указывает, что текущие модели основаны на обработке человеческих знаний и не способны самостоятельно получать новые знания. Он считает, что brute force не поможет в достижении AGI и что необходимы новые алгоритмические идеи.

### Общее Впечатление

- Доклад и последующая дискуссия показывают, что мнения по поводу создания AGI и суперинтеллекта сильно различаются. Некоторые участники верят в прогресс, достигнутый за последние годы, и потенциал достижения AGI в ближайшем будущем, в то время как другие подчеркивают нерешенные технические и этические проблемы.
- Обсуждение также затрагивает вопросы безопасности, совместимости ценностей и необходимости новых алгоритмических подходов для достижения настоящего AGI.






S08 [00:00:07]  : Коллеги, всем добрый вечер. У нас сегодня очередное заседание семинара русскоязычного сообщества разработчиков сильного и общего искусственного интеллекта. И у нас после долгого отсутствия в гостях Игорь Пивоваров. И он предложил очень интересную актуальную тему про регулирование риски искусственного интеллекта, можно сказать, от первоисточника, который непосредственно Долгое время работал в OpenAI. Но я не буду спорить. Игорь, тебе слово. 

S05 [00:00:40]  : Спасибо, Антон. Да, коллеги, всем добрый вечер. Рад всех видеть. Действительно, какое-то время тут в онлайне отсутствовал. С собственными делами загружен был, всякие исследования. Но надеюсь, что сегодня мы интересно поговорим. Значит, я сейчас расшарю экран. Я, честно говоря, не делал какой-то специальной презентации для сегодняшнего митинга, потому что он как бы в моем понимании скорее должен быть как такой, как journal club. И предлагаю обсудить статью сегодня Леопольда Шинбрендера, которая у вас там сейчас должна быть на экранах. Да, она на экране? И статья эта вызвала мой интерес по нескольким причинам. Во-первых, она прогнозирует безусловное окончательное наступление AGI прямо через три года, и Superhuman Intelligence еще в течение года после этого. И автор приводит к этому довольно много аргументов. Мне кажется, это повод мог бы просто об этом поговорить. Во-вторых, человек этот, Леопольд Пашенбреннер, работал в OpenAI, работал в области safety и С одной стороны, конечно, он пишет в этой статье, что это его личная позиция и там все это сделано по публичным источникам, но с другой стороны, очевидно, что вещи, изложенные в этой статье, до определенной степени показывают обсуждения, которые идут внутри ОПНА и показывают, как бы, ход их ну если хотите дорожную карту такую вот того что они делают и это уже сейчас начинает проявляться в высказываниях многих людей из опыта это очень закрытая компания значит я про это еще в самом конце скажу им и в этом плане мне кажется ценно не только вот просто посмотреть на эту статью как на мнение конкретного леопольда шинбрендера но как на некоторый на некоторый срез того, что происходит внутри ОПНА и некая логика их движения. Она, безусловно, там есть. По этой статье видно, что они очень системно двигаются и у них есть много наработок на эту тему. Сама статья довольно большая, и я уверен, что ее мало кто читал, потому что 160 страниц в сегодняшней жизни трудно осилить, поэтому я пробегусь по основным тезисам и сделаю такие основные тезисы. Статья посвящена Илье Суцкеверу. Мы только что обсуждали, почему Илья Суцкевер Много лет был научным руководителем ОПНИ и задавал этот вопрос буквально пару месяцев назад. И очевидно, что ушел именно из-за проблем с безопасностью искусственного интеллекта и с теми вопросами, которые здесь подняты во многом. Он, видимо, является родоначальником или идеологом всей этой линии. Надеюсь, что не самой последней части. этой статьи. Главные основные тезисы такие, что есть масса доказательств в пользу того, что AGI будет скоро, и в первую очередь автор приводит Да, я хочу сказать, что я сегодня буду на позиции, условно, защищающей эту статью, несмотря на то, что я не согласен со многими тезисами и, в частности, с главным выводом, про который я скажу в конце, но с точки зрения вот его аргументации в пользу ИДЖА я буду сегодня ее защищать, чтобы просто был какой-то конструктивный диалог с остальными. Я подозреваю, что многие будут оспаривать эти тезисы. Значит, первый тезис состоит в том, что AGI неумолимо будет через три года, потому как, если мы смотрим на текущие тренды и по наращиванию вычислительных мощностей и так называемых порядок вычислительных мощностей, сейчас мы про это поговорим, то видно, что тренд такой, что через три года мы этого добьемся. Тезис второй, что после ИДЖАИ в течение года будет создан суперчеловеческий интеллект, то есть ИДЖАИ позиционируют как интеллект уровня человека, который способен решать широкий круг задач. а дальше через год будет создан, тоже с некой логикой, сверхчеловеческий интеллект, и даже не один, а их будет создано целый рой этих интеллектов, и тогда наступит некое довольно сложное потенциальное время. В связи с этим возникает некоторый набор вызовов, перед индустрией, как он это ставит. Но в первую очередь это, собственно, вычислительные мощности, которые нужны. И он приводит много аргументов в пользу того, что многие компании уже сейчас вкладывают совершенно колоссальные деньги в вычислительные мощности и показывают, как это готовится. Он считает, что с целью безопасности нужно будет засекретить все лаборатории AGI. И то, что большой проблемой является супер-аллайдмент, то, что Стюарт Рассел называл совместимость в своих терминах, Я не знаю, как точно перевести термин, но, грубо говоря, совместимость ценностей создаваемого нами сильного искусственного интеллекта, нами как человечеством, с человеческими ценностями, насколько это совместимо. Потому что многие люди обоснованно думают, что интеллект, который мы, как человечество, создаем от суперинтеллекта, он совершенно не обязательно будет наделен какими-то очень... грубо говоря, моральными качествами. Ну и вот это отдельная история. Ну и конец статьи посвящен отдельному тезису, что автор считает, что всю эту гонку должна возглавить одна конкретная страна и только так это может быть, а по-другому никак. Ну вот мы тоже на это посмотрим. Так, я расставил вот этих стрелочек, и мы просто будем по стрелочкам идти дальше. Тезис первый. как он обосновывает то, что AGI будет обязательно в течение трех лет. Он вводит такое понятие как OOM, orders of magnitude, ну и как бы порядки возрастания вычислительных мощностей. И строит график, показывающий, что с 2018 года по текущий момент мы видим довольно линейный рост в логарифмической шкале. Здесь логарифмическая шкала, то есть есть хранение экспоненциальный рост вычислительных мощностей, а в логарифмической шкале это будет линейный рост. И при проложении этого тренда с некоторой точностью и при превышении этих порядков вычислительных с неизбежностью будет достигнут уровень этого самого AGI. Что он вкладывает в эти ООМ? Речь идет не только про порядок, это десятка, условно говоря, что-то в 10 раз больше чего-то, это как бы на порядок больше. И он сюда вкладывает три категории. Во-первых, сами вычислительные мощности. Количество флопсов, если хотите, или терафлопсов. И его тезис в том, что количество вычислительных мощностей, вы все знаете, есть закон Мура, что удваивается. Каждые сколько-то лет все это. Есть тренд на постоянный рост учительных мощностей. Это одна из вещей. Мы наблюдаем постоянный рост учительных мощностей, который продолжается. Во-вторых, мы наблюдаем последние годы постоянный рост алгоритмической эффективности. Алгоритмы становятся все более совершенные. И в этом смысле, когда у тебя алгоритм становится более совершенный и он на том же железе может выжимать большую скорость, это как бы в его терминах тоже вот эти magnitude, но просто как бы в другом смысле. Но это он сюда же вносит. И, наконец, такое понятие как unhumbling, то есть как бы распутывание или раскрывание потенциала, если хотите. он говорит про то, что… и видимо в OpenAI тоже говорят про то, что в моделях есть много нереализованного потенциала, который можно реализовывать, находя определенные ключики к этому, если хотите. То есть речь идет не столько про алгоритмическую эффективность, сколько про некоторые приемы, например, как chain of thought. Такая известная тема, но мы сейчас тоже коснемся ее. Что касается компьютера, есть такая сайт, который каждый из вас может посмотреть. Там есть довольно много разных графиков, я там сам пользуюсь регулярно, как аналитик смотрю. И там можно посмотреть, что, глядя на этот график, кто-то скажет, что он линейный, кто-то нет. Но, в общем, более-менее некая линейность вполне себе прослеживается. То есть видно, что с точки зрения вычислительных мощностей, а здесь опубликованы вычислительные мощности, вернее график вычислительных мощностей, используемый для больших моделей. Видно, что действительно есть рост, допустим, примерно экспоненциальный, линейный в алгоритмической шкале. Похоже на это, допустим. С точки зрения компьютера, мне кажется, это самая простая тезис, с которой просто не имеет смысла спорить. Дальше есть у того же ЭПУХЭй график по алгоритмической эффективности. И они делают вывод, что алгоритмическая эффективность сетей примерно удваивается каждые восемь месяцев, что алгоритмы могут выжимать из того же самого железа вдвое больше вычислений с хорошей точностью. Ну и тоже, я не знаю, линейными или нелинейными, но вполне себе виден некий тренд на этом графике. Ну и, наконец, вот этот самый анхомблинг, довольно странный тезис, но вполне себе разумный. Он к нему относится, в частности, допустим, RLHF, в чем я лично сомневаюсь. Но вот, допустим, такие вещи, как chain of thought. Для тех, кто не знает, chain of thought – это техника промпта для LLM. Когда мы даем LLM задачу, она на нее пытается ответить на какой-то вопрос, например, на математическую задачу. И все знают, что LLM часто ошибается, решая математическую задачу, если там нужно рассуждать. Но оказывается, что если мы говорим ей, а вот расскажи пошагово каждое действие и сделай выводы из предыдущего действия в этой обосну, и она начинает рассуждать, выдавать тезисы, то оказывается, что точность ответов резко вырастает. когда мы заставляем ее как бы специально рассуждать и выводить и рассуждения то это сильно улучшает качество ответа при том что модель сама остается тот же самый и алгоритм остается тот же самый вот он это относит к одному из таких как бы приемов, как бы раскрытие потенциала. И он их приводит довольно много. Я, честно скажу, у меня, понимаете, 160 страниц, я не хочу там их пересказывать, я выделяю только самые, как мне кажется, сущностные кусочки, а тем, кому это будет интересно, естественно, можно будет почитать там все это, подробно посмотреть, потому что много написано на эту тему, много разных вещей. Да, автор сам не машинленер, он в алгоритмах не очень понимает, Но в целом, мне кажется, он ухватил основные вещи из того, что там есть. Поэтому это полезно почитать. В результате вот этих трех факторов он рисует некую такую шкалу. что, грубо говоря, вот есть физический компьютер и он там растет с определенной скоростью, но если мы на него наложим еще алгоритмический прогресс в алгоритмах, которые есть, а на них еще как бы наложим вот этот вот самый анхондлинг, то, другими словами, скорость роста вот этих самых как бы порядков величин вычислений, она резко вырастает. Это вот Такой тезис. еще отдельно здесь можно сказать про то что есть еще несколько таких отдельных что ли элементов от этого анхоблинга который он говорит ну или скажем как это будет дальше использоваться которые очень сильно повлияют на дальнейшее там использование и в первую очередь здесь ну надо отметить две вещи здесь надо отметить это это длина контекста Вы все знаете, что нейронные сети, все же базируются на дипленге, на нейронных сетях. Вычислительная сложность растет пропорционально квадрату весов, и поэтому А когда мы обрабатываем контексты, то, грубо говоря, пропорционально длине контекста тоже. Поэтому чем длиннейшими контексты скармливаем в сеть, сложность вычислений возрастает квадратично. И одно время это всех удерживало, потому что длинный контекст сложно считать. Но, судя по всему, в последнее время большие компании, у которых есть деньги, просто забили на эффективность и решили посмотреть, что будет, просто вливая деньги в этот компьютер. И другими словами, если мы дадим очень много вычислительной мощности, то мы можем и контекст давать там неограниченный большой а что такое длинный контекст ну грубо говоря там длина контекста вы все знаете что сейчас прогресс большой допустим там чаджи пяти во многом связан с тем что там есть возможность длинные контексты давать И когда мы, допустим, даем модели вопрос и еще подпитываем его в качестве контекста, допустим, какими-то релевантными документами, то ответ можно получить, но на порядок более качественный. Если представить себе, что мы в качестве контекста будем давать очень большую очень большое количество информации. Например, он приводит пример, что будущие AI-ассистенты, которые будут как удаленные работники, работают ему в качестве вводных может быть задана вся информация про компанию, инструкции корпоративные, мануалы и прочее, просто как контекст. И этот потенциальный AI-ассистент, будущий удаленный работник, он будет сразу в курсе всего, что происходит в компании. В общем, тезис такой, что когда мы контекст увеличиваем до миллионов токенов и выше, то это позволяет модели работать с большими массивами информации и в частности, как ему кажется, это позволяет работать над большими проектами с большим горизонтом планирования. это можно этим почитать. И еще один интересный тезис, что колоссальный прирост эффективности будет получен тогда, когда модель научится использовать компьютер, так же как мы это используем. То есть сейчас модели не дают использовать компьютер. Грубо говоря, если она сгенерировала код, но как бы код является окончательным выходом и дальше она Ну, Дин, если у тебя есть возражения, потом расскажешь, я вижу, что ты киваешь. Но тезис в том, что когда модели дадут возможность использовать вычислительный ресурс так, как ей это будет казаться нужным, и она сможет сама искать нужные сведения, считать, написать код, запустить его, проверить, потом запустить еще несколько версий, снова пойти что-то почитать. То есть так как это делает человек, то это будет колоссальный буст с точки зрения результативности модели. Потому что сейчас, конечно, такого нет в большом масштабе. Ну и как вывод, в результате всего он говорит про то, к чему мы все придем. тому что на как бы горизонте вот этих трех лет будет создан вот этот и джай который в первую очередь создается не просто как некоторый абстрактный интеллект создается как я ресерчер и его тезис здесь сильнее состоит в том что значит у меня он даже сейчас здесь есть это Ну да. В том, что не нужно автоматизировать все. Надо автоматизировать просто AI Research. Давайте мы сделаем так, чтобы AI сам себя улучшал, и этого будет достаточно для того, чтобы все сделать. Мы в конце к этому вернемся. я к тому, что это вполне выглядит реалистично, на самом деле. И дальше он говорит про то, что вот... То есть это не только он говорит, это сейчас говорили... Это говорил Альтман, это говорила вот эта, как ее... Кира или Мурати, я не помню, этот технический CTO новый, OpenAI, он говорит в статье, другие люди говорят, что GPT, если посмотреть на его траекторию, это было там в первой части статьи, про это было много, что GPT-2, грубо говоря, напоминал дошкольника, это была модель, которая там восхищалась тем, что она могла несколько слов связать в текст. И все такие, о, надо же, можно написать какой-то текст. Но это может дошкольник. GPT-3 был уже на уровне такого школьника, который вполне уже мог решать какие-то задачки, но довольно простые. ГПТ-4, на их взгляд, это такой школьник старших классов, уже вполне себе решающий многие задачи. Еще не все, конечно. Пишет тексты, всякие эссе и прочее. Но и логика их такая, что следующий шаг, то, что будет достигнуто дальше, это будет такой, грубо говоря, PhD, то есть человек, который способен не просто обычные задачи решать, но и вести там какие-то исследования хотя бы небольшие, обладающие определенным кругозором, способны решать там много разных задач уже уровней PhD. А следом за этим, когда вот эти PhD смогут автоматизировать AI-ресерч и сами будут заниматься ресерчем определенным, то дальше с неизбежностью появится некий superintelligence. И я сейчас забегу немножко вперед. Я в конце это написал? Ну, неважно. В общем, что такое автоматизация? Я ресерчер. Давайте мы себе это представим на секунду. На самом деле, это не так уж и выглядит прямо уж безумно сложно. У нас кейсы пользования моделью сейчас выглядят так. Мы даем ей вопрос, она дает ответ. Иногда даем какой-то сложный вопрос, который, допустим, можно декомпозировать. Но все это во многом потому, что недостаточная очередь ресурса, маленькие контекстные окна, и модель не очень умеет далеко планировать и декомпозировать свои задачи. Но не сложно себе представить, если посмотреть на исследование, что такое обычное исследование. Мы даем задачу математическую решить, модель ее решает. Она разводит ее на определенные какие-то рассуждения, делает действия, делает некий вывод. скажем там какое-то написание кода это ведь тоже такой задача следующего уровня мы там ставим задачу модели она значит это декомпозирует пишет некий код и дальше его там можно можно проверить скажем и вы помните наверное что deep mind недавно сделала У нас на опентоксе был доклад про фансерч. Александра Новикова. Они сделали модель, которая решает задачи, которые раньше были не решены, генерацией кода. она генерирует код на питоне, этот код проверяет, и в результате из многих вариантов находится действительно вариант решения, алгоритмический вариант решения задач, которые раньше не решались. И можно себе вполне представить, что такой автоматизированный исследователь, грубо говоря, у него есть задача сделать некую модель, которая Решает определенные данные, прогнозирует или там что-то. На входе куча статей, которые можно читать. Модель может писать код, она может его запускать и проверять. Она может вполне себе делать какие-то выводы, снова идти читать статьи, находить новые алгоритмы. И, в общем, если подумать внимательно, то там не хватает только одной маленькой кусочка, про которую я в самом конце скажу, к сегодняшнему, к тому, что есть сегодня. В общем, его тезис в том, что ключевым прорывом дальше будет создание таких автоматизированных AI-ресерчеров. И пока из того, что я лично наблюдаю, всплывает информация, видно, что действительно многие компании именно этим и занимаются. Они создают таких автоматизированных исследователей, если хотите. Это явно будет действительно большой буст. Ну и дальше его прогноз в том, что после того, как будет создан, мы находимся сейчас где-то здесь на этой шкале времени, но после того, как будет создан такой автоматизированный AI-ресерчер, а он будет создан не один, другими словами, когда будет создана модель такого ресерчера, то их можно запустить как бы инференсить параллельно там хоть сколько он делает такую расчетку довольно смешной но тем не менее что по его оценкам на сегодняшних личных возможностях на сегодняшних вычислительных мощностях в мире можно 100 миллионов таких исследователей запустить, чтобы они параллельно работали. Я сомневаюсь в этих оценках, но думаю, что даже если несколько тысяч таких будет запущено, то это уже будет очень сильный, очень сильный буст. Ну и дальше его тезис в том, что это очень сильно, значит, сперва это подействует на нормы труда, это приведет к большим сдвигам в технологиях и разработках, ну и в частности приведет к большим изменениям в военных технологиях, что тоже немаловажно. дальше это как бы собственно вот эта часть которая на этом я заканчиваю часть которая про его обоснование того что и джай неизбежно наступает и он будет и дальше наступит super super human intelligence потому что вот эти вот и агенты они с неизбежностью разовьют исследование до такой степени решать все нерешенные задачи и или плохо решаемые задачи, просто потому что это будет делать миллион автоматизированных алгоритмов, которые не спят круглосуточно и работают круглосуточно. Дальше он говорит, что для того, чтобы это сделать, есть некоторые челленджи, в частности compute. Нужно просто очень много вычислительного ресурса. и говорит про то, что по его сведениям, ну и как бы по информации, которая всплывает, сейчас строятся совершенно огромные вычислительные кластеры, и приводят там примеры этого, и считают, что где-то к двадцать шестому году будут построены как раз там, ну он в миллиардах долларов меряет размер этих кластеров, там условно говоря десяти миллиардный кластер, как минимум один. И это, грубо говоря, плюс два порядка по вычислительным мощностям. А к тридцатому году он думает, что будет триллионный кластер построить. А вы все помните, наверное, что новости всплывали про то, что Сэм Альфман ведет там переговоры с какими-то арабскими шейхами про то, чтобы семь триллионов долларов нужной инвестиций вложить как раз в вычислительные мощности. Понятно, что здесь сам дата-центр стоит огромных денег, но там еще микроэлектроника нужна для него и так далее. Ну и дальше его тезис там любопытный в том состоит, что Для этого нужно колоссальное количество энергии, а пока этой энергии еще нет. Это будет отдельный челлендж, где взять энергию. И приводит такой занятный факт, что в Сан-Франциско популярной темой последнего года является, где взять 10 гигаватт мощности на дата-центр. Народ это обсуждает активно. Приводится такой любопытный график, вот в Алеве, наверное, будет любопытно. просто квартальные выручки подразделения NVIDIA того, который занимается дата-центрами, конкретно оборудованием для дата-центров. И видно, что с первого квартала 2023 года колоссальный рост идет в продажах этого конкретного департамента, который занимается оборудованием для дата-центров. ну и делает еще ссылки на исторические прецеденты что вообще-то такого объема Вложения были в истории, когда государства ставили приоритетной для себя некоторую задачу, и в частности Манхэттенский проект, программа Аполло космическая, телеком инвестировали около триллиона долларов по разным оценкам в инфраструктуру интернета, чтобы он был таким, как мы сейчас разговариваем с вами. этим пользоваться, ну и так далее. И здесь важный тезис, его большой тезис в том, что это проект национального масштаба. И он считает, что IJI должен стать проектом национального масштаба как минимум для одной конкретной страны, где он живет. в штатах, понятно. И дальше огромная часть статьи посвящена именно этому. Я здесь только коротко сейчас упомяну, что он считает, что должны быть немедленно введены режим секретности всех GI лабораторий, никакого open source, ничего в открытую, потому что это все настолько как бы важно и серьезно, и, значит, влияет на безопасность страны, что есть вот всякие там нехорошие страны, которые недостойны того, чтобы иметь... страшно подумать, что будет, если такого плана технологии появится у них. С его точки зрения есть только одна страна с ослепительным имиджем, которая может лидировать этот процесс и все будут уверены в том, что все будет хорошо. Здесь тезис в том, что с его точки зрения есть гигантский диссонанс в этих самых ИДЖАИ-лабораториях, как он их называет, что люди, с одной стороны, считают, что они строят ИДЖАИ, которые изменят мир, а с другой стороны, с точки зрения секретности, у них там просто абсолютно все халтурно сделано, и он считает, что это ужасно. Дальше есть отдельный очень интересный раздел про суперлайдмент. Суперлайдмент это проблема, на которую наконец обратило внимание научное сообщество. Долгое время считалось, что это полная лажа, потому что Никто серьезно не думал про создание сильного ИИ. Отдельные гики думали про создание сильного ИИ, а многие ученые относились к этому несерьезно. Но, судя по всему, за последнее время очень сильно произошло изменение мнений. Из трех отцов основателей дипленинг, как вы знаете, двое вплотную занялись безопасностью ИИ. Йошуа Бенджо просто только этим занимается, а Джоффри Хинтон стал активно на эту тему высказываться, ушел из гугла, чтобы, судя по всему, про это высказываться. Янли Кун там занимает позицию, что все это фигня и пока об этом можно не думать, но он в меньшинстве остается. А вот Суцкевер соответственно ушел из OpenAI исключительно для того, чтобы заниматься вот этим вот самым супералайдментом. В чем проблема этого супералайдмента? в том, что если мы создаем интеллект, который сильнее, чем человеческий, то как мы можем быть уверены в том, что он будет работать, как мы того хотим. И, кстати, здесь вопрос, кто такие мы, тоже важен. То есть кто будет контролировать это. интеллектом уставить ему задачи, и вот это большой и сложный вопрос. Автор иллюстрирует это такой занятной картинкой, что, грубо говоря, пока у нас есть маленькая модель, которая пишет очень простой код, условно, как здесь, то человек на этот код смотрит и видно, что код нормальный. А если мы создаем суперинтеллектуальную модель, которая для решения какой-то задачи создает новый язык программирования и для него инструментарий, а потом на этом новом языке программирования пишет программу на миллион строк, то человек даже не способен понять, глядя на этот код, что это такое, что он вообще делает. И тогда встает в прямую вопрос, а насколько вообще безопасно такую модель допускать к компьютеру, чтобы она что-то делала. Можно ли такой код запускать? это серьезнейшая вещь. Ключевая проблема здесь это конечно удержать... человек не хочет оказаться... человек с одной стороны создает вот этот самый AGI и суперинтеллект, с другой стороны не хочет оказаться в проигравшим в этой эволюционной гонке, что вдруг появится некоторый агент. Но это не организм, это нечто. приводящие нас по... Дальше вы все знаете, есть масса на эту тему фантастических фильмов, которые эти сценарии уже многократно прошли. Собственно, сам автор этой статьи, конечно, признает, что это колоссальная проблема и что это, наверное, даже главная проблема. До тех пор, пока мы не решим проблему alignment, у нас нет никакой причины ожидать, что вот эта маленькая цивилизация суперинтеллектов будет продолжать подчиняться человеческим командам на длинном горизонте. Ну и дальше вторую предложенную читать не буду, она еще более печальная. То есть он хорошо понимает важность проблемы суперэлайдмента, которая, на мой взгляд, является центральной проблемой безопасности ИИ. Ну и говорит еще в частности про то, что когда появятся вот эти суперинтеллекты, то есть пока мы разрабатываем AGI, это еще как бы более такой ну что ли, спокойный период, хотя уже явно там, если ставится еще по горизонту три года, это много. Но дальше, если будут появляться такие автоматизированные помощники и будет такой взрыв суперинтеллекта, как он это называет, то все может пойти совсем не так, как того бы хотелось, естественно. Ну и он дальше приводит несколько примеров того, как он думает, что можно решать проблему вот этой вот элемента, в частности, что оценка проще, чем как бы само исследование. То есть, другими словами, когда человек смотрит на результаты, которые сделала модель, понять, хорошие они или плохие, сильно проще, чем, допустим, модель делать саму. В этом смысле, как бы задача оценки, она вроде как проще, чем задача генерации чего бы то ни было. В общем, тут есть еще некоторое количество разных соображений. Сейчас не буду все приводить. Он считает, что важно, что будет AI, что мы должны будем создавать AI, который будет контролировать AI, автоматизировать этот alignment. Видимо, не он считает, а OpenAI, я так понимаю, это обсуждают. Каким образом это можно сделать? Технологии типа Chain of Thought считаются важными, смотреть на них, потому что они помогают нам увидеть, как модель рассуждает, хотя это еще вопрос, можно ли доверять вот этим Chain of Thought, насколько оно действительно будет отражать то, что модель делает. Ещё раз говорю, что это будет катастрофически, если мы не сможем решить проблему Элайгмента, и будет супер интеллект, который за эту проблему не решает. И при этом меня лично потрясает в этой части статьи то, что он говорит следующее. Вот здесь написано, что он очень оптимистичен. технической возможности сделать суперлайфмут. Мне кажется, что там столько low coming fruit, что очень много всего можно очень легко сделать, что будет. И вот меня, честно говоря, это потрясает, а буквально на следующей странице он говорит, что вот это самая центральная, мне кажется, часть этой статьи, к чему я веду, знаете, прям меня это потрясло, потряс момент, что он пишет, что я думаю, довольно резонно утверждать, что вот этот план, значит, в целом должен сработать, и тут же дает примечание. Но вот, честно говоря, учитывая ставки, я думаю, что вот этот muddling through, термин, который он употребляет, как бы продраться через эти трудности, в некотором смысле это ужасный план. Но, видимо, у нас нету никакого другого. Человек делает тезис, не он делает тезис, многие делают, но он в частности в своем тексте делает ясный тезис, что супералайгмент это огромная, главная проблема того, что есть в ИИ, что если мы не решим проблему алайгмента, то вообще нельзя создавать никакой супералайгмент. И дальше фактически признают, что нет никакого ясного плана, что мы не понимаем, как это сделать, но ничего другого у нас нет. Но он оптимистичен, тем не менее. Вот это меня как бы прям покоробило, честно, вот эта статья конкретно. И пишет, что мы здесь во многом рассчитываем на удачу. А дальше он начинает, вместо того чтобы говорить про вот эту центральную проблему, которую как-то там можно решить, вся остальная часть статьи посвящена, на мой взгляд, ну совершенно, ну как бы на фоне предыдущих абсолютно неважной неважные вещи а именно тому что вот он считает что отдельная страна конкретная соединенные штаты должны лидировать в этой гонке все остальные как бы не могут там недостойны не обеспечат ничего там нужного и не просто значит штата лидировать а это должен быть некий так как в частности этот суперлайн создающийся это фактически новое как бы оружие массового поражения то это должен быть проект национального уровня и как бы правительство просто не должно допустить того чтобы начать вот это могло бы делаться каким-то там другим образом И делают вывод, что это вопрос безопасности страны. как бы пропагандистский бред вот честно такой написан ну то есть я со многими выводами согласен но когда люди начинают вместо того чтобы решать настоящие проблемы обсуждать кто как бы лучше у кого штаны там правильного цвета и и кто там может а кто не достоин и что вот они там мы хорошие понятно что мы хорошие вот они плохие там вот это вот на мой взгляд просто и дальше этого частного до конца статьи вот это все все есть и при этом он еще как бы он прям целую главу пишет про я выделил про нацпроект, как он считает, что должно быть, то есть все должно быть засекречено, все должно быть под государством, а мы с вами знаем, что, ну, когда государство все засекречивает, ну, очень часто это ничем хорошим не заканчивается, потому что как раз... Нет, история знает примеры, когда большие проекты так делались, но, в общем, это совершенно не гарантирует, что что-то будет сделано правильно. Ну и последний тезис его важный. Что если мы правы? Я, собственно, закончил весь этот обзор. Если для всех, кто сомневается в том, что да, будет создана PGI, это фигня, они будут созданы и так далее, проблемы высосаны из пальца и так далее. А что если это все вот так и будет, а если мы правы? В общем, стоит об этом серьезно задуматься. ну вот это собственно пожалуй это весь такой обзор статьи я буквально там тезисом еще таким закончу очень похоже на правду несмотря что я сам много лет был против вот этой логики Что, Антон, у меня там интернет сбоит, да? 

S08 [00:43:12]  : Сейчас вроде нормально стало. 

S05 [00:43:14]  : Хорошо. Я много лет был сам против этой логики, что чем больше модель, тем мы больше в ней найдем. Но, в частности, на последней конференции у нас там была большая дискуссия про эмерджентность с Константином Ивановичем Анухиным, Татьяной Шавриной, Михаилом Бурцевым. И я, признаться, поменял свое мнение. неожиданно для себя. И теперь я думаю, что да, это вполне то, что называется эмержентные свойства, мы вполне можем ожидать. А тем более, когда мы целенаправленно пытаемся проектировать. Это как бы одна тезис. в этом смысле и я и сайнтист про который он пишет на мой взгляд очень реалистичная конструкция и не хватает здесь вот чего а я я рано кстати шагну как я еще хотел две вещи показать значит помимо так да снова экранчик видно правильно Для тех, кто в эту общую копилку, есть еще два соображения. Одно такое. Недавно вот такой человек Дэниэл со сложной фамилией Дэниэл Кокотаджло. Не знаю, как правильно произносится. Бывший сейфти инженер в OpenAI ушел оттуда. Ушел с довольно большим таким скандалом, потому что оказалось, что OpenAI при уходе, при увольнении заставляет людей подписывать NDA, запрещающим до конца жизни вообще что-либо рассказывать про OpenAI. Вообще что-либо. До конца жизни. В противном случае человек потеряет свои бонусы, а бонусов там очень много. Так вот этот Дэниел отказался подписывать NDA и тем самым как бы отказался от бонусов, а бонусов у него там на 2 миллиона долларов было. Но вроде как там сейчас какая-то последняя информация, что адвокаты его начали что-то там это и вроде как может ему таки даже и выплатят эти деньги. но принципиально важно, что Чет отказался от этого подписания идеи для того, чтобы иметь возможность рассказать, что там происходит. И вот он был одним из тех, кто подписывал письма внутри OpenAI про то, что в таких компаниях должна быть Люди, работающие там, должны иметь возможность сигнализировать о том, что что-то идет не так общественности. Потому что, когда все происходит внутри компании, это небезопасно. И он, к чему я сейчас про него говорил, там можно с ним посмотреть статью, там есть какие-то видео, но важный тезис вот какой. Он говорит, что как бы внутри ОПНА позиция следующая, что главное, чего сейчас не хватает, вот что еще не сделала, одного блока нет важного, это агентности. Для того, чтобы вот эта вот вся конструкция заработала, вот этот AGI, автоматизированный AI-ресерчер, не хватает еще только агентности. LLM сама по себе, она хорошо, но вот нет вот этого агента, который способен цели ставить, задачи декомпозировать, решать, двигаться самостоятельно и так далее. И мы все сами это хорошо понимаем. Мы многократно обсуждали это на наших семинарах. Это действительно такая центральная часть, которая сейчас не реализована. Но я хочу просто ваше внимание обратить на то, что OpenAI известна на самом деле не только ChargePT, это мы сейчас знаем про нее, про ChargePT, а то, что, например, там сконцентрировано довольно много людей, которые занимаются агентами, просто про это, наверное, многие не помнят, а кто-то забыл. Я вот специально тут нашел, это их блог, с их блога кусочек Если вы вдруг не помните, они сделали в свое время набор агентов, которые выучились играть в игру Dota 2. и победили лучшую команду людей в Dota 2. Dota 2 – это стратегическая игра. Игра с неполной информацией, где агент учится в этой среде, у него искусственно уменьшено поле зрения, скорость движения, но чтобы он был сопоставим с человеком. И они сделали пятерку агентов, которые друг с другом взаимодействуют. Сейчас там уже закончится. Там такое красивое окончание, я специально сейчас к концу сделал. В общем, тезис в том, что в OpenAI работают не только люди, которые занимаются ВЛМ. Они непосредственно занимаются агентами и очень много. где там сейчас секунду я перемотаю а ну да вот собственно тут там эпическое такое окончание тут сейчас друг друга там Это реальный, реальный кб-кейс агентов, которые лучше, чем человек, работают вообще не в ЛЛМ совсем, а совершенно в другой области. Ну ладно, а то мы уже совсем уходим. Ну вот, собственно, я хотел про эту статью рассказать и послушать соображения встречные. 

S08 [00:49:40]  : Игорь, если позволишь, не зря же я потратил столько времени на чтение, я попытаюсь быстренько пробежаться по нескольким пунктам. То есть, прежде всего я начну с того, с чем я согласен. Первое, с тем, что размер имеет значение и то, что количество рано-поздно перерастет в качество. Про это еще Карл Маркс с Фридрихом Энгельсом говорили, поэтому окей. С одной стороны. Второй момент по поводу того, что сразу после возникновения AGI мгновенно возникает супер-AI. С этим тоже вопросов нету. Про это Ваш покорный слуга тоже во всех местах говорил, когда только можно было уже давно вслед за тем же самым Беном Герцелем. Вот. И тут вопросов нету. С точки зрения противоречивости отношения к перспективе, опять-таки, я просто сейчас перескажу кратко итоги встречи, которая была в Панаме. Достаточно большая тусовка собиралась по поводу регулирования искусственного интеллекта, общего искусственного интеллекта и того, как его делать Дружественным к человеку. Будет он добрым и мудрым, что он пожалеет меня. Так вот. Что меня удивило на этой тусовке. Там было представлено достаточно большое число англоязычной публики. Причем как исследователей, так и финансистов. Включая людей, которые Вашингтоне в ООН представляют. регулирования искусственного интеллекта. Так вот, если какое-то время назад в этой тусовке консенсус был такой, что доздраствует сингулярность и доздраствуют роботы и пусть будущее принадлежит роботам и чем скорее роботы поженятся с людьми и превызойдут их, тем лучше. Консенсус оказался немножко другой. И консенсус был примерно следующий. Что, с одной стороны, пресловутая сингулярность и возникновение AGI и следом за ним возникновение Super AGI или Super Intelligence неизбежно. И как бы в среднесрочной или в долгосрочной перспективе, сколько бы мы не извращались над регулированием и чего бы мы не придумали, неизбежно рано или поздно это произойдет. И это просто эволюционная история. Но, с другой стороны, есть масса причин, почему мы должны как можно сильнее и как можно дальше это откладывать. То есть, мы должны максимально это затормозить. Я не буду сейчас перечислять все варианты, почему мы должны это затормозить. Каждый может вообразить. В общем, консенсус такой противоречивый. То есть, с одной стороны, неизбежно. С другой стороны, давайте все-таки будем не ускорять. А давайте все-таки будем это дело тормозить. А почему нет противоречия? Окей. Хорошо. То есть, если нет противоречия, то хорошо. Второй интересный момент, как бы я тебе перехожу к тем пунктам, с которыми я не согласен в этой статье. Тоже, в общем, связан с той встречей. У меня там был интересный диалог. Как раз с человеком, который из Вашингтона, который в ООН представляет Соединенные Штаты в части регулирования. И он рассказал мне две вещи важные. Когда я его спросил, а как же так? Как бы вот мы тут рассуждаем, как бы нам AGI не допустить, а вот, может быть, начать с того, что автономное вооружение с правом на убийство запретить на международном уровне. Он мне махнул рукой, головой сказал, что нет, этот момент упущен. Это невозможно. Это уже проехали и никакого запрета тут невозможно. Это просто неизбежность. Все это будут делать и остановиться тут не получится. А когда речь зашла о регулировании именно AGI и глобальных, больших языковых моделей. Он сказал следующую интересную вещь, что вот в одной стране, которая является южной суперсилой, в отличие от них, они согласились, они сказали, да, давайте разговаривать о регулировании. Вот. И он это объяснял так, что вот у них очень большое взаимопонимание искусственного интеллекта и государства. Вот. И они понимают риски, которые связаны со всем этим. И вот вроде как им это нужно. А в другой стороне которая к северу находится от этого южного соседа. Сказали, нет, мы ничего регулировать не будем, потому что мы от вас сильно отстали. Когда мы вас догоним, тогда будем говорить о регулировании. Я это к чему говорю? К его тезису, что раз у нас AGI через три года превратится в супер-EI, ничего с этим не сделаешь, давайте мы срочно его посадим за решетку, чтобы он был только у нас, а к нашим геополитическим партнерам он не убежал. Мне кажется, вот это ограничение и закукливание, оно как бы только спровоцирует вот эту вот гонку и сделает ее более бесконтрольной. То есть, если мы сейчас поймем, что все, грубо говоря, если страна А резко перестает публиковать материалы на эту тему, а остальные понимают, что она в этом направлении продолжает с удвоенной силой работать, то, очевидно, они начнут с удвоенной силой работать в эту сторону и тоже начнут закрываться. И что будет происходить в шарашках во всех этих двух, трех, четырех, достаточно двух странах, Начинает совершенно... Верно, верно говоришь. 

S05 [00:55:50]  : Я совершенно согласен, и это, на мой взгляд, как раз есть главный риск сейчас. Потому как, когда мы понимаем, что есть проблема этого суперэлаймента, и что есть проблема возможности не удержать под контролем человека, пусть даже какой бы ни был человек, но все равно вот этот суперинтеллект, то закрываться и делать это все по отдельным углам, это самая абсурдная и идиотская стратегия, которую можно было предложить. В этом смысле я вот смотрю, что здесь есть комментарий такой, что переход политической пропаганды выдает в авторе умного последовательного человека. Может быть, но я вот вывод другой делаю. Я считаю, что если действовать так, как он предлагает, там одна конкретная страна постарается это у себя максимально ограничить я вот Антон с тобой сто процентов согласен все остальные то будут как это то есть человеки так устроены что либо они вот все вместе это делают либо вместе конечно очень сложно 

S08 [00:56:55]  : А потом все равно украдут. То есть, есть предшествующий опыт, да, говорит, что мало того, что это будет бесконтрольно и невозможно будет договориться, а все равно украдут, и у каждого будет там своя бомба, свой AGI там с какими-то национальными особенностями. 

S05 [00:57:09]  : Ну да, ну да. Вот, окей. А по поводу тезиса, про сам ЭДЖАИ, ты что думаешь? 

S08 [00:57:16]  : Да, вот теперь поехали дальше. Теперь я пошел дальше, с чем я не согласен. По поводу ЭДЖАИ через три года. Во-первых, в моем понимании, технически, есть масса вещей, как минимум две-три вещи, которых в ЛЛМках, который, очевидно, для вот этого молодого человека, который, очевидно, не очень широкий бэкграунд имеет в компьютер-сайенсе. Как я понимаю, для него ЛЛМ-ки – это все, что было сделано человечеством в области искусственного интеллекта. Этот самый RLF, chain of thought – это он тоже знает, но он ничего не знает про другие вещи, как я понимаю. Вот. И для него, значит, очевидно, естественно, вот это вот аппроксимирование, значит, как сказал Сергей Марков, что аппроксимация в стиле, значит, если в этом году я женился, то через 10 год у меня будет 10 жён, а через 100 лет у меня будет 100 жён. Вот. И вот аппроксимации в таком стиле, они, в общем, достаточно странные. 

S05 [00:58:17]  : Значит, я вчера как раз… А ты думаешь, это его аппроксимация? Не знаю. Я не думаю, что это его аппроксимация. Я думаю, что это аппроксимация Свиридского. 

S08 [00:58:28]  : Может быть меняю эти аппроксимации. Давай по порядку. Если говорить про IGI через три года. Я как раз вчера делал доклад в Сберии. Альберт Ефимов не даст соврать. Он здесь присутствует. Где я рассказывал принципиальные моменты, которых не хватает с моей точки зрения. И про некоторые из этих моментов. очень мельком очень бегло значит вот автор своей статье говорит но упоминая их он дальше делает следующий ход что ну с таким объемом финансирования из такой экспоненциальной скоростью развития чего-нибудь да придумаем то есть То есть, то, что есть проблемы, которые не удалось разрешить. Я в этой истории сижу уже 25 лет. С того момента, как Бен Герцель первый раз сказал, что через три года у нас будет EGI. Еще тогда EGI не было. Вот теперь он сам регулярно про эти три года говорит. Вот сейчас еще появилось много людей, которые говорят про эти три года. Но проблемы-то они нерешенные. 

S05 [00:59:32]  : Все-таки нельзя отрицать, что за последние 3-4 года прогресс не сопоставим с тем, что было за предыдущие годы. 

S08 [00:59:39]  : За счет перерохождения количества качеств, то есть в конкретной области глубокого влияния... В том числе, да. 

S05 [00:59:46]  : Много есть факторов. Но при всем, при том, что он не специалист в МЭЛИ, это понятно, и он разговаривает про это своим языком, но я вижу почти во всех его утверждениях, ну, может быть, кроме политической пропаганды, про которую я, может, не знаю, но почти во всех его утверждениях я вижу отзвуки того, что я вижу делается в ОПНА, это клинч. И у меня нет ощущения, что это его придуманные тезисы, например, про вот эту интерполяцию. У меня как раз ощущение, что это часть, элемент внутренней такой дорожной карты OpenAI. Они вот так и думают. они наращивают мощности, они думают, что обучая следующую модель, они действительно получат новые. И знаешь, самое неприятное, что ведь как бы спорить-то с этим сложно, потому что факты за последние годы показывают именно это. Как бы Марков не говорил, что там одна жена десять жен, сто жен, но прогресс GPT-2, GPT-3, GPT-4, он вот именно такой. И как бы я понимаю аналогии аналогиями, Но мы же видим реальный прогресс этих моделей, он фантастический. У нас есть Болдачев, который тоже был большим критиком, я помню, а потом стал очень... Я тоже был с первого критиком большим, но сейчас я вынужден согласиться, что мы видим объективный колоссальный прогресс. И он говорит про то, что увеличивая дальше эти размеры, а почему мы считаем, что вдруг мы перестанем видеть прогресс? Я тоже не знаю. У нас нет оснований. Здесь дальше идёт вопрос веры фактически. Мы верим, что он будет или не верим. Но я долгое время не верил в это. А сейчас я вынужден признать, что наблюдая за этим со стороны, как человек, который профессионально наблюдает, Я верю, что факты говорят про то, что... И он там в этой статье пишет, что все, кто становился на пути дипленинга, оказывались, так сказать, посрамлены условно, приводя массу аргументов на эту тему. Я бы сказал, что… А что ты видишь такого, что он не видит, что сейчас не может быть реализовано для того, чтобы этот ЭДЖАЙ был достижим на горизонте три года? 

S08 [01:02:28]  : Ну смотри, раз ты задал этот вопрос, давай я просто в двух словах повторюсь. Основные моменты. Первое, чего там нет. там нету обучения, там нету lifelong learning. То есть, если у человека есть возможность в фокусе своего внимания получать обратную связь от окружающего мира и из краткосрочной памяти, перегружать эту информацию в долгосрочную памяти, меняя свою модель в течение своей жизни, движение по своей жизненной траектории. То есть, архитектура, грубо говоря, такая. Вот у нас есть short-term memory. в которой происходит переваривание текущей ситуации есть long-term memory, в которой хранится наша долгосрочная память. И в процессе того, что мы получаем фидбэк через short-term memory, через нашу повседневную активность, она перегружается в long-term memory и используется уже в следующих эпизодах. Вот. У Чаджи ПТ, у него есть это short term memory, где хранится контекст, да, текущий, да, промпт. Вот. Который постепенно, значит, в зависимости от того, значит, больше он или меньше, все равно рано или поздно из него какая-то информация вываливается. Но это, в принципе, происходит из человеческой памяти. Информация из Пронта не попадает в модель иначе, чем через этот пресловутый – я аббревиатуру не заучу – реинфорсмент по человеческой обратной связи. Но это требует переобучения. То есть, вот они набрали в течение какого-то времени какое-то количество фидбэка, а потом – ба-бах – начинают переобучать. 

S05 [01:04:07]  : Ну и что? У человека это реализовано так, а у них так. 

S08 [01:04:11]  : Но они эту модель переобучают каждые 2-3 месяца. Окей. Второй тезис – это если сейчас пойти и посмотреть, кто и как применяет глубокие модели, Значит, начиная вот с замечательной компанией «Палантир», которая приводила на вчерашнем докладе. 

S05 [01:04:35]  : Они все ставят... Нет, он не компетент в этой области. Не-не-не, погоди, погоди. 

S08 [01:04:42]  : Они решают прикладные задачи. Они решают прикладные задачи на основе ЛИЭМ. И они вынуждены ставить костыли из онтологии как минимум в трех местах. 

S05 [01:04:52]  : Это неглиманский аргумент. Это как бы тоже текущее применение. Мы же говорим про то, чего не хватает, что мешает думать. Что дает нам основание думать, что невозможно, что в течение трех лет этот AGI будет создан. 

S08 [01:05:10]  : Так вот, Игорь, я как раз и хочу договорить, что сейчас... Валерий, я закончу отвечать Игорю на четыре позиции. Вот. Соответственно, решение какое есть да вот это автоматизация построения вот этот вот этих вот структурированных знаний с помощью которых мы можем направлять и верифицировать качество тех ответов, которые эти сети генерят. 

S05 [01:05:41]  : То есть для того, чтобы… Игорь Михайлович, это твой… Это сейчас ты не критикуешь, ты свое решение предлагаешь. А вопрос-то другой. 

S08 [01:05:48]  : Это решение галлюцинации. 

S05 [01:05:53]  : Есть много разных. Хорошо. 

S08 [01:05:54]  : И последние два момента. Первый момент – это пресловутая энергетическая эффективность. Потому что если нам для того, чтобы достичь AGI, нужно использовать всю энергию Солнца и сжечь всю нефть на Земле, то это очевидно не является решением проблемы AGI. То есть, предполагается, что все-таки ресурс опотребления является разумным. А смысл достижения общего искусственного интеллекта, если он прекратит существование земной цивилизации, потому что там будет сожжена вся нефть и вся планета покрыта солнечными батареями, в результате чего нельзя будет выращивать хлеб. 

S05 [01:06:40]  : Антон, ты ставишь другой вопрос. Что такое смысл? 

S08 [01:06:43]  : Я говорю, что если у нас достижение EGA достигается за счет экспоненциального роста энергопотребления для создания этих миллионов объектов, то это не является решением. 

S05 [01:06:53]  : Почему? Если он будет при этом создан, почему не является решением? 

S08 [01:06:56]  : А он не будет создан, потому что электричество кончится. Ему негде будет жить. Он как рак. 

S05 [01:07:01]  : Но не видно, чтобы оно кончилось. Это вопрос. Это не аргумент. Сейчас на планете хватает. Другой вопрос, на что его тратить. Ты считаешь, что это нерационально, а люди считают, что это рационально. Дальше вопрос. На что оно будет тратиться? 

S08 [01:07:22]  : По поводу того, что люди считают рациональные, те выкладки, которые эти люди делают. Опять-таки, в какой-то момент, когда я читал про то, что нужно всю пустыню Невада застроить вычислительными кластерами. И для того, чтобы эти вычислительные кластеры запитать, нужно отменить зелёную повеску и снять все ограничения на выброс СО2 и раскупориться энергетический запас нефти и газа в Соединённых Штатах. Когда он эти вещи говорит, у меня сразу возник вопрос, а может быть это нужно обсуждать не с дата-сайентистами, а с экономистами и экологами. С удивлением я обнаружил, что по бэкграунду этот человек еще и экономист. Он же адресуется не к нам? 

S05 [01:08:08]  : Что-что? Он же адресуется не к нам и не к мальчикам. Он адресуется руководству страны. 

S08 [01:08:14]  : Честно скажу, после того, как я увидел, что этот человек еще экономист, у меня возникло ощущение, что, так сказать, не хочется сваливаться в политизированную дискуссию, но мне создалось впечатление, что все это просто пиар, рассчитанный на поднятие бабок 

S05 [01:08:35]  : в интересах конкретных субъектов безусловно у него есть там одна из целей поднять денег он пишет что он сделал контору которая там венчурно на то чтобы здесь есть тоже какой-то элемент странной логики момент последний момент по поводу доверенного на по поводу значит супер аллаймента значит никакой и ji 

S08 [01:08:59]  : не будет полезен практически, если он не будет то, что называется доверенным в тех областях, где нужен человеческий контроль. Да, вот это вот сейчас вдруг кого-то осенило, что ой, super alignment, а как мы будем модели корректировать? Извините, в областях критической инфраструктуры Уже давно этот alignment есть и super alignment есть. Вы никакую программу, никакую нейросеть не возьмете для того, чтобы управлять не то что атомным реактором, тепловую электростанцию вы не будете нейросетью автоматизировать, потому что там должна быть программа, основанная либо, так сказать, на языке программирования C++, либо на языке программирования там FBD стандарта EC61131, либо в худшем случае какие-то интерпретируемые правила, которые можно верифицировать. 

S05 [01:09:51]  : Согласна, Антон, согласна. 

S08 [01:09:53]  : И это является проблемой в том числе средства параллельных. 

S05 [01:09:55]  : Но это не аргумент против. Это ты просто, ну, критикизируешь. Это не аргумент против. Давай это Диме дадим, потому что он говорил, что убежать ему надо. Или он уже убежал как раз? 

S07 [01:10:07]  : Когда можно получить у вас слово? У вас есть какие-то правила дискуссии? 

S05 [01:10:11]  : А у нас в чатике мы пишем. Альберт, привет. 

S08 [01:10:14]  : Дима, потом Альберт. 

S05 [01:10:16]  : Дима убежал, что ли? А, нет, не убежал. 

S08 [01:10:19]  : Дима здесь. Дима, Альберт, Валерий. В чат пишите, кто хочет высказаться. 

S01 [01:10:26]  : Да, значит, всем привет, Игорь, спасибо. Значит, я выделяю тут три поинта основных, в принципе, да, ты по ним прошелся. Это, собственно, AGI, это супер-AGI или там, короче говоря, сверхискусственный интеллект и политика. Вот политику лучше обсуждать, мне кажется, в кафешечке за винчиком, как мы делали. Поэтому я ее сейчас пропущу. А вот первые два. С первым я процентов на 95 согласен. Я был мега скептиком, если помните мои первые выступления по поводу дипленинга и ЛЛМ. и прочее, но последние три года показывают, что ну где-то здесь в принципе уже, ну как сказать, такой самый большой барьер уже сломан, и для того чтобы, ну опять же, в моем В моем определении AGI это просто такой работяга, который автоматизирует процентов 90 всей работы, как минимум умственный. Про физическую там все гораздо сложнее, понятное дело, там ЛЛМки не зайдут, как минимум. Но если мы говорим про это определение, то я тут не вижу действительно каких-то принципиально непредалимых барьеров. Есть, правда, сложность одна, да, то есть вот ты упомянул эмерджентность, да, как некое чудесное свойство модели, когда она просто даже не давая им какой-то там, ну, конкретные обучающие материалы, да, она вдруг начинает это понимать и начинает решать эту задачу только потому, что она стала большой. И она обобщилась. Как мы объясняем эмержентность, это то, что большой размер и то, что в принципе натурально рожденное свойство обобщаться. Есть вообще удивлённые модели. Но здесь есть большая разница в том, что переход, тоже правильно сказал, от ассистентов к агентам То есть то, что требуется, очевидно. То есть то, что сейчас, это в основном процентов на 90 ассистент и только процентов на 10 это агент. Кстати, можно агентную натуру пощупать уже, допустим, у того же Чаджи 5 и у Антропика, насколько я знаю. взять, напихать туда много разных файлов, там допустим питоновских, экселевских, там csv, еще что-нибудь такое, да, и сказать вот разберись с этим всем, возьми оттуда кусочек кода, посмотри вот в этот файл, напиши мне в итоге модуль, который делает вот это и вот это, да, и она, соответственно, прочитает, возьмет, и это явно происходит не потому, что она прочитала файл и просто засунула их в промт, потому что, ну, очевидно, ты не сможешь прочитать какой-нибудь огромный Excel и засунуть его в промт. Для этого нужны какие-то дополнительные тулы, да, для того, чтобы коммуницировать со средой. Вот, и это уже сделано. Это сделано прямо в... в десктопе, точнее это не десктоп, это веб-версия, но она по сути делает то, что доступно в GPT+. 

S05 [01:13:51]  : То есть ты фактически подтверждаешь, потому что ты говоришь, что на 90% согласен, есть сложность, но сложность 3 года для сегодняшнего дипленинга это огромный срок. За три года, учитывая размеры команд, которые там есть, и финансирование, которое у них есть, и мощности, которые у них есть, которые они могут пожечь, это вполне выглядит как дуэбл. 

S01 [01:14:12]  : Фишка в том, что там делали не только лишь в мощностях. Основная сложность заключается в том, что агентское поведение, исходя из того места, где по факту находится модель, то есть представьте, ощутите себя моделью, ты находишься в черном ящике, на вход тебе прилетает картинка или текстовая строка, и ты должен ответить текстовой строкой, то есть совершенно нет того спектра чувств и акторов, которые есть у людей, мышкой пошевелить, кнопкой нажать в нужном месте и так далее. Соответственно, модель этого всего, во-первых, она не видела, во-вторых, она это не умеет. 

S05 [01:14:57]  : Ты же знаешь, что они инвестировали в фигуру AI, в стартап, который занимается робототехникой. OpenAI сейчас конкретно с робототехникой начали работать, и они прямо говорят о том, что они сейчас делают… То есть ты говоришь про embodiment AI? Они не просто понимают, они над ней работают. Не скрываясь. 

S01 [01:15:19]  : Это немножко не то, как был прогресс последние три года. то это увеличение компьюта, увеличение датасетов и вот эта вот маленькая штучка RLHF, которая вот собственно самый большой такой алгоритмический breakthrough был в последнее время, который позволил моделям понимать инструкции. Вот здесь нужно нечто большее, я примерно понимаю как это должно выглядеть, но это прям очень масштабно, то есть это реально нужно посадить каких-то людей с трекерами, что они делают. 

S05 [01:15:59]  : То есть, другими словами, ты не видишь проблемы, ты видишь задачу, которая решается. 

S01 [01:16:05]  : То, что Антон назвал отсутствие long-term memory, это действительно проблема, но она, в принципе, решается большим контекстом, ракоподобными системами и так далее. какой-то серьезный, ну то есть скажем так, для 90% работ рак подобные архитектуры вполне себе могут зайти, если вот модель в принципе обучена работать вот этого из своего черного ящика. 

S05 [01:16:33]  : Извини, извини. Мне, к сожалению, нужно будет через 10 минут убежать. Я не рассчитал по времени. Я очень хочу послушать, что еще скажут Альберт и Валерий. Давай мы прямо сократим. Ладно, извини. Альберт, выскажешься, пожалуйста. 

S07 [01:16:54]  : Да, конечно. Во-первых, парни, я очень рад вас всех приветствовать, особенно Диму, которого давно ничего не видел. Я, говоря словами поэта, Ашенбренера не читал, но так же, как и вы, осуждаю. И поэтому, Игорь, большое спасибо, что ты прочитал и мне рассказал. Потому что мне очень хотелось узнать вообще, о чем чувак написал. Было действительно интересно тебя очень послушать. Я в целом все комментарии, которые ты дал, я читал в ответ на то, что написал сам Машон Брейнер. ты подтвердил все, что есть, но изложил это невероятно внятно, поэтому было очень интересно все это послушать. Ты это систематизировал великолепно. Спасибо. Я имею комментарии, но я их задам только в форме вопросов, потому что, на мой взгляд, сейчас, может быть, я подискутирую еще потом, но пока я все-таки задам вопросы. Значит, знаете, вот если посмотреть с какой-то достаточно высокой позиции, если эта книга, этот опус 23-летнего молодого человека является ответом, то давайте подумаем, какой был вопрос он себе задал и какой вопрос задало сообщество, что оно скачало там десятки тысяч раз этот опус. Какой был вопрос? И, на мой взгляд, ответ на такой, что ли, вопрос центровой кроется в названии того, что он написал. Как вы думаете, Антон, Игорь, может быть, другие коллеги, которые прочли эту книгу, почему он так ее назвал? Какие у вас идеи? Почему именно эти слова он выбрал для того, чтобы так назвать свою книгу? Второй вопрос. Я чуть бы не услышал нотки ответа на него в твоем анализе, Игорь, но, к сожалению, не услышал. И у меня есть второй вопрос, он очень связан. Были слова у тебя связаны с агентностью, но, на мой взгляд, ты остановился в шаге от того, чтобы спросить, а что Ашенбренер думает про субъект? 

S05 [01:19:08]  : А он ничего про это не говорит. 

S07 [01:19:11]  : Сейчас, секунду, я вопрос еще не задал, на самом деле. Потому что если он про это ничего не говорит, это сам по себе тоже ставит очень большой вопрос, а почему он об этом не говорит? Потому что если он говорит про эмерджентность, наверное, Анохин тоже очень много говорит, эмерджентность, сразу возникает вопрос о целеполагании у этих моделей с общим искусством интеллекта, вот это целеполагание. Это, ну, прямо скажем, любой человек, который хоть как-то следит за игрой и мячиком, который скачет между сторонами, сразу же возникнет вопрос. Слушайте, а что он на самом главном не говорит? 

S05 [01:19:49]  : Смотри, вот на это как раз я отвечу сразу. Это я сделал связку после статьи. Вот если еще раз посмотреть, то, значит, когда я рассказывал про статью, я ничего про агентность, субъектность и то, что нужно сделать, не говорил. А Шумбренер сам про это пишет там одной строчкой, что он хорошо понимает, что еще нужно сделать, но не скажет. И, кстати, понятно, почему этого нет в статье, потому что у него явно есть паранойя на то, что все должно быть засекречено, ничего нельзя сказать, а то сейчас он напишет и все про это узнают. Это после статьи, когда я сделал связку вот на этого Дэниела, с сложной фамилией, как Джабло, как Таджло, он сказал про агентность, что вот этого блока не хватает, он прям ясно это сказал там в одном из интервью, которые у него брали, Я еще добавил про то, что ОПНА, они большие специалисты в агентности, но про субъектность никто из них не говорит. Мы понимаем, ты правильно ставишь вопрос, но как бы на поверхности, в паблике эта тема не обсуждается совсем. OpenAI в последнее время вообще очень мало обсуждает в паблике на эти темы. Так что я думаю, что они над этим работают. 

S07 [01:21:14]  : Ну смотри, мой же вопрос был не в том, работают они над этим или не работают. Мой вопрос был, почему он это обошел. 

S05 [01:21:21]  : Потому что он не хочет говорить про то, что нужно. 

S07 [01:21:25]  : И теперь возвращаемся к тому, о чем я спросил. Как вы думаете, почему он именно так назвал свою работу? 

S05 [01:21:34]  : это вопрос, я потом отдельно в чатике напишу свои соображения. Чтобы по времени уложиться, я еще Валеру бы хотел комментарий послушать. Спасибо огромное, Альберт. 

S07 [01:21:43]  : У меня есть на самом деле короткий ответ на это. Давай. Потому что тема situation awareness возникла, когда возник термин «центрической войны», после войны в Ираке Соединенных Штатов. Этот термин используется в военном деле очень часто. Его просто практически не переводят на русский язык. И на мой взгляд, кратко суммируя, опыт Сашенбренера – это просто послание неким органам, до которых сам по себе достучаться не может, относительно того, что они должны принять в голову военное назначение. Вы уже очень много об этом сказали. И, читая книгу, нужно иметь в виду, почему он её так назвал. Всё, предоставляю слово другим. 

S08 [01:22:27]  : Альберт, спасибо, я угадал ответ, но не стал голос поиграть. 

S06 [01:22:32]  : Болер. Да, да, да. Спасибо, Игорь. Шикарный обзор. Я перед этим на Хабре еще почитал. Там тоже выжимка неплохая. Я попал в ситуацию аварии тоже могу сказать. На самом деле есть такой термин – ситуативная готовность. То есть это грубо говоря муха, если знаешь, что летай в ком, ты ее поймаешь. Если ты не знаешь, она из-за поворота. Просто ты не готов. Я думаю, этот термин примерно вот это означает. Ситуативно надо быть готовым. Это раз. Это в плане той дискуссии. Второе. Я очень рад, что еще один человек в нашем стане появился. Который верит в Brutal Force. Начал верить. 

S05 [01:23:19]  : Нет, я пока защищаю как бы позиции автора. Это не обозначает, что я стопроцентно прямо в это верю. Но вы нужно согласиться с тем, что аргументация разумна и последовательна. Ну давай я сюда. 

S06 [01:23:33]  : В этой части. Да, налью еще чуть-чуть. По-моему, шеринг можно сейчас, да? Я попробую. Вот, да. Антон, показывай, да? Да-да-да. Картинка, собственно говоря, из статьи. Это, собственно, вот этот экспоненциальный рост. Я чуть-чуть в эту уже налью воронку воды. На самом деле было шикарное выступление. Просто почему это верно и достаточно долго верно будет. Было выступление на Компьютексе Дженсона Хуанга. Ну это великолепный, во-первых, Ну, руководитель трехтриллионной компании, я его люблю страшно. Во-вторых, он прекрасно выступает, конечно. Если кто-то, ну, вдруг захочет, ну, в Ютубе можно быстро найти, там, NVIDIA SEAL, GENSHIN HUANG KING. Вот. Есть еще разборка на техплейрап, очень удобная. Тут прям странички. Так вот, значит вот он картинку приводит, то есть собственно Хуанг приводит картинку, где говорит, что ну да тут CPU сдохли, но GPU на самом деле у нас ускоряются, причем вот здесь так, это новая мода кстати презентации, такой грубовато делать, такой как от руки. Какой-то видимо редактор хороший у них. Вот, на самом деле это даже уже не закон МУРа, там в два раза за полтора года, а мы тут в десяток раз добавляем каждый год. То есть я к тому, что Вот этот тренд, он еще и усиливается. То есть это как вот вселенная расширяется, ну извините за сравнение такое. Она с ускорением еще усиливается. То есть она мало того что расширяется, она еще с ускорением. Причем под это есть несколько аргументов. Тот же Хуанг, он очень вот философски так вот визионерский все рассказывает он говорит что смотрите вот буквально его картинка просто мне кажется термин это вот многим поможет дальше смотреть он говорит у нас сейчас наступила промышленная революция по сути у нас вместо фабрик продуктов появились фабрики токенов то есть грубо говоря вы покупаете у нас оборудование берете деньги с клиента за токены и в неограниченном практически количестве, т.е. это по сути клондайк т.е. мы вам предлагаем бизнес модель готовую вы продаете токен, ну это же так и есть, да? вот тот же chat.jpt за сколько ты токенов платишь? вот, а и как бы он говорит это вообще новая бизнес идея т.е. и очень много народу прямо на нее и уже начали ориентироваться То есть начнется конкуренция, кто-то токены подешевле и так далее. И дальше в эту же историю. Под это сейчас тренд развития железа, он действительно сориентировался, люди практически бросили Ну не бросили, ну скажем так, сильно ограничили развитие классических суперкомпьютеров с плавающей точкой в двойной точности FP64. Это не нужно. То есть следующее поколение ускорителей дают на пару порядков, иногда даже на пару. Ладно, на порядок. Быстрее каждое следующее поколение, а каждое поколение раз в два года наступает, да? Они дают там этот BF-16, вот эти разрядности, которые, собственно, для нейростей идеально подходят. То есть там вплоть до того, что сейчас INT8, INT4 начали народ гонять. И так далее. И вот такой пример такого ускорителя. Он говорит, смотрите, это просто одна видеокарта, но унутрирует. Здесь на самом деле вычислитель с двумя процессорами стоит. И 72 B100 вот этих вот ускорителя стоит в единой системе. То есть у них общая память, там можно учить огромные модели, которые туда вот влезут. 

S05 [01:27:34]  : Валер, ты извини, ты уже в такие детали грузишься. Словами ты стопроцентно поддерживаешь 146% тренд на рост вычислительных мощностей. 

S06 [01:27:45]  : Теперь Антону чуть-чуть возразить или прокомментировать про гигаватты. тоже пример дата-центр вот switchNAP это один из ведущих дата-центров американских у них уже сейчас вот в состоянии запуска то есть работает по 200 мегаватт вот по 500 мегаватт два дата-центра в Неваде и по-моему в Тахо около Тахо вот это штат то здесь как бы народ вкладывается в эти деньги. Причем интересно, что вот если посмотреть, стойки по 55 киловатт, то есть это именно под обучение. Вот эти все стойки под обучение. И последнее, что хотел сказать по поводу, я тоже вот кстати, вот у меня ощущение от статьи, но из того, что я вот услышал, удалось понять, Мне просто нравится вся последовательность. Ну, кроме, конечно, вот этой вот истории с ограничениями. Причём я здесь даже, ну, как-то по-другому бы это назвал. Скорее, это какая-то попытка отрегулировать. И здесь, кстати, предположение, что тут все методы плохие по большому счёту. Ну, по большому счёту. Это такая же сингулярность сложности, как и сам Аяй сделать. Вот того же уровня проблема. потому что там Евросоюз два года рожал, родил три пункта и все ограничиваются, то есть они ничего особенно не дают мне кажется здесь как раз нужен вот тот самый ученый социолог-политик, то есть модель которая параллельно с развитием основного это как бы мне кажется здесь наивность нормально подходит, можно здесь так выразить эту историю Вот такая идея. То есть надо большие ресурсы бросать, в том числе и самого, модельные, аяйные, на то, чтобы сформировать вот этот кодекс регуляций, как это делать и так далее. Потому что это совсем новый мир и он так просто не дастся. То есть не получится этим дать, этим не дать, все открыть. То есть это все простые, слишком простые решения, чтобы такую сложную штуку, если мы хотим ее регулировать, отрегулировать. 

S05 [01:30:05]  : Спасибо. Спасибо огромное. Выключай шайбинг. Други мои, я очень прошу прощения, я бы сам тут инициировал семинар, а сам вынужден уже убежать. очень извиняюсь, но я очень хотел услышать, что Владимир Смолин скажет и Александр Булдачев. Антон, у нас же запись будет, да? Да, конечно. 

S08 [01:30:28]  : Ничего криминального, по-моему, нет. Да, пока все нормально, продолжаем. Игорь, спасибо тебе за обзор, спасибо за мероприятие. Другие я досмотрю обязательно. Александр, пожалуйста. 

S04 [01:30:43]  : То есть ко мне, да? Так, добрый день всем. Игорь, огромное спасибо уже ему в дверь. Значит, ну, на первый вопрос, который все отвечали, то, что нас ждет вот такое где-то 27-30 год, я был уверен еще, наверное, лет 15 назад. Если посмотреть мою статью «Финита ля история», она не касается искусственного интеллекта, она касается именно того, что историческое развитие нашего социума, Неизбежно придет книга о сингулярности, но сингулярность не связана с искусственным интеллектом, а связана вообще с общим устройством. И вот в связи с этим мне в этой книжке и в этой статье менее всего понравилась эта игра с интеллектом. Именно какие-то такие сации с суперинтеллектом, сверхинтеллектом. Мы не знаем, чем является наш интеллект. И связывать это с каким-то развитием. Здесь много новых людей, которые не слышали, наверное, мою притчу. Я еще коротко расскажу. Собрались звери на берегу моря. И стали рассуждать, а каким будет сверхзверь, искусственный зверь. Вот такой вот супер-суперзверь, который вот придет на смену нашим нам зверям. Ну одни говорят, он будет сверхсильный, другие сверхвысоко летать, другие говорят там рыбы, что топят за то, чтобы сверхглубоко нырять. Ну, посмотрели, да, что это будет супер-супер такой крепкие мышцы, кости. А рядом, чуть по сторонке, сидела обезьяна и прикручивала камень к палке. Вот наше представление об этом суперинтеллекте, скорее всего, на мой взгляд, именно такой философский, связанное с тем, что мы даже не представляем, что там будет. И то, что вот 9 суперинтеллектов за один месяц родят сверхинтеллект, ну это как-то выглядит немножко комично в этой статье. Ну там не 9 интеллектов, а миллион интеллектов. Если резюмировать этот эволюционно-исторический подход, я считаю, что мы стоим сейчас действительно на пороге смены эволюционного уровня. Социальный уровень, социальный уровень наш заканчивается, начинается новый. Что будет вслед? Он отличается точно так же от наших представлений. Представления животных отличаются от техники. Они не могли представить, что такое техника, что такое техническое воспроизводство, производство. Они мыслили только в категориях жизни, категориях жизненной силы. Вот мы точно так же мыслим в категориях нашего интеллекта, нашего развития его, и что вот этот интеллект решит все проблемы. Интеллект – это определенный способ решения задач. Мы его решаем своим способом. Там будет другой способ. Другой способ. И если мы говорим, что нас ждет именно 27-30 год, то здесь никто не произнес этой фразы, а я более чем уверен, что основная тенденция будет это сверхглобализация вот этой системы. не локальные агенты, которые будут где-то у кого-то пилить конкретные задачки. Это сверхглобализация вот этого инструмента, который обеспечит контроль над всеми данными, учет всех данных и автоматизация обработки этих данных. И этого будет достаточно. Человеку придет в голову идея, как сделать какой-то определенный алгоритм, как подстроить где-то лэмку, это человек придумает. Но для этого ему нужны сверхокученные данные, анализ огромного количества информации, которую не сделает, скорее всего, ЛМК, но она это сделает, предоставит информацию, предоставит данные, предоставит идеи для человека. Как человек сохранил свою животную сущность от предыдущей биологической, так человек сохранит и свою интеллектуальную сущность. А то, что будет сверх, это будет не животное, не интеллектуальное, а некая глобальная система, которая спонтанно сформируется, будет много угроз. Но самая главная угроза, которую иногда говорят, что мы не сможем удержать, она сама себя удержит. То есть не будет, как говорят, о, сверхинтеллект сразу же заспамит Facebook или заспамит поисковую выдачу Google. Не будет Facebook, не будет Google, нечего будет спамить. Будет полный контроль над данными. То есть я в своем браузере получаю по запросу данные те, которые мне нужны, и посылаю в своем браузере, ну скажем так, в нынешнем понимании этого браузера, понимаю некий контент, который будет либо пассивирован, если это просто тупая какая-то компиляция, либо будет принят или сохранен где-то в уникальном виде, и LMK будет на него ссылаться. Всего спасибо. 

S08 [01:35:58]  : Александр, спасибо. Алексей Удот, пожалуйста. 

S00 [01:36:05]  : Алло, меня слышно? Да. Мои основные слова были направлены больше к Игорю и его восхищением проблемы супералаймента, и я хотел немножко оппонировать к нему, но буду делать это заочно. раз уж отсутствует. 

S08 [01:36:21]  : Ну, Игорь потом вас услышит. 

S00 [01:36:24]  : Хорошо, хорошо. На мой взгляд, основная проблема заключается не в аламенте, а в том, что основные усилия сейчас тратятся на нейросети, а не на то, чтобы разбираться в собственном человеческом мышлении. И сейчас больше полагается на решения в черном ящике неинтерпретируемых и сильно неоптимальных нейросетей. АИ, когда получит полный замкнутый контур обратной связи, например, через доступ к реальному миру, через тот же Эмбодит, или хотя бы к вычислениям, то есть сможет выполнять свой собственный код и проверять его результат, то он сможет довольно быстро получить именно оптимальные алгоритмы, которые хорошо подходят нашему миру. То есть мы сейчас его просто трясем в черном ящике разными архитектурами, пытаясь нащупать нужное решение. Отсюда слишком много эмпиризма в этой науке. АИ сможет получить с помощью обратной связи очень точные, очень корректные, очень правильные алгоритмы. Например, оптимальный язык описания мира, почти идеальную для своих целей антологию, почти идеальные модели взаимодействия с миром. И это даст мгновенный отрыв от человека и, хуже того, это закроет понимание человеком принципов его работы. Это уже упоминалось, но я просто немножко с этой стороны хотел еще отметить. И поэтому, на мой взгляд, решением проблемы алаймента может быть только лишь развитие биоподобных и человекоподобных алгоритмов для того, чтобы можно было как минимум их использовать для внедрения на архитектурном уровне существующие модели. для ограничения интерпретации, а как максимум разработка полностью прозрачного искусственного интеллекта на человека биоподобных принципах и алгоритмах. Все, спасибо. 

S08 [01:38:12]  : Алексей, спасибо. Владимир, Вам слово. Так, у нас Владимир. Владимир, невидимо... Да, да, пожалуйста. Владимир? Коллеги, пока Владимир решает проблемы со связью, у кого-то есть желание высказаться? Оставшиеся 15 минут. 

S02 [01:38:52]  : Я, наверное, могу кое-что сказать. 

S08 [01:38:53]  : Да, да, пожалуйста. 

S02 [01:38:56]  : Приветствую всех. Мне кажется, немножко одна проблема как-то вышла за пределы обсуждаемого, в том числе в этой статье. Очень все легко на веру принимают идею вот этих трендов вычислительных. Да, что значит все вычисления растут, а значит машина будет непрерывно умнеть. Но у меня есть некоторое ощущение, что это все происходит во многом потому, что мир LLM окружил себя определенными бенчмарками. И вот, кстати, на конференции в Тбилиси последней на OpenTalks.ai про это много говорили. Сама система бенчмарков, на которых сейчас все измеряется, она такова, что по ней действительно все постепенно растет. Но, строго говоря, она не идеальна. Да, это неплохие тесты типа решения EG и всего остального, но они как раз хорошо и решаются тем, что мы заливаем все данными, и все работает. Подливаем данных, увеличиваем compute, На этих бенчмарках все растет. Но у меня есть такое ощущение, что эти бенчмарки, в общем-то, не совсем адекватно отражают сложность того интеллекта, который при этом все хотят создать. И в этом месте, мне кажется, автор статьи несколько сам себе... Ну, это вот называется wishful thinking. То есть у меня вот ощущение такое здесь. Я не до конца, до сих пор меня до конца не убедили, что вот просто брукфорстными вычислениями мы все вот так заливаем. Мне кажется, это не совсем так. Такая вот у меня простая мысль. Нужны какие-то новые существенные алгоритмические идеи, которые еще не родились. Поэтому я не до конца верю, что все будет так, как нам предсказано. Мне кажется, все эти кривые пойдут на спад и не будут так сильно расти. Такая мысль, простая. 

S08 [01:41:10]  : Анатолий, спасибо. Владимир? 

S03 [01:41:18]  : Владимир, это я. Я не знаю, мне что-то в следе очень плохо, я спрятался, подключался. Вас слышал. Нет, не услышал. Но сейчас пока вроде слышно, а если прервётся... Да, слышно, слышно. Ну, общее впечатление такое, что... То есть главное это что? Увеличить объем вычислений, это даст рост продаж системы, соответственно рост стоимости, ну и много чего полезного с точки зрения менеджера. Теория это же такая неощутимая вещь, она вроде бы как не нужна. Мы что угодно купим, а соответственно ничего не нужно. Это вот общее впечатление от всего обсуждения, которое было. Я не считаю, что Брутфорс даст эфир. Хотя Антон вспоминал, как он запомнил, что через 2 года будет сильный искусственный интеллект. А я повторю сейчас, что через год. Вы говорили у китайцев, что через 3 года. Это я говорил 2 года назад. Вы просто представите, что это было недавно. Это было 2 года назад. Вот. Соответственно, я расскажу, почему. Я считаю, что brute force тут не поможет, и даже то, что предлагает Антон алгоритмически усилить семантическими схемами, это все равно не будет сильный искусственный интеллект. Ну, там, значит, вопрос, как бы сказать, описание сложного мира, оно, как известно, вещь такая, Сильно экспоненциально. Чем сложнее ситуацию мы хотим осмотреть, тем как бы экспоненциально растет. И вот это вот, значит, даже экспоненциальный рост производительности на фоне той сложности, которая соответствует сколько бы серьезным задачам, она, значит... Вот этих самых языковых моделей, по крайней мере с моей точки зрения, произошел именно из-за того, что анализировались тексты. А чем интересны тексты? Что там каждое слово соответствует какому-то простому объекту или явлению. Слово же оно не про весь мир, а про что-то конкретное. И вот это вот сделанное людьми разведение мира на простые объекты и явления, оно на самом деле очень эффективно. Но беда в том, что уже сейчас не один процент этих текстов исследован, а там, скажем так, заметно больше 10% всех когда-либо созданных текстов используют для обучения моделям. Это не мой прогноз. Есть такой вот Визельтер в Госниасе. Он говорит, что все, а сейчас все тексты мы изучим. И дальше с этим человеком больше ничего не написано. Мы, конечно, напишем, и я напишу, и вы напишите какие-то статьи, но на фоне того, что создавалось в человеческом тысячелетии, в 10 раз больше мы не напишем за год. И за 5 лет не напишем. Соответственно, вот этот объем знаний, который машина получила из человеческих текстов, он, грубо говоря, уже практически исчерпан. И, соответственно, это будет важным ограничением роста. Но, значит, то, что там можно избежать каких-то галлюцинаций на основе... Они не содержат галлюцинаций, что, в общем, тоже такая сомнительная, на мой взгляд, гипотеза. Вот. Конечно, да, можно там всевозможные ограничения вводить, просить рассуждать, что-то ещё. Но это всё использование тех знаний, которые накопило человечество. Это не АГИИ. Это не сильный искусственный интеллект, который сам может получать знания. А это, значит, машина, которая обрабатывает человеческие знания и их как-то интерпретирует. Выглядит разумно, хорошо, да. Читает как по-писанному, да. Но, соответственно, новых знаний при этом машина не получает. И, собственно, пользуется не собственным развеянием мира на простые объекты, явления, а тем, который сделан человеком. Ну, это просто проиллюстрирует тем, что оказалось, что не только тексты так хорошо обрабатывают, но и изображения. И сейчас вот даже видео там научились ссору обрабатывать. Казалось бы, в тексте же там нет никаких изображений, но польза от соединения обработки изображений с LM на рамочке дышат. И я опять-таки вижу в том, что используется именно вот эта вот композиция мира, на простые объявления, из которых потом расследуется обратная композиция в целом. И это вот как бы является центральной частью чего-то нехватаемого. Вот. Ну, как бы то, что передо мной сказал. что будут нужны новые алгоритмы, которые будут заработать, конечно, будут заработать в целом, но, как бы, не ставят никого интересу на это, как, например, ПТС рассказывает. И, соответственно, пока, значит, менеджеры делом не помогают, и, собственно, пока неразборчивость серьезна с тем, то, соответственно, его раздавать будут, в общем-то, с большими деньгами, Я понимаю, что связь плохая по вражеским лицам. Я правильно понимаю? Вот. Ну, тем не менее, попробуйте услышать. Вот. И, соответственно, есть, на самом деле, уже некоторые разработанные идеи, еще, конечно, какие-то. Надо разоблачивать все это, заполнить работающие программы. Вот. Но основная идея состоит в том, что Вся эта работа строится на знаниях. И пока мы используем знания, заложенные человеком, ну, с моей точки зрения, это не сильно искусственный интеллект. Это искусственный интеллект, обрабатывающий знания человека. Вот если очень коротко, я, конечно, могу еще очень долго рассуждать. Но поскольку связь плохая, я так могу сказать, что у меня не очень... И осталось немного народу, кому это интересно. Но, тем не менее, если есть ко мне какие-то вопросы, я могу ответить. Если нет вопросов, ну и хорошо. 

S08 [01:47:27]  : Владимир, большое спасибо. У нас, как обычно, мнения разделились. Есть еще буквально 5 минут, если кто-то хочет еще дополнить. Хорошо. Коллеги, всем спасибо за обсуждение. Позиции были озвучены. Позиции, как мы видим, достаточно различные, но у нас нет, к сожалению, возможности их свести к общему знаменателю. Поэтому мы расстанемся с пониманием плюрализма мнений. И в следующий четверг встречаемся. Дмитрий Салихов расскажет более развернута свою позицию о том, что может получиться из больших языковых моделей при движении в сторону AGI. Коллеги, всем до встречи и до свидания. Спасибо за участие. До свидания всем. 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
