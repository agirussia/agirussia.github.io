## 30 мая 2024 - Интерпретируемая обработка естественного языка (известное и неизвестное) - А. Колонин — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/PRmST_EnJsc/hqdefault.jpg)](https://youtu.be/PRmST_EnJsc)


Основные моменты:

 - Докладчик рассказывает о своих исследованиях в области интерпретируемой обработки естественного языка, в контексте развития трансформерных и больших нейросетевых моделей. Он будет сочетать теоретические рассуждения и практические кейсы.
 - Докладчик вводит понятие "символьных моделей" - моделей, где элементы представлены символами (концептами, словами, логическими операциями), которые могут быть нечеткими или вероятностными. Это противопоставляется "несимвольным" нейросетевым моделям.
 - Докладчик демонстрирует, как символьные модели на основе грамматики связей и глубоких шаблонов могут решать задачи обработки естественного языка, такие как определение частей речи, сегментация текста, генерация текста, вопросно-ответные системы.
 - Основная проблема символьных моделей - отсутствие надежных методов их автоматического построения. Докладчик рассказывает о своих попытках решить эту проблему, используя метрики вероятности и неопределенности переходов между символами.
 - Докладчик сравнивает символьные и нейросетевые модели, отмечая, что они могут быть вычислительно эквивалентными, и что механизмы внимания, успешно применяемые в нейросетях, могут быть адаптированы и для символьных моделей.
 - В заключение докладчик обсуждает перспективы интеграции символьных и нейросетевых подходов для построения более интерпретируемых и эффективных систем обработки естественного языка.



S00 [00:00:09]  : Коллеги, всем добрый вечер. Спасибо всем, кто пришел на сегодняшний семинар. Сегодня хочется в очередной раз поговорить о так называемой интерпретируемой обработке естественного языка. При этом я не буду… Причем при этом хочется поговорить об этом, безусловно, в контексте всего того, что говорится и обсуждается в нашем сообществе и за его пределами по поводу современного прогресса или даже прорыва в области трансформеров и глубоких и больших нейросетевых моделей. С другой стороны, не хочется повторять аргументацию «за» и «против». Я буду предполагать, что вся аргументация, которая идет в последнее время в пользу и супротив что она всем присутствующим и тем, кто слушает данный разговор, очевидна и известна. Там, может быть, не все очевидно, кто-то, может быть, с чем-то поспорит, но, по крайней мере, все аргументы будут предполагаться известными. Я к некоторым из них буду отсылаться и буду говорить в этом контексте. При этом отчасти, причем в существенной части, я повторю некоторое положение и представлю некоторые результаты, которые я представлял год назад. Просто я сделаю акценты на некоторых вещах, которые, как мне кажется, в том контексте, в котором мы сегодня обсуждаем эту тему, были мало обозначены в прошлом моем выступлении, но и с другой стороны я покажу все-таки некоторые новые результаты, которые могут быть интересны в разных аспектах. Опять-таки вот в контексте развития современного НЛП вообще и интерпретируемого в частности. Итак, сегодня у нас будет перемешка немножечко общих теоретических рассуждений, потом немножечко прикладных кейсов, потом немножечко больше теоретических и общих рассуждений, потом еще немножечко больше прикладных кейсов. И теоретические рассуждения будут идти в консексте так называемых нечетких или вероятностных, или основанных на мягкой логике, soft logic, символьных моделях. Слово «модели» берем в кавычки, потому что под моделями может подразумеваться все, что угодно. Это могут быть нейросетевые модели, это могут быть модели грамматик, это могут быть векторные модели, это могут быть модели, основанные на логическом выводе. Но мы будем говорить о символьных моделях в том смысле, что мы будем предполагать, что если не вся, то существенная часть элементов этих моделей будет представлена так называемыми символами, где под символом мы подразумеваем либо некоторый концепт, который не выразим в прямом виде человеческими словами, Допустим, нечто желтое, мягкое, сладкое и одновременно кислое, что, видимо, можно проинтерпретировать как апельсин, но не точно. Но, тем не менее, мы можем говорить, что это некоторый концепт, что это некоторый символ. который может быть на самом деле размытым символом. То есть, мы специально говорим именно о мягкой логике, потому что символы являются нечеткими. И точно такими же нечеткими символами могут являться слова или буквы. Более того, некоторые символы могут иметь амонимию или неоднозначность. Допустим, если мы смотрим кружочек, Вне контекста мы не можем с уверенностью сказать, видим ли мы ноль или мы видим единицу. То есть, мы видим некоторый символ, который не имеет однозначного семантического значения с точки зрения отношения его к другим окружающим символам, но когда он оказывается в контексте, мы можем проинтерпретировать этот нечеткий символ как более четкий символ, где кружочек соответствует либо символу ноль, либо символу, соответствующему букве О. И третья категория символов, про которые мы будем говорить, – это логические операции, которые тоже в каком-то смысле их нельзя в явном виде назвать символами, в том плане, что они не означают нечто предметное. Но они, мы можем сказать, обозначают некоторые действия. Например, элементарные символы, используемые в булевой алгебре – это операции и, и, или, and, or, как показано на левом слайде. Вместе с тем мы можем говорить о некоторой мягкой логике или так называемой soft logic, когда у нас самоопределение и операции результатов Соединение некоторых входов может описываться нечеткой или мягкой логикой. То есть, мы можем сказать, что если А и Б с какой-то вероятностью являются истинными, то С с какой-то вероятностью тоже будет истинным. И математические операции соединения выходов a и b и трансформации истинности a и b в истинность c, оно может описываться различными математическими аппаратами. вероятностной логикой в стиле Бена Герцеля и его команды, вероятностной логикой в стиле Евгения Витяева, байесовской логикой, неаксиматической логикой Пиванга. И, наверное, есть другие варианты. Может быть, даже кто-то здесь присутствующий их назовет. Но, так или иначе, мы будем говорить о том, что, и вот, собственно, этот слайд сейчас это пытается проиллюстрировать, что, в принципе, мы можем любое графовое представление знаний, которое здесь показано с правой стороны, где вершинам графа не соответствует ничего, кроме облака связи с другими вершинами графа, Можно привести соответствие на левой стороне некоторому другому графу, который может быть функционально, или с точки зрения тех операций, которые левый граф и правый граф производят над входами, получая некоторый выход. С точки зрения вот этих операций, они будут эквивалентны. или примерные эквиваленты. Разница будет только в том, что в графе с правой стороны у нас не будет четкого ассоциирования всех вершин этого графа с какими-то значимыми, символьными понятиями. А с левой стороны у каждой вершины или у большинства вершин будут некоторые ассоциации, или концепты, или логические операции, которые мы сможем ассоциировать с каждой вершиной. Например, вот здесь показано на этом слайде схема того, как мы можем осуществлять разметку части речи по part of speech taking для текста на английском языке двумя способами. Либо это у нас нейросетевая модель с правой стороны, либо это у нас грамматика связи с левой стороны. С правой стороны мы можем просто построить некоторый классификатор на многослойной нейронной сети, на каком-нибудь перцептроне. где мы натренируем определение части речи для слова в контексте слов его окружающих. То есть, например, если мы возьмем предложение «Айсо да расти со соин», мы наверняка можем на достаточно большом корпусе натренировать нейросетевую модель, которая, если ей на вход дадут позиции слов, окружающих слово «со», она сможет предсказать, что слово «со» в данной позиции является существительным. И, с другой стороны, мы можем построить некоторую формальную грамматику. Это может быть грамматика связей, или это может быть, значит, универсальная грамматика, universal dependencies, где с помощью формальных правил Мы опять-таки можем решить задачу определения того, какой частью речи является слово «со», в зависимости от окружающих его слов. При этом, допустим, если с левой стороны мы здесь посмотрим, мы обнаружим, что мы имеем некоторый семантический граф с двумя скрытыми слоями, где на верхнем слое у нас входящие слова в их последовательности. На втором слое у нас имеются типы связей у грамматики или граммар. На третьем слое или на втором скрытом слое семантического графа у нас операция логической конюнкции. С правой стороны мы можем представить подобную сеть. Естественно, соединение в этом семантическом графе на втором слое происходит по дизюнсу. И, соответственно, если мы понимаем, что слово «со» находится в определенном контексте и связывается, и совпадают два контекста, где слово «со» попадает в категорию, оказывается связанным с одной стороны с одним существительным... Оказывается в определенном грамматическом контексте. то мы можем по операции конъюнкции сделать вывод, что это слово является существительным. Причем, оно может являться существительным, как в случае, если оно оказывается в грамматическом контексте как подлежащего, так и как объекта, так и субъекта. То есть, пила могла что-то пилить, а мы могли что-то делать с пилой. С правой стороны мы точно также можем видеть, что вот эта операция определяется некоторыми неявно выраженными логическими операциями. То есть, если мы предположим, что функции агрегации входных сигналов на нейроны. Функция агрегации сигналов между слоями нейронов линейная. И скажем, что допустим порог срабатывания нейрона заключается из искусственного нейрона имеет единицу. Соответственно, если выход нейрона равняется единице при сумме входов всех входящих нейронов равных или больше единицы, то в этой ситуации мы будем иметь дело с нейроном типа И вот это соответствует этому нейрону на втором снизу слое, когда три или четыре нейрона на втором слое сверху. Если комбинация всех их совпадает, то есть, если у нас 0,3, 0,3, 0,3, 0,3 здесь складываются, то мы получим в сумме единицу, и тогда этот нейрон Работает как И. То есть, нам нужно, чтобы сигнал поступал со всех входов предыдущего слоя. А в другом случае, например, если мы возьмем выход с предпоследнего слоя на последний слой, то у нас функция нейрона на последнем слое может быть другая. То есть, эта функция может быть Или. То есть у нас достаточно сигнала с единицей либо с одного входа, либо с другого входа. И тогда, если у этого нейрона весовые связи всех нейронов на входе равняются единице, он будет работать как суммирующий нейрон. втором снизу слоя каждый отдельный вход меньше единицы, и только сумма всех весов на входе равняется единице, то тогда этот нейрон будет работать как или. При этом агрегирующие или передаточные функции нейронов и пороговые функции у них все могут быть одинаковы, но распределение весов неявно будет говорить о том, что те или иные нейроны работают либо как скорее «и», либо как скорее «или», либо как исключительно «или», либо как исключительно «или», либо как нечто промежуточное, непонятное, то ли это «и», то ли это «или». Ну и, очевидно, можно предположить, что эти нейроны будут не очень информативны. Обратим внимание, что если мы перейдем на левую сторону этой диаграммы, то у нас логика также не обязательно может быть жесткой или детерминистической. Логика также может быть вероятностной. В том же самом Lingramar, например, предусмотрена возможность назначения так называемых стоимости или костов всем связям или коннекторам, с помощью которых описывается грамматика. И мы можем, по сути, осуществлять нечеткие вычисления и вероятностные оценки При принятии решений является ли данное слово существительным или глаголом, и вприменительно к другим задачам по парсингу, генерации, сегментации, которые мы будем на основе грамматики связи рассматривать в дальнейшем. То есть, мне хотелось бы здесь подчеркнуть то, что если мы говорим о символьных моделях при обработке естественного языка, это совершенно не означает, что они основаны на жесткой ибулевской логике. То есть, логика может также предполагать наличие некоторых весов или вероятностей на всех связях графа при описании модели и на всех вершинах графа при динамическом вычислении текущих значений при выводе на основе этого графа. Ну и, наконец, когда мы говорим об эквивалентности, мне хотелось бы еще раз подчеркнуть, что если мы говорим о сложности вычислительной, то мы можем говорить о том, что Весь вопрос заключается в том, каковы у нас функции, с помощью которых мы рассчитываем вероятности или активации, или активности, или экспрессии значений на тех или иных вершинах графа, количеством нейронов, структурой слоев, количеством соединений между нейронов, И является ли эта модель символьной или сабсимвольной? Есть ли у нас этикетки или лейблы на этих вершинах или нет? Это исключительно вопрос интерпретируемости. То есть, если мы берем, скажем так, некоторую нейросетевую модель и умудряемся с помощью какого-то реверс-инжиниринга или различных методов, которые обсуждаются в разных статьях, и в том числе у нас на семинаре был подобный доклад, если мы умудримся на нейросетевой модели каким-то образом расставить концепты на большом числе ее нейронов, она станет в какой-то степени символьной, потому что мы её смогли описать. Конечно, для современных глубоких нейросетевых моделей это невозможно, потому что у нас получается слишком большое число концептов. Но мы можем сказать, что да, у нас много концептов, они просто очень сложно описываются через другие концепты. человеческой памяти и языка может не хватить для того, чтобы выразить всё это многообразие этих концептов. Но мы можем гипотетически предположить существование некоторого суперязыка, который через концепты верхнего уровня может описать концепты любого глубинного уровня. Как, собственно, мы и делаем в естественном языке, говоря о том, что вот есть нечто такое – жёлтое, круглое, кислое и сладкое, невразимое словами, но мы понимаем, что это либо лимон, либо апельсин. Дальше я хотел бы проиллюстрировать, как это делается в грамматике связи. Опять-таки, извиняюсь, если для кого-то это повторюсь. Когда мы описываем грамматику связей, мы описываем множество грамматических категорий. Эти грамматические категории могут складываться в некоторой иерархии. К сожалению, и это одно из больших, с моей точки зрения, ограничений грамматики связей, значит, что иерархия вот этих вот категорий, она не позволяет уходить на уровень семантики. даже минимально глубоко. То есть, количество уровней ограничивается исключительно грамматической структурой, составляющей языка, и исключает имение дела с амнимией и с какими-то семантическими отношениями. Но, тем не менее, мы можем описать такие грамматические категории, как одушевленные существа, Те взаимоотношения, которые этими одушевленными существами могут осуществляться в виде глаголов. Допустим, глагол «честь» и возьмем пару существительных «кэт» и «снэйк». Мы обнаружим, что у нас «кэт» и «снэйк» может оказаться в грамматической позиции, где слева будет определитель. Вот определитель предполагает, что с правой стороны, точнее, артикль предполагает, что связь D направо идёт к некоторой грамматической сущности, которую слева предполагает определитель. То есть, мы видим, что есть коннектор D, который идёт от артикля вправо, и коннектор D, который идёт от соответствующего одушевлённого существителя. картикулю влево. И если у нас типы вот этих вот коннекторов, в данном случае это коннектор типа D Determiner, совпадают, то мы можем выстроить грамматическую связь между этими двумя частями речи. Аналогично мы можем сказать, что у нас Вот эта вот часть речи, которая в данном случае является одушевлённым, существительным, более конкретно, животным, она и может иметь связь, ведущую к объекту. в левую сторону и может иметь связь, идущую к субъекту в правую сторону, где объекты и субъектом могут выступать какие-то определенные части речи в словаре нашей грамматики. При этом мы можем явно указывать так называемые косты или стоимость этих связей. И вот в данном случае, допустим, связь к субъекту имеет вес 0,5. Соответственно, она менее дорогостоящая. А связь, ведущая к объекту, она имеет вес 1,0, соответственно, более дорогостоящая. И, соответственно, если при формировании грамматического разбора, либо генерации речи, либо решении задачи сегментации, практические задачи, которые мы можем решать на основе грамматики связей, если мы эти веса будем учитывать но очевидно что мы будем стараться использовать более дешевые связи да то есть можно сказать что связь от кота и змеи и змеи вправо она является более дешевый то есть более вероятностный то есть мы вот из вот этого распределения связи можем сказать что кот и змея Статистически, в терминах того корпуса, на основе которого строилась эта грамматика, чаще оказываются в роли объекта, нежели в роли субъекта. То есть, они тоже могут оказаться в роли субъекта. мы можем сформировать предложение, где, допустим, man chased cat. То есть, человек гнался за котом. То есть, мы видим, что у нас, например, Допустим, если мы предположим ситуацию, что здесь у нас находится мэн, человек, который гонится за котом или за змеей, то вот эта связь оу, идущая налево, вот эта связь, она является более дорогостоящей. То есть, получается, что реже бывают ситуации, когда за змеей или за котом кто-то погнался, нежели кот. Или змея гонится за кем-то другим. То есть, мы в нашем корпусе чаще встречали фразы типа «со змея погналась за лягушкой», «кот погнался за мышкой», нежели мы встречали фразы типа «машина погналась за змеей» или «человек погнался за котом». И это как раз отражается вот этими костами или стоимостью. Проблемой грамматики связи является то, что в существующих грамматиках вот эти косты или стоимости, они, во-первых, задаются вручную. То есть, не существует на сегодняшний день сколько-нибудь внятной технологии, которая позволяла бы автоматически формировать вот эти вот косты, исходя из тренировочных корпусов, так как это делается в глубоких нейросетевых моделях. Ну и если мы заглянем в существующие грамматики для английского и других языков, В грамматике связи мы обнаружим, что реально эти косты используются в очень небольшом числе случаев, и они добавлялись там просто конкретными лингвистами, которые составляли словари для этих языков для того, чтобы подогнать, грубо говоря, результаты разбора парсера под адекватное поведение на тех тестовых корпусах, на которых они эту грамматику тестировали. Чтобы не зацикливаться на грамматике связей, здесь показан другой способ моделирования языков. Это те так называемые глубокие шаблоны, которые мы развиваем в своих работах. И в этих глубоких шаблонах мы предполагаем, что любая языковая модель в рамках конкретного тренировочного корпуса. То есть, когда мы говорим «модель», мы говорим о том, что эта модель аппроксимирует некоторый корпус. Как она аппроксимирует? Это тема дальнейшего разговора. Но когда она аппроксимирует этот корпус, она его аппроксимирует каким-то графом. Глубокие нейросетевые модели описывают его своими графами. Грамматика связи может описаться своими графами, в которых, к сожалению, нет семантики, и поэтому эти модели очень неточные. Наш подход, который я здесь показываю, он предполагает снятие ограничений на число слоев. или в семантических графах, которые мы можем использовать. И мы можем строить сколь угодно сложные иерархические шаблоны. Вот где элементом любого шаблона является либо последовательность, либо некоторый слот в этой последовательности, где каждый слот может являться либо конкретным ключевым словом, либо дизъюнцией каких-то слов, либо конъюнцией каких-то слов, поставленных в любом порядке. И из вот этих последовательностей коньюнций и дизюнций, трех базовых примитивов, точнее ключевых слов коньюнций, дизюнций и последовательностей, мы можем строить какие угодно иерархические шаблоны. При этом веса на ребрах этих графов, они опять-таки могут тоже быть вероятностными. То есть, вот здесь вот показано, что у нас, например, если мы смотрим шаблон на выявление некоторого лекарства, которое потенциально может снизить ощущение экзистенциальной фрустрации, мы можем предположить, что если у нас используется Т-линол, то или ацетаминофен в качестве лекарства, то толенол будет использован чаще, чем ацетаминофен. А аспирин будет использоваться чаще, чем ацетилсалитиловая кислота. Чисто в силу вероятностных свойств того корпуса, на котором мы тренировались. И отличием нашего подхода от грамматики связи заключается то, что с помощью грамматики связи мы можем решить ограниченное число задач, то с помощью шаблонов различного размера мы можем, в принципе, решить любую задачу. Например, здесь показано, как с помощью этих шаблонов могут решаться три базовые задачи обработки естественного языка. Либо у нас достаточно большой шаблон, который исчерпывающим образом описывает возможные фигуры речи и паттерны, которые встречаются в медицинской литературе. И, соответственно, с удовлетворением этому шаблону говорит о том, что данный большой кусок текста относится к области здравоохранения. Либо мы строим более маленький шаблон, который описывает все возможные способы взаимодействия некоторых медицинских процедур или препаратов с некоторыми симптомами. которые эти препараты должны подавлять. И, соответственно, сформировав этот шаблон, мы можем в большом тексте выявлять некоторые пятна или некоторые сегменты текста, которые описывают. Вот это вот взаимодействие между некоторой медицинской процедурой или медикаментом и симптомом, который они либо снимают, либо не снимают, либо, наоборот, усиливают. Ну и, наконец, более узкие и еще более специфические, еще более точные шаблоны позволяют, собственно, уже извлекать атрибуты. из сегментов текста, описывающих некоторые многопозиционные или энарные семантические отношения. При этом определение атрибута является более широким понятием, нежели Entity Named Entity Extraction, потому что мы извлекаем не Named Entity, а мы определяем атрибуты. Мы определяем семантические значения с семантическим отношением, где, допустим, репортер – тот, кто в данном случае докладывает о определенном взаимодействии определенного препарата с определенным симптомом – это в чистом виде Named Entity. Действующее вещество – это в чистом виде Named Entity, бренд – это в чистом виде Named Entity. А вот, допустим, какой эффект мы обнаблюдаем, позитивный или негативный, надежность определения этого эффекта, собственно, тот диагноз, который мы снимаем или ставим, Это по сути определение каких-то значений, каких-то семантических категорий в той онтологии, которая у нас находится на верхних слоях нашего семантического графа. Поговорим о некоторых практических примерах. На основе технологии глубоких шаблонов в свое время была разработана система, которая позволяла, задавая набор подготовленных семантических шаблонов, применяя их текстом на естественном языке, осуществлять определение атрибутов. в текстах на естественном языке для того, чтобы соответствующие атрибутированные записи можно было помещать в структурированную реализационную базу данных для последующей обработки. Второй пример – это Кейс, который мы реализовали в рамках проекта по финансовому анализу крипторынков. Задача у нас была следующая. Мы пытались поэкспериментировать с тем, как сентимент или тональность, позитивная или негативная, текстах на естественном языке в социальных сетях влияет на рынок. Забегая вперёд, скажу, что с сентиментом получилось не очень хорошо, гораздо лучше получилось с когнитивными искажениями, но начали мы сентимент анализа. И что мы попробовали? Мы попробовали порядка двадцати с лишним моделей определения тональности. Большая часть моделей – это модели с открытым кодом. С левой стороны, второй прямоугольничек – это была наша модель, интерпретируемая, основанная на n-граммах, где особенностью определение сентимента по N-граммам был так называемый принцип приоритета относительно порядка. То есть, если у нас, грубо говоря, есть несколько N-грамм, если у нас, скажем так, в словаре, характеризующем позитивный или негативный сентимент, встречаются N-граммы различных порядков, И если в тексте, ну, допустим, там «неплохо», да? Допустим, у нас встречается н-грамма «неплохо» в позитивном словаре, а н-грамма… а юниграммы «не» и «плохо» встречаются в негативном словаре, да? И если бы у нас в одном месте было там «не» Допустим, получается в одном месте текст, а в другом месте было бы написано «всё плохо», то отдельно взятое «не» и отдельно взятое «плохо» давали бы негативный сентимент. Но если у нас в тексте встречается «не плохо», там все получилось неплохо или там неплохо выступили, то N-грамма, дающая позитивный сентимент, она забирает на себя униграммы или N-граммы более низкого порядка, и алгоритмы работают таким образом, что N-граммы более низкого порядка, они входящие в детектированные N-граммы более высокого порядка, они просто игнорируются. И вот эта наша модель из коробки показала при первом пробеге. результаты сопоставимые на долю процента больше оказалось, чем модель рекордсмен – это модель Берта, натренированная на финансовых данных. То есть специализированная модель на финансовых рынках показала примерно такой же результат, как наша не специализированная модель из коробки. После чего, естественно, у нас стала задача улучшения точности. У нас не было тренировочного корпуса, на котором бы мы могли дотренировать берд. для того, чтобы он существенно улучшил свои данные, и у нас не было возможности создать этот размеченный корпус. Но поскольку наша модель была интерпретируемая, и у нас была возможность интерпретировать все те ошибки, которые оказались выше той точности, которую мы получали, мы смогли экспертным способом, что называется, эту модель допилить, то есть мы просто на экспертном уровне расширили те энграммы, которые перечень тех энграмм различных порядков, которые давали ложное срабатывание и по аналогии с ними добавить те энграммы, которые вроде как должны были бы в других случаях работать. То есть, например, если мы понимаем, что на финансовом рынке используются какие-то определенные конструкции, и эти конструкции, мы видим, у нас не срабатывают, потому что у нас нет, мы добавляем те конструкции, которые мы пропустили, ну и понимаем, какие конструкции надо добавить, добавляем эти конструкции. Вот в результате чего мы очень быстро в полтора раза, даже больше, чем в полтора раза увеличили свою точность. Ну и вот этот вот алгоритм у нас сейчас используется в нашем проекте новостного агрегатора для анализа сентимента новостного потока. То есть мы можем анализировать и сентимент отдельно взятых новостей, и мы можем агрегировать позитивный и негативный сентимент по темам. под которыми агрегируются эти новости и можем даже выявлять так называемую противоречивость. Если у нас какая-то тема получает как позитивные, так и негативные новости, мы можем считать некоторое соотношение позитива и негатива в этих новостях и получаем то, что мы называем меру противоречивости. То есть, если новость одновременно сильно негативная и сильно позитивная, то у нее возникает метрика повышенной противоречивости. Кстати, сейчас в последней версии алгоритма у нас даже по отдельно взятому тексту может считаться как позитив, так и негатив, так и противоречивость по отдельному тексту, не только по всей теме. Ну и вот второй пример того, как мы используем этот сентимент-анализ сейчас в нашем боте, который функционирует у нас в сообществе. когда не падает, иногда падает, но вот если он не падает, то он сейчас с помощью этой технологии, во-первых, определяет сентимент новостей и постов, которые проходят в группе. При этом, если положительность новости превышает некоторый порог, он на неё реагирует улыбкой. или, наоборот, печалью. Кроме того, в какой-то момент мы добавили аналогичную модель по определению грубости или хамства. И на основе этого работает модерация. То есть, если новость имеет показатель грубости или хамства выше какого-то порога, не очень большого, Бот реагирует на эту новость вот такой вот смущенной рожицей с вытаращенными глазами и жалуется админу. То есть, ссылка на текст и сам текст он форвардится админу чата. А если порог грубости и хамства зашкаливает, то новость автоматически удаляется или пост автоматически удаляется из группы, то есть происходит автоматическая модерация. Ну и опять-таки админу летит жалоба на того, кто эту новость написал. Как эта работа описана в нашем посте на медиуме. Дальше пример. Последняя работа – это мой студент китайский сделал в рамках своей курсовой работы. Сейчас подготовили статью на нейроформатику. Он попробовал нашу модель посравнивать с различными моделями определения сентимента для китайского языка. При этом особенностью и дополнительной сложностью оказалось то, что для того, чтобы применять Эту модель, а также другие модели на китайском нужно правильно делать сегментацию. То есть, китайский язык, как мы знаем, он пишется слитно, пробелов там практически нет. Более того, как мы выяснили в ходе другой работы, про которую я, надеюсь, успею сегодня рассказать. Если не затяну, мы определились, что там пробелы в китайском языке примерно так же, как расставляются интонации в русском языке. То есть, они, так сказать, имеют такой эмоциональный характер и интонационный характер. Можно ставить пробелы, можно не ставить пробелы. Но, в общем, смысл мало поменяется. Изменяются только какие-то акценты с помощью этих пробелов во многих случаях. Тем не менее, оказалось, что способ сегментации вкупе с моделью определения сентимента в китайском языке сильно зависит от выбранной технологии. То есть, если брать модель определения сентимента стандартную генеративную, статья, если будет опубликована, я пришлю ссылку, она показала вот такую точность. верхнем столбике. А дальше идут различные способы определения сентимента rule-based. Это снова NLP. И EIGENS – это наша модель, основанная как раз на N-граммах с алгоритмом приоритета на порядок N-граммы. Ну и с левой стороны здесь через плюсик показана модель сегментации и модель определения сентимента. Соответственно, либо разбираем на слова по простому алгоритму и используем либо e-agents, либо снова NLP модель определения сентимента, либо используем алгоритм выделения n-грамм. Как описано в статье, используем либо e-agents, либо снова nlp. И еще используем токенайзер djeba и либо e-agents, либо снова nlp. Определяем сентимент. В общем, показалось, что если использовать токенайзер djeba и нашу модель определения сентимента на основе ngram, точность из коробки получается максимальная. Что нас порадовало. Следующий пример использования нашей технологии, тоже отчасти старая, отчасти свежие результаты другой моей студентки – это использование когнитивных искажений. Когнитивные искажения – это история из психотерапии. То есть, в когнитивно-поведенческой психотерапии диагностика ставится в пространстве десяти с лишним так называемых типичных когнитивных искажений, типа человек думает в терминах «всё» либо «ничего», это all or nothing thinking, или человек преувеличивает как позитивные, так и негативные явления своей жизни. Это overgeneralization. Filtering – это человек игнорирует всё хорошее и концентрируется только на плохом. Катастрофизация – это когда, если есть что-то плохое, то человек считает, что это разрушит его жизнь, ну и так далее. Причём в разных школах когнитивно-поведенческой терапии Используется разная немножечко система классификации. У кого-то 12 этих когнитивных искажений детектируется, у кого-то 14, у кого-то 16. И проблема заключается в том, что размеченных корпусов практически нет от слова совсем. То есть, когда мы начали заниматься этой темой, корпусов найти вообще не удалось, но удалось найти некоторую работу, где были некоторые базовые словари для детекции этих когнитивных искажений на основе н-грамм для английского языка. Мы эти словари взяли, применили к ним свой алгоритм N-грамм с приоритетом по порядку N-грамм более высокого N над более низким N и попытались применить метрики по всем когнитивным искажениям. Посмотреть корреляцию этих метрик с движениями цены, объемов торгов и количества сделок на рынках криптовалют. у нас получилось на верхнем графике у нас синим цветом идет производная цены на то есть это не цена это производная да то есть если цена идет вверх это пик если цена идет вниз это провал вот с Оранжевым цветом идёт некоторая сводная метрика комбинации трёх когнитивных искажений. И мы видим, что в большинстве случаев значимые максимумы, значимые всплески цены, они предваряются значимыми всплесками вот этого нашего комбинированного когнитивного искажения. По самой цене это заметно не очень хорошо, а по объёмам торгов это значительно более выражено. То есть, по объёмам торгов у нас корреляция со сдвигом. Если мы сдвинем график комбинированного когнитивного искажения на один день вперёд относительно графика объёмов торгов или количества сделок, то мы получим достаточно хорошую корреляцию. По цене тоже получим, но там будет существенно меньше на самом деле. аж корреляция это хорошо, мы попытались еще использовать предсказатели на основе нейросетей в данном случае, то есть предсказание строилось по нейросетевому алгоритму, но в качестве так называемых фич, которые мы использовали для предсказания цены на крипторынке, использовались как раз вот все наши когнитивные искажения и в том числе сентимент, и на основе этих предсказаний строились торговые стратегии. И что мы увидели? Мы увидели, что если пытаться строить торговые стратегии, как показано, с правой стороны, не используя предсказаний, то по самым разным стратегиям, вот здесь вот разные прямоугольнички с правой стороны показывают разные стратегии с разными параметрами. Что это за стратегии – это тема отдельного разговора, это не наш сегодняшний семинар. Но мы видим, что практически все стратегии оказались провальными, соответственно, красные стопики показывают убытки, и только одна стратегия показала небольшую прибыль, если мы не пытаемся предсказывать. Если же мы на левой стороне возьмем результаты, где мы пытались опять-таки по всем тем же самым стратегиям еще и предсказывать движение цены на основе наших метрик, полученных из текста, мы обнаружили, что по половине стратегий мы получили существенные прибыли. Эти прибыли гораздо более прибыльные, чем убытки, полученные по оставшимся стратегиям. При этом, что интересно, мы пытались строить предсказания как на основе сентимента, так и на основе когнитивных искажений, так и на основе объединения сентимента и когнитивных искажений. По сентименту получился эффект мизерный, по когнитивным искажениям эффект получился хороший. Если их смешать, то он получился… Примерно такой же, как только по когнитивным искажениям, но чуть-чуть лучше. Ну и третий пример, естественно, если мы говорим о когнитивных искажениях, которые используются в психотерапии, естественно, соблазнительно было попробовать о том, как мы сможем использовать автоматическую диагностику когнитивных искажений в психотерапии. Вот здесь показан пример некоторого гипотетического виртуального диалога, где в помощь психотерапевту, который задает конкретные вопросы пациенту, Система диагностики определяет как когнитивные искажения, так и социальные ситуации, так и эмоциональный контекст на основе предложенной технологии. И, соответственно, в данном случае разметка разным цветом показывает теги, такие как персонализация в качестве когнитивных искажений, увольнение в качестве социальной ситуации, тревога и раздражение в качестве эмоций. Таким образом, в интерактивном режиме система помогает специалисту ставить диагноз. Это результаты нашей работы со СБЕР, которую мы выполняли пару лет назад. Ну и вот результаты студентки, которая тоже подготовила вместе со мной свежую статью, мы ее подали на нейроинформатику. Это пример, это попытка сопоставить точность нашей технологии с нейросетевыми моделями. Удалось найти два датасета, где есть и тестовые, и тренировочные данные для анализа когнитивных искажений на английском языке. Ну и вот результаты, которые мы получили. Катастрофизацию, дисквалификацию, fine positive, fortune telling, mental filtering мы определяем не очень хорошо. На самом деле, как выяснилось, при разборе ошибок их просто было достаточно мало в нашем датасете. А большинство остальных когнитивных искажений, примерно половину когнитивных искажений, мы можем определять с точностью от 0,6 до 0,7, а где-то даже у нас есть 0,9 – 0,8. Ну и в среднем мы с точностью 0,7 можем определять наличие либо отсутствие когнитивного искажения. По горизонтали тут идут различные пороги. Соответственно, мы пытаемся подобрать порог, при котором мы принимаем решение, есть когнитивное искажение или нет. Следующий кейс, переходя от анализа сентимента и когнитивных искажений. Следующий пример будет по поводу того, как мы можем использовать грамматику связей. На основе грамматики связей мы можем строить как стандартное приложение вроде парсеров, для которых она была предназначена, чтобы получать структуру грамматических выражений и определять части речи и роли слов в предложениях для того, чтобы строить уже какую-то семантическую интерпретацию. А также мы показали, что мы можем строить приложение сегментации текста на естественном языке и генерации текстов на естественном языке. Задача сегментации это типичная задача, которая решается при переводе расшифровки аудиотекста. То есть нам нужно не просто распознать все слова, которые идут с аудиопотока, а нам нужно разбить их на предложения. Задача сегментации с помощью грамматики связи решается таким образом, что мы берем поток токенов где каждый токен является словой и пытаемся динамически строить грамматически корректное выражение. До тех пор, пока грамматически корректное выражение строится и его грамматическая корректность удовлетворяет некоторому порогу. Значит, пока это происходит, мы считаем, что у нас идет одно предложение. Как только у нас возникает разрыв грамматической корректности, мы в этом месте говорим, что тут у нас должна стоять либо точка, либо восклицательный знак, либо вопросительный знак. И начинаем строить следующее выражение. Ну а генератор, он решает задачу обратную парсеру. Если парсер у нас разбирает слово на разбирает предложение на некоторую древовидную структуру, то генератор берет поток, набор слов, которые между собой должны быть как-то соединены, и выстраивает этот поток слов в некоторую осмысленную последовательность. Это задача так называемой Surface Language Generation, которая в настоящее время чаще решается с помощью Universal Dependency. Спасибо Николаю Михайловскому, он в группе AGI Bots как раз послал пару свежих ссылок на соответствующие работы. Примеры того, как мы эту задачу решали, есть соответствующая статья по генерации текстов на естественном языке с неплохими результатами. На основе генерации текстов на естественном языке мы также решали на уровне прототипа задачи question answering, где мы не просто генерируем текст, а мы пытаемся еще сгенерировать этот текст в контексте некоторого заданного вопроса. То есть, если у нас есть некоторый текст, которым мы задаем вопрос по поводу того, какое слово в каком контексте используется то или иное слово, оно может выбрать связанные слова из этого текста и построить грамматически валидное выражение. При этом это выражение может не соответствовать тому, что находится в тексте. То есть, в данном случае мы просто не Копипастим предложение с найденным паттерном в тексте, а мы восстанавливаем его, исходя из грамматики. И в сравнении с другими технологиями мы показали неплохие результаты. Но главным вопросом, который остается и применительно к универсальным грамматикам, и применительно к пресловутым глубоким шаблонам, и применительно к грамматике связей, и главная проблема, которая стоит перед подобными подходами, заключается в том, что мы не понимаем мы в широком смысле. Мы не понимаем, как их формировать. Проблема заключается в том, что для того, чтобы создать грамматику связи для какого-либо языка, нужно много человеколет лингвистов, которые будут сидеть и вручную составлять эти грамматические правила и прописывать эти самые косты. И нет способа, по крайней мере, хорошего и надежного способа, я чуть позже пролюбуюсь по некоторым попыткам, которые мы делали, нет надежного способа автоматически эти модели строить. Если глубокие нейроязыковые модели строятся автоматически путем больших вычислительных затрат, значит и мы несмотря на непрозрачность этой модели можем увеличивать вычислительные затраты постепенно увеличивать те корпуса на которых мы их тренируем мы можем повышать их точность то здесь мы даже В первом приближении на сегодняшний день пока не имеем технологии, которая позволила бы автоматически построить хоть какую-то минимально сложную и минимально точную симульную модель. Неважно, вероятностную или невероятностную. Дальше я сейчас чуть-чуть поговорю о том, в каком направлении мы пытались ходить и куда, возможно, имеет смысл ходить. Ну и принципиальным здесь является то, что мы не можем разделить несколько процессов анализа последовательности, которые взаимосвязаны и которые необходимы для решения вот этой вот задачи. То есть, если у нас имеется некоторая последовательность символов, нам для того, чтобы построить некоторую формальную грамматику этой последовательности, нам нужно решать одновременно три задачи. Причем их нужно решать альтернативно, одновременно и делать это на нескольких уровнях. Одна задача – это сегментация последовательности. Мы должны её нарезать на морфемы, на фонемы, на слова, на предложения или на буквы, вплоть до букв, потому что где-то у нас два символа – это одна буква, допустим, «Ы», а где-то вроде как два символа – это Разные буквы. Допустим, ИИИ – это разная буква, а если бы это было И, это была бы одна буква, это был бы один символ. То есть, у нас есть односимвольные символы, вроде И, АЙ, и есть двухсимвольные символы, вроде И. Точнее, АЙ тоже двухсимвольный символ, потому что у нас есть один символ точка и один символ палочка. Просто они вертикально расположены. Итак, сегментация. Второе – это кластеризация. Нам нужно кластеризовать символы, например, все способы написания I, все способы написания A, все способы написания P, все синонимы слова вода, допустим, вода, жидкость. Око и глаз – это тоже синонимы, сгруппировать между собой по семантическому либо по семантическому признаку, либо по идеографическому признаку. Ну и, наконец, третье, нам нужно научиться делать парсинг. То есть, нам нужно их выстраивать в некоторый осмысленный граф, который позволяет строить некоторую иерархию значит взаимоотношений вот этих вот частей которые мы парсим причем парсинг может осуществляться на нескольких уровнях да с одной стороны мы можем осуществлять парсинг на уровне отдельного слова выявляя граф состоит где элементами графа являются приставки корни суффиксы и окончания а с другой стороны мы можем строить графы на уровне предложения, где элементами графа у нас будут являться, соответственно, поддержащиеся сказуемые субъекты, объекты и глаголы. Ну и то же самое видно, что токенизация и сегментация – это, в суть, один процесс, который происходит на разных уровнях. Причем разница между парсингом на уровне слов и на уровне букв или символов, она как бы перетекает с одного уровня на другой. Вот, например, возьмем парсинг на уровне слов в приложении it is impossible. То есть, у нас it соединяется с is, а is соединяется вроде как с impossible. А вот в «Impossible» есть приставка, где «им» – это часть слова «impossible». То есть, получается, у нас вроде как по словам граф состоит из трех вершин, но на самом деле внутри слова «impossible» есть еще маленький подграф морфологического уровня, а не грамматического уровня. А вот если мы возьмем то же самое предложение, которое вместо impossible пишется not possible, то у нас уже будет честный грамматический граф из четырех слов, хотя структура графа будет ровно та же самая. Если мы возьмём в китайском языке, то в китайском языке всё веселее, потому что граф у нас будет один и тот же, символы будут одни и те же, а пробелы, они могут являться произвольными. Если вы возьмёте китайский словарь, то у нас вот это вот выражение с пробелами в гугле, будет интерпретироваться вот как то, что написано наверху. И то же самое предложение, если вы его отдадите в токенизатор джиеба, токенизатор джиеба решит, что тут надо поставить пробелы. А какой-нибудь другой китайский токенизатор решит, что пробелы ставить не надо. А если вы дадите оба этих текста перевести китайцу, китаец скажет, что и в том, и в другом случае смысл будет один и тот же. Следующий вопрос – это вопрос о том, как и на основе чего мы строим языковую модель. на основе тех символов, которые мы насегментировали в процессе нашего анализа, или какие принципы мы будем использовать для того, чтобы осуществлять саму эту сегментацию. Два основных способа, которые могут быть, и о которых мы сейчас поговорим – это, с одной стороны, вероятность А с другой стороны, неопределенность. То есть, если мы имеем любую цепочку переходов различных состояний, неважно, либо это состояние в жизни человека, либо это какие-то фонемы в устной речи, либо это морфемы, или слова, или буквы в письменной речи, мы можем для каждого перехода оценивать такие метрики, как вероятность, какого-то перехода, и неопределенность при этом переходе. Прежде чем сравнить их, я поясню, что в данном случае подразумевается под неопределенностью. Предположим, что мы в гугле набираем выражение «how are» и нам гугл дает подсказку «you». И дальше идут различные варианты того, что может идти после you, но их, в общем, немного. То есть, получается, что если мы набираем how are, то все, что может предложать Google после этого, либо you, либо с какой-то исчезающей вероятностью там будет things. А вот если я наберу how many, то тут вариантов уже возникает масса. То есть, что мы видим здесь? Пока на уровне, так сказать, визуальном. Мы видим, во-первых, что вероятность how are you существенно больше, чем how are things. Это первое, что мы видим. Соответственно, Google в первую очередь предлагает how are you, а дальше там уже пошли варианты. А второе, что мы видим, что после того, как мы набрали how are, неопределенность перехода в новое состояние, она существенно меньше, нежели неопределенность перехода из состояния, когда мы набрали how many, потому что из how many У нас на следующем шаге открыто очень много дверей. Вот здесь список дверей, в которые мы можем войти после того, как мы набрали how many. А если мы набрали how are, то двери всего две – you и things. Это иллюстрируется следующим слайдом. Допустим, мы набираем how many, и после how many у нас может возникнуть people, times, countries, states. При этом каждый из вот этих people, times, country, states имеет свое распределение, имеет свою частоту. Соответственно, мы можем оценивать вероятность. А вот после how are у нас Things мы не рассматриваем, допустим, считаем, что это вероятность ничтожной, по сути, переход только один. Соответственно, у нас на каждой вершине графа, где графом является n-грамма, в данном случае вершиной графа является n-грамма how many, вот эта вот вершина графа при движении слева направо, она имеет оценку неопределенности равной 4. При этом, собственно, сами переходы имеют какие-то уже вероятностные оценки, связанные с частотностью того или иного перехода в рамках физификсированной неопределенности. На основе этих двух метрик мы пытались в той работе, которую я здесь рассказывал год назад, строить систему автоматической сегментации текста без учителя. Мы просто пытались построить систему, которая, потренировавшись на достаточно большом корпусе, и насчитав все возможные графы на всех возможных н-граммах, оценивая либо метрики изменения неопределенности, либо метрики изменения вероятности при движении слева и направо, и справа налево по последовательности символов и н-грамм различного порядка, определять точки разрыва этих двух метрик. И, соответственно, в точках разрыва, то есть при превышении какого-то порога, ставить разрывы в последовательности символов для того, чтобы определять токены. И мы обнаружили, что если мы будем брать за основу метрики вероятности, то надежной токенизации получить мы не сможем. Вот, например, как только у нас появляются какие-то знаки припинания, У нас сразу же всё ломается. Кавычки, апострофы, запятые, они приводят к большому числу фолькс-позитивов и фолькс-негативов, и в некоторых случаях даже слова рвутся на части. А вот если мы возьмем метрику в качестве опорной метрику свободы перехода, то у нас всё значительно улучшается. Например, в данном случае для английского языка, уже для униграм, у нас получается практически стопроцентная точность. Для N-грамм более высокого порядка начинают возникать фальспозитивы и фальснегативы. Ну и вот пример, который мы для английского языка получили в пространстве гиперпараметров. Мы варьировали с различными порогами по горизонтали. Мы варьировали с различными числами N по вертикали. Ну и кроме того, у нас был прунинг частотности по насчитанным графам с различными параметрами спрунинга по низкочастотным переходам. И видно, что для английского языка мы здесь получаем для порога 0,4 и N грамм равные единице получаем точность 0,99 на достаточно большом корпусе. Следующая задача, которую мы решали… Мы решали задачу автоматического подбора этих гиперпараметров, потому что здесь мы нашли этот свит-спот, что называется. Обнаружили, что нам нужна n, равная единице, а порог нужен 0,4, а порог отсечения нам нужен 1. Это мы определили на основе тестового корпуса. А что делать, если тестового корпуса нет, на котором мы можем настроить наши гиперпараметры? И мы стали пытаться посмотреть, а можем ли мы использовать какие-то метрики, которая, не зная ничего о целевом языке, то есть, когда у нас нет какого-то размеченного корпуса, на котором мы можем подогнать свои гиперпараметры, могут ли быть какие-то метрики, которые позволят нам автоматически, максимизируя эти метрики, одновременно найти те параметры, с помощью которых мы будем осуществлять точную токенизацию. И мы попробовали несколько метрик, из которых наиболее интересных оказалось первое. Чтобы было понятно, тепловые карты здесь обозначают f-меру. Соответственно, мы здесь смотрим, при каком значении гиперпараметров f-мера токенизации, то есть точность разбиения токена приближается к единице. И мы попытались покоррелировать эту эфемеру с двумя метриками, которые не связаны с разметкой корпуса. Это чисто информационные метрики. Одна метрика – это коэффициент компрессии, то есть мы просто решаем проблему токенизации как решение проблемы сжатия. То есть, грубо говоря, если у нас тестовый набор состоит из 10 слов «мама», то у нас, соответственно, набор тестового корпуса является 40. В слове «мама» 4 буквы, 10 слов «мама», соответственно, 40 символов. А если мы возьмем один символ «мама» и число 10, повторов, то у нас получится 5 целых чисел. 4 целых числа на кодирование слова «мама» и 1 целое число на слово «десять». Соответственно, 5 вместо 40 мы сжали 8 раз. И вот ровно так посчитанный коэффициент сжатия, как мы обнаружили, он коррелируется с этой f-мерой, которая определяется на основе тестового корпуса. А вторая мера – это Шенноновская антиэнтропия. Ну, то есть, Шенноновская – это, понятно, энтропия, но если взять и трансформировать метрику энтропии, превратив её в антиэнтропию, как мы это делаем, это описано в статье, то мы обнаружим, что вот эта вот антиэнтропия, то есть величина обратной энтропии, она также хорошо коррелируется с мерой токенизации. Ну и мы попробовали посмотреть, как вот эта вот связь. выражается для различных языков – для английского, русского и китайского. Ну и не тратя время на анализ вот этих вот зависимостей, можно сказать, что на достаточно больших объемах корпусов у нас имеется квазилинейная зависимость между любыми метриками. вроде антиэнтропии, коэффициента сжатия и им подобным, и эфмерой. Соответственно, все линии здесь идут более-менее прямо слева-направо. Причем мы видим, что чем более структурированный язык, тем более эти метрики выражены. Для английского они более выражены, чем для русского, а для китайского они выражены более… Так, сейчас я гляжу. Для китайского они выглядят еще хуже, чем для русского. Здесь этого слайда нету. В последней нашей статье показано, что если мы увеличим количество корпуса, на которых мы осуществляем тестирование, то даже для китайского языка вот эти вот кривые становятся более выраженные. Соответственно, то, что мы здесь видим, оно отражает просто то, что мы недостаточно большой корпус взяли, и поэтому вот эти вот корреляционные связи, они такие вот размазанные. Да, причем это мы проверяли на совершенно разных корпусах. С левой стороны у нас английский, русский и китайский на совершенно разных корпусах. Английский на Брауне, русский на Русейдже, а чинез на Русейдже, это корпус русской литературы. значение клюньюз это корпус новостей вот а с правой стороны мы взяли один и тот же браун корпус переведенный на русский на английский и в общем зависимости везде примерно одни тоже характеризуют причем характерная для того или иного языка В итоге результаты, которые мы получили для по-токенизации, для английского, мы получили точность практически 0.99 на основе технологии, основанной на свободе перехода для русского мы получили 1 0 под жирным светом выделено для китайского мы получили существенно хуже но и как мы интерпретируем эти результаты с учетом последних работ которые делали это то что В китайском точность оценки сегментации – это в известной степени дело вкуса, и то, как один токенизатор сегментирует, может сильно отличаться от того, как сегментирует другой. И отчасти мы нашли подтверждение этому следующим образом. В правой колоночке показана точность не сегментации, а точность того, насколько точно мы угадали лексикон. То есть, если центральная колонка говорит о том, насколько точно разбиение на токены совпало, данного тестового предложения совпало с эталонным разбиением этого предложения на токены, то в правой мы показываем точность того, насколько те токены, которые были выявлены в исходной последовательности, совпадают с теми токенами, которые по факту имеются в соответствующем словаре национальном. И мы обнаружили, что несмотря на то, что токенизацию мы не угадали, слова мы все формируем правильно, то есть мы практически не придумываем новых несуществующих слов. Это обнадеживает. Следующая задача, переходя уже к тому, что мы можем делать после того, как мы делаем токенизацию, мы можем пытаться строить иерархии концептов на основе, допустим, алгоритмов токенизации и анализа Свобода переходов или неопределенности на переходах между символами мы можем анализировать и выявлять категории символов. Здесь показано, что мы достаточно хорошо и для русского, и для английского выявляем как согласные, так и согласные, так и символы, так и знаки припинания. По другой работе, которую мы делали при автоматическом построении грамматик для английского языка на основе как парсера, основанного на взаимной информации, это Minimum Sparing Exponent Reparser, так и на основе глубоких нейросетевых трансформерных моделей, конкретно Берта, мы извлекали либо из Берта, либо из парсера, основанного на взаимной информации, грамматические вот эти вот структуры стиля линграммер да то есть грамматические связи между словами между словами в тексте и сопряженными словами и в пространстве вот этих вот на сопряженных слов, и комбинация этих сопряженных слов не просто word embeddings, а именно парных или тернарных комбинаций слов, встречающихся с правой и с левой стороны в соответствующем грамматическом контексте в этом пространстве, Мы строили иерархические системы понятий. Например, здесь показано, что мы достаточно неплохо умеем не только группировать всех слова, соответствующие членам семьи, слова, соответствующие полу, слова, соответствующие каким-то предметам, но и также разделять отдельно слова, которые являются Вот, например, слово «со», оно показывается в кластере со словом «си». И это слово «со», которое обозначено вариантом «би», в данном случае это глагол в прошедшем времени. А вот другое слово «со», которое соответствует слову «со» в качестве существительного, с суффиксом «эй» это обозначено, оно показывается в одном кластере с молотком и телескопом. Следующий пример, как бы я уже завершаюсь. Следующий результат, который я не показывал раньше, вариант построения вопроса ответной системы или ассистента, который позволяет автоматически формировать взаимодействие с пользователем на основе семантической базы данных. где по тематической базе данных мы подразумеваем просто некоторый набор гиперграфовых отношений, по сути, революционных отношений. То есть, мы каждое революционное отношение можем разобрать на небольшой подграф. Или множество связей. Если мы посмотрим на революционную базу данных с точки зрения гиперграфа, мы обнаружим, что каждое соотношение в революционной базе данных – это, по сути, является сабграф. который описывает некоторую сущность, в виде множества отношений различного типа. То есть, множество триплетов является множеством, а множество этих множеств по сути определяет гиперграф. Если нам на вход подается в качестве базы знаний гиперграф из революционных отношений, то, найдя в этом гиперграфе некоторую вершину, от которой мы хотим начать свой поиск, мы можем построить некоторым образом диалог с пользователем, позволяющий ему найти конечную точку в этом герперографе в соответствии с его интересами. Ну, например, допустим, мы решаем задачу по автоматическому поиску чего-либо то ни было. Либо это товары в магазине, либо это комната в офисе, конкретно когда на данном примере это ресторан конкретной кухни соответственно пользователь говорит что его интерес что что он хочет пиццу да мы в базе данных находим множество вот этих вот множество отношений которые связаны с вершиной пицца а дальше мы можем строить некоторое распределение по различным типам отношений и найти такие кластеры из них, где неопределенность выбора минимизирована. Вот, например, у нас существует всего 50 штатов, При этом, может быть, не во всех штатах есть пицца, а количество городов и количество адресов их гораздо больше, чем штатов. И вот в данном конкретном примере, который загружен был просто для демонстрации алгоритма из первого попавшегося тестового набора, Система определяет, что для того, чтобы снизить, получить минимальную неопределенность в ответе на следующий вопрос, самое простое – это спросить, в каком штате ты находишься вообще. Естественно, если мы делали конкретного чат-бот-ассистента, значит соединяем системы геолокации первый вопрос можно снять автоматически из профиля пользователей из геолокации но здесь значит неопределенно снимается в явном виде соответственно сняв неопределенность по штату мы смотрим значит у нас дальше мы можем смотреть пытаться в пространстве вот этих вот трех координат до в одном случае у нас место в другом случае у нас и тип кухни, в третьем случае у нас категория места, в четвертом у нас город. Самое быстрое снятие неопределенности это уточнить в каком городе человек находится. Человек выбирает город, вот в этом городе выясняется всего одно место, после чего человеку уже предлагают, что вот тебе место, которое ты искал, где ты хотел поесть пиццу, согласен ли ты с данным выбором. Человек отвечает «Ес». Соответственно, если человек ответил «Ес», в данном случае система может увеличить какую-то вероятность того, что другому пользователю будет предложено это же решение. Или вероятность может быть не увеличена. Но другой вариант, опять-таки, улучшение вот этой технологии. Это пресловутая Retrieval, модная ныне Retrieval Augmented Generation, когда мы можем вопросы чат-бота, полученные на основе снижения неопределенности, анализируя гиперграф или данные реляционной базы данных по вопросам и ответам пользователя, вопросы пользователя аугментировать с помощью трансформенной LEM-ки для того, чтобы пользователь чувствовал себя более комфортно, что он разговаривает не с тупым роботом, который через OR перечисляет все варианты, а в каком-нибудь более интеллектуальном и развлекающем стиле. Ну и, наконец, да, раз уж мы заговорили про поговорили про сегментацию и вспомнили Retrieval Augmented Generation. У меня недавно как раз был разговор с коллегой, который рассказывал про то, что основная проблема по его опыту в построении систем Retrieval Augmented Generation это правильная нарезка датасетов для собственно на тренировке вот этих моделей, которые будут этот Retrieval Augmented Generation делать, то есть для того, чтобы осуществлять собственно генерацию правдоподобных сообщений пользователей на основе результатов, извлеченных каким-то более менее нейросетевым и более семантическим образом, нужно эту нейросеть натренировать. Так вот, если мы в датасет плохо порежем на куски, у нас качество генерации будет плохое. Если мы его хорошо порежем на куски, качество генерации будет хорошее. Вот и одна из гипотез, которая есть, это вот то, что если попробовать нашу систему генерации, значит, сегментации которая показала вот такие вот хорошие результаты на уровне символов попробовать применить для сегментации на уровне предложений или сегментов текстов, возможно, мы сможем улучшить качество Error-Trivial Augmented Generation для нейросетевых моделей для подобных кейсов. Ну и, наконец, собственно, завершаясь, о том, какое отношение, возвращаясь как бы мысленно к первому слайду, где я пытался утверждать, что мы можем говорить о некоторой эквивалентности вычислительной между нейросетевыми несимвольными и графовыми, символьными моделями, поскольку и там, и там мы имеем графы, и там, и там мы имеем параметры, которые являются либо весами соединений искусственных нейронов, либо весами соединений в семантических графах. Если мы можем говорить об этой эквивалентности, то как мы на это можем посмотреть с точки зрения пресловутого механизма внимания? То есть мы знаем, что у нас прорыв в прикладном применении LLM случился, когда мы внедрили механизм внимания, когда предсказание следующих токенов последовательности оказалось возможным не просто в контексте предыдущих токенов, а во множественном контексте, когда мы можем брать и использовать механизм так называемого многоголового внимания, когда, допустим, контекст токена, находящегося в десятой позиции, плюс контекст токена, находящегося в седьмой позиции, плюс контекст токена, находящегося в третьей позиции, влияют на то, какой токен будет предсказан на основе токена второй позиции. Так вот, данный слайд показывает то, что те же самые механизмы внимания могут быть использованы и на уровне графовых моделей, В данном случае показано, что мы берем, строим некоторую графовую систему, которая предполагает себя как понятие более высокого уровня, допустим, существительные или глаголы. Это максимально высокого уровня абстрактное понятие. Глаголы могут быть либо частями тела, либо какими-то процессами. Точнее, существительные могут быть либо частями тела, либо какими-то процессами. Процессы в одних контекстах могут являться существительными, либо они могут являться глаголами. То есть, если в зависимости от того, говорю ли я о процессе как о некотором объекте, либо процесс сам описывает взаимодействие с каким-то объектом, У нас этот процесс может рассматриваться как на уровне семантики, и как существительный, и как глагол. С другой стороны, мы можем ввести понятия языка, где язык может быть русский, где может быть английский. С другой стороны, у нас может быть наклонение. У нас может быть индикативное наклонение или императивное наклонение. И у нас различные морфологические вариации слов, и различные структурные соотношения этих слов в различных языках, они определяются, соответственно, всеми теми обстоятельствами. который я перечислил. То есть, если мы возьмем, допустим, язык и скажем, что нас интересует, если мы возьмем, допустим, двуязычную систему, где каждое понятие, допустим, «рука» или «давание» с решеткой, здесь показаны абстрактные концепты. Есть абстрактный концепт «руки», есть абстрактный концепт давания, и они могут находить свое отражение как в англоязычном лексиконе, так и в русскоязычном лексиконе. С другой стороны, в русскоязычном лексиконе эти слова могут иметь различные морфологические формы, то есть проецироваться на токены, которые имеют различные морфологические формы в зависимости от того, в каком наклонении они используются. А в английском языке, в зависимости от того, какое наклонение мы используем, у нас будут разные последовательности вовлекать все эти токены. И, соответственно, если мы, к примеру, возьмем и скажем, что нас интересует некоторый процесс, вовлекающий руку и давание и мы хотим скажем что у нас этот процесс должен быть у нас это да мы даже не знаем что это процесс у нас нечто что мы хотим выразить что в данном случае является процессом и процессом связанным с рукой даванием и Если мы фокусируем наше внимание на том, что этот процесс должен быть выражен русским языком, а также мы формируем свое внимание на том, что этот процесс должен быть выражено в повелительном наклонении, что мы не хотим сказать, что кому-то подана рука, а мы хотим сказать подай руку, подай руку, что подай мне руку, то есть мы хотим попросить подать руку, то как бы вот соединение этих трех механизмов внимания приведет к тому, что чисто методами вывода на графе С той или иной вероятностью мы получим, что нам нужны правильные слова, нам нужна подача руки и сама рука в повелительном наклонении, нам нужны эти слова на русском языке, и нам они нужны в правильной последовательности, которая в русском языке соответствует фразу в повелительном наклонении, требующую от человека подание руки. И гипотеза заключается в том, что мы можем такие системы строить на основе символьных систем, в том, что эти системы могут оказаться более интерпретируемыми, и в том, что в вычислительном отношении они могут оказаться более высокопроизводительными, чем нейросетевые системы. Ну и самое главное, Это то, что с точки зрения, поскольку в первом слайде было высказано предположение, что вычислительные эти системы могут быть эквивалентными, может оказаться, что мы на основе в том числе тренировки нейросетевых моделей сможем из них, извлекая структурированные сильные вольные знания, получать более интерпретируемые, более вычислительно эффективные символьные системы, включая системы с нечеткой логикой. Спасибо за внимание. Осталось ли у кого-то терпения на какие-то еще вопросы или обсуждения? Андрей, пожалуйста. 

S01 [01:29:01]  : Добрый вечер. Хотел задать такой вопрос по грамматике именно. Вот вы упомянули... Меня слышно, да? Да-да-да, Андрей. Вы упомянули о том, что, получается, у вас как-то нет общего способа извлечь грамматику связей, кроме как людьми-лингвистами, которые будут ее проводить. Но тут проблема, мне как-то не очень понятно, зачем это вообще нужно, потому что у нас в LLM как это делается автоматически. Вот, может быть, стоит как-то найти способ из LLM извлекать именно грамматику. Потому что грамматика, она как бы от семантики-то отвязана. И вот, допустим, если я скажу... тарабунька заворякал баралышку в аудже тут с грамматикой в принципе будет все в порядке то есть будет просто это предложение будет лишено какой-то семантики вот и возможно там по какому-нибудь вот такому бессмысленному сету обучителя лен. вот. и где мы сможем потом как-то пластеризовать именно грамматику в этом бреде, когда вот у нас с точки зрения грамматики там будет все верно, а вот семантику мы специально занизим. вот. и может быть как-нибудь так и привлекать. просто если Ну, если это делать вручную, то как бы грамматика, получается, даже лингвистами будет бесперспективно труд проводить. Потому что в языке, вот скажем, в русском, у нас есть много... Мы как бы грамматику для себя составили, грамматику русского языка, с помощью которого может работать лингвист, ну, допустим, падежи какие-нибудь. А ведь у нас в русском языке есть какие-то... Рудиментные падежи, с которыми мы саму грамматику-то не включаем, но они есть. Допустим, я хочу выпить чая или выпить чаю. И то и то как бы это верно, но один из этих вариантов, он просто рудиментарный. Потому что раньше у нас падежей было больше и прочее. Так вот, если бы найти какой-то способ именно LLM-кой создавать грамматику какую-то эталонную, вот это было бы неплохо, потому что то, что мы говорим, вернее, то, что мы слышим от лингвистов, и то, что они называют грамматикой, но это ведь не полная система. А проблема LLM-ок, в том, что там помимо грамматики еще нагромождены куча слоев всяких логических. И вот одну кашу от другой отделить очень сложно. И вот мне кажется, если бы делать какой-то бессмысленный набор, который которая семантики и логики вообще не содержала для каких-то пространственных отношений, временных отношений, а содержала именно только грамматику. Возможно, получилось бы ее извлечь в полной мере, но при этом отделить ее от этих компонентов логических, семантических и прочего. Это вам не ремарка, это просто мысль. 

S00 [01:32:41]  : Да, спасибо. Смотрите, во-первых, я согласен с тем, что было бы хорошо это сделать. Во-вторых, я согласен с тем, что отделить грамматику по семантике практически невозможно. Потому что грамматика и семантика, они переплетены в языках. Возьмем, например, мужской и женский род. Причем мужской и женский род и множественное единственное число, они переплетены в английском языке и в русском они переплетены совершенно по-разному. А в китайском они наверняка переплетены каким-то совершенно третьим образом. Но я с вами не уверен, что я согласен. Я не могу сходу это обосновать. Но я практически уверен, что когда мы утверждаем, что глобка куздра кого-то там барканула, когда мы утверждаем, что это грамматически верное выражение, мы говорим это исключительно потому, что «глобкая» и «куздра» нам напоминают какие-то прилагательные и какие-то существительные, которые мы знаем в реальной жизни. И мы по аналогии аппроксимируя, что ага, вот тут у нас прилагательное, вот здесь у нас существительное, а тут у нас глагол, несмотря на то, что в этих местах стоят абракадабры, мы делаем вывод, ага, наверное, вот здесь самая такая грамматическая структура, то есть мы делаем некоторым логическим выводом, там осознанно, неосознанно, мы делаем подмену, понятие да то есть мы ставим на место не существующих концептов некоторые абстрактные грамматические категории из их делаем вывод что это грамматически валидное выражение но это мы делаем потому что у нас есть что называется grounding у нас есть и значит, некоторая модель, на которую мы вот это вот накладываем. То есть, мы осуществляем нечеткий матчинг, значит, расплывчатых концептов на некоторые грамматические категории, а потом строим уже дерево грамматического разбора. Если бы у нас тренировочный набор состоял из абракадабры, я думаю, мы бы ничего не сделали. Вот мне так кажется. Если бы в этой абракадабре не было бы закономерностей, если бы, допустим, вот этого абракадабра подчинялось бы конкретным грамматическим правилам, если бы все, допустим, существительные там имели одни окончания, все глаголы другие, а все прилагательные третьи, если бы они правильно склонялись и спрягались, то тогда бы, наверное, мы смогли бы восстановить грамматику этого языка, но тогда бы это уже была бы не абракадабра, тогда бы это уже был какой-то язык, пусть даже вымышленный, несуществующий, искусственный. 

S01 [01:35:56]  : Но тогда, если с вами согласиться и сказать о том, что семантику от грамматики отделить сложно, если вообще возможно, то тогда в этом случае любая... блин, граммар, получается, грамматика связи будет неполна. нейросетевая модель будет всегда точнее в этом смысле, потому что она как раз-таки в себе сочетает вот в слоях семантику и грамматику в фиче какой-то, который содержит и то и другое, компоненты того и другого. 

S00 [01:36:30]  : Совершенно с вами согласен. То есть, как раз, может быть, я опять-таки недостаточно про это акцентировал, но в моем понимании слабая сторона всех нейросимвольных подходов, основанных на грамматиках, будь то грамматика связей, универсальная грамматика, будь то грамматика связей, заключается в том, что они не предполагают бесшовного интегрированного связывания грамматических слоев языка с семантическими слоями языка, в то время как в нейросетевых моделях эта связь непрерывная. И только построением символьных моделей, где отсутствует эта граница, ага, тут у нас семантика, а тут у нас грамматика, только построением подобных сквозных семантических моделей мы сможем решить, если сможем, если это будет иметь практическую ценность. задачу – догнать и перегнать элелемки в области НЛП. Коллеги, еще есть какие-то вопросы или комментарии? Если нет, тогда всем спасибо за внимание и до новых встреч. Спасибо за семинар. Спасибо. До свидания. 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
