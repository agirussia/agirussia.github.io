## 22 января 2026 - О методах контекстно-эффективной "инъекции" внешних знаний в большую языковую модель - Иван Бондаренко (Лаборатория прикладных цифровых технологий НГУ)  - семинар AGI
[![Watch the video](https://img.youtube.com/vi/CsdTMWA9zk4/hqdefault.jpg)](https://youtu.be/CsdTMWA9zk4)

---

### Основные тезисы доклада

#### 1. Проблема и философский подход
*   **Сила и слабость LLM:** Способность генерировать связные тексты является и причиной галлюцинаций. Модели могут писать гладко, но без связи с реальностью.
*   **Знание как припоминание:** Иван проводит параллель с диалогом Платона «Менон». Знание находится не внутри «головы» (нейросети), а вовне. Задача модели — не хранить все факты, а уметь их «припоминать» (извлекать) из внешней базы.
*   **Мультимодальность:** В 2023 году команда участвовала в соревновании по AGI. Классические архитектуры (текстовая модель + визуальные энкодеры) работают плохо из-за ограниченности проекционных слоев.
*   **Решение (Проект Mino):** Использование мультимодального энкодера и внешней базы данных. Текстовая модель получает описание картинки или звука через поиск похожих текстов в базе (механизм припоминания). Это показало высокие результаты на бенчмарках (Multimodality Arena).

#### 2. Малые языковые модели (SLM) и гипотеза о знаниях
*   **Применение:** Разработка бота для абитуриентов НГУ. Требования: высокая достоверность и устойчивость к нагрузкам.
*   **Гипотеза:**
    *   *Знания о мире* (факты) линейно зависят от размера модели (количества параметров).
    *   *Языковые навыки* (понимание синтаксиса, логики текста) растут нелинейно и достигают плато даже на малых моделях.
*   **Эксперимент:** На бенчмарке MERA (задачи ChGK и MultiQ) подтвердилось, что малые модели (Qwen 1.5B, 7B) отлично работают с языком, но плохо знают факты.
*   **Вывод:** Малые модели эффективны, если вынести знания во внешнюю базу (RAG). Команда дообучила модели **Mino Tiny** (1.5B) и **Mino Light** (7B) на качественных образовательных текстах и инструкциях, получив отличные результаты по работе с контекстом.

#### 3. Эволюция RAG: От векторов к графам
*   **Проблемы классического поиска:**
    *   Простой векторный поиск хорошо работает для простых вопросов.
    *   Для сложных вопросов (multi-hop reasoning), требующих логических цепочек (например, «как открытия кафедры Х повлияли на дипломы факультета Y»), векторный поиск проваливается.
*   **Графы знаний (Knowledge Graphs):** Структурирование информации в виде графа (сущности и связи) позволяет моделировать сложные абстракции и повышает семантическую эффективность контекста.
*   **GraphRAG (Microsoft) и LightRAG:** Существующие подходы хороши, но требуют огромных ресурсов (модели 32B+) для построения самого графа.

#### 4. Библиотека Ragu и мультиагентный подход
*   **Инновация:** Команда разработала open-source библиотеку **Ragu**.
*   **Принцип работы:** Вместо одной гигантской модели для построения графа используется цепочка (pipeline) специализированных агентов на базе малых моделей.
    *   Агент 1: Извлекает сущности.
    *   Агент 2: Нормализует их.
    *   Агент 3: Определяет связи.
*   **Преимущества:** Позволяет строить качественные графы знаний, используя небольшие модели (даже 600M или 7B параметров) и стандартное железо.

---

### Ответы на вопросы (Q&A)

1.  **Архитектура пайплайна:** Процесс построения графа детерминированный (последовательность шагов), а не динамический. Это сделано намеренно для обработки небольших семантических кусков текста.
2.  **Контроль ошибок (Галлюцинатор):** Используется специальная модель-классификатор (дегаллюцинатор), которая проверяет ответы агентов на этапе построения графа. Можно настраивать порог (precision vs recall), чтобы не допустить попадания «мусора» в граф.
3.  **Разбиение текста:** Тексты делятся на смысловые блоки (сверхфразовые единства) с помощью отдельной нейросети, определяющей семантические переходы.
4.  **Обновление базы:** Добавление новых знаний в граф — легкая задача. Удаление устаревших данных (например, отмененных регламентов) реализовано через удаление вершин и связей. Самое сложное — первичная индексация (занимает часы).
5.  **Технический стек:** Сейчас для графов используется библиотека **NetworkX**. В будущем планируется поддержка более мощных графовых СУБД (рассматривают аналоги Neo4j, но с учетом лицензионных ограничений).

### Итог
Доклад продемонстрировал переход от простого RAG к сложным графовым структурам, которые можно эффективно строить и эксплуатировать даже с помощью небольших языковых моделей, используя мультиагентную архитектуру для снижения вычислительных затрат.



**Расшифровка доклада:**



S00 [00:00:00] : Все мы знаем, что мир страдает эпидемией эволюционации больших языковых моделей, но есть люди, которые разрабатывают вакцины для точнее от галлюцинации вот и с помощью этих вакцин есть надежда можно будет ставить большим языковым моделям прививки чтобы они не страдали галлюцинациями например и вам пожалуйста вам слово и поправьте меня если я был не прав 

S01 [00:00:28] : Спасибо большое, Антон. Здравствуйте, коллеги. Действительно, генеративные модели, они имеют много сильных сторон, но их слабая сторона обратно. Это прямое следствие их силы. То, что они могут генерировать достаточно длинные, грамматически корректные, связанные тексты. приводит к тому, что они могут галлюцинировать, делая это без всякой семантической связи с вопросом пользователя, с контекстом в виде знаний об окружающем мире или о нужной нам предметной области и, соответственно, как с этим бороться. Я немножко планирую сегодня о своем опыте рассказать. Вот. Название доклада о методах контекстно-эффективной инъекции внешних знаний в большую языковую модель. Собственно, инъекция внешних знаний, понятное дело, нужна для того, чтобы пресекать галлюцинации и повышать достоверность ответов. Мы об этом чуть подробнее поговорим в ходе доклада. Пару слов обо мне. Я занимаюсь искусственным интеллектом с 2006 года примерно. Я люблю это направление. Изначально я занимался исследованием в области распознавания речи, потом мои интересы перешли в целом на компьютерную лингвистику, на теорию нейронных сетей вообще. И сейчас я работаю, во-первых, в Новосибирском государственном университете, веду курсы, связанные с глубокими нейронными сетями, с искусственным интеллектом, с компьютерной лингвистикой, занимаюсь научной работой. Вместе с дипломниками со своими придумаем всякие интересные методы, новые методы и алгоритмы работы с нейронными сетями, повышение их доверенности, достоверности. Ну и помимо всего прочего, я с некоторых пор также сотрудничал с инверситетом Sirius. Мы занимаемся частным, но очень важным проектом, связанным тоже с повышением доверенности ответов. Мы пытаемся обнаруживать деструктивный контент и пресекать такое вот деструктивное поведение. Но не об этом я буду рассказывать. Я начну издалека. История, о которой я сегодня хочу рассказать, началась в 23 году. Получается, больше двух лет назад на самом деле. Даже три года практически. Мы с моими товарищами и с моими студентами любим участвовать в научных соревнованиях. Вообще говоря, когда мы говорим об исследованиях в науках, критерием истинности в которых является эксперимент, остро стоит проблема воспроизводимости эксперимента. Научные соревнования – это очень эффективный способ повысить воспроизводимость научных экспериментов, поскольку в рамках научных соревнований дизайн эксперимента общий для всех участников, набор данных для настройки и для тестирования тоже общий, методика замеров качества идентична, фактически все эксперименты отличаются только архитектурой метода и алгоритма, который проверяется. И вот на конференции путешествия в мир искусства интеллекта, которая проводится с Бером уже много лет подряд, В 2023 году было предложено очень интересное соревнование под таким названием, супер AI intelligence, супер искусственный интеллект или супер общий искусственный интеллект. Почему так назвали? Речь идет о том, что участникам предлагается построить систему универсального разговорного ИИ, которая могла бы работать, взаимодействовать в различных модальностях, как в текстовой, так и в графической и аудио. Разработчики назвали это сильным искусством интеллекта. Почему вообще говоря они имеют право говорить сильный искусственный интеллект. И что это за термин? Слабый и сильный искусственный интеллект. Я думаю, что все присутствующие так или иначе знают, но буквально пару слов скажу. Когда мы говорим о сильном и слабом искусственном интеллекте, мы говорим вот что. Искусственный интеллект в целом это методы автоматизации поведения человека при решении задач, которые мы считаем традиционно интеллектуальными, то есть задачи простые, деятельность простая, выпить чашечку воды, пройтись туда-сюда, это деятельность, но она интеллектуальная не является. Например, если Мы идем утром в университет, в толпе людей, издалека увидели нашего друга или подругу, помахали ему рукой, сказали «Эй, привет, как дела?» и договорились вместе вечером пойти на семинар, на вебинар в сообщество AGI Russia, то вот это вот интеллектуальная деятельность, узнавание зрения, речевая коммуникация, планирования, динамически изменяющие внешних условия. И вот искусственный интеллект моделирует вот такую деятельность человека. Но при этом есть слабый искусственный интеллект. Это специализированные системы. Например, только система, которая распознает речь, или только система, которая играет в шахматы, или только система, которая распознает номерные знаки автомобиля. Она действительно является средством автоматизации интеллектуальной деятельности только какой-то одной ограниченной деятельности. Когда же мы говорим о сильном искусственном интеллекте, мы подразумеваем, что мы делаем универсальную систему, при этом не просто универсальную, а универсальную коммуникативную систему, позволяющую взаимодействовать с человеком на естественном для человека языке, при этом используя разные модальности. Опять-таки то, что характерно для коммуникации между людьми. Графическая зрительная модальность, текстовая модальность, аудиомодальность и тому подобные штуки. И вот такое соревнование было в 2023 году проведено. Был ряд аппаратных и технических ограничений. Мы там поучаствовали и, собственно, к сожалению, мы толком ничего не успели тогда сделать в рамках отведенного времени. Мы засобмитили решение, которое где-то было посередине. Почему? Это было грустно. Почему? мы пытались сделать решение на базе одной из мультимодальных моделей, публичных мультимодальных моделей, но, к сожалению, было не очень хорошо, и мы решили проанализировать, что мы сделали не так и как можно сделать лучше. Если мы рассмотрим типичную архитектуру мультимодальной неростевой системы, да и в целом неростевой генеративной системы, то мы увидим следующее. У нас есть большая языковая модель, есть некоторый входной контекст, который задается дискретными символами дискретного алфавита, элементами дискретного алфавита, токенами. Эти токены отображаются в некоторое метрическое пространство. И вот цепочки векторов подаются на вход нашей неростевой модели, которая, как правило, обладает трансформированной архитектурой. Более ранние модели основывались на регулятной архитектуре, сейчас, как правило, это слои с механизмом внимания, многоголовочного внимания. Если мы говорим о текстовой модели, то токены это просто токены. Если мы говорим об ультимодальной модели, то часть токенов, часть входного контекста отводится для представления объектов иной модальности. Например, картинка или звук. Как это происходит? Если для текстовой части входного контекста токены как элементы словаря отображаются в некоторые вектора, то для картиночной и для аудиочастей добавляются специальные неростивые модели, так называемые унимодальные энкодеры, зрительный энкодер, аудиоэнкодер, кодировщики. которые отображают объект другой модальности в некоторый вектор высокоразмерный, а потом с помощью специального проекционного слоя отображают этот высокоразмерный вектор в цепочку более низкоразмерных векторов, моделирующих то же самое пространство, в котором располагаются текстовые токены. То есть последовательность является более-менее однородной. Текстовые токены и токены и подстрока токенов кодирующих. Объекты другой модальности примерно по одинаковым принципам подаются на вход языковой модели. Это все хорошо. Но при этом, когда мультимодальная модель строится, берутся готовые компоненты. У нас есть уже готовая чисто текстовая языковая генеративная модель. Готовые унимодальные энкодеры, например, на основе клип или на основе виспера для аудио. И, по сути дела, в такой архитектуре обучаются только вот эти вот две части. Ну или сколько там частей, сколько у нас модальностей нетекстовых, только таких частей проекционных. Простые слои. которые отображают большое высокоразмерное представление, полученное в рамках одного не модельного энкодера в цепочку векторов токенов. Это проблема. Во-первых, у нас ограниченное количество вот этих вот псевдо токенов для различных модальностей. Во-вторых, мы не можем никак управлять знаниями о связях между модальностями. Это проблема, которая типична для мультимодальных систем. В принципе, если решать вопрос фундаментально, то по-хорошему нужно делать сразу модель неростивую, которая знает обо всех модальностях на свете и изначально предобучается с нуля на мультимодальных данных. Это то, за что ротует Ян Лекун, когда говорит о том, что нам нужны модели мира. Но построение такой мультимодальной модели с нуля, с этапа предобучения – это не просто вычислительно-затратная процедура, это чудовищно вычислительно-затратная процедура, которая требует многих тысяч GPU класса H100 и, самое главное, сбалансированного по разным отдальностям гигантского датасета для предобучения. Правила в нашем мире больше все-таки текстовых. данных, чем остальных. Но мы можем пойти другим путем, не первым, как все делают, не вторым, как предлагает Иоанн Лекун, потому что это слишком очень затратно, а третьим путем. Мы хотим, чтобы модель знала о других модальностях. А что такое знание? В данном случае мы вспомнили классическую греческую философию, Вот это вот у нас один из известных философов Сократ. А вот это вот Сократ вместе с своим учеником Платоном, который, как считается, стал основателем философии идеализма. И вот в одном из сократических диалогов Платона, автором являлся Платон, но в честь своего учителя Сократ он назвал свои тексты сократическими диалогами. Диалог назывался Минон. Суть была в том, что его учитель Платона Сократ гулял улицами Афин, встретил юношу по имени Минон и начал с ним беседовать. Этот юноша совершенно не знал геометрию, но в процессе беседы с Сократом он постепенно смог доказать теорему Пифагора. Платон, посмотрев на это, делает вывод. Знание Это что-то, что существует в мире абсолютных идей. Оно не в голове человека, оно вовне. Иначе бы как юноша, который вообще не знал геометрии, смог бы доказать те ремки фигуры? Да никак. И вот, собственно, в рамках его учения об абсолютных идеях знание это механизм, который осуществляется через предпоминание. Мы ничего не познаем, а то, что мы называем познанием, есть предпоминание. Прошло две с половиной тысячи лет и знание через припоминания оказалось эффективным механизмом для построения гибридных систем общего или сильного искусственного интеллекта, где есть где есть нейросеть, которая играет роль естественно-языкового интерфейса и преобразователя текстов на естественном языке, а механизм поиска, механизм отображения в некоторое методическое пространство, семантического отображения в методическое пространство и быстрого социального поиска играет роль припоминать. Здесь можно строить такую архитектуру поиска по-разному. Можно использовать, например, самый такой классический подход. Это мы все объекты, которые хранятся во внешней базе, мы представляем в виде векторов, то есть, опять-таки, отображаем в некоторое пространство, в котором сохраняются, в котором хорошо моделируются семантические отношения между этими объектами. Объектов может быть огромное количество, не тысячи, не две тысячи, а, например, миллионы, десятки миллионов. Нам нужен быстрый ассоциативный поиск, например, приближенные ближайшие соседи. Это достаточно хороший инструмент. И мы можем построить мультимодальный нейросетевой кодировщик. который бы объект любой модальности, текстовые, картиночные, звуковые, отображал бы в некоторое универсальное семантическое пространство, в котором может осуществляться поиск. Одной из таких моделей, наиболее эффективных, но на мой взгляд недооцененных в современном искусственном интеллекте, является модель One Piece. Она фактически является моделью мира примерно Это не генеративная модель, это нейросеть кодировщик, но тем не менее она изначально предобучалась на мультимодальной информации с использованием достаточно сложной составной функции потерь, где одно слагаемое отвечало за объединение разных модальностей, а другое слагаемое отвечало за сопоставление объектов внутри отдельных модальностей. Соответственно, используя такой вот нейросеть-кодировщик и большое количество хранилище мультимодальных данных, отображаемых в некоторое общее веточное пространство, мы построили архитектуру, которая позволяла нам, скажем так, обмануть текстовую модель описывая то, что она может видеть и слышать с помощью развернутого текстового описания. А это текстовое описание формировалось путем припоминания, путем поиска. текстов в базе знаний, чьи вектора максимально похожи на вектора тех мультимодальных объектов, которые поступали в контекст нашей модели. Тут я подробно на этом останавливаться не буду. Тут подробно описано Архитектура решения, это сейчас не представляет большого интереса. Модель отвечает. Модель отвечает на разные модальности, вот про трактора, про автоцензуру, говорит, что вот, мол, все хорошо, не надо цензурировать. И мы, естественно, помимо вот таких точечных примеров, решили получить значимые оценки на бенчмарках. Вот, собственно, Мы использовали известный бенчмарк multimodality arena и сравнивали наше решение, которое называется Mino, в честь сократического диалога Аплатона Minon, с другими архитектурами, более классическими. И мы видим, что наша архитектура Здесь разные способности, которые проверялись. Visual Common Sense, Visual Reasoning, Object Hallucination, Knowledge Acquisition. Мы видим, что зелененькое это такой полигон, который показывает разные аспекты способностей нашего решения мином. Плюс есть еще два минона, миномистраль. Одной версии и другой, но это разные варианты структуры. Мы видим, что Наше решение оказывается по ряду способностей даже лучше, чем решение классические типа лавы или даже чем решение BART, старое название мультимодальных моделей Google. Ок, но это все соревнования. Мы с помощью соревнований проверили, что наш подход, основанный на использовании внешних знаний, на выносе фактологических знаний из нейрочтевой модели во внешнюю базу, оказался хорошим, и, соответственно, настало время практического применения. Во-первых, частное практическое применение – это иллюстрации к новостям НГУ, Новостей выходит много, наша пресс-служба работает очень хорошо, на мой взгляд, но проблема в том, что тексты надо бы иллюстрировать картинками. Это всегда положительно влияет на восприятие. В университете и в пресс-службе очень много картинок, целый банк фотографий, схем, иллюстраций, но как выбрать подходящие картинки, задача сложная. Соответственно, наш механизм припоминания помог построить систему мультимодального поиска, когда журналист, сотрудник пресс-службы вводит текстовый запрос и получает картинку или набор топ-5 картинок, наиболее релевантных этому запросу, и, соответственно, он может выбирать, что же поставить на заставках новости. Использовался вот этот механизм припоминания. Например, можно посмотреть, как это выглядит в тестовую версию этого бота. Мы разворачивали в телеграмме фотографии с неоткрытой дверью МГУ. Видно, что действительно наиболее релевантные фотографии это фотографии действительно с неоткрытой дверью МГУ. была поставлена более общая и фундаментальная задача. В университете есть абитуриенты, которые хотят поступить и думают поступать в НГУ или в какой-то другой курс. Если в НГУ, то на какой факультет, например, какие-то фундаментальные вопросы решают или хотят частные вопросы узнать, где работает приемная комиссия, в какой аудитории, какие даты, какие часы работы у нее. Безусловно, эту информацию можно найти на сайте, но вот ее нужно искать. К тому же сайт не всегда хорошо предоставляет эту информацию. Пытаться организовать поиск по соцсетям университета тоже задача такая достаточно неблагодарная. И возникла идея, а что если мы сделаем деловую систему, которая могла бы взаимодействуя со всеми пользователями, абитуриентами, студентами, сотрудниками на естественном языке, например на русском, отвечать на вопросы разных типов об университете. Соответственно возникла версия Минона для абитуриента. Каковы ее основные особенности? Во-первых, это унемодальность. В Миноне для абитуриентов мы решили сузить задачу и ориентироваться только на текстовый диалог. Но при этом ключевыми требованиями к Минону для абитуриентов являлась максимальная достоверность ответов, Здесь креативность была, мы пожертвовали креативностью в пользу достоверности. Это требования, которые являются ключевыми. Во-вторых, абитуриентов много, может возникать пиковая нагрузка, особенно, например, в дни открытых дверей, в даты приближенной приемной кампании и требования еще устойчивости к нагрузке. То есть Минон для абитуриентов должен соответствовать этим требованиям. По поводу устойчивости к нагрузке мы думали-думали и подумали следующее. Опять-таки, концепция знания через припоминания нам может помочь. Мы можем использовать нейросетевую модель языка малого размера для того и при этом, несмотря на ее малый размер, сделать ее эффективной частью нашей архитектуры. Почему? Давайте разберемся. Во-первых, малоязыковые модели гораздо быстрее. Здесь у меня показан график, который был получен путем экспериментов на одной видеокарте NVIDIA TESLA 100 с 80 гигабайтами видеопамяти. Мы экспериментировали с линейкой различных моделей китайской компании Alibaba. У них была модель совсем маленькая на 500 миллионов параметров, на полтора миллиарда, 3 миллиарда на 7, на 14 и на 72. На 72 нам даже померить на 1 А100 не удалось, потому что для функционирования 70-миллиардной модели без всякой квантизации необходимо 2 А100. Вот у нас есть 32 миллиарда, 14, 7, Мы видим, что по оси y скорость драматически падает. Скорость измеряется в количестве токенов, генерируемых в одну секунду. Это хороший показатель, который не зависит от сложности вопроса и соответственно длины ответа. Какой бы ответ длинный не был или короткий, мы меряем сколько токенов в секунду генерирует модель. Таким образом видно, что малые модели использовать хорошо, вычислительно эффективно. Но насколько они являются умными? И здесь я хочу сказать следующее. Когда мы говорим о способностях современных генеративных моделей языка, мы должны понимать, что у них есть два типа способностей. Два типа знаний. Первый тип это знания о мире и второй тип это знания о языке. Умение перерабатывать информацию на естественном языке. И мы сформулировали гипотезу, что знания о мире линейно зависят от размера модели. Чем нейросетевая модель больше, тем больше знаний о мире она может в себе разместить. Знания о мире представляются в виде параметризованных межнейронных связей. В процессе обучения формируется некий механизм ассоциативной памяти. Чем больше таких параметризованных межнейронных связей внутри нейронной сети, тем больше возможностей сформировать различные элементы сына памяти такая нейросеть имеет. А знания по языке эффективно демонстрируют даже малые модели. Это была наша гипотеза. Для того, чтобы гипотеза стала теорией, ее нужно проверить экспериментально. Мы решили воспользоваться известным бенчмарком Мера, который был разработан альянсом развитию искусственного интеллекта в России, и мы выбрали оттуда две типовых задач, MultiQ и Chagaka. Мы считаем, что задача Chagaka это типичная задача, для решения которой требуется знание о мире, а MultiQ это типичная задача, для решения которой требуется понимание естественного языка, в данном случае русского. ЧГК – это задачки из известной игры «что, где, когда». Для того, чтобы их решить, действительно нужно много фактов знать. А Multi-Q – это специфическая задача, в рамках которой на вход в тестируемые модели дается вопрос и несколько текстов. Внутри которых есть информация, достаточная для ответа на вопрос. И вот проверяется, насколько модель может извлечь информацию из текстов, изгенировать ответ на вопрос. Таким образом, как вы видите, Multi-Tool это типичный тест на умение манипулировать последовательностями на естественном языке. А ЧЛК это типичный тест на запоминание фактов о мире. И вот тут опять-таки мы построили график по оси X. Это размер тестируемых моделей. Опять-таки использовалась линейка Alibaba-Quen 2.5. А по оси Y это F1 мера, критерий качества решаемой задачи от 0 до 1. Чем больше, тем лучше. В идеале 1. И мы видим, что вот та задача, ЧГК красная, которая соответствует знаниям о мире, здесь прям линейный рост. Чем больше модель, тем больше она знает о мире, тем успешнее она решает задачи человека в смысле F1-меры. А если мы посмотрим значимую алтитюду, то мы видим, что Даже малые модели достаточно эффективно умеют манипулировать языком, мы видим резкий нелинейный рост качества решения задачи и уже где-то на 32 миллиардах параметров качество решения стабилизируется. Но при этом даже 40 миллиардов параметров или 7 тебя хорошо показывает, более-менее хорошо. Таким образом, да, действительно, малые языковые модели можно применять, если мы пытаемся заставить их не дать ответ на вопрос на основе своего внутреннего представления памяти, а выносим знания во внешние механизмы. Соответственно, на основе этого мы сделали, сначала мы сделали модель на полтора миллиарда параметров. Как видите, здесь модель на полтора миллиарда параметров показывает себя не очень хорошо. Мы специальным образом дообучили такую модель на инструкционном датасете размером примерно 700 тысяч инструкций, где специально усиливали, развивались способности манипулировать русским языком. Изначально модель крем- это китайская, она являлась мультиязычным, там 29 языков было поддержано, но большую часть текстового корпуса для обучения составляли английский и китайский язык. Мы здесь специально, во-первых, русифицировали модель в плане погружения ее в русское языковое и культурное пространство и усиливали ее способности перерабатывать информацию. Она себя показала действительно весьма неплохо. На батчмарке той же меры мы видим, что в среднем по разным задачам она показывает себя лучше, чем базовая модель, на основе которой она обучалась. 2,5 на 1,5 миллиарда параметров. И в принципе себя показывает на уровне других моделей большего размера. При этом, если мы посмотрим вот эту задачу MultiQ, которая нас интересует, то тогда, когда модель была минутой не выпущена, она занимала 35 место в общем зачёте, но при этом по задаче MultiQ она уже занимала 22 место, обгоняя модели существенно большего размера. То есть та цель, которую мы ставили, мы в общем-то достигли. При этом, если мы посмотрим на другую задачу, например, на известную схему целей винограда. которая тоже так или иначе связана с пониманием языка. Схематерия винограда это тактическое разрешение местоименной анафоры. Кубок не помещался в чемодан, потому что он был слишком большой. Вот он это кто? Кубок или чемодан? Здесь, конечно, задействуется определенная степень знания о мире, но прежде всего понимание синтетической структуры языка для разрешения местоименной анафоры. то Mino Tiny вообще 14 место занимала. Сейчас конечно новые модели добавились, лидерборд немножко поменялся. Но опять таки Mino Tiny модель конечно хорошая, но чересчур лаконичная. Она плохо вела длинные, развернутые диалоги с пользователем, и для преодоления этой проблемы мы сделали модель Minolight. Это модель на 7 миллиардов параметров, которая тоже была основана на модели Quent 2.5 и 7 миллиардов, но мы сделали там ряд улучшений. Мы реализовали то, что называется Continued Pre-Training. Мы дообучили модель, как простую языковую модель, присказывающую следующее слово в тексте на большом объеме русских текстов. По-настоящему большой объем мы не могли использовать. Потому что мы не GPU rich company. У нас нет огромного количества GPU. У нас тут только максимум. Есть узел вычислительный, где 10 GPU класса А100 расположены. Мы можем распровеливать обучение. Либо модели разрезать на узлы, либо делать MapReduce для данных. Но, тем не менее, это не 10 тысяч GPU, как у наших больших компаний российских, тем более зарубежных. Соответственно, мы сделали следующее. Мы проанализировали научную литературу, научные исследования на тему эффективного обучения моделей и выяснили, что ряд исследований показывает, если мы обучаем, да, обучаем такого рода языковые модели не на всех текстах подряд, а только на максимально полезных текстах, то количество текстов, размер текстового корпуса может быть гораздо более меньшим. Например, даже на порядок меньше. Условно говоря, можно использовать уже не миллиард токенов, а, например, 100 миллионов токенов уникальных текстов. Это хорошо. Но, во-первых, что является критерием полезности? Во-вторых, как правильно выстроить процесс обучения? Оказывается, что критерием полезности является то, насколько этот текст эффективно обучает человека, или нервную модель. То есть тексты учебников, методических пособий, монографии гораздо более полезны, чем просто тексты каких-то соцсетей и прочих вещей. Это интуитивно кажется очевидным, логично, но в ряде исследований Полезность таких вот образовательных текстов была доказана экспериментально именно для обучения искусственного интеллекта. Соответственно, мы по определенному алгоритму собрали тексты, имеющие существенную образовательную значимость для русского языка, таких текстов был чуть меньше миллиона, по-моему 800 тысяч сквозь текам, дообучили на этих русских текстах языковую модель предсказывать следующее слово, следующее, следующее. А потом обучили на специальном инструкционном датасете, который был частично синтезирован из очень большой модели, частично был получен путем перевода с детектором галлюцинации хороших английских датасетов, а частично сгенерирован на основе существующих русскоязычных датасетов, таких как Рукоко для обнаружения корреференции нерл, для распознавания минованных сущностей. Это датасеты не инструкционные, но они были сконвертированы в инструкционный формат. И получили модель, которую назвали Minulite, и мы видим, что она, в принципе, лучше и квена базового, и других его вариантов, например, ProAdaptQuen, который сделала наши коллеги Московского госуниверситета. Лучше в целом, лучше по отдельным задачам, в частности, опять-таки, нам нас интересует задача MultiQueue. И тому подобное. Пока что эта модель не в открытом доступе. Точнее так, я ее уже выложил в открытый доступ. но не документировал. Мы сейчас досчитываем эксперименты на различных бетчмарках, не только на мере, чтобы показать эффективность и границы применимости данной модели. Но тем не менее, вот эта вот модель, она позволяет эффективно обрабатывать контекст. Но нам нужен контекст. Как мы уже договорились с вами, что мы будем выносить знания о мире, о предметной области из неростевой модели во внешнюю базу. Соответственно, архитектура, которая нам нужна, это неростевая модель языка, внешняя база знаний. И вот знания, которые необходимы для ответа на конкретный вопрос пользователя, мы будем добавлять во входной контекст модели путем специальной инъекции. А инъекция происходит на основе поискового механизма. Безусловно, можно попробовать разместить все знания, которые у нас есть в нашей базе знаний. А знания, в данном случае сырыми, не структурированными, являются просто набор текстов. Можно попробовать разместить все тексты, которые у нас есть в одном контексте модели, Это выглядит не так уж фантастично. Разработчики современных большевиковых моделей соревнуются, чья модель может обрабатывать более длинный контекст. Те же ребята из Алебабы, например, объявляли о том, что у них есть линейка Квена, которая может миллионный контекст обрабатывать. Ребята из меты, запрещенной в Российской Федерации, говорят, а вот у нас 10 миллионов токенов в контексте может обрабатываться. Но, во-первых, обработка такого гигантского контекста требует все-таки специальной вычислительной архитектуры, и она медленно является. Во-вторых, в вычислительно возможный контекст равно семантически эффективному контексту. Семантически эффективный контекст все равно гораздо меньше. Поэтому лучше пробовать не раздувать контекст модели, пытаясь положить всю информацию из внешней базы знаний, а как-то оптимизировать использование контекста. Самый простой вариант это использовать поисковый механизм, для того, чтобы найти те документы, которые наиболее релевантны вопросу пользователя и распустить в контексте модели именно их. По какому критерию мы будем считать, что мы распоряжаемся контекстом эффективно? Во-первых, по участнительному критерию, чем меньше токенов мы кладем в контекст, тем лучше. модели будет проще обрабатывать, она будет быстрее считаться. второй критерий семантический. чем больше знаний, смысла, семантики мы разместим в нашем контексте, тем лучше. соответственно, нам нужен смысл. давайте поговорим сначала о семантическом критерии эффективности. Эффективность, семантическая эффективность во многом определяется механизмом поиска. К сожалению, поисковый механизм это достаточно простой механизм. Мы можем искать по ключевым словам. старая добрая методика bm25. можем искать по векторам, в которые отображает текст отдельная нерфевая модель маленькая. вот тот самый модельный энкодер, о котором мы говорили в начале. но эта модель маленькая, она многого не знает о мире, и чтобы поиск был реализован эффективно, Вопрос пользователя, который мы ищем во внешней базе, нужно максимально улучшить, максимально подготовить. Первый шаг – это, может быть, например, расшифровка аббревиатур. Когда начинается прием документов в НГУ? НГУ – это аббревиатура, которая мало о чем говорит. Давайте мы попробуем переписать этот вопрос, когда начинается прием документов Новосибирской государственной университет. Это можно сделать разными способами, это можно попробовать сделать классическими методами компьютерной лингвистики. Например, мы делаем морфологический анализ, определяем морфологическую роль МГУ, какой это падеж, какая часть речи существительная, Поддержь, число и тому подобные штуки. Потом ищем в словаре аббревиатур шифровку. Эту шифровку вставляем вместо самой аббревиатуры, генерируем склонение соответствующее поддержу и тому подобные штуки. Получается плохо, получается проблемно, не определяют, не обрабатываются неоднозначности, морфологическая аммонимия, в принципе, аммонимия не обрабатывается. Например, ИКА, это аббревиатура, которая может означать инфракрасное излучение, и институт катализа, или там ИТ, информационные технологии, институт теплофизики, в общем. Короче, такой ряд проблем, и на самом деле для вот решения этой задачи Нерфевая модель языка, модель общего искусственного интеллекта является оптимальной, если мы специальным образом готовим для нее задачи. То же самое касается местоименная NAFTA. Когда начинается прием документов в НБУ? А когда он заканчивается? Это местоимение. Если мы эту фразу, куда он заканчивается, отправим в поисковый движок, найдется всякая ерунда. Нам необходимо переписаться еще на всей истории диалога, что он это прием документа в НГУ. Переписывание позволяет формировать поисковый запрос гораздо более адекватно и сформировать контекст, более симметрически эффективно. Ну и наконец, элипсис это тоже проблема. Когда заканчивается? Это заканчивается что? Непонятно что. Прием документов в НГУ заканчивается. Здесь нам нужно заменить местоимение, а в элипсисе нам по сути дела нужно заменить пропуск. Это все этапы улучшения вопроса. Но Улучшили мы вопрос, дальше мы его ищем. Есть у нас простой вопрос, когда прием документов заканчивается? Классический поиск в некотором метрическом пространстве по критерию косинусной близости работает прекрасно. Об этом я рассказывал в начале своего доклада, когда говорил про мультимодальный поиск. Но, к сожалению, бывают не только простые, но и сложные вопросы. Вопросы, которые требуют многошаговых рассуждений. Например, какие открытия в области синтеза высокоактивных структурных катализаторов, сделанные в институте катализации Сурран, повлияли на тему дипломных работ на УКНГ в последние три года. Вот это вопрос, для которого формировать эффективный и семантический, и вычислительный контекст путем простого поиска в базе знаний невозможно, почти невозможно. Итак, нам что нужно сделать? У нас там первый шаг. Нам нужно найти документы, в которых описаны недавние отрытия или тренды в исследованиях истинного катализа. Во-вторых, нам нужно найти ой, на химфаке, господи, ошибка, на фене, конечно, на НГУ. Нам нужно найти документы, где описано, какие кафедры института катализа представлены на фене НГУ, какие преподаватели там работают. Нам нужно проанализировать темы дипломных работ, как правило, они выкладываются, найти документы, где описываются вот эти работы. Если мы будем пытаться простым векторным поиском это сделать, вот эта фраза очень сложная, если мы ее просто отображаем в одну точку в нашем семантическом пространстве у немодального энкодера, мы ничего нормального не найдем. Можно пойти другим путем. можно решить вспомогательную задачу. Можно сгенерировать простые, синтаксически простые поисковые вопросы, несколько, три, пять поисковых запросов по вот этому сложному вопросу пользователя. Для этого мы опять-таки можем использовать ту же большую языковую модель, малую языковую модель, если мы говорим о оптимизации. Она все равно достаточно большая, 7 миллиардов параметров. Мы можем попросить отобразить вот этот вопрос в цепочку поисковых запросов. Институт катализа, открытие. Институт катализа, факультет естественных наук. факультет естественных наук, дипломные работы, то есть вот такие вот короткие простые запросы, и дальше их искать уже с помощью векторного поиска, либо гибридного поиска, векторный поиск плюс фиксический ПМ25. Это в принципе хорошо работает, но при этом у нас будет очень большая выдача. Мы с помощью генерации вспомогательных простых поисковых запросов Как частично решили проблему семантической эффективности, но мы просто провалили вычислительную эффективность, потому что мы формируем отдельные поисковые выдачи по каждому из простых вопросов, запросов поисковых, их объединяем эти выдачи и получаем гигантскую выдачу на 100 документов, ну на 50 документов. Это проблема. Проблему надо решать. И вот здесь нам на помощь приходит структуризация самой базы знаний. Если мы будем представлять знания о предметной области не просто в виде сырых текстов и их отображение в некоторое схематическое пространство, А будем строить граф знаний, то мы сразу одним выстрелом убиваем двух зайцев. Мы решаем проблему и вычислительной эффективности, и семантической эффективности. Эта идея пришла не только мне в голову и моим коллегам, на самом деле эта идея витала в воздухе и исследования в этой области проводились последние лет 10 точно. Но в 2024 году коллеги из Майкрософта предложили работу, которая позволила сделать эффективный алгоритмический и полезный практический компонент, обеспечивающий построение графа знаний, автоматическое построение графа знаний по старым текстам и эксплуатация этого графа в вопросно-заглядной системе совместно с большой языковой моделью. Эта статья называлась From Local to Global. Local to Global. Graph-Rack Approach to Query-Focused Summarization. То есть как раз название говорит само за себя, что мы с помощью графа знаний моделируем многоуровневые абстракции, возникающие в заданной предметной области. Вершинами графа являются некоторые ключевые сущности, именованные сущности разных типов. Отношениями, дугами графа являются бинарные отношения между этими сущностями. отношения описываются, причем описываются не абстрактно, а исходя из тех конкретных параграфов, элементов документа, откуда они были выделены, происходит мы сталкиваемся с амонимией, что одна и та же сущность играет разную роль в разных документах, мы формируем либо обобщение, обобщенное описание, либо просто контейнируем и получаем такую достаточно интересную структуру знаний в виде графа. И когда наступает период эксплуатации, мы ищем не сырые тексты, мы ищем вершины и дуги, либо по всему графу, либо в рамках каких-то кластеров на графе, каких-то локальных сообществ, которые, например, выделяются лабиринтным лейдером. Таким образом, мы тратим гораздо меньше контекста для представления высокоуровневых понятий, чем если бы мы просто пытались сырыми текстами забить наш контекст. Но при этом сама работа GraphRack, она имела ряд недостатков. Почти сразу были предложены различные решения, оптимизирующие эту идею. Например, известная работа наших китайских коллег про LightRack. Были предложены тоже другие способы построения графа знаний и эксплуатации графа знаний в процессе ответа на вопрос. NanoRack, FastRack и тому подобные штуки. Изначально мы решили построить граф знаний на основе LightRack. Вот тут я показал для иллюстрации фрагменты графа знаний. в процессе построения, у нас есть какие-то вершины, это понятие, например, мера. Мера и описание. не что такое мера вообще, а исходя из конкретного документа, в котором эта сущность была выделена. Мера – это открытый бенчмарк, созданный для оценки качественно-рассказочных нейросетевых моделей, где нейросетями нам демонстрируют свои способности. Допустим, что-то, какой-то другой фрагмент В этом году много что произошло, но в данном случае для этой вершины важно, что этот год, это год изменения звания кафедры, с кафедры теплофизики на кафедры физики неравновесных процессов. Есть отдельные элементы графа знаний, которые описывают какие-то научные документы, там уравнение Янга-Бакстера, это видимо что-то связано, ну это какие-то документы из сайта факультета математики, механики. В общем, вот подобного рода граф строится и дальше контекст формируется тоже в процессе поиска, причем для простоты мы реализуем тот же самый векторный поиск, как и в классическом знании через припоминания. Но в качестве объектов, которые мы ищем, выступают не сырые тексты, а вершины и дуги графа. Мы можем найти какую-то вершину, мы можем сделать несколько шагов по поиску вершин в окрестности и формировать контекст, который максимально эффективен. Он небольшого количества токенов требует для самого представления, большого количества элементов одного контекста, но при этом Он описывает именно те понятия, которые фигурировали в вопросе человека. Тем не менее, подобного рода решения все равно есть недостатки. Серебряная пыль не существует, идеального решения пока еще не найдено. Основные проблемы систем на основе графов знаний заключаются в том, что задача построения графа знаний весьма сложна. Мы упрощаем эксплуатацию системы, упрощая inference, сильно усложняем подготовку к эксплуатации, сильно усложняем индексацию базы знаний. И это проблема. К сожалению, большие языковые модели, они могут галлюцинировать на этапе построения графа знаний. Если мы просим модель сразу обработать какой-то документ, точнее фрагмент документа, выделить из него сущности, отношения, сделать выводы о силе отношений, entity linking их решить, то есть нормализацию их сущности, Сами модели могут с этим не справляться. Например, авторы статьи про LightRack прямо заявляют, что для эффективного построения графа знаний нужно использовать модель не меньше чем 32 миллиарда параметризованных международных связей. Иначе граф знаний получится плохим. Проблема. Это получается построение такого графа знаний долго, вычислительно дорого и не всегда качественно. Из-за того, что задачи сложные. можно сделать? Мы с моими учениками подумали-подумали и придумали. Последний год идет активная, фактически вторая жизнь мультиагентных систем. Сама концепция мультиагентных систем существует достаточно давно, лет 30 точно. Изначально мультиагентные системы рассматривались как средства построение того, что сейчас называется цифровыми двойниками, а раньше называлось имитационное моделирование. То есть мы сделаем имитационную модель предприятия, где агенты моделируют отдельных агентов, моделируют отдельные сущности моделируемой системы. Взаимодействие между агентами подчиняется определенным правилам, происходит по определенным протоколам. Таким образом мы получаем некоторую имитационную модель, туда можем внести какие-то вводные С развитием генеративного искусственного интеллекта мультиагентные системы получили второе дыхание. Сейчас мультиагентные системы рассматриваются как эффективное средство управления сложностью. Когда мы вместо одной большой нейросетевой модели которая решает задачу за один шаг, используя мультиагентную систему, мы тем самым неявно разбиваем сложную задачу на цепочку простых. А задача? Каждую из этих простых подзадач решает отдельный агент. Это может быть одна и та же нейронная сеть, но с разной системной подводкой, с разным системным промптом, описывающим задачу. Например, это может быть агент-планировщик, агент, допустим, поисковик, агент-генератор, агент-критик и тому подобные штуки. Мы решили применить подход, аналогичный мультиагентному подходу для построения графа знаки и разработали открытую библиотеку Oracle, которая основана на принципе цепочки агентов. У нас агента планировщика нету, потому что сценарий извлечения информации из документов у нас детерминированный, Фактически у нас в роли планировщика выступает детерминированный алгоритм, но агенты, которые решают разные задачи политической информации, у нас разные. Тем образом, мы не пытаемся сразу построить по документу некоторый подграф, который потом интегрируется с общим графом. Мы используем цепочку агентов, где один агент, например, извлекает именованные сущности из текста. Другой агент решает задачу нормализации этих именованных сущностей. Третий агент генерирует определение того, что означают эти именованные сущности в данном конкретном тексте. Четвертый агент определяет бинарное отношение между существами и так далее. Таким образом, цепочка агентов решает сравнительно простые задачи по извлечению и обобщению информации. Мы резко снижаем требования к сложности самой неростевой модели, в отличие от того же LightRug, где требуется 32 миллиарда параметров. Мы сделали вариант системы, где 600 миллионов параметров всего агента засчитывается и 7 миллиардов. В принципе, система на основе 7 миллиардного агента, Получше, чем система на 100 миллионов параметров агента, но, тем не менее, даже система с маленьким агентом строит достаточно эффективный граф знаний, который хорошо моделирует информацию. Таким образом, мы довели концепцию повышения эффективности использования контекста, как вычислительной, так и семантической, до некоторого предела. Мы, во-первых, представляем сам контекст в виде графа знаний, что позволяет снизить количество токенов, отводимые на описание необходимого контекста, и повысить вещественную эффективность. Во-вторых, сам процесс построения графа знаний тоже оптимизирован за счет того, что мы рассматриваем выделение, построение подграфов по каждому из документов, как цепочку задач, решаемых цепочкой агентов. На этом я хотел бы закончить. Спасибо большое за внимание. Возможно мой доклад вам показался немножко сумбурным, я до конца и подготовил так, как я хотел бы это сделать. Но, тем не менее, надеюсь, что какие-то полезные идеи, полезную информацию вы из моего доклада подчеркнули. Спасибо за внимание, уважаемые слушатели, и я буду рад ответить на ваши вопросы. 

S00 [00:56:47] : Да, Иван, большое спасибо. У меня есть вопросы. Может быть, пока я их буду задавать, другие желающие подтянутся. У нас в чате... Ой, извиняюсь, я тут слегка простыл, поэтому буду хрипеть. Вопрос по поводу архитектуры. Вы когда сказали... Ой! Извиняюсь. Вы когда говорили про вашу мультиагентную архитектуру, вы сказали, что вас этим управляет некоторый детерминированный алгоритм. Я правильно понял, что просто вы из этих агентов строите некоторый пайплайн, который переводит тексты в графы? Да, абсолютно верно. Тогда вот два вопроса. Лина Бессонова задает, а если бы в этом месте был бы не детерминированный пайплайн, а просто некоторый агент системой микросервисов внутри него, это бы что-то поменяло? 

S01 [00:57:42] : Ну микросервис это вопрос архитектурный, то есть как собственно эти агенты взаимодействуют в виде классов на питоне, либо в виде микросервисов. Это бы ничего не поменяло, но если бы мы использовали не детерминированный сценарий, а какой-то динамический сценарий, то есть некоторый цикл с агентом-планировщиком, это могло бы, с одной стороны, замедлить построение, что минус, с другой стороны, это могло бы, возможно, повысить эффективность построения подграфа по конкретному документу. Но тут вопрос интересный, по-хорошему нужно провести эксперименты. Лично мне кажется, что повышение эффективности вряд ли было бы серьезным, потому что мы сами создали детерминированный сценарий, исходя из анализа того, что является важным для графа знаний. в тексте. И мы свели это все к цепочке достаточно простых задач. Вряд ли бы динамическая обработка с помощью мультиагентной системы с планировщиком могла бы что-то улучшить. Она могла бы улучшить, если бы у нас был большой документ, но мы большие документы не обрабатываем. У нас есть отдельный этап, когда мы большой документ разбиваем на кусочки, причем разбиение на кусочки происходит семантически. Мы отдельную нейронную сеть используем, которая определяет места наиболее резкого семантического перехода и на основе этого делит документ на такие кусочки, которые соответствуют некоторым семантическим единствам, сверхфразовым единствам. А когда мы обрабатываем эти короткие сверхфразовые единства, в принципе, детерминированный pipeline достаточно хорошо извлекает информацию. 

S00 [00:59:29] : Спасибо. И второй вопрос у меня на эту же тему. То есть понятно, что мы разбиваем одну большую задачу на много маленьких специализированных с целью снижения количества ошибок и повышения общей точности. Но глядите, если мы сделали этот пайплайн, допустим, на первом этапе мы выявляем сущности, а на втором мы строим между ними ребра. Если мы определяли миллион, или сто тысяч, или сто кривых сущностей, или часть из них кривые, а потом то есть ошибки же все равно могут быть и потом отдаем эти сущности на создание ребер между ними, то у нас же все равно в итоге получится битый граф, что называется. Вот как вы смотрите на возможность вот именно минимизации ошибок на перепрохождении данных по пайплайну и предполагается ли там какое-то участие человека или там какие-то есть дополнительные механизмы контроля на непротиворечивость данных на каждом этапе. 

S01 [01:00:38] : Спасибо за вопрос. Участие человека не предполагается, потому что слишком трудоемкая задача. Даже по корпусу 3 тысячи текстов, например, у нас получаются сотни тысяч вершин и еще больше дух. Соответственно, ручная корректировка невозможна. Для того, чтобы повысить достоверность извлечения кандидатов в вершины и в дуги, мы сделали специальное дообучение, то есть мы создаем специальную модель, например, маленькая модель 600 миллионов параметров, мы ее назвали RAGU-LM. а модель с 7 миллиардов параметров – это наш Minolight, который обучался в том числе эффективно решать подобного рода задачи. При этом мы рассматриваем вопрос автоматической валидации решения каждой задачи в пайплайне с помощью отдельной нейросетевой модели – дегаллюцинатора. У нас есть входная задача, нейросеть врагу LM или Minolight генерирует ответ, являющийся решением задачи, например, список сущностей или сущность и ее определение, исходя из контекста. Дальше включает, в игру вступает галлюцинатор, который оценивает, насколько сгенерированный ответ является релевантным исходной задачи и тому текстовому фрагменту, в рамках которого задача решается. Замедляет немного процесс работы, но поскольку этот дегаллюцинатор это не генеративная модель, это классификационная модель, то в принципе замедляет незначительно. Таким образом, да, мы рассматриваем возможность автоматического контроля за поведением этих маленьких агентов. 

S00 [01:02:25] : Но смотрите, если я правильно понял, вы валидация делается уже на стадии ответов, а вот сами графы... Нет, это на стадии построения графа именно. 

S01 [01:02:34] : То есть когда модель Roku LM генерирует ответ, это она генерирует ответ в определенном формате, JSON структура, в которой решается конкретно задача по извлечению информации. И вот это валидируется, то есть именно на этапе построения графа. 

S00 [01:02:52] : А вот именно на саму технологию построения графа вы не было мысли сделать какой-нибудь бенчмарк? Ну, грубо говоря, набор стандартных документов со стандартными графами, и вы ожидаете из этих документов получить соответствующие графы и пытаться считать качество построения этих графов на основе данного эталонного. 

S01 [01:03:15] : Это, на самом деле, очень сложная задача. Что является эталонным графом, не совсем понятно. Нам кажется, что наиболее простой способ оценить, что граф хороший, это насколько он помогает различным моделям неростивым отвечать на сложные вопросы, которые включают multiple prison. То есть проблема, на самом деле, правильно сказали, существует. Это проблема не только для графов знаний, для любых моделей представления информации. Те же вот эти нейросети, которые отображают тексты в вектор. Как оценить, хорошие вектора получаются или нет? Сообщество, по сути, пришло к тому, что если эти вектора, бетчмарки составляют следующим образом. Если граф эффективно помогает решать задачи в различных предметных областях, на наборе различных нейронных сетей, значит наша методика построения графа лучше. Вот мы сейчас работаем над формированием именно такого рода бетчмарков и планируем научную публикацию, в которой будет написано Ragu, его методология и оценка его эффективности на бетчмарках. 

S00 [01:04:34] : Спасибо. Еще есть два вопроса от Лины Бессоновой. Во-первых, ошибается ли галлюцинатор? И второй сразу вопрос, какова точность выделения резких семантических переходов? Если этот вопрос понятен. 

S01 [01:04:51] : Да, да. Скажем так, галлюцинатор ошибается, безусловно. у него f1 мера по бинарной классификации примерно 0.82. Но опять-таки, когда мы его используем практически, мы можем балансировать порог, чтобы выкручивать, что нам важнее, precision или recall. Например, если мы готовы пожертвовать частью информации, но не допустить появления мусорных вершин и дуг, то мы можем повысить порог. в пользу точности precision. И таким образом получаем граф, в котором отображены, может быть, не все понятия, но самые важные отражены. И в такой ситуации, на этапе эксплуатации, можем гибридную модель поиска делать. Одновременно контекст формируется из простого векторного поиска и из поиска по графу знаков. Таким образом, мы можем решать проблемы, связанные с потенциальными ошибками в построении графа знаний, не допуская их вообще. Пусть лучше ошибки будут типа пропуска цели, чем ошибки типа ложная тревога. Это один момент. по поводу разбиения на абзации, определения точности систематических переходов. Системного тестирования мы пока что не проводили. То тестирование, которое было, фактически выполнялось экспертным способом. Брались тексты, которые были разбиты на абзацы уже заранее, они слиплялись в один большой текст, запускалась модель, она давала свою версию разлияния этого текста на элементы и, собственно, глазами экспертов оценивалось, насколько эти элементы соответствуют тем абзацам, которые изначально были изложены автором. Здесь пока что числовой оценки нет. 

S00 [01:07:03] : Спасибо. Еще вопрос от Вики Кондрашук. Насколько вычислительно сложно добавлять, удалять неактуальные знания в графе знаний? 

S01 [01:07:14] : Это несложно совершенно. Самое сложное это один раз обработать большой текстовый корпус. Это может занимать несколько часов. А когда граф знаний готов, то его автоматическое редактирование, управление графом знаний, практически несложно. Мы берем новый документ, мы встроим подграф, а дальше автоматически определяем, какое место в общем графе занимает вновь построенный подграф, какие вершины сопоставляются по семантике, нужно ли, допустим, слеплять, допустим, вершины какие-нибудь, допустим, Программа и программа. В одном случае программа образовательная, в другом случае программа это последовательность действий для компьютера, компьютерная. Это одна вершина, либо их нужно расчистить на две вершины, но это решается достаточно участительно и легко. Самая большая проблема, я уже сказал, участительная, это время индексации. В базовых подходах графом RAG она вообще очень большая, мы попробовали в RAG ее оптимизировать, но все равно это часы. Но тем не менее, она считается один раз. Когда нам нужно актуализировать, это гораздо быстрее. 

S00 [01:08:27] : А вот вы не рассматривали такие два кейса? Вот я просто сейчас думаю, вот один кейс, допустим, там взять, к примеру, НГУ с документами НГУ, да, вот в вашем примере, или там вот мы обсуждаем у нас в сообществе, допустим, есть расшифровки семинаров, да, каждую неделю добавляется новый семинар, и в этом плане мы, казалось бы, можем просто регулярно обновлять базу данных, добавляя туда новые подграфы, обрабатывая новые документы, правильно? То есть мы не перечисляем информацию, мы просто проиндексировали новый документ, подгрузили его в базу данных. А как быть с тем, например, если какие-то документы теряют актуальность? Вот, допустим, мы загружаем-загружаем документы, допустим, какие-то, ну, допустим, берем организацию, где есть некоторые регламенты. Вот, и вот мы добавляем, добавляем регламенты, а потом выясняется, что какой-то регламент отменили, да, и он теряет свою силу. Вот как можно ли выкорчевать из графа базы данных под граф соответствующий отмененному документу? Вы не думали про такой кейс? 

S01 [01:09:29] : В данном случае реализовано простое удаление вершин. Есть у нас какой-то регламент, который является самостоятельной сущностью и моделируется с отдельной вершиной. Существует график знаний, его можно удалить, соответственно, с удалением всех дуг, которые связывают другие вершины с удаляемыми. Пока что реализовано вот так. Более сложного механизма удаления мы над этим думаем, но пока не реализовали. 

S00 [01:09:54] : Ну то есть там как минимум должен быть очевидно какой-то счетчик ссылок, чтобы случайно не удалить вершины, которые повторно используются двумя документными графами, чтобы мы, удаляя вершины, связанные с одним документом, не удалили те вершины, которые связаны с другим документом, еще актуальным. 

S01 [01:10:16] : Ну да, безусловно. 

S00 [01:10:19] : Хорошо, еще у меня пара вопросов по поводу, вот вы говорили про поиска. А как вы осуществляете поиск вершин и ребер? То есть у вас, если я правильно понял, у вершин есть какие-то текстовые описания, генерации? Да. и вы их в векторной базе данных складываете и по векторной базе данных пишете. 

S01 [01:10:40] : и для вершин и для ребер тоже есть текстовое писание, что означает вот это ребро. 

S00 [01:10:46] : то есть, грубо говоря, когда вы генерируете RDF триплет, у вас для каждого, и для вершины, и для ребра, генерируется некоторый текстовый пилот, по которому вы можете делать индексацию. 

S01 [01:10:58] : да, верно. 

S00 [01:11:00] : понятно. Спасибо. Коллеги, есть еще вопросы к Ивану? Вопросов нет? Ну, тогда хочется поблагодарить Ивана за доклад. Было очень интересно, особенно вторая часть. То есть, первая часть, она такая была более познавательная. Вот, а вторая часть, она была, по-моему, совершенно практичная. Вот, я думаю, многим здесь присутствующим будет полезно. Кстати, базу данных вы какую используете для графовых представлений? 

S01 [01:11:37] : Вот это хороший вопрос, на самом деле. Мы даже вот внутри нашего коллектива дискутируем. Сейчас мы используем библиотеку NetworkX для представления графиковых моделей. Некоторые из членов нашего коллектива говорят, давайте Neo4j использовать. Но, например, мне кажется, это плохим решением, потому что то же Ragu, которое мы делаем, оно является open-source. В Neo4j есть инженерные достоинства, но юридические недостатки. Это не является в полной мере open-source система. Сейчас один мой коллега, мой тезка, Денис. Вот небольшой бенчмар изделия различных графовых баз знаний, соответственно, сколько они производительными являются, ну и в то же время анализ лицензии. И вот мы будем думать для по-настоящему большого масштаба, когда у нас там не сотни тысяч, а десятки миллионов вершин что лучше всего использовать. Neo4j, конечно, поддержать надо, потому что тот же конкурент в виде LightRack поддерживает несколько вариантов представления графа. В Postgre есть плагин, вот он его поддерживает. Neo4j, тот же NetworkX как базовый вариант, а у нас пока только NetworkX в рагу. Но нам нужно сделать ставку на более эффективные ПДСУБД для графов знаний, если мы хотим масштабироваться, хотим, чтобы наши пользователи работали с большими графами. 

S00 [01:13:04] : Понятно. Хорошо. Иван, большое спасибо за доклад. Коллеги, всем спасибо за участие и за вопросы. До новых встреч. В следующий четверг у нас будет Татьяна Шаврина. Тема доклада еще точно неизвестна, но, очевидно, она будет рассказывать что-то новая из мира больших языковых моделей и мультиагентных систем. Возможно, это будет связано с автоматизацией научной деятельности. Но точно название доклада Татьяна обещала дать в воскресенье. Всем спасибо. До новых встреч, Иван. Вам успехов. 

S01 [01:13:38] : Спасибо за то, что пригласили. Спасибо за ваше внимание. До свидания. 

S00 [01:13:43] : Спасибо. До свидания. 









https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
