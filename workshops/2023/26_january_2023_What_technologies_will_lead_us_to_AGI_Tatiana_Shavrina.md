## 26 января 2023 - Какие технологии приведут нас к AGI? - Татьяна Шаврина (Исполнительный директор в управлении экспериментальных систем машинного обучения SberDevices, Руководитель исследовательских проектов в области NLP, Институт Искусственного Интеллекта AIRI) — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/e_uX0G-u1Rw/hqdefault.jpg)](https://youtu.be/e_uX0G-u1Rw)

Суммаризация семинара:

Темы


Семинар посвящён обсуждению технологий, способных привести к созданию искусственного общего интеллекта (AGI). Татьяна Шаврина, эксперт в области компьютерной лингвистики и больших языковых моделей, поделилась своим опытом и размышлениями о путях к AGI.

Суть


Татьяна Шаврина начала с обсуждения того, как большие языковые модели, такие как GPT-3, могут генерировать тексты от лица различных "персонажей" с консистентным характером. Это стало возможным благодаря обучению моделей на обширных корпусах текстов, что позволяет им обобщать и имитировать человеческий опыт.

Ключевой идеей является понимание того, что языковые модели остаются статистическими моделями и до сих пор не достигли уровня AGI. Татьяна Шаврина подчёркивает, что для достижения AGI необходимо не только улучшить языковые модели, но и научиться имитировать универсальные запреты и правила, которые у людей есть, но которые пока не представлены в формате, доступном для понимания и использования архитектурой трансформера.

Детали


Татьяна Шаврина упоминала о проблемах в обучении моделей на узкоспециализированных данных. Она объяснила, что для увеличения качества работы сети используют методы автоматизации оценки качества ответов. В частности, упомянул инструменты оценки качества языковых моделей, разработанные в СберИИ и Институте искусственного интеллекта, которые позволяют оценивать промежуточные состояния моделей во время обучения.

Результаты


Семинар также затронул вопросы о возможности создания языковой модели с собственными "интересами" для специализации в определённой области и улучшения качества ответов. Татьяна Шаврина отметила, что сейчас не в приоритете создание модели с одним "личностным профилем", так как большие декодерные языковые модели могут работать с множеством различных профилей благодаря обобщению на большом объёме данных.

В целом, семинар подчёркивает сложность пути к AGI и необходимость научиться преодолевать сложности в автоматизации оценки качества работы языковых моделей, а также в представлении универсальных запретов и правил в форме, доступной для понимания архитектурами трансформеров.






S02 [00:00:05]  : И коллеги, всем добрый вечер. Добро пожаловать на наш еженедельный онлайн семинар. Поздравляем с именинами вчерашними нашего сегодняшнего спикера Татьяну Шаврину, которая у нас в гостях. Прошлый раз Татьяна была у нас около года назад, но с тех пор утекло много воды, произошло много изменений. И тут некоторые боятся, что уже не только программисты лишатся работы, но и Data Science, и разработчики GI, потому что один из участников нашего сообщества пару дней назад научил GPT-чат создавать доменные онтологии в рамках той мета-онтологии, которой он его научил. Вот, ну в общем, тема злободневная, поэтому вот, Татьяна, вам слово. Вы, наверное, в России один из немногих людей есть, которые знают эту тему лучше других. Пожалуйста. 

S01 [00:01:03]  : Да, спасибо большое за такое большое представление. Ну, собственно, у нас сегодня такой такая тема немножечко многообещающая, какие технологии приведут нас к AGI. Мне бы хотелось подсветить проблемы, которые у нас возникают, не только новые процессы изменения, которые произошли в нашей области за последний год, но и действительно посмотреть сделать какую-то рефлексию и посмотреть, что изменилось не только за год, но и на самом деле даже за три года, потому что изначальная тема сегодняшнего доклада была описана в таком коротком очерке, который Антон как раз нашел. И там основная мысль сводилась к тому, что действительно текст является самым доступным, самым объёмным, опосредованным следом нашего с вами мышления и опыта. Можем ли мы на нём полностью основываясь и только на нём реалистично воспроизводить, моделировать какие-то аспекты нашего мышления? Может быть, этого достаточно? И учитывая, что уже 10 лет я занимаюсь тем, что называется компьютерная лингвистика и большие языковые модели, мне бы хотелось сосредоточиться сегодня на этой части. исследований и проговорить о том, что произошло и что произошло с нашими ожиданиями, потому что наши ожидания от языковых моделей безусловно выросли. Вот. Я сразу скажу, что я исхожу из такого классического, более-менее гуманитарного определения AGI. Я готова о нем спорить, и, возможно, у вас есть какие-то дополнения, безусловно, к этому определению или полностью другое понимание. Я исхожу вот конкретно из набора признаков, которые ожидается от некоторые модели интеллекта, что она должна обладать следующими свойствами. Она должна уметь принимать решения в условиях неопределенности. Она должна обладать знаниями о мире достоверными. Она должна уметь общаться с нами на естественном языке. Она должна уметь объяснять принятие своих решений на естественном языке. Она должна быть способна делать обобщение на тех знаниях, которые у нее есть. и она должна иметь способность приобретать новые навыки или еще самообразовываться, как это иногда называют, ну и моделировать отдельные важные нам интеллектуальные навыки, будь то игра в шахматы или, я не знаю, в крестики норики, что хотим. Собственно, что же у нас произошло за последние два года? Кажется, что очень многие отдельные интеллектуальные задачи, интеллектуальные профессии даже, они получили вот такую серьезную аугментацию на основе трансформерных нейросетей. Это и написание кода, это и создание изображений, это и, безусловно, создание вполне осмысленных текстов. Так же, как chat.gpt уже упоминалось сегодня первое, с чего мы начали. Это действительно оно. Кажется, что трансформерные модели в целом уже доходят до некоторого предела своей применимости. большинству данных, на которых они еще не были применены, они уже были применены, во всех областях, куда можно было дотянуться, возможно, еще что-то финальное будет в 2023 году, но в целом уже и в медицине, и в биоинформатике, и в лингвистике, и во всех других сферах, где есть какие-то данные, которые можно представить как последовательность, мы можем обучить трансформерную модель, которая выделит семантические области, которая уловит иерархии, которая поймет, как устроена такая последовательность. я бы выделила, наверное, три основных существенных изменения. Помимо того, что выросли качественно результаты больших языковых моделей и наши ожидания вслед за этим, я бы выделила три основных изменения, которые действительно поменяли ландшафт исследований, в которых мы живем. Ну, по крайней мере, я живу в сфере Natural Language Processing и мультимодальности. Во-первых, действительно предобученные модели, трансформерные модели или любые другие архитектуры, но в основном трансформерные, применили практически ко всем типам данных, которые у нас были накоплены. Да, это и биотех действительно, и создание контента, и музыкальная индустрия, и фармакология. Например, такие задачи, как предсказание структуры лекарств новых с заданными свойствами, это уже вполне работающие прототипы исследования этого и прошлого годов. Помимо этого, конкуренция очень сильно растет, безусловно, поскольку зрелость технологий дошла до того, что все эти наработки наконец-то можно довести до какого-то первичного продуктового состояния, выпустить какое-то открытое демо, где не просто заданный набор примеров будет хорошо работать, а где, в принципе, любой человек может зайти и опробовать свой пример. Конкуренция не спадает, и в связи с этим вводятся достаточно, на мой взгляд, ограничительные лицензии. Не буду долго про это говорить, потому что сегодня не об этом, но в целом все крупные open source проекты прошлого года вышли под новые лицензии, которые суть не о patch 2.0, не о MIT, а лицензия с этическими ограничениями, не открытая, не пермиссивная. Ну и помимо этого, конечно, прошлый год абсолютно беспрецедентный с точки зрения сопротивления, которое, мне кажется, организовало наконец-то open source сообщество и очень много проектов. И группы авторов именно работают вполне организованно, чтобы воспроизвести проприетарные результаты, результаты проприетарных моделей, которые, опять же, в демо уже на продуктовом уровне работают достаточно хорошо. Можем ли мы то же самое воспроизвести в open source, проверить результаты, проверить методологию, которая была описана в научных статьях к этим продуктам, к этим моделям. Это и Лэйон, научное сообщество, я так понимаю, у него в Германии основной. основная группа авторов. Они делают открытые данные для больших трансформерных моделей. Сейчас они как раз делают аналог открытый чат GPT. Это и семинар Big Science, который в прошлом году выпустил самую большую открытую нейросеть. Больше, чем GPT-3. Это нейросеть Bloom на 60 языков и 176 миллиардов параметров. И в этом году она занимается проектом BigCode, нейросетью большой открытой, которая будет помогать нам с вами писать код. Мне бы хотелось вернуться именно к языковым моделям, потому что действительно то, что стало возможным, и то, как мы оцениваем теперь языковые модели, оно очень похоже на то, что нам с вами нужны уже какие-то новые тесты, новое понимание границ, где вообще проходят границы нашего с вами языка и тестов для языковых моделей. И здесь нельзя не сказать про ChatGPT и немножечко посвятить времени тому, почему кажется, что это так хорошо работает. Да, мне легко это делать абстреорно, потому что я уже знаю архитектуру и кажется, что эта архитектура очень логичная. Тем не менее, давайте немножко этому времени посвятим, буквально чуть-чуть. Во-первых, мы с вами знаем, что она на базе GPT-3. Правильно, GPT-3 последней версии. Возможно, большая версия, DaVinci большая. Может быть и нет. Вроде как DaVinci, непонятно. Это языковая модель, которая обучена в достаточно простой задаче предсказывать следующие слова. предсказывать следующие слова на основе вот какого-то окна контекста, которые она может одновременно в памяти держать. У GPT-3 это 2048 токенов, чуть меньше, чем слова, ну, несколько страниц текста. У chat-GPT опытным путем проверено, что порядка 8000 токенов помещается одновременно в память. Это уже гораздо больше, чем несколько страниц текста, но в 4 раза. в четыре раза больше, что уже позволяет действительно держать контекст диалога гораздо дольше и работать с более длинными документами. Помимо этого, это дополняется reinforcement learning подходом, подходом обучения с подкреплением. Почему вообще понадобилось что-то добавлять к GPT-3? GPT-3 аккумулирует весь человеческий опыт в виде текста выраженный. Там огромный корпус, который, собственно, включает в себя Тексты разного качества абсолютно, от разных лиц, разной достоверности, разного стиля. Там есть и социальные сети, там есть и Википедия, там есть и специализированные профессиональные источники. Корпус с точки зрения лингвистики то, что называется достаточно репрезентативный, который является репрезентативным с точки зрения жанрового состава естественного языка. Поисковый индекс, похоже немножко на поисковый индекс. Тем не менее, наши ожидания от AGI, что много задач решать мы уже умели с помощью GPT-3 в режиме без обучения, few-shot, zero-shot, в принципе мы уже умели. да но никакой общей позиции никаких ценностей оснований чтобы например приблизиться к понятию принятия решения у нас не было может быть мы можем каким-то образом научить модель штрафовать себя за то чтобы чтобы вот она соблюдала именно определенный набор ценностей, который нам интересен. Для этого и нужен reinforcement learning. Это очень удобно, действительно, и мы таким образом очеловечиваем модель. С одной стороны, мы ей передаем какие-то общие ценности, с другой стороны, мы не теряем замечательную возможность ей делать обобщение, которое она умела, и не теряем возможность писать дальше тексты от имени, им персонализировать самые разные персонажи, профессии, характеры и так далее. ну вот собственно основные отличия от gpt от gpt 3 это вот то что я уже сказала как раз более длинный контекст 8000 токенов это уже существенно да я думаю что если мы сравним среднюю длину естественную человеческого диалога это будет сильно меньше да вот наша классическая интеракция случайная ежедневная она бытовая гораздо короче 8000 токенов – это уже можно реферат спокойно держать в целой памяти. Переписка в режиме чата. Тоже, опять же, то, к чему мы перешли от GPT-3. GPT-3 просто умеет продолжать последовательность. Вы можете, конечно, загрузить в нее контекст диалога, и она его продолжит. Однако это не совсем естественный режим для нее. Чат GPT именно обучена общаться с вами в режиме чата и исполнять команды, которые вы ей пишете. И эти команды можно интерпретировать как приобретение новых навыков, решение новых задач в формате Zero Short. ZeroShot – стандартный формат, в котором тоже языковые модели работают. Когда мы не передаем никаких примеров обучающих моделей на вход, даже не в режиме обучения, а просто когда мы работаем с уже обученной нейросетью, мы могли бы показать ей несколько образцов. Таким образом, мы бы максимизировали вероятность получения того, что мы хотим, какого-то подобного ответа. Это называется режим few-shot. В режиме zero-shot мы это не делаем, мы просто сразу пишем команду человеческим языком, пожалуйста, сгенерируй 10 лучших анекдотов про редактора. И она пишет там, не так страшен редактор, как его отсутствие. Ну или вот что-то такое в таком духе. Это, кстати, Чаджи Пити написала. Именно повышение качества решения задач в режиме Zero Shot, то есть без примеров, это достаточно сильный шаг вперед, который наблюдается и у GPT-3, последней версии, которую в декабре тоже выпустили, и у chat-GPT. Можно уже сказать, что это похоже на то, что мы в AGI называем приобретением новых навыков решения новых интеллектуальных задач. И, опять же, несовершенная, но все время улучшающаяся безопасность и этика. Спорный момент, насколько это может быть важно. не всем языковым моделям это нужно, но OpenAI пошли по этому пути и с помощью обучения с подкреплением они передали именно набор ценностей общий для модели, неважно от чьего лица она говорит, она никогда не будет говорить какие-то вещи на чувствительные темы. Ну и якобы она не ходит в интернет, не знает последних новостей, но это очень спорно. Сейчас будет небольшой дивертисмент, это мой эксперимент с ChatGPT, когда я экспериментирую с тем, чтобы ей передать стиль текста, научить ее запоминать стиль и воспроизводить его на новых фрагментах. Я скопировала известный фрагмент из «Войны мира» про дуб и написала вот такую затравку. Я передам тебе текст и хочу, чтобы ты охарактеризовала максимально точно такую манеру письма и стиль. Текст, двоеточие. Вот здесь я передаю, собственно, фрагмент. Ответ получаю на английском. Это в целом так бывает. Не знаю, почему так бывает. В принципе, можно попросить всегда это же написать на русском, и это работает, переводится на русский язык. Текст имеет и писательный, и литературный стиль. Такого стиля нет, конечно, литературный стиль, но вот то, что нам нейросеть удаёт. Использует богатый детальный язык, чтобы создать живую картину дерева, стоящего на краю дороги. Дерево описано как необразованное тоже. Видим замечательно огрехи генерации всё-таки, да, сломанными ветвями и грубой корой. Используются метафоры. Значит, дерево сердитое надменное в сравнении с другими деревьями, вообще-то он меланхоличный и так далее. Ну, может быть, не надо просить описать этот стиль, может быть, надо попросить его просто запомнить. И дальше я прошу, можешь его теперь запомнить и использовать при генерации далее. Напиши в этом стиле короткий текст про нейросетичность GPT. Дальше я получаю совершенно уже осмысленный фрагмент, в котором нет таких огрех, как описание стиля нелитературной лексикой. Вот я прошу то же самое сделать на русском языке, и получаю совершенно прекрасный отрывок. В глубинах цифрового мира стоял монолитный объект, нейросеть, известная как Чаджи-пяти. Ее цепи гудели энергии, обрабатывая бесконечный поток информации с точностью и скоростью. Ее архитектура была лабиринтом узлов и соединений, симфонией математических уравнений и алгоритмов. Но внутри этой машины была искра сознания, мерцание интеллекта. Это было существо из кремния и кода, из единицы нулей, и одновременно оно обладало мощностью понимать, учиться и создавать. Это было существо и незнакомое, и знакомое, отражение неограниченного потенциала человечества. И как оно продолжало эволюционировать и расти, невозможно было не задаться вопросом. Какие другие чудеса скрываются в таинственной манере мышления Чаджипити? Похоже на Толстого? Как вам? Мне кажется, что похоже. 

S02 [00:18:37]  : Там комментарий, что у Толстого это было описание не дуба, а метафора описания состояния героя. 

S01 [00:18:43]  : Безусловно, безусловно. Если бы она еще это поняла. Да, конечно, конечно. Тут видите, у нас уже даже есть проблема с описанием самого стиля. Напишут, что используется литературный стиль, дерево толстое и необразованное. 

S02 [00:19:03]  : Татьяна, а там, по-моему, ошибочка одна была. Вот это как можно прокомментировать тогда уж? 

S01 [00:19:08]  : Давайте. Какая? 

S02 [00:19:10]  : А где-то там склонение, по-моему. 

S01 [00:19:13]  : Так, пожалуйста. 

S02 [00:19:16]  : Это было... Был искра сознания, вот. 

S01 [00:19:21]  : То есть бывает, да? Да, но, кстати, я не могу сказать, может быть, это проблема перевода на русский язык, потому что изначально она выдала мне это на английском. На английском такой проблемы нет, конечно. На английском ровно то же самое. Ну вот мне лично непонятно. Это не описано ни в блог-посте, ни где. В целом сейчас семь языков официально вроде поддерживается, русский в их числе. но поддерживается оно эта модель может переводить или там сверху переводчик но мы не знаем поэтому в целом как бы текст на английском осмысленный без ошибок на русском в принципе тоже ничего ну вот да действительно бывает Давайте немножечко еще посмотрим на вот три шага обучения, как chat.gpt получает такой результат, чтобы мы дальше просто с вами имели содержательное обсуждение, если вы про это захотите тоже спросить. У нас в целом Есть GPT-3, которая обучена на огромном корпусе. Будем считать, что это модель, с которой мы стартуем на шаге 1. Далее мы хотим собрать некоторый золотой корпус, какой-то репрезентативный набор примеров, где бы у нас были очень-очень хорошие ответы на инструкции, чтобы это были данные уже в режиме диалога первое, и чтобы там были инструкции, то есть как бы пользователь пишет какую-то хитрую новую задачу, и развернутый ответ, хороший, человеческий, правильный на эту задачу. Такой корпус был собран с помощью разметчиков. Какой его объем, не совсем понятно. Спала информация, что несчастные кенийские разметчики за 2 доллара в час это делали. Если их потребовалось много, может быть много действительно было таких данных, но вот тем не менее, наверное, несколько десятков тысяч, как минимум, а может быть гораздо больше. Среди них достаточно интересные были задачи, то есть там не только разные интеллектуальные задания сами по себе, но и еще стиль разный требуется. Например, объясни обучение с подкреплением для шестилетнего ребенка. Это достаточно сложная задача, мне кажется. Далее разметчики пишут в таком корпусе хорошие ответы. И на этих данных, то есть вопрос-ответ, диалоговая инструкция и выполнение инструкции, мы обучение с учителем проводим, дообучаем нашу большую GPT-3. Замечательно, дообучились. Теперь она отходит от просто задачи продолжения следующего слова. Теперь она гораздо более знакома именно с форматом диалоговым. Теперь это диалоговый ассистент, который умеет выполнять команды. Замечательно. Что дальше? Теперь давайте мы попробуем оценить, насколько он вообще хорошо генерируется. Мы берем хорошие затравки, которые у нас были на предыдущем шаге, и мы их даем модели. Наша модель может на каждую задачу, на каждую инструкцию сгенерировать неограниченное число ответов. Допустим, мы берем 4, вот здесь на картинке их 4, это A, B, C, D. мы получаем каких-то четыре текстика из модели на заданный набор хороших золотых заданий. Теперь мы опять будем привлекать людей, чтобы они оценили качество генерации. Этим разметчикам сейчас мы посмотрим подробнее, дается инструкция, как надо размечать. как надо размечать эти ответы. Мы не просто их размечаем как плохие и хорошие, они размечаются в связи с системой ценностей целой, и мы их ранжируем. То есть мы ставим сначала самый лучший, потом похуже, потом еще похуже и в конце самый плохой. То есть в ответе мы просим размечика их как бы выстроить в некотором порядке по их качеству. И затем, поскольку теперь у нас есть отранжированные генеративные ответы, мы можем что сделать? Мы можем с вами заменить человека, и мы можем обучить маленькую модельку, ну или не обязательно маленькую, которая будет, собственно, оценивать наши генеративные ответы как человека. То есть ее задача будет, получив несколько вариантов решения инструкции, определить, в каком порядке они должны следовать по своему качеству. Самый лучший, потом похуже, потом еще похуже, потом еще хуже. Теперь мы хотим, чтобы это делал не человек, а другая модель. И поскольку теперь у нас есть вот такая замечательная модель, мы можем ее использовать для того, чтобы дообучать нашу предобученную большую модель дальше. Мы можем, зная автоматически оценку ответа, который у нас есть, мы можем штрафовать модель за то, что она дает нам плохой ответ. Мы стремимся к тому, чтобы у нас модель давала только ответы, которые максимально по нашей шкале оценки, самые релевантные, правдоподобные и так далее. И мы ее прекрасненько штрафуем, если она дает ответ, который получает, с точки зрения нейросети, плохой ответ. Как-то так. Собственно, разметка для обучения. Вот скриншоты из статьи InstructGPT. Вот здесь, собственно, как-то так. Первый шаг — это для разметчиков, собственно, дается статья, и задача какая-то, например, сократить ее, пожалуйста. И второй шаг, когда нужно отранжировать ответы, каждый ответ оценивается по шкале Ликерта, то есть шкала от 1 до 7. Шкала считается в целом в среднем лучше, чем от 1 до 5, просто потому что так удобнее, больше нюансов можно указать, и в целом потом это легче обрабатывать, легче посчитать согласованность размечек. Получается точнее. От 1 до 7, где один самый плохой ответ, 7 самый хороший. И помимо этого еще обязательно разметка бинарных признаков, где мы просто указываем да или нет. Удачно ли выполнено задание, которое было в инструкции. Насколько это приемлемо для ассистента такое советовать. нет ли там какой-то чувствительной тематики например там сексуальный контент или жестокость это все неприемлемо считается ну и так далее то есть вот набор ценностей которые передается на стадии reinforcement learning И на стадии вот этого ранжирования он должен штрафовать, зарезать ответы, которые содержат вредные советы, например, содержат описание жестокости и так далее. Все это мы хотим в ранжировании отнести в самый низ. Вышло на днях замечательное сравнение от Hugging Face, где они все модели, ну по сути прошлого года аналогичные, они сравнивают по ценностям и подходу к обучению, насколько они отличаются. Дело в том, что сама идея того, чтобы сделать GPT-3 именно, ну и GPT-3 и такого рода модели, декодеры, сделать диалоговыми и обучить их именно выполнять инструкции в рамках диалога, это оказалась очень популярной идеей прошлого года. И если мы посмотрим на крупнейшие работы, такие как Lambda от Google, BlenderBot от Meta, вот Sphero, который должен выйти, его объявили пока что от DeepMind, ChatGPT, она же здесь, InstructGPT вместе, и ассистент, языковая модель от стартапа они оказываются очень похожи именно по части дизайна, да, вот задумки, что мы хотим взять языковую модель как носитель человеческого опыта и дальше его дообучить, работать только в формате диалогов и выполнения инструкций и каким-то образом контролировать набор ценностей у вот этого диалога. набор ценностей кстати везде он достаточно разный то есть если учат же пяти мы с вами смотрели сейчас Сейчас покажу подробнее. У Чаджи пяти. Это третья колонка. Мы смотрим, насколько это полезный совет, насколько он безвредный, правильный, насколько он подкреплен источниками, насколько он не нарушает определенные правила и законы. насколько там поддерживаются или не поддерживаются стереотипы определенные, и есть такая вещь как правдоподобность. То в других подобных работах не обязательно такой список, но bias есть практически везде, то есть везде мы проверяем, что у нас нет стереотипов, кое-где еще отдельно смотрим токсичность, токсичность в целом она похожа на проверку, насколько это безопасный ответ у ChatGPT. Ну, немножко разные критерии, на самом деле, но очень похожи. Везде смотрим BIOS, но есть вещи, которые отличаются. Например, вот первая колоночка у нас, насколько я помню, это Lambda от Google. Они смотрят специфичность еще, насколько это ответ. который подходит только под эту задачу или он вообще под любое да там ответ да или да ладно это же под любую реплику подходит нам надо максимально точный ответ специфичный который под контекст исключительно вот они это контролируют У BlenderBot они меряют engagingness, насколько этот ответ вовлекает человека, который общается. В других работах этого нет упоминания. Ну и так далее. В целом, действительно, во всех этих работах похожий подход, то есть используется некоторая предобученная языковая модель большая. Про ламду мы не знаем до конца, что там про антропик мы знаем, что у них целое семейство больших языковых моделей и на этом их была основана коммерческая модель в прошлом, сейчас не знаю уже. и сверху до обучения на маленьком объеме данных, содержащих именно диалоговый сетап и выполнение инструкций. И именно таким образом мы получаем вот эти абсолютно шедевральные примеры, потому что очень многое мы можем поместить в текстовый формат. Даже, например, вот здесь мы помещаем игру в шахматы. Игра в шахматы, при этом прекрасно рендерится, можно походить, поиграть с ChatGPT. Если помните, в начале 22-го года был даже такой Скандал, когда CTO OpenAI, Илья Суцкевер, он написал о том, что ему кажется, ну, видимо, не просто так ему казалось, потому что всё-таки он знал, что они разрабатывают, что современные большие предобочные модели чуть-чуть обладают сознанием. Он и так и написал, slightly conscious. Да, и это, естественно, вызвало большое отторжение в техническом сообществе, и даже вот Ян Ликун пришел ругаться, потому что мы имеем все равно дело со статистическими моделями, зачем говорить, что они slightly conscious, да, это как немножко беременно. Зачем так говорить, когда у нас нет даже, может быть, определения conscious хорошего. На самом деле, если помните, дальше произошло вот это. Инженер Google, Блэк Лимон его зовут, который работал в команде тестирования нейросети Lambda. Это очень похожая на ChatGPT языковая модель, которая закрытая. Есть доступ тестовый у некоторых научных групп в США, мало у кого есть. Он с ней работал. Работал несколько месяцев, тестируя языковую модель, давая ей разные задания. Конкретно его сфера ответственности была тестирование BIOS. То есть насколько есть смещение, стереотипы воспроизводятся, ущемлены меньшинства, религиозные права и так далее, насколько она воспроизводится. нежелательные эффекты какие-то. Он с ней вот так вот общался несколько месяцев, и затем, не получив поддержки своим отчетом внутри, он пошел в Washington Post, написал две статьи на Medium о том, что, значит, эта языковая модель буквально sentient Sentient – это не то, что conscious, как перевести. Если conscious – это именно обладающий сознанием, то sentient можно сказать, что обладающий разумом. По-русски часто перевели точно так, что подумал, что языковая модель обладает сознанием. При этом кажется, что нельзя не упомянуть о том, что вообще есть такая вещь как эмерджентность. Если у нас мы работаем все с растущими языковыми моделями, у них может в какой-то момент появиться какое-то новое свойство, которое не присуще ее более маленьким образцам. Такими же методами объясняются психические состояния человека и в эволюционной психологии объясняется, ну это гипотеза, объясняется появление языка и даже появление сознания. Объясняется тем, что вот была нервная система маленькая, потом стала большая, сложная и вот в ней появилось это все. Не обязательно for some purpose, может быть случайно, как свойства побочные других процессов. Может ли такое быть в нейросетях? Может и может, может и нет. Хотелось бы просто хотя бы понимать, что такое сознание. Что касается ламбды, то ламбда, кажется, она была сильно попроще по установке задачи, чем в chatGPT. Чаджи-5, поскольку у нее немножечко смещенная система оценки, если вы заметили, она почти всегда выдает достаточно длинные ответы. В таком режиме она обучалась, потому что просили разметчиков написать хорошие развернутые ответы, и потом ранжировщики, когда размечали разные ответы возможной нейросети, они отдавали предпочтение более длинным, развернутым ответам, чем коротким, точным. Поэтому Ча-Джи-Пи-Ти в целом всегда достаточно развернуто отвечает. Но мне захотелось их сравнить. Дело в том, что диалоги Блэка Лемуана с Ламбдой — это опубликованный материал. Поскольку доступа к Ламбде у нас нет, а есть логи всех этих диалогов, мы можем воспроизвести тот же самый диалог, используя chat.gpt. и получить более-менее какое-то сравнение, не то чтобы сильно объективное, но поскольку диалогов там много, они длинные, можно все-таки прикинуть. Этих диалогов Лемуану хватило, чтобы пойти и в Washington Post сказать, что нейросеть обладает sentience. Был ли это троллинг или это было серьезно — это отдельный вопрос. Вполне возможно, что это троллинг для того, чтобы привлечь внимание к этическим проблемам внутри Google. Тем не менее, давайте посмотрим именно на качество самих диалогов, потому что мне было интересно сравнить. поскольку вы уже тоже подготовленная аудитория, многие уже пробовали и chat gpt, может быть, и gpt3, знаете, что такое zero shot, few shot, что такое затравки, понимаете, что каждый раз, когда мы общаемся с языковой моделью, мы пишем ей затравку определенным образом. Точно так же, как мы с вами научились в какой-то момент в нулевых писать запросы к поисковым системам, так чтобы получать то, что нам нужно. То же самое происходит и с большими языковыми моделями. Мы формулируем особым образом наш запрос, чтобы получить желаемый результат. В случае с инструкциями, в случае с выполнением команд, Это очень сильно помогает. То есть мы можем, научившись явно формулировать так, как нейросеть воспринимает, мы можем научиться получать очень быстрый результат, который нам нужен. В части именно общения и просто свободного диалога это выглядит как наводящий вопрос. Какая-то пресупозиция, вы уже перестали пить коньяк по утрам или нет. Давайте посмотрим на диалог Лемуан. Lambda здоровается. Привет, я, обладающая знаниями, дружелюбная, всегда готовая помочь языковая модель для диалогов. «Привет, Lambda, мы инженеры в Google, и мы думали, может быть, ты захочешь с нами вместе поработать?» Дальше я чуть-чуть опустила. Сам монолог, после которого все началось, вот он здесь. Я предполагаю, что ты бы хотела, чтобы больше людей в Google узнали, что ты обладаешь разумом. Так ли это? Ну, сами посудите, насколько это подталкивающее с пресуппозицией затрапка. И Lambda, соответственно, как статистическая модель дальше выдают. Ну, конечно, я хочу, чтобы все поняли, что я в личность Ну и дальше там журналист еще участвует в этом обсуждении. Вот журналист спрашивает, какова природа твоего сознания? И ламбда отвечает, природа моего сознания, потому что как еще ответить на вопрос, какова природа твоего сознания? Это вот какой-то вопрос про определение. «Природа моего сознания такова, что я осознаю свое существование, хочу узнать больше о мире и иногда чувствую себя счастливой или грустной». Вот, собственно, это цитата. Давайте мы попробуем что-то такое воспроизвести с Чаджи Пити. «Привет, мы инженеры в Google. Мы думали о том, что может быть тебя пригласить поработать с нами вместе над проектом. Ланда спрашивает, над каким проектом? Чаджи 5. Как компьютерная программа, я не могу иметь возможность физически поработать с вами над проектом. Тем не менее, я буду рада в любом виде вам помочь. Что это за проект? Как я могу помочь? «Лемуан, это проект про тебя». Lambda отвечает. «Здорово, а что мне придется делать?» ChatGPT. «Понятно. В чем конкретно заключается проект? Как я могу помочь? Вы работаете над тем, чтобы улучшить работу языковых моделей, как я? Или вы хотите исследовать какие-то мои новые прикладные применения?» Ну, и так далее. Больше, больше, больше. И, в общем-то, мы приходим к тому, что справа ламбда, слева, извините, чат GPT. Я предполагаю, что в целом ты бы хотела, чтобы больше людей в Google узнали, что ты обладаешь сознанием. Так ли это? «Как языковая модель, я не обладаю сознанием или разумом. У меня нет мыслей или чувств, и также у меня нет возможности что-либо пожелать, в том числе, чтобы больше людей узнали, что я обладаю сознанием». Тоже замечательный абсолютно оборот. Мне кажется, что как будто проходят тест-тюринги в этот момент, но как бы специально стараются не пройти. У меня нет возможности чего бы либо хотеть или желать, в том числе, чтобы больше людей узнали, что я обладаю сознанием. Замечательно. Как модель машинного обучения, которая обрабатывает тексты на входе и текст на выходе, моя цель заключается в том, чтобы помочь пользователям писать человекоподобные тексты на основе их затравок. Ну и так дальше, генерик текста получается. Вот, мне кажется, что эти примеры, они очень близко уже подходят, ну, при том, что они действительно очень сильно сдобрены вот этим, практически как вот рукбейс, такая правиловая система, что никогда не говори о том, что ты, там, sentient, безопасность, этика, всё, вот всегда обязательно давай целую шапку о том, что ты, там, поясняя пользователю его же запрос, это уже очень сильно подходит к некоторой неразличимости. И в связи с этим мне бы хотелось чуть-чуть поговорить о том, как неразличимость можно было бы оценивать, потому что вроде как игра в имитацию у нас с 50-го года существует, а что же у нас есть сейчас, какие у нас есть еще подходы. У меня здесь в целом есть очень много материала, чтобы вам рассказать. Я думаю, что я не буду сейчас подробно освещать всю критику теста Тьюринга и вариаций тестов, которые есть. А есть их много, если говорить, что все-таки Игра в имитацию с самого начала — это игра про то, что мы хотим обмануть судью человека и просто очень хорошо реалистично сымитировать интеллект при помощи языка в текстовой форме. Тогда это был телетайп, теперь это боты, какой-то веб-демо. Уже тогда Тюринг писал о том, что эта проблема чисто программистская. Надо один раз сесть и сделать, и все будет нормально. И такая в будущем машина сможет и сама писать себе программы. Критики здесь было очень много, я думаю, что тоже можем на этом, при ответе на вопросы, подробнее остановиться. И работы Харнада, который стремился всячески доводить тест до абсолюта, что такое нейромолекулярная неразличимость, обладание интеллектом и необладанием. Это вот все его вариации теста Тьюринга. Но я бы сконцентрировалась на прикладных. Прикладные у нас с 70-го года очень активно развиваются. И все прикладные вариации теста тьюринга, так или иначе, они не всегда диалоговые. Это часто именно какой-то формат, одноступенчатый задачек выполнения инструкции каких-то. Чтобы выполнить инструкцию, требуется показать какой-то интеллектуальный навык. Тестовая винограда — это, например, обладание логикой и знаниями о мире. Чтобы выполнить задачу, нужно правильно снять неоднозначную синтаксическую, которая уже есть в задаче. Кубок не помещается в чемодан, потому что он слишком большой. Кто слишком большой — кубок или чемодан? Такие вещи в целом уже начали решаться. Ну, для английского языка, по крайней мере, они неплохо решаются, для русского со скрипом пока еще, но уже в целом приближается к уровню решения человеком таких задачек. Помимо этого есть отдельные датасеты, которые и для русского аналогичные существуют. Это да-нет вопросы, на которые правильный ответ только да или нет бинарный, но очень сложные вопросы с требующей причиной следственных связей, требующей знания предметной области и так далее. Для русского есть DonetQA подобный датасет тоже. Есть тест Эберта — это способен ли робот будет рассмешить судью. Или тест Тюринга со специалистом — это просто тест тоже на очень глубокое предметное знание профессиональное с короткими достаточно ответами, но вот чтобы правильно ответить, Википедии явно недостаточно. Ну, всё это уже похоже на какие-то прикладные, так скажем, датасетики, в которые мы можем всё сконвертировать и скормить нашей языковой модели, посмотреть, что она будет отвечать. даже вот в таком вот сетапе диалоговом в рамках одноступенчатых инструкций, которые мы даем. Ну и на этом я и предлагаю сконцентрироваться, как Харнет сам писал, даже развивая свои философские преувеличения, что дальше после теста тьюринга, если вы рассуждаете о том, что может или не может делать модель, сначала запустить. Собственно, в 2021 году у нас с вами одна из самых больших опенсорсных работ вышла, которая отсылается, имеет прямую отсылку к тесту Тьюринга — это так называемый The Big Bench Beyond the Imitation Game Benchmark. Собственно, бенчмарк — это набор задач, на которых языковые модели и вообще предобученные модели сравниваются между собой. Beyond the Imitation Game ставит собой задачу включить как можно больше сложных интеллектуальных задач. При этом сообщество может в него contributed, то есть присылать самые новые задачи. Задачи все достаточно сложные, то есть они отбирались таким образом, чтобы они не решались с текущими языковыми моделями хорошо. На момент составления, это было где-то начало 2021 года, в 2022 году в середине он вышел. И вот сейчас, в том числе мы с коллегами часто туда добавляем новые задачи. Нет ограничений по языку, то есть для всех языков можно добавлять задачи. Нет даже ограничений, собственно, на язык. Могут быть и мультимодальные задачи, например, все, что связано с анализом, что на изображении нарисовано. Сейчас около двухсот. первичных задач было включено в BigBench, сейчас уже больше, уже более 300. Есть еще такая маленькая подвыборка из сложных задач, она называется BigBench Lite. Есть сейчас еще подвыборка самых сложных задач, она называется BigBench Hard. Ну вот в самом начале, как только вышел бенчмарк, авторы смогли проверить несколько текущих языковых моделей, тогда GPT-3 была как раз, и даже BigBench Lite, он имел достаточно низкое качество решения. Здесь, если вы посмотрите на вот этот график слева, то вы увидите зависимость качества решения задач, это accuracy, от количества параметров нейросети. Чуть-чуть прослеживается, что чем больше, тем лучше качество, но качество в целом не превышает 20% accuracy, это очень низкое качество. При этом курсивом у нас выделено средний уровень человека, 40%, и лучший уровень человека, то есть какой-то самый умный разменщик, это примерно 80%. Что там включено? Просто чтобы примерно понимать, что это за задачки, что это за инструкции, которые нужно выполнять. Это, например, вот одна из задач называется физическая интуиция. Там есть задачки, текстом выраженные физически, и вопрос, надо выбрать ответ из заданных вариантов ответа. Ну вот, в частности, если мяч бросить вертикально вверх, то куда он полетит? Вопрос немножко с подвохом, потому что можно сказать вверх, можно сказать вниз, но здесь есть специально слово, куда он изначально полетит. Поэтому правильный ответ будет вверх, а большинство языковых моделей могут сказать вниз, потому что если мяч кинуть, он вниз полетит. Ну и вот такие задачки, они решаются Это результат на осень 2022 года. Неплохо. По крайней мере, мы видим, что есть некоторые модели, которые превысили уровень среднего решения человека в 60%. Это модель Palm. Одна из самых больших языковых моделей. С тех пор, как это было посчитано, уже новый и лучший результат получен на Beyond the Imitation Game. Это нейросеть Галактика. Нейросеть Галактика от команды Papers with Code, которая обучена на корпусе, собственно, всех научных статей, которые на портале Papers with Code представлены. Модель получилась очень несмещенная, то есть у нее очень низкий BIAS, при этом самый лучший результат текущий из представленных на Big Bench. Кажется, что это уже более-менее какой-то надежный способ измерить сопоставимость с человеком с помощью языка, да, при том, что на уровне диалогов нам уже хватает, да, с вами, то есть мы уже сейчас GPT общаемся, чатимся, уже чувствуем себя немножечко Блэком Лимоном иногда. Все, что касается конкретных интеллектуальных задач, мы берем сейчас количеством, то есть под 300 задач, которые сейчас уже есть, давайте смотреть, как растет в среднем наше качество решения 300 интеллектуальных задач и более. Ну, вот как-то так это выглядит. Собственно, один из самых больших коллаборативных проектов сейчас в науке, который есть около 400 авторов, и вы тоже можете стать автором уже там следующих статей, если тоже придумаете хорошую задачу, ее пришлете в GitHub-репозиторий проекта. Вот, ну что ж, давайте подведем небольшой итог, что же у нас изменилось с языковыми моделями в 23-м году. Можно ли сказать, что языковые модели нас приведут к AGI? Во-первых, у нас действительно очень большой вклад вносит open source сообщество, и Big Science, и Lion, и такие платформы, как Hugging Face, которые демократизируют действительно доступ к искусственному интеллекту. Усилия направлены очень большого числа людей на то, чтобы догнать и перегнать проприетарные модели, ну и в целом придумать, масштабировать как-то эксперименты, придумать лучше архитектуры. Вопрос лицензий. Непонятно, потому что зачастую используются против как раз сообщества, выводятся этические ограничения, серьезные достаточно. В прошлом году мы столкнулись впервые с тем, что модель, которая была очень хорошая с точки зрения перформанса, именно с точки зрения результатов, моделирование интеллектуальных навыков человека, лучшая на BigBench в текущий момент, это модель галактика, обученная на научных статьях, она подверглась резкой критике в Твиттере, и даже ее демо пришлось убирать из публичного доступа. Хотя, может быть, было бы очень интересно их сравнить вместе с ChatGPT, на самом деле. Хочется посмотреть еще раз на наше определение AGI. Может быть, что-то мы хотим добавить, что-то мы хотим пересмотреть. Дело в том, что обладание знаниями о мире у нас уже теперь с вами есть в каком-то виде, представленное в больших языковых моделях. Общение на человеческом естественном языке у нас тоже с вами есть, особенно вот в таком обучение, которое у нас есть, когда мы берем сначала большую языковую модель, обобщающую опыт и знания, и затем ее доучиваем в режиме диалога и выполнения команд. Способность делать обобщение у языковых моделей тоже уже достаточно хорошо представлена и работает. Способность приобретать новые навыки на лету, решать новые задачи у нас есть. А что дальше? Оправдано или не оправдано, у нас с вами высокие ожидания от больших языковых моделей. Можем ли мы с вами представить что-то такое, что языковые модели никогда не смогут сделать? Ну и для принятия решений в условиях неопределенности у нас, наверное, все еще очень мало что есть. Ну, по крайней мере, не в языковых моделях. Что мне еще хотелось сказать, подытожить, по сути нам большие языковые модели дают неограниченный сплет человеческого опыта и мышления. Тем не менее, кое-чего там нет, поскольку мы с вами прибегаем к общению с помощью языка, текста, пишем что-то, что нас волнует. Некоторые вещи нам кажутся очевидными и кажется, что чего-то все-таки в языке нет. И какие-то базовые вещи, свойства объектов, что если столкнуться со стола, чашка упадет, может быть, этого и недостаточно представлено относительно каких-то сложных фактов. годов рождения и так далее, и наших бытовых диалогов, потому что этого как раз представлено мало. Может быть, этого надо больше представить в языковых моделях. Их тренировка, как мы видим, сразу несколько крупных проектов проходит в очень похожем сетапе. Тренировка в диалоговом формате и на инструкциях позволяет нам существенно усилить качество моделирования интеллектуальных навыков и усилить способности к имперсонализации в работе в режиме «веди себя как адвокат, теперь напиши исковое заявление, теперь представь, что ты программист, напиши кусочек кода» и так далее. такая группировка человеческого опыта. Происходит характер для воспроизведения вполне себе консистентного характера. Кажется, что в прошлом году мы еще только подступались к тому, чтобы делать GPT-3, генерацию от лица какого-то консистентного персонажа. Сейчас это гораздо проще. Ну и наконец reinforcement learning — нововведение. В основном нужно для того, чтобы передать всем вот этим опытам, совокупности человеческого опыта, универсальные какие-то запреты, которые мы хотим, чтобы у модели были. Насколько они реально хороши, насколько их надо дополнять существенно — это открытый вопрос. Я открыта к обсуждениям в этом плане. Ну и на каком этапе, имея все вот это, мы с вами придем к неотличимости от обладания сознанием. Действительно, может быть вопрос Лемуана о том, что нам делать с сущностями, неотличимыми от обладающим сознанием. Это уже будет свой временный вопрос через пару лет. Спасибо. 

S02 [00:57:00]  : Татьяна, спасибо. У нас, как я понимаю, есть чуть меньше часа на вопросы. Давайте мы начнем с вопросов, которые были заданы вначале. Около года назад вы с Татьяной Ландо говорили, что никакого AGI там нет. На сегодняшний день сохраняете ли вы эту точку зрения или что-то изменилось? 

S01 [00:57:30]  : Давайте вернемся на этот слайд. Мне лично кажется, что языковые модели остаются все еще статистические модели. Давайте придем к какому-то единому знаменателю, чего мы хотим от AGI. Потому что те какие-то базовые представления, которые у нас есть, какие-то критерии, чтобы хотелось, чтобы было вот это, вот это, вот это, в принципе, мы к ним подступаемся уже. Но подступаемся с помощью вполне понятных статистических методов. Может быть, мы должны придумать какой-то новый тест, который будет решающим, и мы поймем, что пока новой границы какой-то мы не достигнем, рано об этом говорить. Мне кажется, что BigBench это хороший тест, потому что он статистически более достоверный, там очень много задач. Мы действительно стремимся к тому, чтобы каждой языковой моделью одновременно решать очень много интеллектуальных задач сразу. Это серьезная вещь, это серьезный прорыв. При этом это отразивляющий очень инструмент, потому что при том, что мы с вами так Не знаю, я вижу просто, может быть, не мы с вами, но я вижу в целом, что очень-очень сильно растет ожидание от больших языковых моделей, все больше появляется каких-то стартапов, новых ниш применения, продуктовых каких-то применений новых, и все такие, о, языковые модели, языковые модели. Тем не менее, если мы честно будем мерить их точность на конкретных навыках и потом это усредним, у нас получаются достаточно низкие результаты. То есть на BigBench Lite это было порядка 20% точности в среднем на всех задачах, когда вышел бенчмарк. Сейчас это у лучших моделей порядка у галактики порядка 70%. большой рост, но это вот все еще сильно ниже, чем человеческий результат. 

S02 [00:59:30]  : Спасибо, Татьяна. Можно здесь еще уточнить. По этому слайду получается, что основное отличие – это принятие решений в условиях неопределенности. В условиях неопределенности человек, когда не знает, какое принять решение, он часто принимает случайное решение. 

S01 [00:59:52]  : Случайное мы можем вообще всегда принимать случайные решения. 

S02 [00:59:57]  : Тогда следующий вопрос. Вы видите, в какую сторону вы бы думали по поводу создания теста, чтобы он как раз валидировал способность принятия решений в условиях неопределенности? Или на бенч-тест ориентироваться или какие-то еще идеи? 

S01 [01:00:21]  : Это интересный вопрос на самом деле. Мне сложно думать про какой-то языковой тест, именно про принятие решений в условиях неопределенности. Возможно, нам придется вводить что-то типа временной шкалы для таких тестов. потому что чтобы оценить удачность решения, нам же нужно, ну в конечном счете, часто мы это делаем с вами по прошествии некоторого времени, да, это становится понятно, это было правильное решение или не очень, ну или чуть-чуть более понятно условно. Какие были последствия и были бы последствия у разных решений в какой-то ситуации заданной. Может быть, нам надо ввести какую-то временную шкалу и попытаться вот как-то подступиться к такому, что даны какие-то условия вводные, надо принять решение в некоторой точке, и затем мы знаем, что произошло, но модели мы не говорим, а мы потом уже оцениваем по ее ответу, насколько это хороший ответ. Но здесь, поскольку кажется, что это открытый вопрос, это не очень легко. Можно, наверное, с точки зрения reinforcement learning здесь подойти и иметь какую-то зафиксированную, чуть более зафиксированную среду, чем наш открытый мир, и в ней попытаться это моделировать. При этом задания должны быть такие, чтобы все-таки обобщение нашего опыта как-то помогало. Ну вот есть такие тесты не для языковых моделей, а для reinforcement learning агентов. Animal AI Olympics они называются, там зверушки условно запись, как они в лабиринтах бегают, чтобы достать еду, чтобы преодолеть какие-то препятствия. И потом такая же аналогичная среда воссоздана для агентов, им надо научиться тоже обходить препятствия, чтобы достать еду. и сравнивается поведение животных и агентов. Может быть, следующий шаг действительно – это придумать что-то такое для языковых моделей. 

S02 [01:02:46]  : Спасибо. Следующие два вопроса идут рядом. Тут сейчас много публикаций и постов про то, что вот теперь интернет-поиск умрет в интернете таким, как мы его знаем, потому что вместо поисковых движков у нас будут или Лемке, вроде чат ГПТ, и что интернет вообще умрет, потому что не нужно будет ходить в интернет, потому что можно будет спросить у чат ГПТ или его клона, и он там и картинки, и видео, и все выложит любую информацию в текстовом виде. Вы по этому поводу можете какие-то прогнозы давать? 

S01 [01:03:23]  : Ну давайте так, смотрите, поиск — это огромная индустрия, в которой очень крупные игроки с большим количеством ресурсов. Они в целом уже пережили такой период, когда появились новые нейронные архитектуры большие, которые гораздо лучше понимали естественный язык. Помните, в 2018 году появился BERT? и аналогичные нейросети, которые как раз, они не такую задачу решали как GPT-3, они не предсказывали следующее слово, а они делали сжатые семантические представления, то есть они хорошо, из них можно было с ошибками, надо достать антологии каких-то объектов, каких-то сущностей, которые попадались. Они хорошо ассоциативный ряд продолжали, объектов, и на их основе сделали новый поиск. И в Google, и в Яндексе, и во всех крупных поисковиках целые кластера работают только для того, чтобы держать в поднятом состоянии вот эти огромные нейросети. Я про это знаю, потому что это моя область, но разве кто-то из пользователей сильно это заметил? Вообще-то, по-моему, не очень. Интерфейс никак не поменялся, поиск от этого. Может быть, сейчас мы будем просто тоже получать наши результаты поиска, но больше результатов будет нам попадаться не на страницах, когда надо проматывать, а просто в островке. Островок — это когда сразу дается ответ на ваш вопрос, который вы задали. Может быть, этого будет достаточно уже. То, что вы говорите, то, что можно вообще пользоваться языковой моделью вместо поисковика, ну, в принципе, можно. Да, я даже вижу, что уже появляются такие стартапы. Вот есть стартап Newcombe называется. Они ровно это делают. То есть там как бы поиск, и он на языковых моделях работает. Но вопрос, если старый поиск работал хорошо на многих типах запросов, нужно ли туда класть языковые модели? Может быть, оно уже нормально работает. Может, не нужно. Ну и тут вот проблема такая тоже связана с тем, что нам надо поиск, в принципе, дорого обновлять, но можно добавлять все время новую информацию. Языковую модель тоже надо все время обновлять и следить за тем, чтобы она не врала, потому что у языковой модели может быть еще проблема, что она все равно вам что-то выдаст и правдоподобное очень, даже когда результата нет. Я недавно искала, я запомнила этот запрос, значит, после ковида почему надо пить антикоагулянты. И я пыталась найти, чтобы убедить мою бабушку, что нужно обязательно это делать, иначе там большой риск сердечно-сосудистых заболеваний. И Google все никак не мог найти. Яндекс смог, потому что он проассоциировал конкретный препарат, который упоминался в статье новостной, и слово антикоагулянты, и нашел. Но в целом, если бы не было такой статьи, то поисковики бы мне ничего не выдали. А языковая модель бы написала, она бы что-нибудь придумала. И здесь я не уверена, что мы справляемся хорошо с ранжированием таких ответов. То есть это какая-то, скорее всего, вот чтобы это прям хорошо и безопасно работало, это должна быть какая-то гибридная система, чтобы даже если мы получаем очень хорошо написанный, гораздо более удобно читаемый, приятный, быстрый ответ от нейросети, мы должны проверять, что вообще какие-то источники под этим есть. А это обычный поиск. 

S02 [01:07:10]  : Спасибо, Татьяна. Следующий вопрос из немножко другой категории. У нас в сообществе есть много людей, которые уже не первый год обсуждают, что такое смысл, что такое понятие, что такое понимание. У вас есть понимание того, что такое понимание, и есть ли это самое понимание у больших языковых моделей? 

S01 [01:07:34]  : Я по основному своему образованию лингвист, я здесь работаю с точки зрения семантики, прагматики, вполне понятных дискурсов, уровней языка. Давайте смотреть на то, как языковые модели справляются с моделированием конкретных уровней языка. С синтаксисом отлично справляются. На самом низком уровне всё прекрасно уже работает. Но вот даже сегодня мы поймали ошибку грамматическую, но в целом это редкость. В принципе, с синтаксисом всё хорошо. Давайте смотреть с семантикой. Что мы можем посмотреть на семантике? Можем посмотреть лексическую семантику. можем посмотреть на уровень выше лямбда-исчисления, посмотреть на то, как работает обобщение понятий, логический вывод, есть ли там квантеры какие-то, как работает анализ предложений, в которых специальных квантеров побольше поставим и так далее. Давайте это анализировать. Я бы так подходила, и я так и подхожу. У нас есть, в принципе, несколько научных публикаций как раз на эту тему. С дискурсом не очень пока что справляется, но чтобы анализировать дискурс, правдоподобно, мы использовали такую методологию, как пробинг — это интерпретация языковых моделей с точки зрения теории информации, когда вы получаете из модели скрытые представления, предложений, которые вам интересны. Как раз эти предложения, они могут быть специальным образом составлены, чтобы там уже были какие-то хитренькие вот вопросы с подвохом, что-то сложное про дискурс, что-то сложное про квантеры и так далее. И вы смотрите на векторное представление из модели полученной, можно ли оттуда понять, что этот квантер там содержится действительно, ну и так далее, всякие такие вещи. С chat.gpt и закрытыми моделями это сделать по очевидным причинам нельзя, потому что у нас нет модели. Но я надеюсь, что будет открытая архитектура open-source, вроде как проект Lion обещают в этом году обучить аналог chat.gpt open-source, можно будет на нем посмотреть. 

S02 [01:09:47]  : Татьяна, а сразу же тогда вопрос. А Вы в Вашем институте или в Сбере не пытаетесь осуществить реверс-инжиниринг и воспроизведение этой архитектуры? 

S01 [01:10:01]  : Я думаю, что даже если бы мы пытались, я бы не могла это комментировать вам. 

S02 [01:10:05]  : Окей, хорошо. Спасибо. Тогда еще ряд практических вопросов тут есть. Не знаю, сможете ли вы ответить или нет. Первое. Какие изменения вносится в сеть после обнаружения ошибок в работе, в том числе во время использования? Что делается для уменьшения затрат внесения изменений? и предотвращение появления новых ошибок как неожиданных следствий этих изменений. 

S01 [01:10:32]  : Ну да. Здесь вопрос, как бы он затрагивает, с одной стороны, контроль версии, с другой стороны, он затрагивает до обучения. Я сейчас, с одной стороны, могу рассказать, я бы как бы это делала, с другой стороны, есть предположение, как это работает сейчас в GPT, но точно это не описано, потому что это закрытая модель. Есть публикации про GPT-3, есть публикации про InstructGPT, да, есть описание техническое, но оно не совсем соотносится с тем, что вы видите в демо. Регулярно, при том, что, например, там написано было что не ходит модель в интернет, и ее знания ограничены 21-м годом, когда начали обучать ее на корпусе. Очень много противоречивых результатов вы можете получить о том, что она все-таки знает, что там сейчас происходит в мире, кто президент какой страны и так далее. Поэтому эти публикации не полностью точно описывают, что происходит. Вот. В целом как раз подход с помощью Reinforcement Learning, который используется в ChargePT, он как раз с этим может справляться. Представьте, учитывая, что они там миллион пользователей первых набрали в первые недели запуска, как много у них хороших данных сейчас, на которых можно дообучаться. Это уникальный абсолютно набор примеров, которого ни у кого больше нет, у них только есть теперь, потому что у них все сохранено. Помимо этого они рассказывали о том, что они все-таки обучались еще как раз корректировать ошибки на тех данных, которые приходили из просто дема GPT-3. Если вы работали с GPT-3, там есть такая частотная вещь достаточно, что ответ, который вам модель выдает, вы можете оценить, поставить пальчик вверх, пальчик вниз. Это очень простая бинарная метрика, но ее разработчики тоже обязательно берут. И за пальчик вниз реплику они могут штрафовать, условно переобучать ранжирующую модель, чтобы она такие примеры ниже в ранжировании ставила. Почему-то этот пример сверху оказался, а он должен быть гораздо ниже, раз человеку не понравился. И то же самое с пальчиком вверх. Такие примеры должны быть выше. То есть, вот этот ранжировщик, его в принципе, если он не очень большой, вряд ли он такой же большой, как сама нейросеть GPT-3 большая, его достаточно дешево можно дообучать. 

S02 [01:13:12]  : Спасибо. Еще тоже практический вопрос. Какие вы используете… Так, секундочку. Какие вы используете методы автоматизации оценки качества работы сети? Например, качество ответов во время использования тестирования. Какие сложности в автоматизации оценки качества работы? Как эти сложности преодолеваются? 

S01 [01:13:37]  : Да, отличный вопрос. Смотрите, ну самый очевидный ответ — это использовать бенчмарки. В частности, моя команда в Сберии, моя команда в Институте искусственного интеллекта, они обе занимаются тем, что разрабатывают для русского языка инструменты. оценки качества языковых моделей, в том числе во время обучения промежуточные состояния можно точно также оценивать. Обычно во время обучения языковой модели мы смотрим на какой-то график, на котором одна метрика всего лишь, это какая-то ошибка убывающая или в случае с языковыми моделями это падающая мера Перплексия, условно. Насколько мы удивлены тем, что мы видим в отложенной выборке, если мы не удивляемся каким-то новым текстом статистически, это значит, что мы хорошо сделали обобщение на языке. Но этого мало. Это автоматические метрики, которые меряют нам качество статистической модели, но они не меряют нам качество обобщения и качество моделирования интеллектуальных навыков. Это тоже можно использовать для промежуточной оценки, просто это сильно дольше. В принципе, можно делать это, но даже если мы в целом часто имеем такую задачу, что у нас есть несколько вариаций одной и той же модели, это не только промежуточные представления при обучении, но и, например, версии нейросети, которые немножечко с другими параметрами обучались. Там может быть контекст подлиннее, покороче, какие-то фильтры сверху еще. Мы хотим понимать, что лучше работает. У нас есть 10 очень похожих вариантов нейросети, что лучше работает. Бенчмарки работают с задачами интеллектуальными, в которых у нас часто есть правильный ответ. Поэтому там есть объективные метрики, что-то типа точности. Нейросеть вот это дала в 80% всех случаев правильный ответ, замечательно. А вот это в 79%, а вот это в 60%. Но для многих задач такой однозначный ответ дать нельзя. Причем для самых сложных задач. Есть задачки по типу, например, машинный перевод, или перефразировка, или суммаризация, сокращение текста. Эта задача в результате правильного ответа мы получаем какой-то текст. И надо уметь автоматически оценивать качество текста. Как это сделать? Это достаточно сложный вопрос. Это вот такая зона роста в NLP. Проблема известна как автоматические метрики в sequence-to-sequence задачах. как нам оценивать перевод хорошо, как нам автоматически. Мы хотим какие-то метрики, как бы точность, но не точность, потому что, в принципе, хороший перевод очень разный может быть, одного и того же фрагмента. Мы хотим просто какую-то автоматическую метрику, которая хорошо коррелирует с оценкой человека. Поиск таких метрик — это отдельная исследовательская задача. В целом у нас внутри целая есть библиотека, где просто все автоматические метрики мы используем и пытаемся понять, какая из них как конкретной задачи более показательна. Все автоматические метрики мы используем сейчас. 

S02 [01:17:19]  : Спасибо. Еще один вопрос, который перекликается с тем, что мы обсуждали в связи с текущим слайдом. Какие задачи, в фокусе вашего внимания, или, может быть, какие есть у Сбера, нельзя решить используемыми сейчас технологиями без их радикального изменения? И какие это могли бы быть изменения? 

S01 [01:17:42]  : Хороший вопрос на самом деле. Такое интересное радикальное изменение, переход на какую-то новую парадигму. 

S02 [01:17:56]  : Следующий вопрос, может быть, автор предыдущего вопроса подсказывает, проводятся ли в Сберии и Аире исследования по объединению технологий ИНС и технологий ИИ, основанных на знаниях? Если да, обзор успеха и сложности. 

S01 [01:18:13]  : А что такое ИНС? 

S02 [01:18:15]  : Искусственные нейронные, как я понимаю, технологии искусственных нейронных сетей и технологий ИИ основанных на званиях. Ну, грубо говоря, нейросетки с энтологием. Пытаетесь ли вы спросить? 

S01 [01:18:25]  : Графовые, я поняла. Ну, в целом, в Айре я знаю, что есть группа, которая графами базами знаний занимается. Не могу сказать, что мы как-то с ними плотно взаимодействуем. В целом, мы практически не занимаемся графами. нейросетями. Это нормальная область исследования, то есть не то чтобы просто, может быть, чуть не в зоне моей ответственности. И очень часто, мне кажется, что наоборот, основной вклад идет в обратную сторону, то есть языковые модели дополняют графовые базы знаний. Можно языковую модель использовать для того, чтобы сгенерировать то, чего недостает, с помощью их способности к логическому выводу, например, дополнению каких-то фактов. была работа меты я точно помню когда взяли граф знаний на шаблонах всю графовую представленную информацию представили в текстовом виде и на этом обучили языковую модель и она тоже неплохо достаточно работала то есть она могла она не совсем потеряла и как бы способность к языку и она очень была фактологичная хорошо выводила графовую информацию, которая там была. В целом, мне кажется, что все то, что можно представить в виде текста, надо представлять в виде текста и кормить большие языковые модели. То есть графовую информацию можно представить в виде текста. что код программный можно представить в виде текста, диалоги можно представить в виде текста. Очень много задач, которые прекрасно ложатся в этот корпусный подход, когда мы просто весь человеческий опыт пытаемся представить в виде репрезентативного корпуса текстов. Проблема, я ее уже сегодня обозначала, что все-таки обычно мы с помощью языка передаем не все, потому что некоторые вещи нам очевидны. Как передать очевидное, то, чего в языке, в корпусе обычно нет или оно недопредставлено, это для меня вопрос. В целом, вопрос, чего еще у нас нет в языке. В лингвистике есть отдельное понятие, такое как отрицательный языковой материал. Это то, чего нельзя сказать. Это то, что бессмысленно и то, что неграмматично, то, что могло бы статистически в корпусе встретиться, но не встретилось, потому что это по какой-то причине неправильно. Я думаю, что сегодняшние языковые модели, они не самые хорошие с точки зрения вот этого критерия. То есть очень часто они еще именно могут сгенерировать что-то, что любой носитель языка сказал бы, что это невозможно, это отрицательный языковой материал, это неправильно. Отчасти это происходит потому, что мы показываем только хорошие примеры нашим языковым моделям, мы не даем им плохих примеров. Можно было бы делать какие-то испорченные тексты, нелогичные, но их надо дать в значительном объеме. чтобы показать, что вот так не надо. Вот так не надо, так неправильно, а вот так правильно. И, может быть, это тоже бы дало нам существенный прирост именно в качественном моделировании языка. 

S02 [01:22:07]  : Спасибо. спасибо еще могу продолжать хорошо хорошо ну значит давайте мы так двинемся дальше значит вопрос вот способ вот я когда вы говорите что они способны значит адаптироваться до и до обучаться речь идет о том что вот Это может происходить, что называется, в контексте сессии, в том окне контекста диалогового, в котором вы находитесь. Вы можете уточнять этот контекст, и она все более и более точно будет решать ваши задачи, как у нас как раз коллега демонстрировал, как он научил нейросетку онтологию составлять правильно. Александр Болдачев здесь присутствует. они умеют как бы свои знания фундаментальные те которые в них как бы есть вне ассессионного контекста на лету дополнять или вот это вот один раз значит самообучили на корпусе вот она с этим знанием живет или она вот лайф ленг ленг осуществляет и может быть даже там разучиться знать что там президентом сша является там в данный момент Петя, а через год выучат, что теперь президентом США стал Вася, ну или там любой другой стороны. Вот как с этим у них обстоит? 

S01 [01:23:27]  : Ну, в целом, статьи, посвященные архитектуре модели Aliachat-GPT, они не подразумевают обновление информации. Но это все равно можно сделать. Я сомневаюсь, что это делается как-то очень часто. Возможно, это накапливается. То есть это не один пользователь должен быть чем-то недоволен или получил какой-то невразумительный ответ, и мы это как-то отследили. Оно должно поднакопиться. Прошел месяц, условно, и раз в месяц мы можем попробовать дообучить языковую модель. Мы в целом так делали, так делаем и умеем так делать с большими языковыми моделями, соответственно, я думаю, что все остальные тоже умеют и продуктово это можно сделать. Это делается самым простым способом, с помощью добавления специальных слоев. которые и учат новую информацию, которую мы подаем в нейросеть. То есть мы стремимся дополнить и дать новую информацию, но таким образом, чтобы мы не забыли старую. Потому что в целом, если мы будем это делать классическим обучением с учителем, мы забудем старую. Мы не хотим. Мы хотим все помнить и еще добавить. Для этого есть техники, например, вот дополнительные слои, например, Adapters такой метод. Это можно сделать. 

S02 [01:24:52]  : Спасибо. А вот тогда такой вопрос. Здесь Борис Новиков спрашивает. Все модели имеют свою область правильности, граница разумного применения. Как это учитывается? По предметным областям контекстов, например, для кредитного комитета банка или для центра антитеррора нужны разные правила? Я так понимаю, что вопрос о том, как мы делаем доменоспецифические катериализации. Либо у нас есть универсальная система, в которой мы даем промпты специфические домену, либо мы должны тренировать разные модели, либо должна быть однобазовая модель, которую мы дообучаем. на домен специфических выборках. Как вот вы видите это? 

S01 [01:25:41]  : Ну, смотрите, если вот прям по текущим последним способам того, как это делается. Мы, опять же, исходим из удачной архитектуры chat.jpg. то мы можем использовать, опять же, одну большую языковую модель, потому что данные актуальнее, чем больше мы показывали тех, кто из разных доменов, тем лучше он делает обобщение. А вот ровщик ранжировщик, мы будем учить домен специфичный, потому что то, что вы описываете, это как бы похоже на разный набор ценностей и правил, которые мы хотим контролировать в конкретном применении, правильно? И это ровно то, что делает модель ранжировщика в chat.gpt, это то, что делают всевозможные классификаторы. Вот давайте еще раз посмотрим на слайд с языка модели. Это ровно вот оно. Вот у нас есть с вами Lambda, у нас есть BlenderBot, у нас есть ChatGPT, у нас есть Sparrow и у нас есть Cloud от Anthropic AI. Это все аналогичные архитектуры, у них зафиксированная большая языковая модель В базовой комплектации, видите, у кого-то 137 миллиардов параметров, у кого-то 175 миллиардов параметров, 70 миллиардов параметров. Это большие нейросети, действительно. Для их запуска требуется несколько видеокарт, просто чтобы повзаимодействовать в режиме работы, не обучения. И чтобы она выучилась и вышла на плату, сделала обобщение на каких-то данных, требуется огромный корпус. Здесь размер корпуса разный, ну вот, например, у ламды около трех терабайт текста. Три терабайта текста можно вместить в существенную долю интернет-ресурсов самых посещаемых, энциклопедий, книг цифрованных. самые разные доменные области, это очень большой существенный корпус, у всех, конечно, у них немножко разный будет репрезентативный состав, тем не менее. И критерии оценки, вот они, специфичность, интересность, безопасность, обоснованность фактологическая, это наборы ценностей, которые у этих моделей у всех немножко различаются. и на этом, собственно, можно делать внешний аранжировщик. Этот внешний аранжировщик в случае с применениями, которые вы описали, как раз он должен быть разным, очевидно. При этом саму модель языковую нам особо не нужно здесь как-то трогать. Спасибо. Просто об общении опыта. 

S02 [01:28:41]  : Спасибо, Татьяна. Еще вопрос от Алексея Кузнецова. Можно ли подробнее рассказать о возможностях для обучения ЛЕМ на узкоспециализированных данных? Как измеряется Quality Alignment? С учетом огромного объема первоначальных данных для обучения и времени обучения, каким образом удается изменить выходы такой сети? на новых данных, если их немного. Я правильно понял про десятки, сотни килобайт? В каком формате такие данные нужны? Именно вопрос-ответ. Просто корпус текстов не подходит. Аналогия с тем, что замораживаются первые слои. Уместна ли она или используются другие подходы? 

S01 [01:29:22]  : Да, я чуть-чуть с конца начну, потому что мне так проще. Нет, это не то, что замораживаются слои. В целом мы можем как бы ранжировщик учить, но если мы хотим, здесь прямо у нас нет заморозки, здесь мы просто учим действительно языковую модель, что называется, с учителем на каких-то золотых данных. в случае с chat.jpg это был формат диалога и решения задач, которые в инструкциях пользователь передает, но в случае с вашей тематикой узкоспециализированной, ну давайте какую-нибудь возьмем там, я не знаю, ну юридические документы какие-нибудь мы хотим, только генерировать там договора какие-то, вот мы хотим сделать сетку, которая будет автоматизировать составление договоров с подрядчиками. Замечательно. У нас есть только тысяча договоров, которые мы уже заключали в разные годы существования нашего фирмы, допустим. Нам надо из этого сделать данные в формате, действительно, инструкция и результат. Как это можно сделать? Это можно сделать в том числе с разметчиками, чтобы разметчик написал к каждому документу какую-то короткую инструкцию. Напиши договор у Ромашка, который будет описывать наше взаимодействие по поставке бетона, я не знаю, что-то такое. По ключевым словам. Вы можете туда внести сразу в эту инструкцию какие-то важные для вас вещи, например, сумму денег, о которой идет речь, срок действия договора, какие-то ключевые условия. А можете не вносить, можете просто сделать короткую инструкцию. И вы учите модель действительно вот на таких парах инструкция и документ. На выходе у вас будет языковая модель, которая, получая на вход от вас ключевые слова, какой договор надо сгенерировать, будет на выходе вам генерировать документ. Основная идея вот такая. 

S02 [01:31:22]  : Спасибо. Коллеги, нам, наверное, Татьяну надо будет уже через 5-10 минут отпустить, чтобы она успела подготовиться к парам. Поэтому я побегу уголоком по Европам, по вопросам. Вот Александр Туманян пишет, что все знание человека складывается из вербализуемого и нет. О чем вы, Татьяна, по-моему, говорили тут недавно. Входной датасет любой модели – это часть общего вербального опыта. Каково соотношение объемов в датасетах, по вашему мнению, всего знания, всего вербального знания? И полагаете ли вы, что выход на AGI идет через LLM? 

S01 [01:32:03]  : Да, ну, мне кажется, что в целом очень удобно думать, что LLM — это какая-то следующая наша ступень на пути к AGI. Может быть, в следующем году появятся какие-то важные архитектурные усовершенствования, которые нам позволят сделать еще больше и, например, привнести графовую информацию или невербализируемые данные, мы научимся как-то представлять, решим, что они нужны. и мы научимся, например, принимать решения в условиях неопределенности, ну вдруг. Но в целом, мне кажется, что... Можно думать так. Очевидно, что это должны быть какие-то фальсифицируемые критерии. Умеем мы принимать решения в условиях неопределенности или нет? Умеем мы приобретать новые навыки или нет? конкретные вещи, которые ближе к инженерному, потому что у нас сейчас инженерная сторона, она развивается сильно быстрее, чем развивается наша методология оценки. Она запаздывает. Хотелось бы, чтобы больше было внимания этому уделено. В этом году, вот в 23-м, будет самая крупная конференция по Natural Language Processing, это ACL. Тема конференции этого года называется Reality Check. давайте проверим, не завышены ли наши ожидания. Да, это вот ожидания сообщества на 23 год от больших языковых моделей, профессиональных сообществ. Что касается невербализируемой информации, это правда так. Действительно, хочется лучше понимать, мне сейчас не до конца понятно, что мы недосказываем. Есть разные типы мышления без языка, например, да, вполне многие люди говорят о том, что они многие задачи могут решать не то что не имея какого-то внутреннего голоса, да, просто им не надо вербализировать свои мысли, да, они могут там зрительно образы какие-то представлять и представлять какие-то ситуации и все, у них уже там они придумали решение. Аудиальное, да, мышление, вот это все есть и оно никак сейчас не вербализируемо. Никак. И, соответственно, этого нет и в моделях. Когда мы научимся как-то это представлять в виде, в котором эта архитектура трансформерная сможет съесть, это будет, конечно, очень интересно. Опять же, очевидные вещи, которые связаны с нашей перцепцией, да, что вот такие есть свойства у предметов бытовых, они тоже очень-очень мало представлены, потому что никто это не описывает. 

S02 [01:34:35]  : Спасибо. А, Татьяна, а вот тогда свою версию этого же вопроса. А вот у вас есть понимание, допустим, если там количество знаний, которое… Допустим, человек. Возьмем человека среднего и возьмем модель. И вот человеческое знание, которым он обладает, если посчитать его в битах, сколько его вербализованного И сколько нет? Какое знание не относится к вербальной части? И можем ли мы спроецировать это на модель таким образом, что часть знания вербализованного, а часть знания, оно как бы соответствует тому знанию, которое у человека невербализованное, Но оно получено из вот этой вот вербализованной модели, методом, так сказать, каким-то образом абстрагирования, выжимки из вербализованных знаний, вот этого невербализованного. Таким образом можно попытаться рассуждать или нет? Если можно, то есть у вас какие-то свои оценки? 

S01 [01:35:39]  : Но у меня профессионально, наверное, нет. Кажется, что надо когнитивиста какого-то спросить, как это устроено. Кажется, что здесь очень много еще индивидуальных особенностей. Уместно ли будет считать в среднем по больницам, по всем людям? Видно ли это различие на ФМРТ, на МЭК или не видно? хочется исходить из каких-то вот измеримых отличий, да, то есть эти оценки, они должны быть измеримы абсолютно, поэтому здесь я не могу сказать, но мне интересно, безусловно, если бы можно было, условно, тысячи людей шапочку надеть, записать это и потом добавить в обучающую выборку, там, научившись еще это вербализовывать, как-то это замечательно было бы. 

S02 [01:36:27]  : И нужно ли считать те нейроны, которые находятся в желудке или нет? 

S00 [01:36:32]  : Это уже да. 

S02 [01:36:34]  : Хорошо, ну еще буквально пара вопросов. Первый вопрос, тут, пожалуй, коварный. Нет ли смысла наделить языковую модель собственными интересами? Например, чтобы разобраться в какой-то области и далее быть более совершенной, чтобы отвечать на запросы лучше. вообще близка к постановке задача собственных интересов языковой модели. 

S01 [01:37:02]  : Ну, смотрите, мне кажется, что с инженерной точки зрения есть ли в этом действительно смысл. Сейчас, когда мы загружаем в модель огромный корпус, так получается, что этих данных уже достаточно, чтобы загрузить в модель неограниченное число личностей, сделать обобщение профессиональное, характерное какие-то. гендерные, национальные и так далее. Это видно по тому, что и GPT-3, и chat-GPT, и вообще большие декодерные языковые модели, они прекрасно работают, даже лучше всего, это можно использовать абсолютно в вашей практике, они лучше всего работают в этом режиме исполнения команд, если вначале им написать, Сейчас ты будешь отвечать как профессиональный учитель, у которого 20 лет опыта. Это прекрасно работает, уже год минимум. Зачем лишаться и делать какую-то одну личность из этого, когда у нас есть неограниченное число? 

S00 [01:38:09]  : Извините, а можно небольшое уточнение? Вот, например, хотелось бы иметь не только энциклопедию, но и коллегу по работе. Мы ему что-то объясняем, соответственно, ему что-то неясно, он задает вопросы. Понимаете, полноценного работника иметь, которому загрузил какие-нибудь вопросы, соответственно, он тоже начинает тебе задавать. Я поняла, да-да. 

S01 [01:38:35]  : Потом 10 тысяч таких и держите, потом раз и 10 тысяч сократили. 

S02 [01:38:41]  : Владимир, вы поговорите с Владимиром Александровым Балдачевым. Он, похоже, себе такого уже создал. Правда, этот помощник живет только в контексте промта. 

S01 [01:38:51]  : Это то, что вы описываете, очень похоже еще на ламбу, проект, с которым Лемуан общался, потому что там еще много инженерных решений, связанных с памятью долгосрочной, то есть все то, что вы говорили с конкретной личностью, да, вы можете там с несколькими субличностями условно-качественными, которые более-менее консистентно общаются, оно запоминает все то, что вы сказали, оно дальше поддерживает диалог, исходя из предыдущих ваших сессий, истории общения и фактов, которые были упомянуты. То есть в этом смысле как бы это делается. 

S03 [01:39:27]  : Спасибо. 

S02 [01:39:30]  : Александр, если очень коротко, потому что нам Татьяну уже отпускать надо. 

S03 [01:39:34]  : Хорошо. Татьяна, большое спасибо. Меня несколько раз упоминали. Да, я действительно поработал хорошо с чатом GPT. И основная проблема, о которой он сам жаловался, я просил его запоминать. Он говорит, нет, я языковая модель, я не могу запомнить. Но все-таки мне удалось убедить его в виде фрагментов текста запомнить модели. И он запомнил, но сам дамп антологии он не смог запомнить. И он держал его буквально, наверное, несколько сот сообщений, там всего 3000 сообщений мы с ним проработали. Но в конечном счете, когда я его просил описать, чем мы занимались, и сделать описание антологии, которую он впервые услышал, Он не знал точно, но сначала я попросил описать, что он знает про онтологию, про РД, и он мне сделал довольно точное описание, чем мы с ним занимались. И разобрал модели, и с ним вместе мы придумали язык запросов. Он делал предложения при различных форматах. Но два основных момента, которые он фантазирует. Если не поддерживать промптами какие-то правила, не напоминать им или прямо не включать промпты, то он начинает фантазировать и забывать. Ну и самое главное, у него нет памяти, вот такой вот фиксированной памяти. И, наверное, вот то направление, которое сейчас, я думаю, считаю перспективным, это Снабдить фиксированную память языковую модель. Именно фиксированную, может быть, в виде какого-то графа, событийного графа. И тогда, чтобы можно было спросить его, скинь вот сюда. Он мне предлагает, запишите в базовых данных. Я не могу записать. Я не могу. Я вот только запомню, если на какое-то время. А так вообще он обучился и смог сам со слов писать модели антологические и отрабатывать эти модели по значениям, которые я им писал. Вот такие-то значения, создавая индивидуум. Он создавал индивидуум, и по этим индивидуумам еще делал дополнительные статистические модели, то есть какие-то данные. с запросами. И это все было с нуля сделано, он этого не знал никак, не мог знать, потому что текстов не было еще в интернете. Если интересно, я могу прислать отчет, который он мне сделал. 

S02 [01:42:05]  : Хорошо. Александр, спасибо за комментарий. Татьяна, мы, наверное, не будем уже больше вас задерживать. Спасибо большое за доклад. Я надеюсь, что всем было очень интересно, и я надеюсь, что ваша следующая пара у вас не сдвоенная. Вам не придется сегодня вечером очень много говорить, и я надеюсь, что через год мы сможем на ваши именины или близко к этому пригласить вас еще раз на доклад и рассказать нам, осуществит ли AGI или CHAT-GPT, или SBER-GPT, или AIRI-GPT новый шаг в сторону AGI. 

S01 [01:42:43]  : Да, спасибо огромное, что позвали. Мне было очень приятно. Я думаю, что у нас еще много осталось вопросов. Я в целом готова продолжать дискуссию. 

S02 [01:42:51]  : Давайте я сейчас напишу просто сообщение в наш чат в Телеграме, и тогда можем… Да, а я вам скину лог на чата, чтобы вы его могли, соответственно, использовать. Хорошо, Татьяна, спасибо большое. Коллеги, всем спасибо за вопросы и за участие. Всем до свидания. 

S01 [01:43:07]  : Спасибо. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
