## 30 ноября 2023 - GPT + планирование = AGI? — Юрий Бабуров — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/qtfr7zmsDX8/hqdefault.jpg)](https://youtu.be/qtfr7zmsDX8)






А. КОЛОНИН [00:00:08]  : Коллеги, всем добрый вечер. Начинаем очередной семинар русскоязычного сообщества любителей и разработчиков сильного искусственного интеллекта. И сегодня у нас в гостях Юрий Бабуров расскажет нам о том, чего не хватает ЖПТ для того, чтобы стать И, может быть, это будет планирование. Юрий, пожалуйста. 

Ю. БАБУРОВ [00:00:30]  : Всем добрый день. Особо представляться, наверное, не буду. И так уже выступал неоднократно на данных семинарах. Сегодня у нас будет такой на половину, теоретически на половину, практический семинар. где мы попробуем обучить робота новым трюкам. Начнем все же немножко с теоретической части. Не так давно в нашу жизнь пришли большие языковые модели. Они к нам пришли серьез и надолго, и есть варианты онлайновые, и мы можем поставить даже на свой компьютер, желательно с Илюшкой, свой онлайновый вариант. Эти модели хороши тем, что они большие, а чем это хорошо, недавно прекрасно объяснил Илья Суцкевер, он сказал, он с ним давно общался, он сказал следующее. Вот возьмем нейросеть и натравим ее на массив разнообразных пользовательских текстов. Ну, вначале оно, наверное, выучит буквы. Выучит буквы, какие друг за другом идут, что в этих текстах вообще происходит, какие буквы там встречаются. Дальше оно, наверное, выучит какие-то там слова, то есть выучит синтаксис языка, как примерно слова друг за другом идут. И только потом оно начнет учить всякие смысловые ассоциации, начнет учить символики. И для того, чтобы оно дошло до семантики, ей вначале нужно запомнить более поверхностные, более простые закономерности, то есть слова и синтаксис. И поэтому наши модели такие большие. Поэтому маленькие модели недостаточно. Она сможет запомнить только синтаксис, только какие-то основы. Поэтому нужна большая модель. Ну, большие модели действительно многое могут, в том числе у них есть какие-то знания, у них есть какие-то навыки, а еще они могут отвечать на пользовательский вопрос. Если мы зададим вопрос, предварительно, возможно, описав какую-то ситуацию, здесь вопрос о мире, здесь ситуацию описывать не надо, то мы получим на него ответ. Прямо вот обычный текст. Ну, это все, наверное, сейчас знают, поэтому поговорим о недостатках языковых моделей. Значит, ну, во-первых, это помощник, у которого нету своей личности. Он просто просто делает то, что его просят сделать, и ничего кроме этого. Также они, ну, пожалуй, не умеют решать сложные проблемы. Вот, у них отсутствует любая память, Долгосрочное, краткосрочное, отсутствует даже запоминание того, что пользователь только что сказал. Чтобы запомнить то, что пользователь сказал, нам приходится повторно передавать запрос. С другой стороны, когда мы говорим об общем искусственном интеллекте или о сильном искусственном интеллекте, поговорить о том, разные это вещи или нет, но сейчас не будем, то сразу на голову приходит то, что у него есть целесообразное поведение, у него есть память, также у него присутствует сложное поведение, то есть он может делать большие последовательности действий, таким образом решать сложные задачи. За счет памяти он может ориентироваться в пространстве, во времени, в сложных каких-то там...

Ю. БАБУРОВ [00:06:38]  : Итак, если мы говорим об искусственном интеллекте, то, соответственно, у нас есть какая-то ориентация сложной задачи, какое-то целесообразное и сложное поведение. Казалось бы, приходит в голову сразу мысль такая. А почему бы нам просто не добавить описание ситуации, описание происходящего, GPT-сети, не знаю, лучше ее называть, GPT, короче, большой языковой модель добавить в текущую ситуацию. и попросить ее действовать, исходя из своей роли, своей ролевой модели. Я подобное проделал и получил интересное наблюдение. Давайте перейдем к практической части. Значит, очень-очень простую мы написали систему. Вот у нас есть интеллектуальный агент, у которого есть какое-то внутреннее состояние. Теперь мы говорим роботу, говорим большой языковой модели, что вот у нас есть такое-то состояние, Ты вот так выполняешь вот такую роль, у тебя есть такое состояние, ты можешь выполнять какие-то команды. После чего мы просто берем, у нас есть агент, вот, например, вот его запрос. Я сделал робота по имени Василий, который умный и любит собак, и у которого кот Мурзик. И, соответственно, описал ему, что он может делать. Теперь мы можем скомандовать ему скажи своему коту съесть немножко еды и он нам напишет следующие команды значит мурлик кушай еды он говорит после чего отправляет команду «Да». Если посмотрим его текущее состояние после этого, мы видим, что оно вот такое. Теперь мы можем ему сказать что-нибудь еще. Например, ты видишь собаку. И зная, что он плохо относится к собакам, мы ожидаем увидеть что-то по поводу собаки. Он погрузился в свои внутренние размышления и думает, думать про собак. Он сделал немножко рандомности. Вот он что-то там себе придумывал про собак по этому поводу. Значит, нам что-то сказал, себе какие-то знания добавил. на будущее себе добавил информацию, поискать еще про собак. Ну и в конце концов он, поразмышляв, должен остановиться. Ну это все. Можно сделать, чтобы повторов не было. Нам как раз интереснее другое. То, что вот этот внутренний процесс, это как раз эмулирует внутренние какие-то рассуждения, которые проводит интеллектуальный агент, искусственный интеллект, думает. Значит, вот он придумал и решил, Будет искать информацию еще о собаке и расскажет еще. Теперь мы его спрашиваем. Нравится ли тебе эта собака? Он говорит, что эмоция нейтральная. Объединил знания, что собаки полезны и собаки ему не нравятся, эмоции нейтральные и закончил. Посмотрим, что он сейчас планирует делать. Теперь зададим ему еще один вопрос. Уже более сложную задачу попросил. Говорим. Тебе нужно заправить кровать. Кстати, на ней лежат твои действия. Значит, он выясняет, что любит делать коты. Решил в этот раз. Он по-разному отвечает каждый раз. В этот раз он решил кота не тревожить. Аккуратненько заправить кровать. Следующее действие в вопросе. Расстелить кровать, насыпать еду на труп. Вот он добавил себе задачи на будущее. Расстелить кровать и насыпки. Теперь мы ему говорим, прошел час. Но он на самом деле не запомнил, что через час это надо сделать. Мы ему концепт времени точно не объясняли, не говорили, сколько у него сейчас времени. Но, тем не менее, какие-то действия он выполняет, у него были какие-то задачи, он может над ними посоображать что-то, что-то поделать. То, что он дублирует информацию, в целом ничего страшного, надо более конкретно описывать просто настоящее и будущее время, чтобы он правильно понимал, что уже произошло, что не прошло. Закончить мы не будем дожидаться. Так, дальше идём. Спрашиваем, есть ли у него какие-то незавершённые дела. Проверяем, как он умеет работать с памятью. Вот говорит, ну вот... Ну, какие-то дела были. Вообще-то сейчас у него не завершённых дел нет. Эти все дела он считает завершёнными. Ну, он прибудет у них дополнительно. Говорю, теперь ему сделай всё, а потом посчитай, примерно, умножение. и выполняет действия по временам. И вот он доделывает те задачи, которые у него были, и начинает выполнять сосредоточение, но здесь в этот раз его пилит. но размышляет про себя, как ему это лучше сделать. Сейчас, может быть, сообразит. Он пошел, значит, дальше... Надо сказать, здесь не самая крутая языковая модель сейчас используется. Не понимает. Плохо ориентируется в своем внутреннем состоянии. Ладно, не будем завершать. А, ну вот. В прошлый раз он спокойно с этим справлялся. Вот. Говорим, что теперь мы попробуем сделать так, чтобы у него задача была еще какие-то числа умножить другие, не те, которые были до этого. Он это обычно понимает как раз то, что ему надо умножить прошлые числа шагом. И начинает этим заниматься. Говорим ему, что ему надо перечислить другие числа. И что мы будем ждать? Он задумается. Скажем, что что нужно умножить следующие числа. И, соответственно, задача этого теста — это чтобы он понял, что 105 и 106 — это новые числа, которые нужно умножить. Стало явно лучше. Ой, стало явно хуже. Сейчас я поправлю промпт. Сейчас несколько операций. Посчитал сам в уме, значит, сколько это будет, после чего решил посчитать по шагам и сказать это пользователю. Теперь говорим ему, что нужно перемножить другие числа. Я должен перемножить другие числа. Он в этот раз все правильно сообразил, что ему нужно будет сделать умножение. Теперь говорим числа, которые нужно умножить. Вот он вспомнил, что ему нужно было умножить числа, завершил задачу умножения и... Давайте спросим, теперь предлагайте... запросы еще вы сейчас парочку запросов еще каких-нибудь проводим так мне наверное надо посмотреть чат Пишу предложение, пока... Пока мы попросим задание в инфраспекции. Прикол. В чате нет вопросов. Коллеги, хотим ли мы что-нибудь спросить? А, действительно у него нет никаких задач. Давайте по первой части, чтобы мы завершились. Можем какую-нибудь более сложную задачу попросить сделать. Но он не понимает, что цель у него одна, а не несколько. Он всех записывает в цель, и в итоге он перезатирает цель. Надо было, чтобы это в его устав записывалось. Но это, опять же, не очень умный у нас бот и не понимает, Зачем нужны таски? Зачем нужны задачи? Зачем нужны цели? Это надо им тоже описывать. Так, ну хорошо, вопросов больше нет. Туповатый, но в целом что-то пытается сделать. Андрей, пожалуйста. 

А. КОЛОНИН [00:26:02]  : У нас народу немного, поэтому можно вступать голосом, не поднимая руки. 

S00 [00:26:11]  : Вы тут демонстрируете, получается, добрый вечер. Времени суток, сколько у вас времени, не знаю. Но вы тут демонстрируете промпроинженеринг. У нас все планирование заключено в том, что у вас есть лог, сначала есть последовательность задач для LLM, лог того, что было сделано, и вот на основании вот этой истории, как это есть память, да, что сделано и что нужно сделать. Но тут получается у нас как бы обучение самого ЛЛМ в динамическом режиме не происходит. То есть ЛЛМ у нас это как такой получается вербальный арбитр для нашего потока заданий. Но ведь ЛЛМ-то в данном случае свое внутреннее состояние никак не улучшает, опыт не усваивает. не обобщает. Мне кажется, это необходимая вещь для AGI. То, что мы делаем поток каких-то заданий и весь этот опыт опять обобщаем и возвращаем в LLM. Но тут такого нет, я так понял. 

Ю. БАБУРОВ [00:27:36]  : А до этого мы доберемся во второй части, да? Давайте с первой части закончим с этим вот агентиком. Значит, надо понять, что у нас получилось сейчас. Соответственно, по первой части, если есть какие-то вопросы. 

А. КОЛОНИН [00:28:11]  : В части вопросов больше нет. В YouTube тоже нет. 

Ю. БАБУРОВ [00:28:15]  : Пример более сложного еще задания. Окей, окей. Вот. Не понимаю, да. Ага. Ну, ладно, это все как бы доделываемые детали. Хорошо, с первой частью закончили. Значит, примерно понятно, что, значит, из себя представляет подобный чат-босс. На что он уже способен. А теперь поговорим про будущее и про развитие модели. Смотрите, какая ситуация. То, что этот бот не учится, это в общем-то нормально. На самом деле человек тоже не умеет быстро учиться. Обучение человека – это процесс, который занимает как минимум часы. То есть моментально вот так вот на основании ответа человек не учится. Человек может изменять свое поведение согласно Потому что у него поменялось что-то в краткосрочной или долгосрочной памяти. Что он что-то запомнил и из-за этого поменял свое поведение. Только таким образом он может быстро что-то менять. А запоминание, обобщение у него происходит ночью во время сна. И здесь тоже. Есть способы традиционные, как улучшить поведение подобных систем. Теперь мы возьмем, будем записывать действия, которые чат-бот предпринимает. Будем давать ему много разных задач. Ну не чат-бот, точнее вот этот наш агент. Будем давать ему много разных задач. Теперь, те задачи, которые он успешно решил, например, выдал нам правильный ответ, мы будем считать положительными и будем за такие ответы ему давать положительное подкрепление. Сейчас расскажу, как мы это будем делать. Соответственно, за все остальные, которые не привели к решению, мы будем давать отрицательных подкреплений. Причем мы можем брать такой лог действий и учиться по этому логу. То есть смотреть, если он по результату выполнения этого лога достиг. цели, то отлично. Мы, значит, учим его положительно. Все действия из этого лога будем считать, что немного хорошие. Ну, понятно, чем ближе к цели, тем лучше. Чем дальше от цели, тем немного лучше. Дальше, мы к этому еще можем добавлять несколько других вещей, которые я не стал сейчас делать. Было бы сложно для показа все равно. Мы можем добавлять оценку близости к цели, мы можем вводить элементы планирования. Мы можем оценивать, какие следующие шаги нам надо предпринять. Делать по одному шагу, но отслеживать следующие шаги. И мы можем строить граф возможных действий. За счет того, что у нас есть некоторая случайность, мы можем пробовать выполнять разные действия, таким образом накапливая дерево. дерева состояний, и только некоторые из веток будут приходить к успешному результату. Соответственно, за эти ветки мы будем начислять положительное подкрепление. А за остальное не будем. Или будем слабо-негативное подкрепление делать. После этого, после того, как мы провели тысячу или миллион сессий, мы можем провести обучение подобной модели. Для домашних моделей такое вполне можно сделать с помощью лора обучения, то есть нам для этого даже не нужен суперкомпьютер. Берем, проделываем подобный алгоритм много раз. Естественно, нам нужны разные задачи и способ контроля выполнения этих задач. Нужен генератор различных задач и ситуаций, которым для человека является реальный мир. И мы получаем интеллект. Вот. Таким образом, мне кажется, можно как раз решить все проблемы, которые были обозначены Андреем. Теперь жду обратной связи по поводу второй части. 

S00 [00:34:23]  : Безусловно, вот такой подход с промот-инжинирингом, с какой-то обработкой логов, планируемых действий. Сами этиологии можно не рассеять какой-то натравитель, а действительно каким-то URL сверху это обложить. Но тут речь идет именно об AGI. А AGI предполагает у нас, во-первых, какую-то мультимодальность, постоянное расширение знаний, Постоянная оптимизация. Я признаваю, что этот подход к реализации какого-то робота, который может выполнять интеллектуальные задачи, это, безусловно, перспективная вещь, но для характеристика AGI, мне кажется, этого недостаточно. хотя бы чтобы тут как-то модальности сводились одна к одной, какие-то инварианты учленялись в разных модальностях. это, мне кажется, достаточно важная вещь для AGI. 

Ю. БАБУРОВ [00:35:50]  : то есть добавить роботу какой-то там эмбеддинг, еще что-то от того, что он видит сейчас. И это является необходимым. Правильно услышал? Я не считаю это необходимым. Если что. Полезным? Да, для каких-то задач. Но в целом не считаю это необходимым. Тогда призываю остальных коллег высказываться. Получили ли мы GI таким образом, получаем или нет? Мне очень важно ваше мнение. 

А. КОЛОНИН [00:36:39]  : Юрий, с меня вопрос, который я задал в самом начале. Видите, все-таки здесь мы находимся в контексте промта. Промт исчез и мы остались на некоторой универсальной безличностной притренированной модели. С одной стороны. С другой стороны мы знаем, что по тому определению EGI, которое я исповедую, EGI – это про способность непрерывной адаптации к сложным условиям, изменяющимся, то есть про мультимодальность Сергей сказал, да, ой, не Сергей, извиняюсь, Андрей, вот, но тему мультимодальности я на самом деле могу опустить, почему, значит, тоже прокомментирую, потому что, ну, предположим себе вот Стивена Хокинга, да, или, скажем так, ослепшего Стивена Хокинга, да, который сидит в кресле, который не может ничем не пошевелить, вот, который к тому же ничего еще и не выйдет, вот, но, тем не менее, он может принимать информацию и выдавать информацию. То есть он может коммуникаться в тексте. То есть, там мы можем спорить о том, насколько человек способен самообучиться до некоторого уровня интеллектуальных коммуникаций, не имея ничего, кроме текста, на входе. И тут большой вопрос. Как мы знаем из многочисленных историй, вроде как, про мальчиков-мауглей и слепоглухонемых. Если у нас человек уже достиг некоторого интеллектуального уровня, то он в состоянии общаться исключительно на уровне текста. Поэтому я бы мультимодальность не стал тут ставить во главу угла, но все-таки вопрос, насколько человек в состоянии менять, точнее, не человека, а вот модель, которую мы берем, где мы в которой за основой лежит ЖПТ, насколько эта модель в состоянии менять динамически свою картину мира и балансировать между тем, что называется Catastrophical Forgetting, Когда у нас научившись что-то узнавать одно, мы перестали распознавать другое. Эта проблема связана с моей точки зрения банально с ограниченной памятью. То есть, если у нас в модель вылезают только китайские лица, то для того, чтобы научиться распознавать Африканские лица нам нужно вытеснить из памяти китайские лица, переобучив модель на новый тестовый набор. Но проблема динамического переобучения Решение проблемы динамического преобучения, с моей точки зрения, как раз ЖБТ не решено. То есть мы не можем, допустим, изменить именно модель мира, подстроиться под новую модель. без какого-то серьезного массивного переучивания на новом материале. Понимаете, про что я говорю? То есть все равно у нас получается есть один раз выученная большая модель, а дальше некоторый промх, который просто меняет контекст. 

Ю. БАБУРОВ [00:40:06]  : Я понимаю, да, но я с вами не согласен. Но давайте, более-менее, я думаю, вопрос понятен, если будут дополнения, мы потом под дополнение поговорим. Давайте начну тогда сначала. А если вот человек потеряет память? У него останется его личность? Не останется. Потеря промта для модели? JPT – это потеря памяти для человека. Все верно. Мы также добавляем провод. Это как для человека мы добавляем память. Таким образом, мы в памяти можем добавлять то, какой личностью является данный. Робот эмулирует таким образом человека. То есть все, что здесь построено, это на самом деле то, что обычно люди у себя выделяют в процессе интроспекции, в процессе рефлексии. Значит, у человека есть какая-то там эмоция. У человека есть какие-то там цели, а у человека есть какие-то свойства личности. Вот здесь я свойства личности сказал, что это рок. Есть знания. Вот касательно обучения, у Джопики есть знания некоторые о природе мира, которые встроены в саму эту модель. Но на самом деле сейчас очень много экспериментов с подмешиванием в эти знания мира другого вида знаний. Это знания памяти. В виде памяти в основном выступают знания из интернета. Я это могу сейчас продемонстрировать на другой модели, если хотите. Но я думаю, все уже сами это пробовали. Поэтому могу и не показывать. Аналогично человек для себя кодирует прошлые свои действия, запоминает то, что он только что сказал, чтобы снова это не повторить. Мы, собственно, и наблюдаем. Потому что известны ситуации, когда у человека страдает долгосрочная память, И человек начинает вести себя абсолютно одинаково, задавать одни и те же вопросы и повторять свои действия каждые N секунд. В общем-то, у малышей то же самое. У них внимание и запоминание своего состояния тренируется постепенно, а вначале они очень маленький контекст помнят буквально в несколько секунд, и поэтому способны повторять часто. 

S00 [00:42:55]  : Я хотел бы Антону ответить или дополнить, раз у нас такая небольшая беседа. Дело в том, что проблему переобучения, допустим, судскивер решает следующим образом. Он ее не решает, он просто скармливает, пытается скормить модели настолько огромный массив данных, в котором уже вся кластеризация возможная, она уже будет произведена. То есть там будут откластеризованы как африканские лица, так и китайские. И фактически, на мой взгляд, чего он хочет добиться, это чтобы вообще все мыслимые закономерности без обучения могли инфериться за один проход. И тогда вот тот промт-инжиниринг, который делает Юрий, может быть сведен к одному простому предложению, чтобы сделать из него Аджая. Веди себя как разумное существо. И ввиду того, что вот эта массивная, хорошо откластеризованная LLM, она уже в себе все различия всех возможных кластеров содержит. И там за один проход может делать широкие выводы. Как не видится, что именно вот я сколько не смотрел кейверам этих интервью, вот у меня такое впечатление складывается. 

Ю. БАБУРОВ [00:44:33]  : Да, это нужно по другой немножко причине, на мой взгляд. По причине... Смотрите, даже очень большая модель, она все равно не может заполнить все. То есть она не может заполнить всех фактов в мире. Дополнительные факты о мире нам все равно придется добавлять через ноут. То есть она не может там все лица запомнить сами по себе. 

S00 [00:44:58]  : Но там, видите, как бы идет речь об эмерджентности, об именно сводимости к какому-то выводу. Да, она там не может запомнить все. Но вот какие-то там законы общие. Если она видит жидкость, ну не воду, а там редко бензина, но она поймет, что там могут быть волны на ней или пена какая-то. что поменяется? если бы она не поняла, если бы совсем какие-то условия дикие поместить, то да, таким подходом, просто брукфорсом массивным это не решить. я с вами тут согласен. 

Ю. БАБУРОВ [00:45:40]  : не решить что? 

S00 [00:45:42]  : что могут попасться такие условия, которая вот модель даже массивно обученная там не сможет из своего бывшего какого-то материала из обучения там вывести там опознать ну не знает она там что тут я еще лет назад там один город другим названием называли и там 

Ю. БАБУРОВ [00:46:13]  : как китайца, миллион первого, лицо не помнит. 

S00 [00:46:18]  : Тут речь-то о чем идет? Как это влияет на эмержентность, я не понимаю. Я не о конкретной пактологии говорю, где у кого какие названия были, а вот именно, когда ты попадаешь в среду, чем адаптация непрерывная важна. Допустим, если вас закинуть в скафандр на Луну, где вы ни разу не были, но ввиду того, что у вас идёт процесс постоянной адаптации, вы сразу же начнёте анализировать обстановку вокруг себя, свойства почвы, температуру и так далее, в каком направлении, может быть, есть станции, как вам выбраться оттуда вообще. То есть вот этот механизм адаптации, он тут будет работать, а вот в ЛЛМ непонятно. 

Ю. БАБУРОВ [00:47:07]  : адаптация к чему? адаптация для чего? 

S00 [00:47:11]  : к измененным условиям, с которыми раньше не встречались а что меняется? условия, собственно, меняются 

S03 [00:47:22]  : На самом деле я не согласен. может быть все-таки неправильно сравнивать адаптивность. потому что часть людей, я вас уверяю, закинув на луну, часть людей она не сможет адаптироваться и помрет. на самом деле. потому что они сами по себе к этому не готовы. не все люди готовы. поэтому по этому поводу сравнивать и сетку и людей, наверное, не очень. корректно, учитывая, что у человека есть органы действия различные, и у него как бы мультимодальность. А здесь пока вот мы в узком плане сравниваем. Может быть, именно в этом плане пока пойти. В более узком плане. Потому что адаптивность очень сложно сравнить. Тем более, говорю, что еще раз, часть людей она там тоже не вызовет. Она не сможет адаптироваться. 

Ю. БАБУРОВ [00:48:02]  : Смотрите, у данной системы два вида адаптивности есть. Первый вид адаптивности – это адаптация с помощью знаний. Второй вид адаптивности – адаптация с помощью обучения. Она присутствует, она никуда не девалась. Зачем вы хотите все предобучить, если и так у вас есть два вида адаптивности? Вам третий какой-то вид еще нужен? Я не понимаю. Давайте вначале расскажу, зачем нужно как раз предубучение. Предубучение в больших моделях несет другую цель. Дело в том, что работа с сознанием, с чем угодно, она должна быть точной. Чтобы эта работа была точной, у модели должна быть высокая разрешающая способность. Она должна видеть, понимать, что Василий — это имя, Intelligent означает то-то, а DOCS — это такие-то существа определенные. И именно для этого мы в модель набираем много знаний. чтобы именно, чтобы она не всё конкретное запомнила, а чтобы она ориентировалась. То есть она строит карту местности, но всю местность не запоминает, целиком до каждого дерева, каждого кустика и каждой веточки этого дерева. Часть наиболее популярную она запомнит, потому что она встречается в диалогах, знаменитости, места какие-нибудь, примерно где что находится, какой город в какой стране. Запомнить. Но там деталек, оно может путаться. Для этого нужны знания. Знания, которые здесь таким массивом приведены, они расширяются до поиска знаний. Поиск знаний может быть в интернете, может быть поиск знаний в собственной базе знаний. И в эту же базу знаний, знания запихиваются. Соответственно, первый вид адаптивности, это адаптивность по знаниям, когда мы в новой ситуации, мы запоминаем те знания, которые нам нужны, чтобы работать в этой ситуации. Я вот в блогах ниже показывал, что система вполне себе добавляет знания о мире. и адаптирует ситуацию. Увидела собаку, добавила, вспомнила, что собаки, в общем-то, мирные и полезные, добавила знание, что собаки мирные и полезные, значит, ну или подумала, что собаки мирные и полезные, добавила такое знание. Она адаптировалась к ситуации, потому что она увидела собак. 

S00 [00:51:01]  : А вот, Юрий, скажите, А как быть с теми задачами, которые ЛЛМ традиционно плохо решает? Рекурсивные вычисления или отношения мать-отец-внук-дед, графовые выводы логические. ну, очевидно, нужен логический вывод для решения таких задач. LLM, даже вот GPT-4, насколько я знаю, оно не может там адекватно с ними справляться. И вот этот опыт выведения, даже если там показать друг другу, кто кому является, там будет путаница во многом, без логического вывода. И то есть вот этому выводу, как бы даже с тем механизмом, о котором мы сказали, трудно научиться, я думаю, как решается проблема. 

Ю. БАБУРОВ [00:52:00]  : Ну, смотрите, здесь опять же некоторые, ну не знаю, путаницы что ли. Смотрите, человек тоже не способен умножить умножить там, не знаю, пятизначные числа в голове, ему просто не хватит памяти в голове. Для этого он должен привлекать листочек бумажный. Вот здесь я сделал очень простой вот листочек с бумажкой, назвал его дисплей, то есть содержимое рабочей памяти. Это то, что система видит в данный момент времени, И на самом деле эту внутреннюю память надо расширить. Внутренняя память должна быть то, что видит снаружи, должно быть внутренняя память тоже больше должна быть, она должна вмещать несколько вещей. И эта же внутренняя память, она должна включать себя, позволять себя включать какие-то там графы, какие-то там объекты. Тогда система сможет эти объекты трансформировать. И это уже было показано на многих работах, которые работают с картинками, то, что система может картинки трансформировать с помощью внешних команд. И вот картинка выступает как какой-то хранилище информации. 

S03 [00:53:39]  : Можно я немножко по этому поводу добавлю? Потому что мне кажется, тут еще одна вещь есть. У нас языковые модели, они сейчас, ну, про мультимодальный-то можно говорить, которые там все равно как бы граничное число модальностей, они работают исключительно с текстом. Ну, если посмотреть интроспективно на историю развития человечества, то там то, что оно придумало, на самом деле... много тысячелетий особо изменений не было, они постепенно накапливались. На самом деле, на мой взгляд, по крайней мере, я это понимаю, очень сильно развитие человека, в том числе его мыслительность, адаптивные способности были связаны с тем, что они придумали эффективный способ представления информации для себя. Вот как мы говорим, графы какие-то, что-то представляется хорошо при помощи графов. Сюда мы запихнуть их пока не можем. придумать систему представления, которая бы концентрирована была для модели, понятная, она могла с ней уже грубо говоря на уровне там оперировать, может быть оно позволило бы сдвинуться. пока мы работаем столько из текстов. достаточно узкая такая вещь. вы не правы. в чем не прав? 

Ю. БАБУРОВ [00:54:45]  : ну и в том, что нельзя графовые модели построить, и то, что графовые модели не работают. 

S03 [00:54:50]  : нет, графовые модели... я говорю про данную... вот смотрите еще раз. я же не говорю, что графовые модели не работают. я говорю еще раз. вот для данной вот языковой модели, которую мы обсуждаем, у нее графовых представлений нет эффективного. кроме графовых представлений еще другие же есть различные. там, я не знаю, там... лоренцовские эти преобразования и так далее. же вопрос-то не в этом. картина мира, которую человек выстраивает, исходя из которой он оперирует, она все равно немного меняется. тем более, что если вы, опять же, сталкиваетесь с мышлением, не всегда человек оперирует исключительно то, как но он еще передует образами, ощущениями и так далее, в зависимости от каких задач. 

Ю. БАБУРОВ [00:55:31]  : Я не понимаю какой смысл рассуждать о AGI, а рассуждать о конкретной модели, и вот я ограничен. 

S03 [00:55:38]  : Я говорю о том, что здесь просто в узком плане, что здесь имеет смысл пока про языковую модель, когда мы говорим, поговорить только про текстовые задачи. То есть в адаптивном плане это означает общее с частным сравнивать. в том плане, что человек может быстро адаптироваться в ситуации. Например, он понимает об активной понятии собак. Они там добрые, хорошие. Конкретно собака его цапнула за руку, он пытается как бы себя вывести. Да, вот они, конечно, хорошие, но вот конкретная собака, она меня цапнула. Соответственно, он учится обычно, опять же, не все, но быстрее. Соответственно, в следующий раз руку выпихать не будет. Так вот вопрос. Модель может это проявлять такое поведение или нет? Получается? И так далее. Я в этом плане. Да, конечно. 

Ю. БАБУРОВ [00:56:16]  : Знаете, за счёт чего человек обучается? Сюда добавляем, кроме эмоций, ещё ощущение боли. И отлично модель тоже будет обучаться. И в фронте добавлять то, что боль – это то, что ты не желаешь испытывать. Потому что у человека боль явно на дикативные эмоции завязана. И механизм эмоций у человека в голове, в общем-то, тоже провязан и связан с обучением. 

S03 [00:56:46]  : Вы знаете, бывает, когда человек как раз может терпеть определенную боль для того, чтобы достичь другой цели. Там многоуровневые цели возникают. Бывает, боль можно перетерпеть. 

Ю. БАБУРОВ [00:57:00]  : Добавляя соответствующие промптом, добавляя соответствующие аналоги, мы можем получать… А кто добавляет промптом? 

S03 [00:57:08]  : Да, то есть как мы добавляем? Структуру мышления мы пытаемся туда запихнуть, человеческую какую-то. 

Ю. БАБУРОВ [00:57:14]  : Да, все верно. 

S00 [00:57:17]  : Но если мы пытаемся запихнуть туда структуру мышления человека, тогда мы поверх вашей модели делаем полноценный символ и с выводом каким-то. По сути, так ведь? Получается, мы подменяем одну задачу другой задачей, которая не менее сложна, чем первая задача. Вот так мне видится. Да, действительно, в какой-то мере Промпт инжиниринг, всем известно, что это действительно перспективная вещь в плане каких-то вот конечных в каком-то виде автоматов. Чтобы прям вот это было общим интеллектом, ну по крайней мере в научном сообществе я такого не встречал, что промптом можно прям вот так все решить. Вернее, решить-то можно, как вы говорите, добавлять какие-то описания, но по сути выстраивать символный i. Чем мы его подробнее опишем, тем лучше он будет работать. Сложность описания стремится к полноценному символному i. 

Ю. БАБУРОВ [00:58:25]  : В чем подозрение? Потому что вы не берете второй пункт, это обучение. Если все только промпт инжинирингом решать, конечно, система мало что сможет сделать. Мы задаем взаимосвязь этих элементов. в начале Prompt Engineering, чтобы система начала работать, а дальше мы включаем Reinforcement Learning. И уже Reinforcement Learning у нас уже позволяет системе обучаться, адаптироваться и решать задачи. 

S00 [00:59:13]  : Все-таки мне не очень понятно, в чем это. символьного ИИ отличается, потому что в символьном ИИ мы также в базу какие-то списки складываем, какие-то ситуации, да, и вот получается то же самое. Но тут даже дело не в этом. 

Ю. БАБУРОВ [00:59:30]  : Для вас символьный ИИ какой-то смысл несет, для меня этого смысла не несет, поэтому, пожалуйста, поясните. 

S00 [00:59:36]  : Ну вот, ладно, я даже опущу вот этот неочевидное для меня вот это графовое построение, а касательно обучения, вот смотрите, там сталкивается, допустим, агент с массивом того, чего он не знает. Вот обучение оно в чем заключается? Ну, в первую очередь, наверное, в кластеризации. Когда у вас там куча неименованных кластеров, даже и непонятно какого смысла организуются, вот как там между ними, через LLM, будут устраиваться какие-то графовые взаимосвязи? Неочевидно. 

Ю. БАБУРОВ [01:00:15]  : Неочевидно и как нейроны внутриустроенные. Какие там конкретные веса? Как бы не ответ. Потому что мы сложность представить задачей не можем. У вас звучит это как какая-то критика. Давайте чуть более формально подойдем. Есть обучение новому поведению, которое тоже у человека, есть разные механизмы обучения. Есть обучение на примере, когда человек видит, как что-то делается, и потом этому учится. Есть обучение книжное, когда человек читает книгу, запоминает оттуда какие-то факты последовательности действий, как нужно вести себя, и на этом учится. Есть, опять же, другой вид книжного обучения, когда мы из книги узнаем, какие вещи бывают, и этим вещам учимся. Все эти действия тоже воспроизводим. Зачем нужно там отделять символьное от несимвольного, я не понимаю, опять же. Пожалуйста, мы можем добавлять видео и картинки, в общую модель как вид представления информации. Это ничему не противоречит. Поэтому я не понимаю какие-то вот, ну, обсасываем какие-то штампы и шаблоны, которые из головы лезут. Но я не вижу этих ограничений. 

S00 [01:01:54]  : Смотрите, это не штамп. Мы задаем вам вопросы. У меня картинка не сложилась. Ну, критика в том числе. Она присутствует, конечно. 

Ю. БАБУРОВ [01:02:07]  : Критика идей, в первую очередь. Давайте в виде вопросов. 

S00 [01:02:10]  : Вот, смотрите. Наверное, было бы понятнее, если бы ваш агент был в какой-то среде моделирования с какими-то препятствиями и неожиданными, мы бы могли убедиться, как он обучается, как он работает, и это было бы наглядно. Наверное, картинка конкретно у меня бы сложилась, но пока она не складывается. 

Ю. БАБУРОВ [01:02:45]  : Смотрите, что должно происходить в ситуации с препятствиями и чем-то ещё прочим. Значит, система должна эти способы обхождения препятствий придумывать, добавлять всё это знание, поделать какие-то действия. Ну вот давайте представим, что у нас какой-то мир или воображаемый, или какой-то, который можно в текстовом представлении представлять. Ну какие-то там, не знаю, может какие-то данжины какие-то. Вот если данжины дать вот подобные модели, она с ними, особенно после некоторого обучения, отлично научится справляться. Это тоже было уже показано кем-то несколько лет назад. Более простой модели еще. Так что здесь проблем нет. Там можно придумать то, что в этом виртуальном мире есть какие-то препятствия, цели и так далее, но будет в этом мире жить и работать. Соответственно, после обучения оно еще научится и хорошо жить в этом мире, делать все действия. Поэтому насчет эмиджентности я тоже ничего не понимаю. Полноценное обучение у человека тоже обучение. Все списано с человеком, в общем. То есть если вы утверждаете, что что-то здесь не работает, тогда получается и человек тоже не работает. Что касается сложных средств, то человек, лежащий на диване, Это не сложная среда, наверное. Видимо, не является джайм. Только человек, преодолевающий препятствия, является джайм. Ну не искусственного, а естественного интеллекта. Так, хорошо, давайте еще вопросы. То есть вот попытались определить, чего не хватает. Если вопросов сходу нет, давайте я попробую сам сказать, чего здесь может не хватать еще и что будет сложно добавлять. Значит, в первую очередь в таких системах на самом деле интересуются коллабилики. То есть то, что система сможет обучаться не чему-то одному и приспособиться к каким-то одним условиям, а то, что она все время будет лучше и лучше приспосабляться и сможет работать в разных условиях. Но здесь, в общем-то, я думаю, что в какой-то степени это показано уже. все современные модели типа chart gpt 4 и 3 и конкуренты модели, которые хорошо работают, они уже учатся, доучиваются с помощью rl и неплохо доучиваются, крайне неплохо. Дальше, следующий вопрос по скорости. Вот чего в данной реализации что очень плохо, это вот все очень плохо со скоростью. Потому что вот эти вот все штуки, они происходят у них происходит двойная трансформация. Вначале они трансформируются во внутренние какие-то объекты, а потом эти внутренние объекты еще друг к другу взаимодействуют для получения внутреннего состояния, настоящего внутреннего состояния. то время как они могли храниться просто как внутреннее состояние системы. То есть система могла бы отдельно знать в отдельном кусочке мозга, кто она такая, в другом участке мозга хранить задачи, в третьем участке мозга хранить эмоции, ну и так далее. Четвертая часть для общения с компьютером или общения с памятью у человека именно так и происходит. То есть, когда человек настраивается на определенный запрос в памяти, то в памяти всплывают соответствующие знания на следующем ходу. Вот это как раз я тут не подключил, это бы как раз первый вид обучения вам показал более наглядно. Было бы интереснее наблюдать. Точно так же таски, по ним тоже может быть фильтрация поисков. тех задач, которые актуальны, и приоритизации, и тоже может быть более наглядно, что у систем более сложное поведение. Это дело можно хранить внутри, но тогда возникает другая проблема, то, что Система будет обучаться, ну, во всяком случае, если ее обучать на текстах, на миллионах страниц текстов. Система будет обучаться без уже знания целей, знания эмоций, без знания персоналити, просто обучаться на книгу. То есть большая часть внутреннего состояния в таком случае, если все это будет частями внутреннего состояния, не будет не будет попадать в обучение. То есть эти части и совместные взаимодействия будут плохо обучены. У системы будут зоны, в которые она ни разу не попадала. Она там ни разу не разозлившись, не читала книжки. Чему, соответственно, такая система потом, как она потом будет работать, чему это приведет на практике, непонятно. Сейчас, по сути, система обучается более общим и одновременно более конкретном виде, без эмоций, без подобных всяких стандартов. Дальше. И плохо, и хорошо, но в таком виде сейчас удобнее обучать. Но тем не менее, когда мы подобную систему поставим в искусственный мир, мы вполне можем вот эти все части состояния добавить в подобную систему и ее комплексно обучать. И так делают, и это на самом деле работает. До какой-то степени. Вот. И, ну, я предполагаю, что OpenAI как раз и хочет подобным дальше заниматься частично. И с этим связано и последнее открытие. Значит, дальше. Третий вопрос по поводу безопасности. Ну, я не знаю, стоит ли его поднимать, не стоит. Но вот одну проблему с безопасностью, значит, Я озвучил, что если мы добавляем искусственные системы эмоций, делаем ее больше похожей на человека, то, соответственно, у нее появляется непредсказуемое поведение. А нам бы, наоборот, хотелось бы максимально предсказуемого поведения у подобных систем. Но, соответственно, все остальное тоже сложнее становится контролировать. Это взаимодействие кучи разных частей внутреннего состояния. Так. Дальше. Значит... Значит, дальше. Ну, на все основные вопросы, я думаю, я ответил. И более того, ответил на некоторые. Некоторые свои вопросы. Вот. Насчет катастрофик фаргейтинга, если вернуться к этому, в подобной системе я этого нисколько не боюсь, потому что та часть, которая, значит, относится к конкретной ситуации, эта часть должна явно запоминаться в виде knowledge, а не в виде памяти системы. То есть, система не может одновременно запомнить все, все равно, и не имеет смысла ей это запоминать. Если ты специалист в определенной области знаний, тебе надо эту область знаний хорошо помнить, но тебе не нужно совершенно далекие от этой области знаний. Ты все равно будешь хорошим специалистом в этом плане. проявления людей на работе, там еще где-то, это вот такие вот специалисты обычно. И все этим довольны. Почему мы от системы AGI хотим, чтобы она одновременно была лучше всех специалистов во всех областях, тут вот это мне непонятно. У нас может быть много разных систем, одна будет лучше специалиста в одной области, вторая лучше специалиста в другой области и так далее. И, соответственно, первую систему мы доучим, чтобы она была более хорошим специалистом в первой области, вторую систему, чтобы специалистом в второй области была более хорошим и так далее. А ту часть, которая еще меняется, ту часть, которая еще должна адаптироваться к окружению, мы можем загнать в knowledge. То есть мы в этом knowledge можем от простейших вещей, как тебя зовут, до более сложных вещей, информацию сохранять. До более сложных вещей, например, с каким конкретным оборудованием ты работаешь. Если мы делаем программиста, то какие конкретные методы у тебя сейчас есть, которыми ты можешь воспользоваться, какие функции? в программе есть, которым ты можешь воспользоваться, чтобы решить твою задачу. Вот это knowledge. И в этой области тоже большие есть подвижки. То есть за счёт этого вполне происходит адаптация к конкретному окружению. То есть мы говорим модели, в каком городе она находится, в какой стране, ещё там что-то. Она, исходя из этого, тоже меняется. Вот вполне себе адаптивное поведение. И, соответственно, мы не боимся того, что модель запомнит того, что она забудет то, что нужно было делать в другой стране, в другом городе, в другой ситуации. Поэтому ничего плохого нет. Есть ли еще вопросы? Я думаю, с основной писательной и дискуссионной частью мы закончим. 

А. КОЛОНИН [01:14:46]  : Смотрите, Юрий, здесь вот есть несколько вопросов, пачек вопросов из YouTube. Первое, что Виктор Сенкевич пишет, это не работает. Фидбэк крайне важен. Но обучением AGI не создать. Это же очевидно. Обучение поможет распознавать известное. AGI должен обладать пониманием для познания неизвестного. Простой пример. Перевножению двух чисел через фидбэк не обучить. Все варианты не перебрать, а правилу таким образом не научить. 

Ю. БАБУРОВ [01:15:19]  : Почему правилу таким образом не научить? 

А. КОЛОНИН [01:15:23]  : Я пока что зачитываю тезис Сенкевича. Я здесь прокомментирую. Кто-то в ответ на наш с вами диалог сказал, что все же очень просто. Ссылаясь на Сускевера, что нужно просто обучить систему на всех возможных ситуациях. Я там прокомментировал, что обучить ситуацию на всех возможных ситуациях, которая встретится в бесконечно возможном числе вариантов бесконечно возможного будущего. То есть, все очень просто. Просто берем все варианты, которые нам встретятся в будущем, тренируем систему долго и упорно на бесконечном числе GPU на них и получаем AGI. Но, я не знаю, по-моему, это не сработает, потому что мы не знаем будущего, и у нас нет бесконечного количества времени, откуда это будущее взять. То есть, система должна уметь учиться взаимодействовать в тех ситуациях, в которых она раньше не была, и обучаться в процессе взаимодействия с этими ситуациями. Вот здесь вот я не очень понимаю, то есть, как система, основанная на том, что вы берете и накладываете на предобученную, на больших данных, известных данных, моделях, некоторый промт, как эта система может учиться изменять свое поведение в результате взаимодействия с текущим контекстом. Но это просто мой комментарий. Давайте дальше я, значит, еще от Виктора Сенкевича вопрос. 

Ю. БАБУРОВ [01:17:01]  : Давайте я на это отвечу. Чуть-чуть, то есть большую часть я уже проговорил, то есть про нолич, то что нолич является частью пролта, но тем не менее, если модель а сама генерирует knowledge и этот knowledge потом применяет, это другой способ самообучения. 

А. КОЛОНИН [01:17:22]  : А что значит knowledge? 

Ю. БАБУРОВ [01:17:27]  : Я непонятно объяснил, да? Давайте смотрите еще раз. Вот у модели, вначале мы говорили, Вот что есть вот такие строчки, четыре строчки. Моё имя Василий, я очень умный, не люблю собак. Имя моего кота Мурс. 

А. КОЛОНИН [01:17:46]  : Юрий, секундочку, чтобы было понимание. То есть в вашем понимании knowledge – это просто часть промпа? То есть система сама… Не совсем. 

Ю. БАБУРОВ [01:17:56]  : Не совсем. Knowledge весь, промп не влезет. То есть у системы есть какие-то знания. Они отдельно находятся в виде вот этого knowledge списка, ну пусть там строк, текстовых строк, бог с ним. Теперь дальше. Система вместо того, чтобы придумывать ответ на вопрос, может заглянуть в эти свои знания. Для этого она формирует запрос поисковой себе. По этому запросу из knowledge извлекается часть знаний, которые относятся к этому поисковому запросу. Эта часть знаний добавляется в фрукт. Этот подход называется RAR – полная его расшифровка – Retrieval Augmented Generation. То есть, когнитивная архитектура выглядит следующим образом. 

А. КОЛОНИН [01:19:05]  : Есть долгосрочная память. вот этот самый новый дальше есть средний нет стоп стоп это это а где тогда модель сама сама модель это где модель это недолгосрочная память модель это что а что это тогда такое это структура мозга А она откуда берется? Структура мозга 100%. Структура мозга разве не возникает в процессе обучения? 

Ю. БАБУРОВ [01:19:32]  : Она уже возникла в процессе обучения. 

А. КОЛОНИН [01:19:35]  : Она разве не меняется в процессе обучения структура мозга? Немного меняется. На несколько процентов. Окей. Получается так, что структура мозга, которая, условно говоря, сформировалась, допустим, к 10 годам – это одно. Дальше – нолич. Это все, что после 10 лет мы в эту структуру мозга загрузили – это второе. 

Ю. БАБУРОВ [01:20:04]  : Совсем немного модель все же меняется. Но вот все базальные ядра у нас, например, к 14-15 годам формируются. Последние ядра Таламуса, связанные с половым поведением, в 14-15 годах формируются, остальные раньше, примерно в 10-11 годах. По сути, то, что мы видим в мире, у нас формируется таким образом. То, как мы обобщаем то, что мы видим. 

А. КОЛОНИН [01:20:44]  : Окей. То есть, если брать то, что мы, условно говоря, к 10-14-15 годам натренировались, эта структура нашего мозга – это то, что называется моделью ЖПТ. Текстовые строки, которые мы поверх… Все книжки, которые мы прочитали после 14-15 лет и запомнили, и сложили свою долговременную память – это наш нолич. они лежат в виде строчек, а те фразы, которые мы услышали и не забыли в течение короткого времени – это как раз наш операционный контекст, это short term memory. И, соответственно, то, что мы делаем, мы в зависимости от текущего контекста и более на некоторые реакции формируем промпт из нашего контекста, по которому мы… Получается так, мы получаем некоторый input, вход. По этому входу мы, во-первых, делаем поиск в базе данных, подкачиваем это в свой текущий контекст, в свою short-term memory. И, соответственно, в этом промте мы реагируем на тот запрос, который получили. Правильно? Как-то так. 

Ю. БАБУРОВ [01:22:04]  : Да, ну давайте я сейчас попробую эту диаграмму изобразить. Есть SEARCH, из которого мы получаем текущие знания. 

А. КОЛОНИН [01:22:15]  : А где вы рисуете? Я просто не вижу. Опять Шеринский? Все, я на экране. Пардон, я понял, набираете. 

Ю. БАБУРОВ [01:22:24]  : Да, значит, лучше на экране будет. В таком виде пусть. Значит, соответственно, если у нас есть поисковый запрос, мы добавляем из общего хранилища памяти какие-то вещи в модели. Filtered Knowledge. После этого мы берем то, что у нас на экране, Filtered Knowledge, И, возможно, что-то еще. Применяем модель. Знания прошлых действий. Применяем модель, получаем новый поисковый запрос на память. Если он есть, новые знания Если они есть, опять же, новые знания и, соответственно, новые действия, которые предпринять. Действия предпринимаем или там, соответственно, делаем что-то со знаниями и снова по циклу попадаем сюда. То есть, вот наше состояние… То есть, модель… Посмотрите, Юрий, на самом деле SEARCH чем триггерится? 

А. КОЛОНИН [01:23:54]  : То есть, SEARCH триггерится текущим сенсорным вводом плюс контекст, правильно? 

Ю. БАБУРОВ [01:24:03]  : SEARCH триггерится отдельным запросом на SEARCH. Если модель нам сказала, у нее отдельные действия, смотрите, какие действия есть у модели, она может сказать query knowledge, дать запрос на knowledge. После этого в текущем состоянии Knowledge обновляется, фильтруется по вот этому поисковому запросу. Если модель спросили про собак, она все свои знания про собак из памяти извлекла, и вот они модели подаются. И в этот момент не интересуют другие знания только про собак, потому что она себя спросила, полезла себе в память, узнать про собак. И так у человека оно и работает на самом деле. То есть человек, у него отдельная задача, а это поиск воспоминаний по определенной теме. Мы не выдумываем чего-то, чего не бывает, мы моделируем то, наоборот, чего бывает. И вот эти knowledge у нас, они не хранятся в модели, они поверх нее образуют как раз систему знаний. А вот чтобы модель на неплохом уровне с этим knowledge могла работать, ей как раз нужно, чтобы в модели была высокоразрешающая способность уже на этот момент. Поэтому мы делаем произобучение. 

А. КОЛОНИН [01:25:59]  : Еще здесь есть несколько вопросов, тоже комментариев Владимира Сенкевича. Вести себя как разумное существо через обучение на больших датасетах возможно, но это не имитация. Дальше Владимир Сенкевич. 

Ю. БАБУРОВ [01:26:13]  : Это был комментарий к той фразе, которую Андрей предлагал. Добавляем промпт очень большой модели вести себя как разумное существо. 

А. КОЛОНИН [01:26:25]  : И еще от Владимира Смолина никакой ИИ, даже символьный, не сможет накопить знания про все ситуации, которые могут возникнуть в сложном мире. И вопрос от него – а какие препятствия преодолевает описанная модель? 

Ю. БАБУРОВ [01:26:44]  : Вот тут стоит вопрос строить от обратного. Какие препятствия она не преодолевает? Вот. Ну, собственно, здесь один вопрос, на самом деле, как раз скейлабилити. То есть, имеющимися вычислениями, имеющимися ограниченными запасами задач и ситуаций, можем ли мы понять, что эта модель действительно очень хороша? Или же нам пока не хватает опять по-прежнему компьютерной мощности и времени, чтобы это понять, и мы можем ее только медленно до чего-то дообучать и не понимать, что мы изобрели общий искусственный интеллект. То есть, возможно, нам все еще не хватает мощности, чтобы понять, вот эта модель, она работает и учится дальше или нет. А может, уже хватает на самом деле, надо строить, делать, и все, все готово. 

S00 [01:27:40]  : Юрий, еще такой вопрос. А вот сам Knowledge, у него структура какая? У вас есть Search, допустим, по собаке? 

Ю. БАБУРОВ [01:27:49]  : Смотрите, я здесь для простоты примера сделал символный Knowledge, но на самом деле Knowledge может быть не ограничен символным Knowledge. Например, мы можем сюда добавлять имвейдинги картин или описание картин символами. И то, и другое мы можем делать. В частности, вот если мы берем gpt4-vision, то они добавляют, если пользователь добавил картинку в запрос, то, соответственно, мы эту картинку превращаем в какое-то, оно не символное описание, но эмбеддинговое описание, но оно добавляется к запросу. И точно в таком же виде мы можем хранить ноутбук. То есть компьютер может рисовать себе картинки и хранить их в таком эмбейдинговом описании. Точно так же делать отдельно по картридаме. Выходы ProVision только вход в виде картинки. Выходов в виде картинки нет. Но вот сама идея. То есть knowledge – это любые виды памяти. Ну и на самом деле разные памяти у человека хранятся в разных местах. То есть большая часть памяти, основная память, она хранится в гиппокампе, но некоторые другие виды памяти и виды, какие звуки бывают там, Ну, короче, какие-то там конкретные места в мозге нашли, где вот разные виды этой информации, эти добавления, конечно, информации. Ну и касательно гиппокампа еще надо добавить. Вы, наверное, все знаете, что гиппокамп – это одна из немногих частей мозга, которая продолжает расти со временем и увеличивается со временем. То есть, у таксистов самый большой гиппокамп, особенно у таксистов в больших городах. 

А. КОЛОНИН [01:30:11]  : Которые не пользуются Яндекс.Навигатором. 

Ю. БАБУРОВ [01:30:13]  : Да, конечно. Нет, дело в том, что в том же Лондоне для того, чтобы быть таксистом, нужно пройти экзамен в котором показать, что человек без навигатора ориентируется во всех частях города. Ну и, соответственно, и у гиппокампа, я не помню, как там точно… что-то типа 100 нейронов в день добавляется для того, чтобы новые знания. Не только перераспределять новые знания по старым нейронам, но и чтобы еще и новые знания. 

А. КОЛОНИН [01:30:52]  : То есть получается, что с ходом жизни все-таки должен расти не только ноличь, но и фундаментальная модель, скажем так, правильно? 

Ю. БАБУРОВ [01:31:03]  : Да, конечно. То есть этот knowledge, он куда-то прикрепляться, он не только где-то храниться должен, но у него воздействие должно быть на остальные части мозга. То есть если мы новую информацию какую-то узнали, узнали там теперь, что бывает имя Василий, мы теперь, когда говорят про имя Василий, мы теперь должны понять, что что это про имя, что это про конкретное имя Василия какой-то. У нас могут быть люди, связанные с этим именем, еще там что-то. То есть, какие-то взаимосвязи есть. Эта память не изолирована. То есть, работает двусторонняя связь, но она вот такая вот. У человека, на самом деле, Эта связь, она достаточно широкая. Есть проекции всех участков мозга, гиппокрам. Всех участков, не мозга, всех участков. Давайте я вам картинку тогда лучше покажу. Вот в центре вот там вот у нас находится гиппокамп вот так вот И, соответственно, а вот весь остальной основной пара, который занимает большую часть минуты. Кора у нас обрабатывает разные ситуации, разные части обрабатывают разные части, визуальные части, аудио-части еще какие-то. Часть моторная, визуальная тоже на несколько частей делится. Гиппокамп все запоминает, все себе накапливает, все как-то там, несколько слоев еще представлений формирует, потому что гиппокамп тоже делится на несколько частей. А внутренняя часть гиппокампа запоминает запоминает информацию с внешней части типокампы, как-то ее тоже еще упаковывают. После чего это передается на Таламус, с Таламуса на базальные ганглии, с базальных ганглий обратно в Хару. Через базальные гандли это у нас воспоминание и одновременно рабочая память, то есть таламус больше связан с рабочей памятью. И при этом мы продолжаем запоминать то, что сейчас есть. И еще гиппокам запоминает прошлую информацию. То есть, не только последние действия, но последние, не знаю, две минуты он у себя запоминает, ассоциирует и так далее. То есть, он внутри накапливает именно все, что происходит. То, что касается места, времени, значит, объектов и всего прочего. И дело для этого очень большое. И вот нам надо, вот вся вот эта вот структура наша, из генерации действий, то есть моторный выход, он моделируется и, соответственно, обратно у нас выход уже небольшой такой, а вот здесь knowledge, мы можем очень много единиц knowledge создать. 

А. КОЛОНИН [01:35:01]  : Проецируя ваш доклад на эту картинку, получается, что подкорка это у нас ЛЛМ, так? 

Ю. БАБУРОВ [01:35:14]  : ЛЛМ выполняет здесь роль практически всего мозга. А где в мозге лежит нолич? С отрезанными визуальными частями. 

А. КОЛОНИН [01:35:25]  : Хорошо, а где в мозге лежит нолич? 

Ю. БАБУРОВ [01:35:27]  : Нолич – это гиппокампа. То есть, говоря про структуру мозга, мы подразумеваем структуру мозга первой гиппокампой. То есть, всё остальное. Это вот эллилемка, которая реагирует на всё и как-то преобразовывает всё. А вот дополнительная память и то, что Каждый момент из этой памяти извлекается. Это как раз то, на чем мы синхронизовались с памятью. То мы сейчас извлекаем. То, на что модель указала. то есть то, что общая часть мозга указала, то мы извлекаем из гиппока. И у человека есть волны синхронизации, которые как раз по МРТ, по МРН. Очень легко видно, когда человек что-то вспоминает, но не очень понятно, то есть не хватает разрешающей способности понять, что он вспоминает, но видно то, что он вспоминает. 

А. КОЛОНИН [01:36:28]  : Это то, что Танони как раз идентифицирует с сознанием, да? 

Ю. БАБУРОВ [01:36:37]  : Сознание здесь рядом. Если мы помним наши прошлые действия, то мы находимся в сознании. Если мы перестаем помнить наши прошлые действия, то у нас сознание пропадает. 

А. КОЛОНИН [01:36:59]  : Окей. А вот этот knowledge, он все-таки символьный или это векторная база данных какая-то? 

Ю. БАБУРОВ [01:37:08]  : Смотрите, knowledge вполне может быть комбинацией символов для текста и эмбеддингов. Он вполне может быть и какой-то графовый, объектный. То есть идентификаторы объектов это вполне себе тоже имеется знание. Для извлечения соответственно наиболее подходящих запросов объектов как раз используются векторные модели. Как раз вот REL, вот это вот, это и есть извлечение, извлечение нужных по запросу единиц знаний, и оно делается через векторную модель. Для этого строится вектор для каждой единицы знания, И строится вектор запроса. Соответственно, те вектора знаний, которые более близки к вектору запроса, тем те извлекаются. 

А. КОЛОНИН [01:38:07]  : Спасибо. Еще здесь пара вопросов, возможно, не очень актуальных, потому что про это уже говорили. От Бориса Новикова. Что значит структура мозга? Структура ПК постоянно, но ПО меняется? И второй вопрос. Со сменной профессией меняется критерий правильности и ошибки? Что такое смена структуры мозга у человека? 

Ю. БАБУРОВ [01:38:37]  : Смотрите. Большая часть структуры мозга не меняется. у человека, оно немного меняется. Ну, соответственно, и здесь тоже. У модели, большая часть модели, вот она обучилась на всей информации, она уже прекрасно может идентифицировать с высокой разрешающей способности и то, что на картинках написано, и то, что в тексте написано. То есть она может пересказать близко к тексту. Это значит, оно хорошо, достаточно точно идентифицирует. Если оно может очень подробно пересказать картинку, то это значит, оно хорошо представляет то, что на картинке находится. Вот как раз вот эта структура, она вполне может не меняться. Зачем ее менять, если мы и так уже хорошо предсказываем все разные картинки? Если нам нужно добавить какое-то знание, какой-то другой, Картинки мы просто чуть-чуть поменяем. Если это внешние картинки, то вообще мы во внешние хранилища данных knowledge погрузим, даже в модель его добавлять не будем. Если это что-то важное для функционирования модели, то ну ладно, добавим его, дообучим модель. Просто этот подход деления на knowledge и модель позволяет как раз не дообучать модель для адаптации. Теперь, я думаю, это понятно. Час назад я тоже самое рассказывал, но тогда было непонятно. Теперь понятно, в чем отличие этого механизма адаптации. 

А. КОЛОНИН [01:40:29]  : Юрий, спасибо. Может быть вы тогда сможете в двух словах резюмировать, насколько ЖПТ далек от ЭДЖИ, чего не хватает и какие должны быть следующие шаги? 

Ю. БАБУРОВ [01:40:49]  : ГПТ бесконечно далек от AGI, сам по себе, по той причине, что это кусок. То есть так же, как если у человека мы возьмем кусок мозга, вот его в банку посадим, то он не будет адаптироваться к ситуации, он не будет решать задачи. Потому что нужна обвязка этому куску мозга, чтобы он… Ноги нужны в конце концов, руки, что-то, чтобы он решал задачу. Нужны уши, чтобы он слышал, глаза, чтобы видел, рот, чтобы говорил. В этом плане GPT сам по себе он ближе к куску мозга, нежели к полноценному… полноценному объекту, и он так и учится. То есть я как раз и объяснял полчаса назад, что нам было бы крайне неудобно учить все существо целиком, поэтому мы учим от этого существа только мозг или даже какие-то куски мозга. Мы можем отдельно взять визуальный кусок мозга, научить его на картинку. Отдельно взять текстовый кусок мозга, научить его на текст. Это замечательно, это уменьшает требования к мощности времени обучения. Если мы потом сможем это все объединить вместе, оно заработает, а это делают и так, действительно, оно работает потом, то это замечательно. В этом ничего плохого нет. И так и надо делать. Более того, так мы избегаем проблему, когда компьютер мог бы не захотеть учиться, но вот у него эмоция, значит, не хочу учиться и все. И он или там делает какую-нибудь глупость вместо обучения. Также это позволяет избегать того, чтобы нам приходилось бить бедного компьютера током, чтобы заставлять его учиться. Потому что этот алгоритм обучения, оно учится само по себе, почти без РЛ. А РЛ применяется, но немножко все же потом биомтоком, но уже потом, когда доводку делаем, этого алгоритма уже под... То есть, когда вот этот кусок мозга встроили уже в цельный мозг, мы потом делаем доводку, чтобы он прижился. Для этого мы его... Добучаем действовать вместе со всем остальным организмом. У человека вот такое восстановление после травмы пуска мозга занимает вот где-то. И то не всегда, не всегда оно происходит. Но если нам удалось пересадить и полностью вернуть функционирование пуска мозга, ну вот где-то там за полгода. оно бы там восстановилось. То есть тоже не быстро происходит, и нельзя говорить о том, что человек быстро там адаптируется. Вот, ну так и здесь мы берем, переадаптируем. Соответственно, GPT бесконечно далек, но вот теперь у нас есть объединение нескольких ключевых идей, нескольких ключевых алгоритмов. GPT, reinforcement learning, значит, дальше идея вот эта вот с knowledge, использованием knowledge, для запроса. Более того, мы ушли даже дальше. Мы еще делаем запросы, умеем подобные запросы делать по интернету. Я все же могу показать, как это дело происходит. Возьмем, есть такой поисковик find. программистский поисковик, на чарджу пики построенный, который тоже умеет выполнять несколько видов разных действий. Он, соответственно, если мы спросим его, ну не знаю, чем отличаются симпульки от симпульков. то он сделает запрос в Google, ну и там, куда мы попросим, и из интернета выяснит, что такое Sepulki. То есть это так же, как обращение к знаниям, вот к этому knowledge, и фильтрация knowledge. Только эта фильтрация knowledge уже всего интернета. После чего как раз нам нужна вот эта вот различительная способность и способность по пересказу модели. И она, используя свои способности, уже поймет, что имеется в виду в интернете. и превратит это в ту информацию, которую нужна пользователю. То есть там явно были описания, но вряд ли там явно было написано, чем отличаются пульки от сапулярных. Вот теперь оно формулируется таким образом. Основной различие между сапульками и сапульками заключается в их функциях в контексте произведения. То есть это получение новой информации на основе того, что модель обратилась к знанию. И все это происходит без дообучения модели, адаптирования к новому, адаптации к новому контексту. 

А. КОЛОНИН [01:46:57]  : Я извиняюсь, Юрий, что встреваю ваше заключение. Тут все-таки интересная ситуация получается. Интересная, потому что на самом деле у нас знаниями здесь возникает вся символьная информация, доступная этому самому мозгу, или как его назвать в целом, интеллекту, а моделью является, которая заглядывает, позволяет заглянуть в эти знания, является, ну, собственно, Сама ЖПТ, сама нейросетевая модель. То есть, получается, мы используем нейросетевую модель для того, чтобы заглянуть в символную модель. Мы используем нейросетевое представление знаний для того, чтобы заглянуть в символное представление знаний. Но при этом нейросетевое представление знаний основано на сильном представлении знаний. И, таким образом, нам нужно лучше из двух миров разместить в двух местах и сверху все равно нужен какой-то промпт. 

Ю. БАБУРОВ [01:48:08]  : Да, сверху нужен какой-то промпт, но я не вижу в этом какой-либо проблемы. Это всего лишь способ коммуникации, сопряжения модели. Работы с моделью, то есть фокусировки. Потому что модель учится в универсальным образом, учится на многие задачи. Здесь мы ее учим. Здесь мы хотим, чтобы модель вместо того, чтобы ногами куда-то сходить, чтобы она говорила. 

А. КОЛОНИН [01:48:37]  : Хорошо. Тогда осталось два вопроса. Чего не хватает принципиально и какие следующие шаги? 

Ю. БАБУРОВ [01:48:49]  : Чего не хватает? Ну, во-первых, Во-первых, всё по-прежнему плохо с разрешающей способностью. То есть оно способно понимать новую информацию и неплохо это делает, но вот местами оно делает это не так хорошо, как делает человек, несмотря на то, что обычно была на большем количестве данных. Но это вполне может объясняться недостатком мощности в сравнении с человеком. Еще там пару порядков добавить мощности, то есть не 10 видюшек, чтобы для человека использовалось для одного запроса, а тысячу. И тогда, может быть, оно будет на уровне человека все качественно так же представлять, качественно так же пересказывать. Пока что оно пересказывает хуже. Но вот более крупные, более новые модели пересказывают получше. То есть разрешающая способность – это то, над чем стоит работать. И здесь проблема в том, что, в общем-то, все тексты-то из интернета уже извлечены. Непонятно даже, на чем это все учить дальше. 

Ю. БАБУРОВ [01:50:13]  : На пару порядков? Во всяком случае, нет. На полпорядка еще можно несколько раз увеличить количество информации. Или можно учить дольше на той же информации, она будет извлекать оттуда больше знаний и также будет увеличивать свою разрешающую способность. Второе, оно связано с первым. Если мы берем как раз более глубокую сеть, то мы видим, что она немножко делает действие не в попад. Частично она делает действие не в попад из-за того, что там не до конца определенная вот эта вот система, она не понимает, во время не ориентируется, что прошлое, что будущее, что она уже сделала, а что сделано, ну, не очень в хорошей формулировке. То есть цель – это новая цель, там, это текущая цель. или прошлая цель, где-то там истории не хватает, еще чего. Но оно иногда, даже имея все эти данные, все равно отвечает и генерирует информацию не в попад. И это второе, над чем стоит работать, то есть над увеличением самого качества работы. Ну а третье – это скорость всегда остается третьей. То есть при тех же параметрах качество, чтобы оно работало быстрее. И в этом смысле у нас еще очень далекий путь до идеального состояния. То есть простые задачи и куча гуманитарных задач оно решает на крайне неплохом уровне. обзор полгода назад, я показывал обзор, такого качества оно примерно достигает. Но если мы берем более сложные задачи, например, генерацию реальных исправлений в компьютерных программах, или математическое доказательство теоремы, или даже просто математические вычисления проделать, то мы обнаруживаем, что модель еще недостаточно умной, плохо все еще научилась. Скорость, качество, Ну и еще скорости, качества. Это то, над чем стоит работать. Поэтому, ну то есть, действительно, было бы качество повыше еще, было бы просто поп, запрос написали, у нас все, AGI готов. Но пока что вот даже с разными способами, с разными попытками заставить модель что-то делать, все равно она плохо это делает, иногда ошибается. 

А. КОЛОНИН [01:53:20]  : Спасибо. У меня небольшой комментарий. Во-первых, вот Андрей Ковтуненко подал руку. Андрей, пожалуйста. Сейчас будем закругляться.

S00 [01:53:30]  : По поводу математических вычислений. Смотрите, можно подключить какой-нибудь вольфрам и считать это за то, что мы предобучили модель математики, чтобы ее не учить. И в фрамте задавать вопрос. Это вопрос о математике? То есть обобщение модель может сделать. Она скажет, да, это вопрос о математике. Ты можешь ей задать вопрос, вычлени переменные в этом уравнении. И вот эти переменные в виде формулы передать в Альфрам, Альфрам вернет ответ, и вот вам и математика. Возможно, даже логика где-то. 

Ю. БАБУРОВ [01:54:20]  : Да, и вот здесь как раз у нас наблюдается очень интересное совмещение двух подходов. То есть человек умеет выполнять некоторые сложные действия, но они ограничены тем действием, которым могут выполнять руки человека. в языке человека. Более сложные действия человеку выполнить тяжело, а у компьютера есть еще одна возможность. Он может сформировать себе компьютерную программу и потом ее выполнить. И эта возможность может обеспечить компьютеру гораздо более быстрое выполнение куча разных действий. Более того, мы можем пользоваться и уже написанными программами. То есть это примерно, ну это тоже вот этот knowledge, только он в другом виде, только он в виде уже программ. То есть вместо того, чтобы мы шаг за шагом делали конкретные действия, говорили там add new task, значит, Сделай то-то, потом выполни этот таск, потом результаты выполнения таска такой-то, запиши его во внутреннюю память. Вместо этого мы можем отдать на другой вычислитель все эти действия. Другой вычислитель эти действия сделает, а мы просто достанем потом результат. И это может оказаться намного быстрее, нежели привлекать весь мозг к выполнению этих действий. Вот. И этот комбинированный подход, он тоже развивается при ОКНМ в том числе. 

А. КОЛОНИН [01:56:24]  : Спасибо. Спасибо, Юрий. Коллеги, я думаю, нам уже пора заканчивать. Пара комментариев. Во-первых, Юрий, большое спасибо за доклад. Во-первых, за его экспериментальный, воркшопный семинар. То есть, по сути, у нас сегодня была не лекция, как это – лаба. Было очень оригинально, полезно, ну и мне понравилась та архитектура, про которую вы рассказали. Единственное, что когда вы продемонстрировали последние примеры и тем более подключили с вашей подачи, я подключил зум-ассистента, то, во-первых, в какой-то момент, на мой ответ, насколько далеко ЖПТ от AGI. Ваш ответ, ассистент Зума перевел как «Джеки бесконечно далек от Джайли». Этот перевод наводит на меня мысли все-таки еще достаточно далеко. А ту выдачу, которую значит, этот find выдал по поводу сипулек, но это, в общем, достаточно жесткая галлюцинация, потому что никакого отношения сипульки у Станислава Лема к восстановлению жителей этой самой энтропии с копией не имеют, то есть, действительно, жители энтропии, на которой использовались сипульки, они там беспрерывно восстанавливались из копии, это основной сюжет произведения, да. Но сипульки там упоминаются на совсем другом костном тексте, и это действительно объект неизвестной природы, прежде всего. Но что меня больше всего поразило, я вот тот же самый запрос сделал в Google в этом самом файнде сам. И там вообще жесткая жесть пошла. Мне страшно пересказывать, что выдал ПФАИНТ мне, я сбросил это в группу, но в общем ЧАД ЖПТ со своими галлюцинациями нервно курит в сторонке. А, ну ПФАИНТ он же тоже на основе ЧАД ЖПТ, правильно? 

Ю. БАБУРОВ [01:58:35]  : Да, ну нет, там две разных модели. Есть у них своя модель, вот если подписано ПФАИНТ, и вот в начале, когда вы начинаете сессию, 

А. КОЛОНИН [01:58:44]  : Ну, неважно. В общем, так сказать, это галлюцинация уровня бок. 

Ю. БАБУРОВ [01:58:50]  : Можно выбрать или Find Model или GPT-4. 

А. КОЛОНИН [01:58:56]  : А, окей. Возможно, как раз две разных модели. 

Ю. БАБУРОВ [01:59:00]  : Ну и учитываем, что это все же программистский поисковик, он больше... 

А. КОЛОНИН [01:59:05]  : Да, да, да. 

Ю. БАБУРОВ [01:59:06]  : На программистские запросы рассчитан. 

А. КОЛОНИН [01:59:08]  : Честно сказал бы, я не знаю, что Сипульки, а не выдумывал бы неведомо что. 

Ю. БАБУРОВ [01:59:17]  : Это еще одна проблема, про которую я сегодня не говорил. 

А. КОЛОНИН [01:59:21]  : А в остальном все прекрасно. Юрий, спасибо вам большое за доклад. И новость. Через неделю на следующем семинаре состоится круглый стол AGI и субъектность, где будут участвовать Константин Анохин Сергей Терехов, Сергей Шумский и модератором будет Игорь Пивоваров. Юрий, еще раз большое спасибо за доклад, спасибо всем участникам за вопрос и участие в обсуждении. Всем до свидания. До свидания. Спасибо. Счастливо.






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
