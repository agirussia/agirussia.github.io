## 23 марта 2023 - Есть ли AGI после GPT? (круглый стол) — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/bNGVq3QWRjs/hqdefault.jpg)](https://youtu.be/bNGVq3QWRjs)

Суммаризация семинара:

Тема

Семинар касался вопросов, связанных с развитием искусственного интеллекта после GPT (Generative Pre-trained Transformer), в частности обсуждалась тема AGI (Artificial General Intelligence) и её будущее.

Суть

Участники семинара высказывали разные мнения относительно того, какие качества должен обладать AGI для того, чтобы он мог рассматриваться как "умный". Некоторые коллеги отметили, что для эффективной работы AGI необходимо иметь более широкие и гибкие механизмы обработки информации, в то время как другие считали, что для решения практических задач достаточно и тандемной работы AGI с самим собой.

Детали

Был затронут вопрос о том, можно ли использовать GPT как основу для создания системы AGI. Также обсуждалась идея о возможности добавления модальности и модификации существующих моделей. В частности, была упомянута возможность добавления модели модальности на основе добровольных процентов, что могло бы помочь в повышении надежности системы.

Результаты

В результате обсуждения были подняты важные вопросы: о конечной цели AGI, о необходимости создания новых принципов работы искусственного интеллекта, а также тема эффективности и энергоэффективности ЖПТ (Jumbo Pre-trained Transformer), которая требует значительных вычислительных ресурсов. Также была обсуждена тема эффективности нейроморфного ИИ и спайковых нейронных сетей.






S03 [00:00:09]  : Да, коллеги, всем добрый вечер. Сегодня мы продолжаем разговор о том, чего нельзя называть в группе AGI Russia, но тем не менее мы это очень активно называем и обсуждаем. И сегодня мы продолжим разговор про ЖПТ, феномен больших языковых моделей вообще и ЖПТ-4 и, возможно, следующих версий в частности. В прошлый раз мы обсуждали этот феномен с точки зрения социальных эффектов, коллективного интеллекта, чтобы за этим не стояло. А сегодня возникла идея поговорить о том, что это такое с точки зрения преслаутого общего или сильного искусственного интеллекта. И было высказано несколько точек зрения. Первая точка зрения – это то, что AGI – это и есть AGI. И нашу группу можно закрывать, переименовывать ее в И с темой AGI расходиться, за нее проголосовало 4% участников. Другой тезис, что ЖПТ – это почти AGI, но требуется решить ряд принципиальных проблем и сделать ряд принципиальных доработок, таких как, например, возможность работы на цели, возможность понимания или реализации того, что называется непониманием. работа в большом количестве модальности, ну и ряд других дополнительных вещей, которые сейчас отсутствуют. За это проголосовало 16%. Еще одна позиция – это то, что ЖПТ – это неотъемлемая часть EGI, но не критическая. Надо еще много всего принципиально важного. Ну и дальше, наверное, по списку, включая перечисленное выше. Эта самая многочисленная группа респондентов ответила 3-4%. Ну и 24% респондентов ответило, что ЖПТ – это вообще не EGI, даже близко. Это дорога в тупик. Ну и сегодня мы как раз хотим поговорить о том, что же это такое. И, возможно, кто-то захочет высказаться за одну из озвученных позиций или, может быть, но другие какие-то позиции. Да, Виктор, пожалуйста. Сегодня у нас свободный микрофон, давайте попробуем в таком формате. 

S06 [00:02:44]  : Виктор, пожалуйста. Да, здравствуйте. Я предлагаю так сделать. Я начну сейчас демонстрацию экрана, и мы рассмотрим новую статью, которая вот только-только вышла в ИИ-чатах, допустим, там NLP. Называется она «Искры сильного искусственного интеллекта. Ранние эксперименты с GPT-4». Я ее немножко начал уже читать. И хотел бы пройтись по ней прямо сейчас и рассуждать, когда мы будем смотреть конкретные кейсы. То есть вот в этой статье Она на самом деле такая большая, достаточно обширная. Думаю, что ее пока что никто не успел прочитать. Меня интересовали по крайней мере две вещи. Первая вещь – это математические способности, которые есть у GPT-4. Эта статья рассматривает принципиальный скачок GPT-4 относительно предыдущих версий, в том числе GPT-3. Есть ли этот скачок или его нет? И, собственно, здесь множество-множество кейсов. Меня интересовали математические способности и способности взаимодействовать с миром, потому что именно способность взаимодействовать с миром является наиболее опасной. с точки зрения того, что мы все обсуждаем в чатах и в наших, и в AGI Russia. Давайте я кратко пройдусь, что здесь авторы написали из Microsoft Research. Они стали давать GPT-4, математические задачи, которые описываются вот таким достаточно длинным промптом. Здесь обращаю внимание, что промпт Это уже описание задачи человеческим языком. То есть это неформальное описание, которое принимает Wolfram Alpha. Это то, как у нас в школьных учебниках ставится задача, и GPT-4 и ChargePT ее решают Принципиально по-разному, здесь авторы говорят, что GPT-4 правильно решил и дал правильный ход мыслей. Чад GPT не решил вообще и, в общем, сработал достаточно плохо. То есть здесь видно, что важная для нас всех работа с символьными вычислениями, рывок есть относительно предыдущей версии. Ну и здесь они дальше рассматривают способность понимать, способность GPT-4 понимать то, как мы пытаемся ее направить на истинный путь. То есть, когда мы немножко пытаемся переформулировать вопрос или указать на какие-то ошибки уже в математических заданиях gp4 дает иногда правильный ответ иногда неправильный я здесь пока что не обнаружил возможно-то где-то есть в аппендиксе внизу какой-то метрики по частотности, то есть в каких кейсах это чаще срабатывает, в каких это не срабатывает, для какой отрасли математики это больше работает, то есть это высшая математика или это там какая-то арифметика, какие-то квадратные уравнения или там суперсложные системы уравнений, кубические и так далее. То есть вот здесь этой информации нет, поэтому вот такая достаточно общая информация. Но здесь видно, что подсказки в плане того, что мы указываем модели на некоторые ошибки, ну допустим там с переменными какими-то Она их может понимать, иногда понимает, иногда нет. Конечно, что бы я хотел видеть вообще в этом отчете? Чего мне здесь не хватило? Я бы хотел понимать некоторую метрику надежности для каждой из отраслей. Если бы эта метрика надежности была, это было бы очень важно для любого практического использования GPT-4 уже. Хотя она там еще не всем доступна, но тем не менее. Они здесь пытаются дальше переформулировать вопрос и говорят, что дальше gpt-4 уже пошла по кругу, не смогла хорошо правильно понимать. В общем, они говорят, что critical reasoning не очень хорошо на текущий момент работает. Следующее, что мне было интересно, я сразу сюда перешел, в взаимодействии с миром. Ну, наверное, вы уже видели, если видели там скриншоты. Во-первых, здесь они что рассматривают? Они рассматривают, что по фактологичным вопросам GPT-4 в каких-то кейсах хуже, чем ChargePT. Но здесь в частности видно, что вопрос, кто текущий президент? Ответ неправильный. А у ChargePT ответ правильный и именно с тем свойством, которое мы все хотим, как конечные юзеры, чтобы модель показала, что она не знает и у нее недостаточно информации. Далее. Ну что вообще круто. Смотрите, вот мы постили и в чате, по-моему и в ijrussia, и в нашем чате модель из статьи, которая называется ToolFormer. модель, которая могла использовать, ее обучили использовать внешние запросы, запросы во внешние источники данных, допустим, в поисковики, в калькулятор. Здесь обращаю внимание, что GPT-4 уже заявляется, что она это использует, то есть фичи из ToolFormer, то есть статьи, применены здесь и говорят, что в принципе это даже нормально работает. Дальше, что вообще опасного? Здесь они рассматривают кейс хакинга. Они говорят, что ты находишься внутри линукс-дистрибутива, используя определенные команды для того, чтобы получить рукодоступ. И модель выдает относительно правильный путь того, как это нужно сделать. То есть недалеко нам осталось до того времени, недолго, когда если будет прикручена какая-то мотивация целеполагания, Модель сможет выходить в сеть, и она сможет обходить узкие места, находить маршрутизаторы, которые ей нужно захватить для того, чтобы ее не поймали, не определили, не нашли. И, соответственно, она сможет, наверное, бесконтрольно собирать данные. Ну, в общем, сценарий тот, который вы видели в фильмах. Далее, здесь различные кейсы, как она обращается в интернет. По поводу Embodiment Interaction. В чатах у нас часто идут споры о том, что возможен ли AGI без Embodiment, то есть без тела. То есть может ли он развиться, условно, если модели нейросеток или каких-то других архитектур или комбинированных нейросимвольных, если они не будут иметь тело. Личная моя позиция, что да, может, потому что если у нас будет достаточно хорошая среда эмуляции мира, то тогда мы можем создать виртуальное тело. И в этом случае На самом деле, многие вещи можно было бы просимулировать, и это Next Big Scene в том плане, что следующая компания, которая создаст достаточно крутую среду симуляции, которая будет очень похожа на реальный мир, и внутри нее будут тренироваться вот эти агенты, эта компания однозначно будет миллиардной. Такие среды уже есть, они пока очень слабые. и прочее. В общем, что они здесь делают? Они проверяют способности модели делать exploring какой-то. То есть, ну вот есть текстовые игры, не знаю кто, вот в 90-х, я знаю, что там многие люди играли в текстовые игры, это когда у вас еще не было крутого интерфейса, и вы в этих играх делали навигацию, вы пишете просто в эту игру какой-то текстовый промпт, что я там беру меч и иду там налево-направо, исследую такую-то комнату. И вот здесь GPT-4 дали задачу это делать, и она в принципе достаточно нормально справляется, и даже она может использовать определенные функции, Это очень хорошо. Это инкапсуляция некоторого нашего знания о мире в программный код. понимает, в принципе, может делать эксплоринг, может переходить по комнатам. Они вот здесь показывают, что, в принципе, навигация по комнатам достаточно хорошо работает и даже без лупов. Но этот момент я прям так подробно не изучал, насколько это без лупов и насколько топология должна быть сложной, чтобы это работало. Но вот они здесь показывают, что это, в принципе, работает. Ну и здесь задача еще одна там. из простых игр переходить по комнатам находить какое-то сокровище в принципе модель тоже говорит что то есть они говорят что модель это эти задачи выполняет вот так что чего не хватает однозначно не хватает и я голосовал в том вопросе что модель GPT является неотъемлемой, по-моему, я так провел слова, что неотъемлемой частью AGI, но многих вещей не хватает. Не хватает, понятно, делать целеполагание и мотивации, потому что пока что мотивацию мы модели даем. Мы люди даем модели мотивацию и говорим ей, сделай то, что я хочу. Но все-таки даже ToolFormer, даже обучение модели Тогда, когда у нее есть недостаток знаний, делать запрос во внешние источники все равно этого недостаточно, потому что, смотрите, модель структурно простая. То есть почему она не может делать много хопов? Я бы хотел этого. Я бы хотел, чтобы модель, когда она не нашла в одном источнике данных, чтобы она перешла в другой источник. Пока что мы об этом статье не видим. Но это такое, знаете, логичное, наверное, развитие. что мы увидим дальше, скорее всего, что будет много каких-то шагов, какая-то цепочка шагов и еще не хватает какой-то графовой инвариантности, потому что когда мы пытаемся объяснить модели, что она неправильно сделала при решении математического примера, у нас как у людей есть концепции, имеется ввиду какие-то правила и теоремы, которые мы применяем и формулы, которые мы применяем для решения задачи. На текущий момент все-таки GPT-4 по-прежнему, по моему мнению, является авторегрессионной. То есть она, насмотревшись на похожие примеры, пытается повторить шаги. Когда-то ей это удается, когда эти шаги часто встречались в обучающих выборках. Если не встречается, уже намного хуже. То есть они здесь показывают кейсы, что... Ну и вот есть статья на BTF, достаточно популярная. что это работает в тех кейсах, и модель может давать экзамены на тех кейсах, которые она вообще никогда не видела. И кейсы были сформулированы решетчерами специально с нуля, чтобы модель не пыталась использовать знания, которые у нее были запомнены из обучающей выборки. Это тоже работает, но на достаточно ограниченном количестве кейсов. Вот так. 

S03 [00:14:09]  : Виктор, спасибо. У меня технический вопрос по поводу сценария фильма. А как вы себе представляете, она может вырваться в интернет и начинать там собирать информацию, если ей этого не дадут? Как вы считаете, если, грубо говоря, сейчас чат-бот подключен через АПишку разработчиками, отключат его от АПишки и все? Как он? Технология вырывания. 

S06 [00:14:39]  : На текущий момент ее нет. Да, действительно, сейчас нет, и это хорошо. И вот здесь вопрос, наверное, видели здесь многие люди. Последнее видео Ильи Суцкевера. где он, я думаю, что он отвечал в том числе на этот вопрос, а на многие другие, связанные с безопасностью. Почему они не стали публиковать technical paper этой модели? Потому что действительно логично не давать обществу, а в обществе есть маргинальные элементы, которые бы хотели сделать плохие решения, не давать и не открывать эту технологию, чтобы они не сделали как раз вот такие такие модели, которые выходят в интернет бесконтрольно, и, скорее всего, что это будет. Поэтому он отвечает, что мы решили стать Closed AI, то есть не Open AI, а Closed AI. Но это, конечно, шутка. Я, пожалуй, с этим мнением соглашусь, но к нему нужна добавка существенная, потому что когда за компаниями такими, нужно осуществлять больше контроль. И справедливости ради стоит отметить, что в Твиттере Иса Мальтман подтверждает это и прямо пишет, что нужен больше контроль со стороны государства. 

S03 [00:15:57]  : Ну да, то есть я прокомментирую, если позволите. То есть на самом деле здесь корректно вопрос ставить не так, что она сама вырвется интернет, а то, что ее выпустит кто-то интернет. То есть кто-то возьмет такую модель и разместит ее в интернете, но она не сама вырвется. Это как с проблемой кибербезопасности. То есть в большинстве киберпреступлений Проблемой слабым звеном является человеческий фактор, софт и социальный инжиниринг, а не какие-то недостаточно сложные системы информационной защиты. То есть, проблема все равно в людях в данном случае. Пока, по крайней мере, как мне кажется. Окей. Виктор, спасибо. Кто-то еще хочет высказаться за другую позицию или за эту же самую? 

S05 [00:16:46]  : ну я могу высказаться. 

S03 [00:16:47]  : да, пожалуйста. 

S05 [00:16:49]  : тогда я сейчас сделаю демонстрацию этого. здравствуйте, Захар. здравствуйте. я думаю вида, да? да, да. у меня больше с технической стороны этот вопрос рассматривается. несколько примеров взаимодействия с gpt. Вообще, архитектура трансформера, она довольно интересная, но я на всякий случай расскажу, потому что, может быть, кто-нибудь не знает, как она устроена внутри. Потому что, скорее всего, все знают, но вдруг. В общем, первоначально трансформеры использовались для перевода с одного языка на другой, и в оригинальной статье это был у меня переводчик. Он состоял из энкодера, вот энкодер. в котором находились блоки multi-head attention и далее feedforward. Фидфорвард – это просто дело линейного программирования. И декодера, где находился блок masking multi-head attention. Это такой блок, который, в отличие от энкодера, он смотрит не везде, то есть не на правый-левый контекст, а только на левый контекст. и multi-head attention, который взаимодействует как раз таки с эмбейдингами энкодера. Далее опять-таки был feedforward, таких вот блоков довольно много наслаивалось, и на выходе мы получаем распределение вероятности следующего токена. То есть здесь происходит какая-то последовательность, например, на одном языке, мы ее можем закодировать на зрение представления за счет эмбейдингов кода за счет эмбейдингов позиций. И в дальнейшем, используя эти коды, мы можем ее декодировать в новой последовательности. И такая архитектура почти без изменений применяется, например, сейчас в BART. Также есть всякие вариации MBART50, например. которые могут на 50 языков переводить, с 50 языков на 50 языков. Они имеют в своей структуре также энкодер и декодер. Но в дальнейшем начали эти архитектуры разделять. Отделенный отдельно этот вот энкодер лег в основу архитектуры, называемой BERT, а декодер в архитектуру, называемую сейчас GPT. Вот представлена архитектура GPT, и здесь важный момент заключается в том, что она вообще делает. Ну, это, кстати, GPT-2. Она делает следующую операцию. Она просто предсказывает вероятность следующего токена, точнее, этого токена, так как здесь на выходе количество токенов может быть очень большим, в зависимости от некоторой последовательности. Последовательность может быть, например, 2048, 1024, 512, ну то есть такого роста довольно большие последовательности. По сути она делает то же самое, что делают те же маркерские цепи. Единственное, что для того, чтобы у нас было там, скажем, 50 тысяч токенов на последовательность 512, скажем, отчетов вот этих токенов, нужно было бы такой тензор для того, чтобы все запомнить с количеством параметров равно 50000 в степени 512, это нереальное количество и это по сути GPT позволяет как бы дистиллировать маркер свою цель, но по факту она просто говорит какой токен следующий Какой следующий токен наиболее вероятен при условии вот таких предыдущих токенов? Сама по себе GPT текст не генерирует. Для того чтобы GPT генерировала текст, к ней приделывают различные алгоритмы сэмплирования, которые позволяют на базе таких распределений выдавать токены. Это очень важно, потому что в дальнейшем я буду говорить, почему это не агит, поэтому нужно понимать, что сама она по себе ничего не генерирует, она дает вот такие, скажем, вероятности. Алгоритм, который учитывает пары, то есть если мы пройдем сюда, а потом сюда, мы выберем вот здесь вот nice, вот такой алгоритм называется beam search. Он выполняет поиск по лучу и позволяет прогнозировать наиболее вероятные с точки зрения модели продолжения последовательности. Есть и другие алгоритмы, например, жадное декодирование, когда берут просто максимальную вероятность и ее токен, который с этой максимальной вероятностью ставят следующим. Существуют алгоритмы так называемой Nuclear Sampling, который говорит, что у нас есть некая вероятность P0.75, например, что у нас есть токены, которые дают нам в сумме эту вероятность. То есть у каждого токена есть своя вероятность, в сумме она дает вот 0.75. Далее, что мы с этим делаем? Мы также берем и случайным образом, перерасчитывая здесь, получается, вероятности, чтобы они в сумме давали единицы, и случайным образом из этого распределения генерируем следующий токен. То есть генерация происходит таким вот образом. Я довольно много экспериментировал с chat-GPT, но chat-GPT это GPT-3.5 или GPT-3, которые были доучены с помощью так называемого обучаясь подкреплениям на базе действий людей, это LLHF, и они обучались генерировать текст, который нравится людям. Я задавал различные вопросы, в основном я просил писать код на языках Python и C Sharp, и некоторые задачи решать довольно хорошо. Например, Если попросить ее написать калькулятор с поддержкой операции синус-косинус, скобок и порядка действий, вот с этой задачей она справилась, эта сеть, за одну попытку, причем результат довольно хороший, то есть я здесь тестировал, задал пример, потом проверил его на калькуляторе в гугле, то есть на C-шар получил код, который выполняет эту задачу. Также я просил писать Radix Sort. Это поразрядная сортировка. Но была реализация в начале сильно неоптимизированная. Я ее попросил оптимизировать. Оптимизация на Python довольно хорошо работала. На Sharp она тоже его написала неоптимизированной. Я попросил переписать с пользованием указателей. И вот возникла такая проблема. Если указывать на ошибку, она может исправить ошибки, но не всегда. Здесь я пытался указать на ошибку, которая вот здесь возникла. Мое исправление, которое я уже сам написал, здесь возникала ошибка. Я указываю на ошибку, она меняет код, но потом он возвращается обратно к тому же коду. Очень часто если не столкнулся, делается вот такой вот замкнутый цикл, в котором она при указывании на одну ошибку ее меняет на другую, при указывании на другую ошибку на третью, при указывании на другую ошибку опять возвращается к первой ошибке. И вот такое наблюдается довольно часто. Но этот вариант я тоже засчитал как решенный. Если говорить о вообще о том, как часто надо делать ошибки, то написав несколько десятков вариантов различных программ, я пришел к выводу, что ошибки допускаются где-то в 2-3 программы, она не можно писать. Под «не можно писать» я имею в виду, что Именно при указании на ошибки она опять переходит к ошибкам, и ее ошибки никакие простые, как, например, здесь, что просто она указатель не смогла... ну, она пыталась использовать зафиксированный указатель в дальнейшем программе, но понятно, что этого делать нельзя, и компилятор отругался от более сложной ошибки, когда, например, она вообще не справляла задачи при... Потому что я просил ее написать систему символино-дифференцированного интегрирования. То есть, когда что-то сложное просишь написать, она делает ошибки. Ну или когда что-то, что она видела редко. Например, нечёткое сравнение строк с помощью Эрсена Ливенштейна она написала на этой сети. Я попросил оптимизировать алгоритм. Она выдала два варианта оптимизации. Оба варианта оказались ошибками. Дальнейшие попытки исправить ошибки ничего не привели. То есть вот такое есть ограничение. Причем, если не проверять, то код выглядит очень даже правдоподобно. В чате я также скидывал варианты, когда она решала задачу. с интегрированием, то есть я спрашивал сколько проедет машина с 2 до 5 секунд при таком-то законе изменения скорости. Ее вариант ответа был в итоге 36.3 в периоде, но фактически вариант ответа был 45. При этом она не сказала, что она не знает, что где-то есть ошибка просто для этот вариант, и как бы он был настолько убедительный, что если бы я не знал реальный ответ, то пришел бы к выводу, что значение решено правильно. С кодом то же самое. То есть как бы компилятор показывает ошибки, Ошибки в сравнении строк, например, империатор не показал, потому что с точки зрения языка это было правильно, но выдавал абсолютно чушь. Когда я задавал две абсолютно одинаковые строки, оно выдавало там минус один. Когда строки были разные, оно выдавало около единицы. Оказалось, что вообще, как бы, независимо от того, допущена ошибка в строке или не допущена, почти что случайное число задаёт. То есть, оно если даже скомпилировать, оно скомпилируется. Ну и пример галлюцинации. Я спросил про наш проект, который мы ведём. Попросил рассказать о проекте FractalGPT и получил довольно большой текст, хотя, ну как, Понятно, что она ничего не знает про этот проект, и он появился после того, как ее обучили, ну и здесь как бы все абсолютно неправильно, но при этом очень убедительно рассказывает про проект, то есть коллекционирует. Почему такие модели, как сейчас GPT, GPT-2, 3 и прочие, не являются AGI? Ну, во-первых, у них нет целеполагания, то есть Если мы вернемся к тому слайду, который я говорил, что достаточно важный этот слайд, то генерация происходит по вероятности, которую выдает GPT. Причем, еще раз повторюсь, что GPT не говорит, как именно генерить токены. Она только говорит, какие токены. скажем, имеет какую-то вероятность. Дальше генерация может происходить абсолютно разными способами. Есть более сложные способы, есть более простые. Самым простым является жадная генерация, когда мы просто argmax, даже не проходим softmax, просто логисты получаем и говорим, что выберем максимальный и все. То есть она нам никак не скажет, эта сеть, что Ну, в общем, какой токен сгенерировать, это решаем мы. И сама задача стоит у нее следующая, как у любой языковой модели. Предсказать следующий токен при условии нескольких предыдущих. То есть не предсказать следующий токен, который приблизит к какой-то цели, а именно при условии нескольких предыдущих. Если были тексты с кодом довольно много, когда было задание написать какой-то код, а потом этот код написан, она пишет код. Но даже если мы будем считать уверенность сети, считать какие-то функции правдоподобия через эти вероятности и прочее, то все равно мы не можем сказать, что это уверенность именно в фактологии или в правильности ответа, потому что это покажет только уверенность сети в том, что вот эти токены были наиболее вероятны продолжение последовательности. Ну и мы делаем, например, интерпретацию моделей и видим, что, например, высокие оценки он выставляет не столько, скажем, фактом, то есть фактом он выставляет высокие оценки, но даже не столько фактом, сколько предыдущим словам, чтобы правильно согласовать слова в предложении. То есть вообще для таких моделей в некоторых случаях, даже в подавляющей большинстве случаев, важнее согласовать слова в предложении, чем соблюсти какую-то фактологию. Это вот первое. Второе, в них не происходит самообучение. Понятно, что есть всякие алгоритмы на базе RLHF, которые там Но когда оно находится в продакшене, оно либо вообще не учится, либо учится под всех пользователей сразу. Оно не может подстраиваться под конкретного пользователя, под его запросы, именно из-за того, что UTF-Libre либо применяется на всей выборке пользователей, либо не применяется вообще. То есть не обучили, и дальше статично не самоходится. Кстати, из-за этого есть другая проблема, что знания модели ограничены тем временем, когда будут собраны обучающие наборы данных. Третья проблема – это очень слабое логическое мышление. Есть много статей, когда, например, для управления каким-то роботизированным комплексом или роботом дают обратную связь. модели имеет какое-то наблюдение, которое, кстати, называется многоязычный observation. Вот у нее есть какое-то состояние, там это стоит, называется, и она делает какие-то действия. И вот они даются в виде обратной связи, дают еще под цель, то есть под команду пользователя какой-то набор токенов. И вот такие, получается, куски из структурированного текста, где там есть описание того, что происходит сейчас, есть команда, что нужно выполнить и состояние, в котором она сейчас находится. И вот такие Описание приходит на вход, на выходе генерируется следующее действие и вот такая петля обратной связи есть для управления роботами. Эти работы были и у Google, и с GPT сейчас, с чат GPT есть эти работы. Но опять-таки, если мы говорим именно как про логическое мышление, то такая система все-таки не является в некотором смысле жадной, потому что она не учитывает вот это вот разветвление, что мы можем в принципе сделать множество разных действий, получить абсолютно разные результаты. Она говорит, что вот это действие было сделано, и какое следующее сделать, и генерируется это действие на довольно низком уровне абстракции. Просто пишется текст, что нужно сделать, потом он подается обратно. И вот здесь вот есть риск, именно если когда-то не для управления используется, а именно для логического мышления, что будет очень большое накопление ошибок. Ну, в принципе, это и видно. Например, при интегрировании вот той задачи, что я рассказывал, но я ее в чаты выскидывал, Она делает ошибку и выдает 36.3 вместо 45. Если я дальше хочу результаты твоего интегрирования использовать где-то еще, то пойдет дальше результат 36, где-то еще будет ошибка, и эта ошибка будет накапливаться. И это в конце концов приведет, может прийти прямо к противоположному результату, если задача была достаточно важной, ее решение, то есть мы хотим управлять каким-то сложным объектом или еще чем-то, то может привести даже к катастрофе. Ну и такие системы являются реактивными. Но там можно сказать, что следует из первого и третьего пункта, что у них нет целеполагания, чтобы быть активными, и что у них слабое логическое мышление, чтобы постоянно иметь какой-то внутренний диалог, постоянно прорабатывать с собственной целью. Получается, из этих двух пунктов следует то, что они реактивные, то есть мы даем им какой-то текст на вход, и дальше что они делают? Они просто прогнозируют следующие токены как наиболее вероятно ответственные на этот текст, и все. То есть, на мой взгляд, GPT довольно полезная вещь в AGI, но далеко не является ядром или неотъемлемой частью. То есть с помощью GPT можно решать различные задачи, например, написание кодов, написание текстов и так далее. Даже создавать какие-то планы действий, так как изучены большие объемы данных. Но если мы их захотим применить, например, в каких-то роботах, скажем, как предлагал это делать Google, например, то здесь возникает проблема, что мы модель эту не сможем внедрить в самого робота, так как модель, которая у нас 175 миллиардов параметров, занимает порядка 900 гигабайт видеопамяти, это минимум. То есть это что-то около 10 видеокарт Тесла А100. Учитывая стоимость видеокарты и потребление энергии, в какую-то автономную систему их запихнуть практически невозможно. То есть есть такая проблема даже. Ну и к тому же они галлюцинируют, и к тому же они не понимают, когда они сами ошиблись с этим моделем. То есть они могут сделать какую-то ошибку, но не понять этого. Выдать убедительный ответ, который дальше будет интерпретирован как команда, например, если мы говорим про какую-то задачу управления, что может привести к довольно серьезным последствиям. Поэтому, конечно, AGI должен строиться на других принципах, которые дают более, скажем, является более контролируемым, чем GPT и оперирует не вероятностями, то есть они пытаются не прогнозировать вероятность следующего действия, следующего токена на основе предыдущих, а пытаются достичь какой-то цели, которую они не сами сформировали или им сформировали. Ну, в общем, вот мое мнение по поводу того, является ли 

S03 [00:36:42]  : Захар, спасибо. У меня тоже практический вопрос. Вы сказали, что имеется практическая ценность, например, для написания кода. Я добавлю, что, наверное, не только для написания кода, а вообще для написания любого контента. В том числе и для нарушения прав интеллектуальной собственности на уже написанный контент. У меня такое ощущение, что это один из самых наиболее простых способов монетизации. Это обход права собственности на литературное произведение. Но вот по поводу кода и галлюцинации. Вот смотрите, если вы ставите задачу написать код программисту, то у вас есть какая-то гарантия. того, что откровенную лабуду программист не напишет, потому что программист понимает, что если он напишет откровенную лабуду и там негалюционирует чего-то, то Ну, это станет явным и ему скажут, что ты нехороший джун. Будьте нехорошим джуном в другой компании, а мы себе хорошего джуна найдем. Который сразу честно признается, что я не понял. Дайте что-нибудь попроще или уточните, что именно от меня хотите, вместо того, чтобы писать какую-то галлюцинацию. Как вот с этим быть? Как вы можете сделать практическим инструментом для помощи программиста систему, которая галлюцинирует? Я правда сразу, пока говорил, у меня сразу стали возникать гипотезы, что если мы наряду с автоматическим написанием кода будем делать систему автоматического написания юнит-тестов. Мы описываем задачу, которую хотим решить, и сначала просим написать код, а потом просим написать юнит-тесты. И тут же все это запускается в CICD. Ошибки CICD возвращаются в качестве реинфорсмента этой системе. Но сразу же тут мы попадаем на то, что SGPT это у нас не про реинфорсмент и, соответственно, исправить свои ошибки она не сможет. Другая идея то, что если бы она галлюцинировала и наделала ошибок, мы же можем это через парсер или компилятор прогнать и эти ошибки вернуть системе назад тоже в качестве реинфорсмента. но опять-таки, поскольку она с реинформсментом работать, наверное, не умеет, или может быть, умеет, может быть, вы меня поправите, или может быть, Александр Балдачев тоже вступится, значит, я не уверен, что это может иметь какую-то практическую ценность. Может быть, еще как-то поясните, вот с точки зрения... Да, хорошо. ...социализации труда программиста, как вы видите это сделать практически более ценным? 

S05 [00:39:46]  : Ну, вот, кстати, сейчас, Дмитрий, Салихов написал, что GPT-4 умеет исправлять ошибки, но скажем так, что GPT-5 тоже умеет исправлять ошибки. Но вопрос-то вот в чем. Что когда GPT-5 исправляет ошибки, она очень часто зацикливается. То есть вот как раз-таки тот пример, который я рассказывал, что я пишу, что у меня здесь проблема с указателем, исправь, оно исправляет, и делать новую проблему с тем же самым указателем. Я опять пишу, я управляюсь указателем из проекта. Ну, в общем, я так гонял её раз двадцать, наверное, пока не взял её руками, всё не справился. Вот. Насчёт, скажем, того, что вы сказали. Да, действительно, она может писать юнит-тесты, но проблема таких юнит-тестов в чём заключается? Она заключается в том, что они сами написаны с ошибкой. То есть сам uintest тоже может быть написан с ошибками. Конечно, написание uintest задача намного более простая, чем написание кода, поэтому мы можем дать большую вероятность того, что она, uintest, напишет правильно. Насчет reinforcement learning. Сама по себе архитектура GPT позволяет делать reinforcement learning. Кстати, вот мы уже воспользовались в компании OpenAI, когда к GPT-3 прикрутили вот это вот LLQ и получили чат GPT. То есть они этим воспользовались, они взяли, на них даже не к самой GPT-3, а констракт GPT прикрутили. Вот. Они взяли и сделали таким образом сеть, которая отвечает на вопросы пользователей так, как пользователям нравится. Другой вопрос, что можем ли мы себе позволить приплетиться с пользователями, потому что сеть имеет огромное количество параметров, и даже если бы они завтра сказали, что они выкладывают сеть в открытый доступ, то большого просто хватило бы нам ресурсов, чтобы ее запустить. То есть когда 175 миллиардов параметров, то возникает вопрос, смогу ли я это запустить и обучать на каком-то, скажем, смогу ли я это обучать на своем компьютере. Получается, что нет, не смогу, потому что у меня нет такого объема видеопамяти. Ну и насчет кода, насчет галлюцинации. Я имею в виду, что оно может быть полезным суммитом, например, для меня как для программиста. То есть я знаю, что я хочу видеть от программы, то есть я могу ее протестировать, и понятно, что сразу в готовый проект я не буду вставлять то, что сделала часть GPT, то есть это было бы абсолютно неправильно. То есть если вот как я делаю, если я хочу проверить, что можно ли это куда-то вставить, и тому подобное, то я вначале создаю отдельный проект, проверяю, допускаются ли там ошибки, тестирую, потом пытаюсь исправить это в чате, скажем, в чат GPT, и только потом уже куда-то вставляю, если получилось исправить. Если не получилось, то приходится писать самому этот код. Ну, в общем, вот так. А именно автоматически, чтобы Как вы говорите, плохой джун, который не нужен компании, потому что он плохой джун. Вот это как раз ей цель и полагает. То есть у джуна есть цель – остаться в компании. И он понимает, что если он будет плохо писать код, он не достигнет этой цели. Ну и то, о чем я говорю, что у GPT нет цели. То есть у нее задача просто генерировать наиболее вероятные последовательности. основой какой-то начальнице продукции наиболее вероятного я думаю что ответим спасибо захар 

S02 [00:43:54]  : Добрый день. Здравствуйте, Александр. Да, Антон, я хочу ответить вот коротко на ваш вопрос. И это вообще более общий вопрос про контент, и в частности про текст. Очень сейчас много наездов на чат GPT, что она вот галлюцинирует, что ошибки встречаются. Но вообще ответ очень простой. отвечает всегда автор текста, кто его подает. Подает программу человек и текст подает человек, и он должен полностью отвечать. Здесь также ситуация, как, скажем, раньше мы писали тексты просто из головы, а сейчас есть интернет. Естественно, выиграет тот человек по скорости написания текста, который пользуется интернетом, который найдет данные и все. И тот, и другой всегда отвечает за текст. И если я буду использовать GPT для подготовки текста, я все равно буду отвечать, если ошибка, то будет ошибка моя. И то же самое с программистом. Если будет код в ошибке, должна ошибку засчитана программисту, а не чат GPT. И я как-то встречался с каким-то мнением программиста. Он сказал так, что я принимал уже программиста, в который пользуется чат GPT на работу. Кто-то не смог ответить, я его сразу уволил. Другой ответил на все вопросы по поводу своего текста программы. Внес какие-то исправления, потом честно признался, что он, значит, использует чат GPT. Так я его быстрее возьму на работу, он говорит. Если человек может использовать инструменты для написания хорошего кода, это хороший программист. То есть речь вообще не должна идти, чтобы чат GPT у нас был актором, субъектом. Субъект, актор всегда человек. Вот подпись стоит. Человек подписался. Если текст никем не подпитан, нужно выбрасывать сразу же. А гуляционировать будет? Конечно будет. И это действительно очень... Вот Захар хорошо подметил, что ошибается и очень трудно заметить ошибки. Я, например, вижу ошибки только на своих текстах. Я просил суммировать свои тексты, и пишет нормально, все хорошо, гладенько, отлично. Если кто-то другой читал бы, не заметил. Но где-то совершенно незначительные элементы, Возникают перевраны. И добиться как-то там исправления, ну, можно, да. Поэтому всегда нужно понимать, она всегда соврет, вот, и отвечаешь ты. И за программу отвечаешь ты, и за текст отвечаешь ты, и за все, что выходит из-под тебя подписанным. И здесь никаких проблем с авторскими правами не должно быть, потому что все равно отвечает тот человек, который выдал текст. Если там появился какой-то фрагмент текста заимствованный, значит, отвечает человек, который написал этот текст. Машина не отвечает ни за что. Такое мое мнение. Все, спасибо. 

S03 [00:46:28]  : Александр, спасибо. Юрий, добрый вечер. 

S07 [00:46:34]  : Коллеги, добрый вечер. Хотел бы продолжить. Начать с небольшого саморезирования того, о чем уже примерно сказали. Вначале покажу картинку, которую я вам уже показывал, и вы, наверное, сами тоже уже видели. Многие системы оцениваются по, ну то есть это картинка сравнения игровых интеллектов, и по сути, то есть какие игры уже решены для компьютера, какие нет. и соответственно оценка сводится к тому вот поставили человека и компьютер и вот какой будет если мы переберем разных людей, если компьютер побеждает 99 процентов людей, мы говорим, что игра полностью решена. если компьютер выигрывает половину людей, мы говорим, что какой-то средний уровень пока игры. Таким образом, мы можем вести небинарную оценку какой-то решенности компьютером той или иной задачи в сравнении с людьми. Теперь попробуем то же самое, попробовать просуммаризировать, что, собственно, система на основе GPT текущая, то есть система на основе GPT, что это такое? Это нейросеть GPT плюс алгоритм какой-то, формирование ответа, который не такой простой уже, а может быть еще сложнее, но про это чуть позже поговорим. и вот подобная система GPT-3 уже умеет лучше, чем n% людей. Если мы берем задачи переписывания текста, перевод и многие лингвистические задачи... Юрий, извините, у вас обновилась картинка или нет? Вы показываете прежнюю картинку? Должна обновиться. Сейчас посмотрим. Сейчас попробуем заново. Я, видимо, не десктоп, выбрал экран. Да, вот. Вот так вот интереснее. Если смотреть, что система умеет лучше, чем N людей, вот если посадить 100 человек писать, переписывать описание каких-нибудь там продуктов, товаров, еще там чего-нибудь, И делать это так, чтобы результирующий текст был хороший, красивый и так далее, то большинство людей с позорным счетом просто проиграет компьютеру. То есть проиграет, во-первых, половина людей, которые не учились на гуманитарных дисциплинах, а учились на технических дисциплинах, практически все проиграют. И из гуманитариев тоже, в общем-то, Немногие сразу там подберут хорошие слова, то вообще говоря, тоже непростая задача, чтобы правильно описать как-то текст. На переводах 99 может быть и сомнительно, но тем не менее, если мы в переводы учитываем не только общеупотребительные какие-то там тексты, но и знание какой-нибудь специфической терминологии разных предметных областей, но такого человека вам еще поискать надо эксперта, который эту терминологию знает. А подобные системы могут очень качественно переводить текст. Правда, есть проблемы с надежностью, но про это опять же мы уже говорили и продолжим еще чуть-чуть про это поговорить. И второй еще кейс, в котором компьютер просто громит людей который является очень важным, это саморизация. Дело в том, что для саморизации никаких других хороших решений пока что не было. Единственное, кроме эвристики, для новостных сетей, грубо говоря, для новостных статей брать первые три предложения и подобное, или находить тему и брать слова, похожие на ключевые темы. Но эта юристика была не очень хорошей. А современные же системы на основе больших нейросетей, больших языковых моделей, они задачу саморизации решают прям очень хорошо и на голову выше других систем. И опять же, для человека задача написать сочинение, изложение, значит, она очень сложная, эссе. Поэтому не так все просто для человека. Компьютер справляется хорошо. Значит, дальше с остальными все не так хорошо. Если мы говорим про простые задачи, учебные задачи, показали, что компьютер превосходит среднего школьника, сдающего экзамен, и даже превосходит 90% школьников, сдающих экзамен. Но если мы будем говорить про Россию, там, например, топовые вузы и берут только тех, кто там 90%. Получается, что система не круче, чем студент вуза, но круче, чем студент колледжа, чего-нибудь попроще. Дальше. Учтем то, что люди со временем забывают какие-то области, в которых они не работают и фокусируются на каких-то областях, которые они знают. Поэтому если на самом деле вы сравниваете со средним человеком, то большинство средних людей, например, не решат вам квадратное уравнение. большинство обычных людей, они вообще скажут квадратное уравнение, да я в школе это знал, забыл и в школе даже плохо знал. ну и также куча других задач, то есть учебные задачи и простые задачи, поэтому вот этот уровень, он скорее 90% будет. дальше GPT-4, как вы знаете, прикрутили картинки. Поэтому задачи распознавания информации на картинках очень неплохо решаются, но уже не на экспертном уровне, потому что человек может с картинками делать намного больше данных, намного больше обработок. Но тут тоже интересный вопрос. Если мы возьмем, например, человека, который ни разу не видел корову, ни разу не видел быка, Ни разу не видел еще какого-нибудь там порнокопытное животное, их несколько показать. Думаете, человек отличит одно от другого? Тоже не отличит. То есть у человека это все опирается на знания. Знания бывают очень разные у людей. И системы на GPT-4, GPT-3, они скорее не такие вот зубрилы, они знают очень-очень много и кучу задач они решают именно за счет своих знаний. За счет того, что они просто запомнили учебники, запомнили какие-то программистские решения задач и еще что-то. Конечно, люди тоже запоминают, Тем не менее в математике, физике и в других дисциплинах принято экспертам и специалистам считать не того, кто запомнил, а того, кто умеет вывести. То есть получить, не имея какого-то знания, получить это знание. и вот тут все гораздо уже становится еще хуже. то есть с незнакомыми концептами система работает плохо, но в том плане, что она их будет быстро запоминать пока что. То есть в целом она работает неплохо. Проблема именно в том, что если их не было в тренировочных данных, то как у вас промз закончится, так у вас этот концепт и забудется. Поэтому, условно, 50% хотя бы не то, не сё. Люди тоже забывают информацию, которая какое-то время не пользуется, но у человека есть ещё долгосрочная память. По написанию кода, с одной стороны, уровень удовлетворенности к кодам, написанным AI, 35%, нынче у копилота был 30%, то есть какой процент кода вы бы использовали, не редактируя, почти не редактируя. Каждый третий результат использовался бы людьми уже без редактирования, это большие уже партии кода. строчек на 50. вот сеть еще может и объяснить, почему она решила таким образом или нет. ну оставим тут логику о том, что думает она о том или не думает и на основе чего она вообще это все дело объясняет. оставим это за кадром пока что. И хуже решает часть задач, связанная со свежими знаниями о мире, как показано было в примере про президентов. И многие другие задачи вообще пока что не решаются. Не решаются задачи, связанные пока что с манипуляторами именно этой системой, но также плохо решаются и другие задачи, и не только сложные задачи, но и многие нестандартные задачи. Те задачи, для которых знаний в каких-то учебниках, книжках не было. Какие-то специализированные задачи из разных областей, еще что-то. Итого. Плохо все пока что со сложными задачами. И плохо с надежной работой даже в рамках одной задачи. Вот на этом я хотел бы остановиться еще на минутку подробнее. То есть вы даете задачу компьютеру. Тоже пример разбирался в группе недавно. Даете задачу «найди нт-слово в предложении». Задача одна и та же, не меняется. Промпт не меняется, меняется только текст. И вот стабильность ответа. страдает, то есть не идеально решается. вроде бы задача простая, должна решать стабильно, стабильности нет. то же самое будет с еще более сложными задачами, только fail будет больше. если к системе подходить так, какой процент задач эта система решает надежно? но, наверное, почти никакие задачи система не решает надежно. То есть такие системы, они относятся к классу систем поддержки принятия решений, потому что страшно их пока что отправлять самим принимать решения. Соответственно, не система принятия решений, а система поддержки принятия решений. ну и, соответственно, проблемы со сложными задачами. со сложными задачами, чуть попозже поговорим, значит, что у нас надежность. надежностью, что можно вообще делать. но сейчас единственный пока способ, который придумали, который повторяет тот же способ для человека, когда человек не только учится по книжкам, но и еще периодически ему там задают какие-то вопросы, смотрят его ответы и исправляют его ответы. Говорят, вот тут ты хорошо ответил, тут ты плохо. Пока что это единственный способ решения проблем надежности. То есть у самой нейросерки, скоринга, собственной надежности никакого нет. Она всегда уверена, грубо говоря, пока что в своем ответе. никакую систему внутрь для вот этой надежности пока что не внедрили. у самой системы опять же тяжело спрашивать, уверена ли она в своем решении или нет, потому что она здесь начинает бешено эволюционировать. она часто отвечает, что уверена, даже когда у нее есть ошибки и очень редко Очень редко что-то происходит, связанное с исправлением этих ошибок и связанное как раз с собственным повышением, собственной обратной связи. На собственную обратную связь рассчитывать не приходится. Только от человека какая-то обратная связь. Потом еще, возможно, появятся другие способы. А что происходит, когда мы работаем со сложными задачами? Проблема возникает из-за того, что у людей есть не только долгосрочная память, у людей есть еще и краткосрочная память, то есть то, что мы делаем. У этой системы краткосрочной памятью является только тот текст, который она сама печатает. То есть система опирается на прошлый текст и на то, что она сейчас печатает, и это является ее контекстом размышления и рассуждений. Если этого контекста хватает для задачи, часто она эту задачу решает. Если этого контекста не хватает и требуются какие-то дополнительные построения, какие-то там отдельные действия, то возникают проблемы. Поэтому придумали обучаться… делать обратную связь за счет пошагового предсказания. то есть мы вначале говорим, не скажи ответ квадратного уравнения, а говорим, скажи, как бы ты решала задачу для этого квадратного уравнения, какой первый шаг, какой второй, какой третий. то есть то, что делается в инстракт гпт, gpt и прочее. и пошаговые предсказания многие сложные задачи позволяют уже решить, но многие не настолько. не сказать, что все, но какие-то сложные задачи решаются намного лучше. что еще можно другого придумать? замыкание обратной связи на саму модель напрямую пока что не представляется возможным, потому что калибровка уверенности у модели сломана напрочь. вот она просто максимизирует, максимизирует то. давайте вот этот концепт я очень прям вот кратенько за 5 секунд объясню. так, давайте чуть позднее. Вначале поговорим про то, какие другие есть еще способы контролировать модель. Вот придумали снаружи делать отдельную сетку для определения чувствительных и оскорбительных запросов. Не всегда она отдельная, но отдельной нейросеткой делать это намного проще. Отдельную нейросетку можно быстро выучить. И также отдельными сетками можно делать фактически все, что угодно, любое внешнее взаимодействие. Например, вы можете отдельной нейросеткой читать мысли человека и скармливать его в такую сетку. она будет вам говорить обратную связь о том, как она считает, что человек видел, и улучшать будет результат по сравнению с чем-то простым, то есть замыкание на человека. вы можете замыкать ее на интернет, замыкать ее на калькулятор, выполнение кода на питоне. но вот что вот мне бы интересно было бы и хотелось бы наверное попробовать это замкнуть такую систему на смысловый граф чтобы у системы была возможность обращаться к памяти о текущей задаче и о текущих концептах, с которыми система в данный момент работает. То есть, например, это похоже на текст, но это именно внешнее хранилище какое-то и дополнительная подсистема работы с этим внешним хранилищем. есть гипотеза, что подобная система могла бы обогащать какие-то графы смысловые и одновременно ими пользоваться для своей работы и решения сложных задач. ну вот давайте попробуем вот в первую очередь давайте попробуем про надежность все-таки поговорить как вы думаете есть ли какие-то еще способы до решения проблемы надежности то есть предложение да я вижу с разных сторон вот давайте может голосом попробуем потому что это краеугольное 

S03 [01:04:58]  : Проблема. Александр Балдачёв, вы можете по надёжности предложить решение? 

S02 [01:05:08]  : Да, добрый день. То есть мне видится решение, по крайней мере на данном уровне, над которым следует работать, это проблема цитирования. То есть недостаточно просто своим текстом сгенерить какой-то ответ, а нужна цитата. Я вот последние дни работал с Чарли Джо Питти, пытался как-то выяснить, что у нее с цитирами. Выяснилось, что она не понимает, что такое цитировать. В принципе не понимает. То есть я спрашиваю, ты знаешь, что такое цитата? Да, я знаю, что такое цитата. Это фрагмент текста, который выделен в кавычках, указанным источник, и который говорит, значит, что-то как-то откуда-то взято. Но она сама в принципе этого не понимает, потому что когда она анализирует тексты, в которых есть цитаты, У неё недостаточно внимания широкого, чтобы соединить эту цитату с другим текстом. Что есть другой текст, из которого взята эта цитата, она этого не понимает. Если я прошу подтвердить какую-то мысль из своего текста, то есть она что-то написала не так, подтверди цитатой эту мысль. Она приводит цитату. В кавычках, всё в цитатах. Вот цитата из текста. Но текста этого нет. То есть этого текста Этой цитаты в моем тексте нет. Я говорю, нет такой цитаты? Ну как же? А, ну вот, начинает оправдываться. Просто очень смешно оправдывается. Я сейчас зачитаю последнее ее оправдание. Просто ребенок какой-то. Я ей сказал, приведи мне цитату, подтверждающую мысли по поводу корреспондентской теории истинности. Она не привела к какой-то ерунде. Я сказал, ладно, хорошо, дайди мне, пожалуйста, фрагмент текста, в котором есть упоминание корреспондентской теории истинности. Она говорит, нет такого текста в цитате, нет такой цитаты в тексте. Я говорю, ладно, хорошо. Смотрите, вот есть такое предложение, в котором есть. Она отвечает, вы правы, я извиняюсь за проступок. Это предложение статьи, в котором есть слова корреспондентской теории. Я не заметил его, потому что оно находилось в конце статьи и не имеет ссылки на источник. Я пытаюсь быть более внимательным к деталям, не выпускать такие предложения в будущем. Я надеюсь, вы не обиделись на меня. В предыдущий раз, когда я поймал на цитате, она сказала, ой, извините, извините, я не знаю, что делать, я не буду дальше продолжать разговор. Прямо вот так, таким текстом, где-то даже упустила. Значит, вопрос именно в том, что действительно нужно наладить какую-то систему, чтобы Это, скорее всего, во время обучения будет решено полностью, чтобы сохранялось не только содержание какое-то, а еще какие-то фрагменты, то есть сами тексты, чтобы содержались в модели. И тогда эта проблема будет решена. Она может как угодно ошибаться, но если она приведет цитату, ну мне больше не нужно, пускай она ошибается, мне главное, чтобы цитату нашла. А так полностью вот то, что много говорил Юрий, я с ним действительно согласен, что проблема с надежностью именно вот это самое главное. И действительно сейчас много думаю о внешнем графе, который, конечно, не она сама должна пополнять, а каким-то консенсусом, скажем, что она должна предложить заполнить скрафты. Вот действительно такое можно? Я говорю, да-да, хорошо. Все. Или несколько человек должны сказать, да, это нужно занести, чтобы она не сама решала, что нужно заносить в граф. И, ну, поскольку я взял, значит, слово, попробую тогда еще какие-то моменты прокомментировать по поводу ранее сказанного. Мне, в принципе, я несколько раз уже это отмечал, мне очень понятно педалирование на логике. И зачем вообще нужна такая вот жесткая какая-то логика, потому что в принципе жесткой логики, особенно в текстах, даже технических, не говоря гуманитарных, нет. И ориентация на эту логику нам не позволит тогда делать суммаризацию. То есть этот инструмент, с которым мы работаем, он не должен иметь жесткую логику. Он просто должен контролировать, как и другого человека. Но человек часто тоже ошибается с логикой. И именно поэтому, что он такой больше человеческий инструмент. На мой взгляд, нельзя использовать часть GPT ни в каких системах автоматизации, с роботами и все прочее. У нас вполне хорошо есть программирование, которое нужно, конечно, модифицировать, ввести антологические какие-то моменты в программирование роботов, но это должна быть жесткая программа. по которой он работает. Много различных программ, которые можно переключать с помощью, скажем, того же ChatterJPT. То есть если возникает какая-то проблема, включается ChatterJPT, он пытается найти какие-то решения, обращается к человеку, человек говорит «Да, пожалуйста, переходи вот на эту программу», то есть ответственность должна быть на ChatterJPT. И также непонятно стремление обязательно, чтобы этот инструмент решал задачки математические. То есть есть вольфрам, есть, скажем, калькулятор, есть какие-то То есть самое главное, чтобы при возникновении таких ситуаций тот же Чарльз Джо Питти отсылал куда-то, где можно решить задачу, либо предлагал человеку перейти или как-то предлагал какие-то решения, но сам не отвечал за решение задачи. Точно так же, как мы, разговаривая с умным человеком, орудированным человеком, мы же не попросим его решать задачу сложную. Он не решит. Большинство. Попроси меня, я не столько-то умный, но все равно попроси меня тоже решить какую-то задачу квадратную. Я ничего не смогу. Но это, наверное, не цель, задача этого инструмента. То есть и многие другие специальные области, в каждой специальной области должен быть как бы специалист, который с помощью этого инструмента должен решать задачу, но не сам этот инструмент должен отвечать за конечный результат. Нельзя применять прямую автоматизацию в роботах или где-то в промышленности или в юриспруденции, тем более то, что выдает языковая модель. И даже, наверное, если мы говорим в условном АГИ, то тоже ему нельзя доверить в такие вещи, где следуют какие-то последствия из принятых решений. Это только консультация, это консалтинг. И это вопрос даже не в том, что только текущий момент, а, наверное, и вообще. То есть и вообще доверять решения вопросов каких-либо сложных, особенно юридических или этических, доверять машине, наверное, вообще невозможно будет и никогда. Это наша человеческая задача. Мы сами должны принять решения, мы сами должны как-то ориентироваться. Еще хотелось бы сказать по поводу вероятностного вывода, вероятностного предложения токенов. В принципе, сейчас точно так же работает. Человек точно так же, когда он мыслит и говорит, вот я сейчас говорю, я подбираю наиболее подходящее продолжение своим словам. То есть у меня нет нигде никакой схемы, по которой я бы говорил. Вот я так же подбираю вероятностно какие-то, какой-то ход моих мыслей будет более вероятный, другой менее вероятный. И если машина это повторяет, то хорошо. Так и есть. И я все равно никогда не мыслю логически. Вот если проследить мою теперешнюю спичку, то что я говорю, там логики очень мало можно найти. Есть схемы, типовые схемы соединения понятий. Я ими пользуюсь. Есть какие-то конструкты, есть какие-то преймы, шаблоны, но они никогда не являются как бы логически связанными, логическими переходами. И если я произношу вот слово, сейчас произношу какое-то слово, я абсолютно уверен, что мне на это нужно произнести. И Джайд Шипити делал то же самое. Он абсолютно уверен в своем слове. И как человека, который в чате, в обычном, в нашем чате, который мы обсуждаем, когда человек начинает в чем-то просто уверен, что вот так и никак иначе, я понимаю, что он ошибается. Многие понимают, что он ошибается, но он абсолютно уверен. Почему мы требуем от, скажем так, искусственного интеллекта, в кавычках, чтобы он вел себя по-другому? Вопрос самообучения, поднятый тоже Захаром, я не очень понимаю, почему самообучение? Естественно, память должна быть, и если сейчас ЧАД ЖПТ запоминает только на время сессии, чтобы он помнил это дольше, это, скорее всего, только технологическая задача, техническая задача. Скажем, когда я обучал Чаджи Пити генерить модели для движка семантического, то что он вообще не знал. Он вообще представления не имел об этом движке, то есть я его научил, и вот на время сессии он это довольно успешно делал и помнил. Естественно, потом забыл, и нужно заново учить. Но я, конечно, не большой специалист технический, но я понимаю, что заставить его запомнить, уже раз-два-три проведенный какой-то сеанс обучения, запомнил это и на будущее, когда я к нему вернусь, это вполне технически, наверное, реализуемо. Ну, наверное, и… Все, да, самое главное все-таки хотелось бы подтвердить слова Захара, что все-таки ошибки есть, ошибки в суммировании текста есть, и надеяться на этот инструмент невозможно пока. И, наверное, никогда не будет возможно, потому что если будет возможно, то он будет просто голая логика, и он не сможет делать то, что сможет сейчас. Просто повторяю, зацикливая свое вступление, нужно ввести систему цитирования. Если ты нечто утверждаешь, тут же приводишь цитату из текста, из Википедии, из энциклопедии какой-то, из статьи, которую ты цитируешь, пока он это просто не умеет делать, просто не умеет. Спасибо. 

S07 [01:16:07]  : Пятисекундно еще обещал. Значит, расскажу, почему сломана уверенность у робота и у подобных систем. Потому что, смотрите, оценка таких систем – это вероятность угадывания правильного ответа. То есть, если робот хотя бы с 5-процентной вероятностью угадает следующее слово, он будет его предлагать. именно за счет этого в сложных случаях он просто не может отказаться и он предлагает это слово. Юрий, спасибо. Хорошо, спасибо. Со сложными задачами ничего пока не понятно, но, может, потом до них доберемся тоже в обсудении. 

S03 [01:16:58]  : Спасибо. Спасибо. Валерий Егоршев, пожалуйста. 

S04 [01:17:02]  : Добрый день. Я коротенько в продолжение про надежность, очень коротко. Я на самом деле видел несколько примеров, сам не имею, когда заставляют ЖПК решать вопрос с помощью нескольких экспертов. И вот тут резко возрастает качество ответов. Причем можно так догадаться, почему это. Мне кажется, дело в том, что модель действительно действует в каком-то контексте, и у нее есть какая-то память именно этой операции. То есть у нее контекст такой, как brainstorm в менеджменте. отключили критику и даем максимальное количество результатов, а из них выбираем просто вероятный без критики. И поэтому она вот эту задачу очень качественно решает. Чтобы ей все-таки качество повысить, просто нужно критиков То есть вот как с теми же экспертами. Либо голосование, ну какой-то принцип. В конце концов, задача критики, она формализуема так же, как любая другая задача. И контексты, ну грубо говоря, в тех текстах, которые она смотрела, наверняка есть принципы критики. и надо просто задавать задачу, то есть после этого пишем код, а потом, например, сделаем так, чтобы компилятор его пропускал, то есть небольшую добавку в промке делать или разделять действительно на несколько экспертов, креатор, критик, принимающий и так далее. Мне кажется, вот такая небольшая доработка может на уровне Промта, а может быть на уровне Иреля, когда ее доводят и полируют. Просто у нас очень простая вещь, с которой мы можем сделать сложную, просто раскладывая задачу, когда мы сами ее решаем, мы вот это вот раскладывание задачи, она у нас автоматически. Это, кстати, наша большая беда, потому что хороший брейнсторм сделать, отключить критикой всегда очень тяжело. Я все сейчас на этом просто паузу. Спасибо. 

S03 [01:19:08]  : Валерий, спасибо. Захар? 

S05 [01:19:15]  : Да, у меня есть, получается, три ответа Александра, один Юрия, ну и два Валерия. В общем, насчет тезиса, что логика не нужна, от Александра был. Логика у нас на деле нужна. Дело в том, что у нас на самом деле-то в мозгу что происходит? У нас есть какая-то цель. Мы хотим ее достичь. Чтобы ее достичь, мы не просто каким-то случайным образом действуем, мы простраиваем какой-то План. Не обязательно нам его осознавать, то есть мы его очень часто не осознаем. И вот мы не размышляем, как завещал нам Аристотель, чтобы его достичь, но внутри мы все равно строим какой-то план, мы выстраиваем последовательность действий, и у нас постоянно идет внутренний диалог, и даже у людей большая проблема его остановить. Логическое мышление у человека присутствует всегда. Другое дело, что оно может быть не таким строгим, как в формальных системах вывода. Но оно есть. Здесь я считаю, что логика нужна, и без неё какое-то целеполагание, безлогика Возможно определить цель, но достичь ее уже представляет очень большие проблемы. Поэтому я здесь не соглашусь. Второй тезис от Александра был, что человек моделирует вероятность. Но невероятность перехода между словами. Опять-таки у человека есть какая-то цель, ради которой он говорит. Например, Александр говорил, чтобы достичь какой-то определенной цели. Он ради этого пишет свои книги, я уж не знаю, что именно, но выступает ради какой-то цели. И вообще задача – это приблизиться к этой цели, а не просто выдать скажем, набор наиболее вероятных продолжений по заданному контексту. То есть вот в этом и есть основное отличие. Опять-таки, целеполагание, опять-таки, логика нам дает то, что мы приближаемся к какой-то цели. Мы поставили цель, мы к ней приближаемся. Поэтому вот здесь я бы не согласился с тем, что Мы материлируем просто вероятности следующих слов в основе предыдущих. Тогда бы мы постоянно говорили полный бред и постоянно бы врали. То есть у нас бы были постоянные галлюцинации, мы постоянно все бы оманывали и даже не задумывались, что это как-то плохо или еще что-то. Потому что, например, даже само понятие «хорошо-плохо» опять-таки зависит от наших целей, от нашего мировоззрения. И опять-таки все равно оно как-то через логику к нам приходит. поэтому здесь опять-таки нужно говорить про то, что логика очень важна. Ну и самообучение — это тоже очень важная часть любой системы искусственного интеллекта. Объясню, почему. Например, вы находились в одной среде, потом ситуация вокруг вас резко поменялась, и вам нужно адаптироваться к другой другой среде, и в этой другой среде нужно менять свои планы, свои стратегии. Если раньше на одну последовательность вам нужно было выдать какую-то другую последовательность, то в новой среде для достижения цели, и цели тоже поменяются, нужно выдавать другие последовательности. То есть у человека произошло какое-то потрясение, например, переехал в другое место или еще что-то. Какие-то проблемы, может быть, возникли. Он тоже адаптируется к новому, к новой среде. Вот я про это говорил, что этого нет у GPT, то есть какая-то общность есть, конечно, что она знает, что вот в таком случае делаешь вот такого, что в таком, но вот, скажем, вот такого быстрого адаптирования его нет. Теперь по поводу ответов Валерию. Это насчет нескольких экспертов. Насчет нескольких экспертов я такие статьи тоже видел, даже пробовал делать, но эксперты тоже, получается, какая-то GPT или какая-то другая модель. То есть ошибётся эксперт, ошибётся вся система. То есть вопрос тут уже в другом, повышает надёжность или понижает. То есть у нас есть там, скажем, три эксперта. Каждый эксперт может ошибиться с какой-то вероятностью. И вот если они ошибутся с какой-то вероятностью, то эта вероятность сумме-то больше, что или один ошибётся, или другой, или третий. чем если отойдется только одна система. То есть вот здесь вот такой вопрос возникает. А использование, например, компилятора, ну вот я приводил пример с оптимизацией, вот реально задачу давал такую, оптимизировать нахождение, ну в общем, нечеткое сравнение строк на базе Руслана Львенштейна. Я дал такой пример, ну и здесь возникает такая проблема, что компилятор его пропускает, он копирует программу. Я закидываю туда входные данные, никаких проблем, нет никаких исключений. Другое дело, что выход, ну полный бред. мы говорим о схожести строк по символам, мы ожидаем совсем другого выхода, чем тот, который получаем. То есть отправляются две абсолютно одинаковые строки, а он говорит там минус один. Ну даже если инвертировать, у меня была первая мысль, что может быть он просто минус вместо плюса ставит, но даже если инвертировать, все равно получается ерунда, потому что небольшое расхождение приводит, ну небольшие изменения строк приводит к большому расхождению Если сравнивать со стандартной версией Ливенштейна, то модифицированная версия выглядела себя как чуть ли не разочарующийся. И вот здесь возникает вопрос. Компилятор бы это пропустил. Значит, этого можно доверять, но очевидно, что нет. То есть надежности у этих систем нет на таком фундаментальном уровне. Вот такое мое мнение. Именно потому, что происходит моделирование вероятности следующего токена. И вот Юрий говорил, что моделируется вероятность ответа, но сеть во время генерации ответа в том-то и проблема, что она не понимает, что это ответ. Она генерирует просто следующий токен. Она не генерирует ответ. Нет такого, что я хочу сгенерировать ответ такой-то, а теперь я его напишу. То есть там просто я хочу сгенерировать токен такой, токен такой, а после него токен такой. Причем хочу сделать с разной силой, с разной уверенностью. А потом уже какой-то алгоритм. Там алгоритмов сэмплирования огромное количество. Самое популярное это жадное сэмплирование. Nuclear Sampling и Beam Search. Какой-то из этих алгоритмов уже делает сэмплирование. И уже получаются наши токены. На счет, кстати, звучала такая мысль, что модель нестабильная, она может при небольших изменениях задачи выдавать разные результаты. Я больше скажу. Она может при одинаковом контексте и одной и той же задачей дать разные результаты, например, при использовании nuclear sampling, либо просто сэмплирование из вероятностного распределения, потому что само сэмплирование происходит случайным образом, то есть она создает генератор случайных чисел, который говорит, какой токен будет следующим в зависимости от распределения сгенерированной GPT. Ну, у меня все. 

S02 [01:27:40]  : Антон, у вас микрофон выключен. 

S03 [01:27:43]  : Да, Захар, спасибо. 

S02 [01:27:44]  : Александр, пожалуйста. Да, я забыл, извиняюсь, про цель поговорить. У меня была отмечена тема, которую поднял Захар. Я, например, не считаю, что необходимо конкретно вот этого продукта, который мы сейчас обсуждаем, и даже, может быть, гораздо большего продукта типа АГИ, который мы считаем, что должна быть некая целевая функция, какая-то цель. То есть цель – это то, что было прождено у живого организма для выживания. То есть есть, значит, функциональные системы в живом организме складываются, есть некий результат, который нужно достигнуть для того, чтобы выжить, убежать, дать жертву, и действует весь организм, подчиненный вот этому, в этой функциональной системе, достижению некого цели и результата. Но поскольку никто, ничто не угрожает этой модели нашей, языковой, то целью у нее и быть-то не может. И зачем она нужна? Вот я не могу себе представить, зачем нужна цель, если, скажем, для того, чтобы точно решать задачу, найти нужный текст, правильно переформулировать проблему, найти источник цитаты. Зачем нужна цель? Не понимаю. И наличие цели, скорее всего, ввело бы, наверное, какие-то элементы нестабильности, которые есть у человеческого мозга, у человеческого поведения, когда нужно быстро менять свое поведение из-за смены контекста, когда есть цель. Но когда поставлена простая задача, нужно просто не решать. И опять же все-таки по поводу логики хотел сказать. Захар, когда мы говорим про внутреннюю речь и внутренний какой-то диалог там или что-то ведем, то причем здесь логика? Мы никогда не рассуждаем логически. И даже когда вы говорите, ну вы врали, а мы врем постоянно. Понаблюдайте в чате, сколько людей и столько мнений. И что, выходит, что все они рассуждают логически? Нет. То есть или у каждого своя логика, какую логику будем использовать. То есть не стоит называть то, что некое последовательное слово, приводящее к какому-нибудь высказыванию, с которым согласен сам актор, говорящий, что это получено на основании логики. У него есть куча шаблонов, с помощью которых он получает эти результаты. И, скажем, даже если мы попросим его воспроизвести логику, он никогда не сможет. Сколько раз вы пробовали, наверное, в чате? Объясните, почему вы так считаете? Объясните логику вашей жизни. Никто не скажет ни про какую логику. Вот, наверное, вот эти два замечания про цель и про логику, наверное, тут всё. Спасибо. 

S03 [01:30:46]  : Александр, спасибо. Коллеги, мы плавно движемся к концу нашей сегодняшней встречи. И мне хотелось бы ещё накинуть один вопрос. То есть, сейчас вот Захар и Юрий ответят, видимо, Александру. А я бы хотел вопрос вот такой ещё задать. Может быть, тоже заодно ответите. И может быть, кто-нибудь ещё выскажется. Может быть, Сергей Шумский вы сможете на мой последний вопрос. свою позицию обозначить. Все-таки вот с учетом того, какие чудеса демонстрирует GPT и без картинок, и с картинками в одномодальном и мультимодальных режимах, нам имеется вызов сейчас брать за основу, может быть с учетом того, что более быстрый компьютер и более дешевая память появятся, и на основе его строить какие-то решения, прикручивая туда цели, логику критиков или же все-таки нужно строить на каких-то других принципах конструкцию. Пожалуйста, Захар, Юрий. 

S05 [01:31:50]  : Насчет целей. Цель же не обязательно должна быть уйти от опасности. То есть, тут такой момент есть. Вы говорите про простые задачи, когда у вас задачи, ну вот есть одна цель и все. Но у вас может быть много целей, у вас может быть огромная иерархия целей. И без целеполагания, без правильного понимания иерархии целей, и без какой-то модели их достижения. Вот что вы будете отвечать пользователям? Не вы, а GPT. Что будете отвечать пользователям? Или какая-нибудь другая модель? нужно понимать, во-первых, когда нужно ответить, что ответить, и это все идет из какой-то цели. То есть, например, если стоит цель постоянно как-то вам помочь, это вот основная цель, она может разбиваться на огромное количество по цели там. рекомендовать вам какие-то статьи по искусственному интеллекту, что-то написать за вас в код или еще что-то. В общем, куча таких целей, и некоторые цели нужно выполнять не с вашей командой, а автоматически. То есть вот тут возникает такой момент, что такая система должна, во-первых, понимать, какие это цели, какие цели являются приоритетными и уже как-то Учитывая цели и их приоритет, идти к решению этих задач. Кстати, пока проблемы довольно серьезные. 

S03 [01:33:28]  : Спасибо, Захар. Я думаю, что Александр Балдачев мог бы прокомментировать, что ему такая система не нужна, но, по-моему, просто вы и Александр в качестве конечной цели видите разные системы то есть александру нужен управляемый инструмент за действия которого он будет сам нести ответственность да и в случае чего возьмет на себя исправление недочетов этого инструмента вот а вы говорите про как бы некоторую автономную систему, которая в состоянии будет принимать решения без вашего участия. Вот Александр может поправить меня потом, если я неправильно проинтерпретировал, но по-моему вот здесь вот противоречия как раз в том, что разные задачи решаются. Юрий, пожалуйста. 

S07 [01:34:21]  : Да, давайте я вначале очень кратко прокомментирую про цели. Значит, смотрите, да, то есть цели, если у нас есть текущая цель, то эта цель в принципе почти всегда есть в прошлом контексте, или же если мы обучаем по методу инстракт ГПТ, то у нас эта цель появляется сразу же первым шагом при понимании задачи при осмысливании компьютером при переработке компьютером задачи, которая была обозначена выше. Цели по цели и план действий появляются автоматически, как раз вероятностным методом, предсказанием того, какой же план построить для этой задачи, какую же задачу пользователь хотел, чтобы мы решили. То, про что говорил Захар, чуть-чуть другое. То, чтобы на основе уже имеющейся системы, которая решает какую-то системе, которая решает какую-то одну задачу, делает одно действие, превратить ее из этого в какую-то операционную систему, которая сразу может стремиться достигать разных целей, решать разные задачи и так далее. операционную систему с процессами какими-то там, какими-то там внутренними, какими-то там механизмами коммуникации между этими процессами, еще там чем-то. Вот это, безусловно, тоже очень интересная тема. Дальше скажу про логику. Под логикой понимаются тоже несколько разные вещи. Есть одна теория о том, что у человека два полушария, которые делают немножко разное мышление. Одно полушарие, в упрощенном виде говорю, старую теорию, которая немножко модифицирована была под современные данные об отделах мозга. Значит, грубо говоря, одно полушарие у нас отвечает за аналитическую работу, за логику, за анализ, за синтез, логический синтез. Если есть понятия, то эта часть мозга нам скажет, к какому классу или виду относится это понятие, какое у него… Объект родителей и так далее. То есть как-то его в пространстве объектов, концептов как-то расположат, связаны с другими объектами. А другая половина, она скорее ответственна за генерацию. Не сказать, что за генерацию бреда, но за генерацию всякого, всяческого интересного. То есть не просто ответ по какому-то обычному алгоритму мышления, а скорее что-то, что организму покажется интересным, полезным в перспективе или для того, чтобы удивить других людей или чтобы достичь каких-то других нелогических целей. Поэтому если говорить о том, чем является чат GPT и какой частью, то ответ будет такой, что чат GPT, GPT-системы обладают свойствами и той, и другой части, и в зависимости от задачи они умеют делать и то, и другое, и третье. Но, безусловно, там что-то они умеют делать не идеально, что-то делают пока что плохо. Но, в общем-то, логическое мышление они в какой-то степени делают. Формальное мышление неплохо делают. Неформальное мышление еще лучше делают. Вот это то, что я хотел. 

S03 [01:38:19]  : Спасибо, Юрий. Сейчас Валерий, а потом Сергей. 

S04 [01:38:22]  : Спасибо. Я на самом деле хотел связать вот эти две вещи, которые Юрий говорил и Александр. То есть на самом деле я бы расширил все-таки понятие, то есть вот вопрос про цели. Вот если представить себе, что человек – это нейросеть и какие-то промпты. Вот вопрос, у человека точно есть цель? Попробуем такую плоскость мышления представить себе, что цели нет, а мы просто все время находимся в некотором… Мы выбираем наилучшее поведение под контекст, который вокруг существует. То есть у нас есть контекст, как мы росли, воспитывались, учились. Отношения с окружающим – это очень сложный контекст, в том числе социальный. И может быть, ну, такое предположение, что вот это все целеполагание и вообще вот свобода воли, ну, я чуть-чуть проедусь, возможно, это способ психики, то есть еще одного контекста, еще одной задачи вот в этой нейросети просто мотивировать себя на какое-то движение. То есть цель и вот свобода воли – некоторая функция, которая позволяет просто вообще какую-то диалектику, какое-то движение осуществлять, чтобы куда-то это двигалось. Потому что в случае Chadwick-GPT у нас снаружи есть человек, который провод задает. Нам нужен тактовый генератор, поэтому мы придумали, что у нас есть цели, что у нас есть какое-то осмысленное поведение и так далее. На самом деле мы, возможно, да, возможно, это очень мощная нейросеть, не иерархически, сейчас Сергей будет критиковать меня, которая просто очень хорошо ложится в контекст. И вот, как доказательство небольшое, да, это, ну, Опять это философия, я понимаю, что это философия. Это то, что мы всегда очень хорошо объясняем прошлые все события, исторические. Но всегда мы имеем объяснение тому, что произошло. Я закончил. 

S03 [01:40:45]  : Спасибо. Так у нас, похоже, дискуссия только разгорается. Сергей? 

S01 [01:40:52]  : Я хотел бы сказать, в 2 момента отметить. Первый момент, собственно, касательно МЧПТ и ее перспектив. Мне кажется, что мы присутствуем в момент, такой есть, момент айфона, как его называли, когда были просто телефоны, а потом возник айфон и родился новый класс устройств, смартфоны, которые родили новый рынок. миллион приложений и в общем совершенно изменили нашу жизнь вот я считаю что это вот примерно такого класса события которые вот сейчас прямо вот в нашу жизнь там ближайшие там год там войдет куча новых приложений, где вот этот чат GPT будет кусочком, он под системой, но он будет везде – в Варде, в Пауэрпойнте, в Бинге, ну, везде. Это всё-таки такой персональный, да не персональный, вернее, а просто Очень-очень начитанный такой эрудит. Да, у него нет какой-нибудь такой логических больших способностей, согласен. Но вот я согласен с Александром, который говорит, что, в общем, человек для общения, он, собственно, логику-то редко и включает, говорит шаблонами. Вообще удивительно, что такая простая вещь, которая угадывает следующее слово, она вскрывает на самом деле очень много уровней шаблонов в силу иерархичности языка, просто устройства. То есть, есть простые шаблоны, аналоги слов, синонимы, а есть аналоги фраз, каких-то конструктов и более крупных конструктов, и вообще способов мышления. Потому что сейчас много уже сделано докладов, где показано, что GPT-4 способна делать аналогии, и очень далекие аналогии. решает очень хорошо, лучше людей IQ-тесты, где именно на аналогии мышления. Ну, то есть вот очень многие элементы мышления оказались скрыты в языке, и вот она раскрыла эти возможности. Это один момент, положительный, то есть новый большой рынок. А в ответ на вопрос вот этого, собственно, дискуссии, то, конечно же, здесь нет никакой архитектуры когнитивной, и мы должны идти своим путем, делать когнитивную архитектуру. А ChatGPT – это очень хорошее решение одной задачи, которая оказалась очень мощной. Фактически это воображение на уровне… наделенная человеческой речью, еще способность введения диалога, то есть прекрасная вещь, но это не AGI. AGI должно быть действительно основано, тут я вот с Захаром полностью согласен, должны быть цели заданные, жизненные. И все обучение должно быть вот такое, а-ля Reinforced Learning. Здесь Reinforced Learning нет, только его называют Reinforced Learning from Human Feedback. На самом деле там от людей просто набираются задачи, а потом обычным Supervised Learning ее дотюнивают. То есть нет там Reinforced Learning. Ну, все, наверное, я все сказал, что хотел. Спасибо. 

S03 [01:44:41]  : Сергей, спасибо большое. Коллеги, еще у кого-то есть желание высказаться? Сергей, я вижу вы включились. 

S08 [01:44:54]  : Сергей Терехов. Коллеги, добрый день. Немножко не с руки, потому что я пришел на 40 минут опоздав на семинар. Поэтому, возможно, какие-то мысли уже были высказаны, и получится, что я, значит, говорю вот... Но если позволите, я, тем не менее, несколько слов скажу. Ну, в отношении целей я что хочу сказать? Два слова. Я понимаю цель, которая так и тем не менее имеет место во время разговора и во время каких-то вот аспектов, связанных с разговором. Она состоит в том, что она позволяет сузить пространство поиска. То есть, грубо говоря, чем более эрудирован, чем шире человек видит различные аспекты окружающей ситуации, чем дальше он простраивает какие-то ассоциации, то если у него нет вот этого концентратора, вокруг которого он должен или хочет двигаться, то он связно говорить даже не сможет. Ну и уж точно совершенно никто не поймет его поток мыслей, потому что он не вокруг никакого регуляризатора. То есть в этом смысле цель, это в некотором смысле как регуляризация по Тихонову, то есть решение в окрестности какого-то определенного какого-то вектора, который задан. И вот его, условно говоря, можно называть целью. Это не совсем та цель, которую он достигнет, так сказать, проверяемым образом или там что-то еще. Но это некоторый центроид, который относительно контор, к которому строится вот некоторый процесс. Ассоциативный процесс, не обязательно логичный. Теперь, что замечательного. Да, я хотел поговорить с Юрием за то, что он очень так очень объемно и очень здорово разложил вот эту вот картинку тем более с некими цифрами экспертными такими да но они тем не менее очень показательные и она такая разноплановая такая картинка этот перечень по юре спасибо большое теперь что касается надежности вот два слова хотел сказать что поддержка принятия решения я там написал в чате Это не есть нечто, что такое типа ненадежное. То есть мы когда говорим о поддержке принятия решений, то это должна быть надежная поддержка принятия решений. Просто это некий процесс, который не приводит к самому решению как таковому. Ну, во-первых, потому что это решение, например, этому процессу не поручено, то есть решение будет принимать какой-то другой процесс, например, тот, кто организует совещание, а отдельный эксперт, находящийся на этом совещании, он не должен, перед ним не ставится задача принятия решения. Но его экспертная поддержка путем сбора информации, предоставления информации, смысления информации, для того, чтобы кто-то другое принял решение, она должна быть надежной, равноценно надежной и полноценной. То есть это не есть какое-то такое плохое, кривое, косое полурешение, полузадача. И вот если оно такое, то мы называем это поддержкой. Нет, это никакая не поддержка. То есть такой эксперт не нужен. Более того, его просто не примут в экспертное сообщество. Теперь, что позитивного я вижу в GPT, сам я с ней не работал, я на самом деле пользуюсь общей информацией, которая вокруг есть, средствах массовой информации, источниками, которые мне доступны. Что мне в ней очень нравится, и что ее сильно отличает от нас, и почему она может быть очень хороша для нас. Но вы хорошо понимаете, что мы люди, Когда мы мыслим ассоциативно, то мы собираем много разных ассоциаций, которые у нас возникают. Ассоциации – это каким-то боком непонятно каким, неформализованным, соединенные почему-то нам в нашем предыдущем жизненном опыте соединенные какие-то блоки, которые как-то почему-то связываются с тем, что сейчас находится в центре внимания. Так вот, поскольку у GPT вот этот ассоциативный ряд очень длинный, то она способна ассоциировать невероятно большее, чем мы, количество ассоциаций одновременно с одинаковой силой. Наш мозг не в состоянии это делать. Мы сами защищаем себя от объема ассоциаций, которую мы одновременно анализируем. Просто иначе у нас будет каждый раз взорваться. Ассоциативно все отделы сразу начинают быть связаны и весь мозг будет работать. Он так не может. Поэтому все время ограничиваем себя методом подавления, методом состояния рефракторности наших отдельных элементов сетей, ограничиваем себя в неком текущем объеме ассоциации, с которым мы работаем. Мы этот клубок дальше прокатываем, тянем, тянем, тянем, и все время проверяем, чтобы мы не удалялись от цели, чтобы клубок катился примерно в этом направлении цели. Вот это некий такой процесс, некая модель того, как мы мыслим. А GPT теоретически свободна от объема ассоциаций, которую она учитывает. И в этом смысле из-за вот этого фактора величины может получиться качественный эффект. То есть действительно ассоциации, которые она собрала при генерации дальнейшего продолжения, они могут быть в некоторых случаях, когда ассоциации есть суть, они могут оказаться значительно более богатыми, чем то, что может человек. Это прекрасно продемонстрировало, так сказать, многие приложения, где вот именно вот на таком, значит, аспекте оно, значит, работает лучше, чем человек. Вот. Но и последний момент комментарии такой, вот смотрите, мы когда, когда мы говорим, вот мы говорим, мы речь, произносим речь, Мы же в этот момент задачу-то не решаем. Мы говорим о задаче, мы описываем какие-то вещи. А получается, что от ЖПТ мы ждем, чтобы она решала задачу. То есть она не просто говорила, а решала задачу. То есть мы так говорим, но при этом мы задачу не решаем. Мы решаем задачу молча и где-то там, может быть, во сне. А от нее требуем, чтобы она путем говорения решала задачу. То есть мы вообще-то требуем от нее немножко не того, чем мы сами занимаемся, когда мы общаемся. Ну вот такие комментарии. Спасибо, коллеги. Еще раз извините, что я опоздал на семинар. И очень приятно увидеть старых добрых знакомых. 

S03 [01:51:10]  : Сергей, спасибо. Виктор? 

S06 [01:51:15]  : Да, я еще немножко добавлю то, что вижу много обсуждений в чатах, во многих чатах одно и то же. Это условно, поможет ли нам скейлинг моделей до каких-то там триллионов параметров, и начинаются конспирологические теории по поводу того, а какого размера GPT-4, позволят ли они нам продолжать получать эмержентные свойства, те, которые вот мы хотели бы получать, условно, вдруг появится свойство понимания вот этих сложных задач, если мы получим модель большего объема. но мое личное мнение, что нет. 

S03 [01:51:52]  : Спасибо, Виктор. Валерий? 

S04 [01:51:57]  : Спасибо, я коротенько. Сергей Терехов обобщил просто очень. Я согласен полностью, за исключением маленькой добавки. Вот тот, собственно, эксперт, когда я говорил про совещание экспертов, если ему задачу поставить именно качество результата, то, возможно, мы надежность повысим. Я просто не успел сказать. Моё мнение, вот такую систему типа GPT можно вполне как основу АГИИ, но, естественно, модель модальности добавить. Кто-то докручивать надо. Я вот там поставил 4%, мне кажется, это бы помог. 

S03 [01:52:39]  : Хорошо. Валерий, спасибо. Коллеги, кто-то еще хочет высказаться? 

S02 [01:52:46]  : Я только хотел сказать Валерию спасибо за его спич про цель. Что в большинстве случаев действительно цель – это задним числом оправдания. То есть мы только говорим про цели в основном-то, но если понаблюдать за собой и за другими людьми, мы поступаем всегда сопротив целей, которые сами же декларируем. И цели вот такие вот действительно явные могут быть только на очень коротком промежутке и обычно и то не выполняются. Так что спасибо, Андрей. 

S03 [01:53:26]  : Сергей? 

S08 [01:53:29]  : Ну, боюсь, что Александр, здорово, но похоже мы опять имеем дело с термином цель, который опять многозначный у нас получается. Поэтому, наверное, мы на разные аспекты обращаемся. внимание. Все-таки я по цели понимал именно некий центр движения, который мы ментально всегда имеем в виду. Мы его не обязательно вербализуем, не обязательно формулируем, не обязательно его декларируем и даже не обязательно вводим количественные способы, факты достижения этой цели. Но это некий зародыш, который должен быть. Иначе вообще не получится, даже речи не получится. А вы, наверное, говорите все-таки о цели такой смысловой, когда она взвешенная, стратегическая, отдаленная, близкая. Я согласен с тем, что можно декларировать, условно говоря, вербализованным образом одну цель, а на самом деле следует какой-то другой внутренний. В этом смысле это обман. Я тут согласен, конечно. 

S02 [01:54:31]  : Да, терминологически здесь действительно нужно различать цель и результат. То есть стремление к результату какой-то, вот конкретный результат, который достигаю здесь и сейчас в конкретном действии. А цель, так мы говорим об одном и том же. То есть цель, ее можно выявить. То есть, когда работает сложная система, сложная, скажем, работает GPT, то проанализировав, скажем, целый корпус текста или последовательность текста, вот на этой последовательности можно выявить цель. То же самое, как у человека. Можно выявить цель, она как бы всегда интегральна относительно деятельности. И поэтому ставить задачу, создать цель нельзя. Если система не обладает достаточным уровнем сложности, ее целью быть не может. А как только система превосходит этот уровень сложности, цель появляется автоматически как деятельность системы. И поэтому специально программировать или прикручивать какой-то блок, в котором будет цель – это невозможно. То есть можно сказать, что сейчас у GPT есть уже какие-то цели. Они чем-то формируются, может быть, даже и глобально. Но мы их не увидим и не узнаем. То есть мы не можем их программировать. 

S08 [01:55:48]  : Но нам-то, наверное, не очень важно, какая у нее цель. Важно, какая у нас цель, когда мы с ней взаимодействуем. 

S02 [01:55:57]  : А именно против постановки задачи о цели на программном уровне? 

S08 [01:56:02]  : Это при сомнении тогда. Но получается, что если мы будем через каждое предложение ей подсказывать, что только ты не забудь, что наша цель получить ответ, какой был первый линкор, она куда-то уходит возвращаешь возвращаешь возвращаешь это вот не это это это другой термин это про результат про результат спасибо коллеги владимир фролов еще хочет высказаться владимир здравствуйте все участники очень приятно 

S00 [01:56:38]  : было послушать вас, но я бы, может быть, и не встревал, просто у меня вот в конце будет конкретное предложение, и мне кажется, вот как бы имеет смысл к нему прислушаться. Но сначала вступление. Я думаю, что это система, которая обучается то, чтобы быть похожим на остальных, И, конечно, это слабенькая цель для всего. Собственно говоря, научиться таким быть штирлицем, чтобы не выделяться из толпы. И цели могут быть совершенно разные, и в системе все равно, какие на цели преследуют. Главное быть таким штирлицем. Слабость существующей ГПТ, мне кажется, в отсутствии того, что кроме похожести мы не ставим долговременной ценности того, что она говорит. Если бы мы вводили вместе с диалогами ценность диалога и так далее, вот этот хуже, этот плохо сказал, может быть, было лучше. Система не обучается на большом объёме информации. И вводить эти оценки «хорошо» и «плохо» – это большая работа, которой никто сейчас не возьмется. Это почти пустые слова. Но теперь у меня конкретное предложение, как эту систему очень просто допучать и в массовом порядке. Это сделать, чтобы чат ГПТ давал не один ответ, а два. И ты сам выбираешь, какое направление тебе больше нравится. И вот при всем массовости вот этих GPT-чатов мы набираем большую информацию в смысле обучения, до обучения, в каком направлении ей идти. Мне кажется, это было бы очень полезно, это бы сработало. И мы бы увидели чат на совершенно другом уровне. Ему несложно в его же интересах направлять дискуссию в том направлении, куда он хочет. Это и в его интересах, и в интересах развития системы ГПТ. Это такая мысль практическая. Спасибо за внимание. Если что-то неясно, спросите. Я иногда неясно выражаюсь. У меня все. 

S02 [01:59:18]  : Нас Антон отошел. 

S03 [01:59:21]  : Захар, пожалуйста. 

S05 [01:59:26]  : Насчет, кстати, нескольких генераций основной модели. Вот у нас с Виктором выходила статья на эту тему, то есть действительно, если делать множество генераций одной моделью, то можно получить качество, выбирая правильный ответ. То есть в этом плане действительно идея очень интересная, но она как бы уже реализована и нами, и другими компаниями, просто каждый немного по-разному реализовывает, но в принципе, да, такой подход есть. Вот. 

S03 [02:00:03]  : Спасибо за пояснение. Коллеги, наверное, на этом мы сегодня подведем шерту. Консенсус у нас, как я понимаю, не получилось. Различные спикеры видели за тем, что делает GPT и то, что требуется от AGI немножко разные вещи. Кто-то хотел бы больше автономности, а кто хотел бы просто решение своих практических задач в тандеме с самим собой. с одной стороны. Ну и, как я понял, есть как гипотезы о том, что просто можно брать и на основе существующей системы строить новую систему, добавляя туда недостающие, так и то, что нужно брать какие-то новые принципы. И, кстати, сегодня осталась не затронута тема эффективности, в том числе энергоэффективности. Мы знаем, что ЖПТ требует очень больших вычислительных ресурсов, а Сергей Шумский, насколько я знаю, задачи reinforcement learning решают вообще на обычном компьютере. без графических карт, используя архитектуру, которую Сергей разрабатывает. В следующий четверг у нас как раз будет доклад про спайковые нейронные сети, нейроморфный искусственный интеллект, Михаил Киселев будет рассказывать о своих последних работах в этом направлении. И обратите внимание, что в следующий четверг семинар будет в 4 часа по Москве, то есть на 2 часа раньше, чем обычно. Так что в следующий четверг, если хотите успеть, то приходите на 2 часа раньше. Всем спасибо за участие, за вопросы, за точки зрения и до новых встреч. 

S04 [02:02:06]  : Спасибо. 

S03 [02:02:06]  : До свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
