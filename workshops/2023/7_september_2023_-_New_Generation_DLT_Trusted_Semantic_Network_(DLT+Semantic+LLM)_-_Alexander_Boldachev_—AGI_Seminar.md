## 7 сентября 2023 - Новое поколение DLT: Trusted Semantic Network (DLT+Semantic+LLM) - Александр Болдачев —Семинар AGI
[![Watch the video](https://img.youtube.com/vi/FQQ7umzDXhI/hqdefault.jpg)](https://youtu.be/FQQ7umzDXhI)




А. КОЛОНИН [00:00:01]  : Коллеги, всем добрый вечер. Сегодня у нас в гостях снова Александр Болдачев, и он нам расскажет про новое поколение децентрализованных платформ на основе распределенного реестра, семантики и больших языковых моделей. Александр, пожалуйста. 

А. БОЛДАЧЕВ [00:00:19]  : Спасибо, добрый день. Сегодня я представлюсь немножко пошире, чем обычно представляюсь, потому что с 2016 по 2020 год я работал системным архитектором блокчейн-платформы Apla. И сейчас являюсь автором концепта и системным архитектором платформы Trusted Semantic Network. Это условное название, которое фигурирует последние лет пять, но оно будет изменено. Сейчас оно как бы нерабочее такое название, но здесь я представляю систему именно так. Работая в АПЛЕ, я занимался полной архитектурой системы, платформы и в большей степени приложениями. Мы делали приложения Proof of Concept, пилоты. MVP для Эмиратов, Индии, Люксембурга, Газпром, нефти. Я разрабатывал сами приложения и управлял производством. До десятка программистов работал над этими приложениями. Хотел еще пару слов сказать по поводу предыдущего моего семинара про движок, семантический движок, на котором работает вот раз и Semantic Network. Там появился один комментарий в YouTube, что, мол, все свалено в одну кучу, и человек еще по-русски неплохо говорит. Значит, здесь вообще как бы засмущался, поскольку уж здесь-то все действительно свалено в одну кучу. Вот сейчас будет. То есть, как мы видим, три разных технологии. Ну, а по поводу языка, значит, в конце презентации будет страничка с публикациями. Я, конечно, больше писатель, чем рассказчик. Поэтому если кого заинтересуют какие-то концепты, какие-то идеи, какие-то решения, то прошу читать статьи. То есть есть и публикации в журналах, много статей на Хабре, много статей специфических по блокчейну было написано в специализированных журналах, изданиях. То есть можно все это найти. План у нас сегодня такой, мы быстренько пробегаемся по трем технологиям, что они из себя представляют на данный момент, что они нам дают. Потом пытаемся их скрестить, и в конце расскажу более подробно об использовании языковых моделей, поскольку профильный у нас семинар, и на каких-то более-менее таких понятных примерах, юзкейсах. Trusted Semantic Network, та платформа, та система, которой сейчас я занимаюсь разработкой, она формально называется так – публичная, криптозащищенная, дестерилизованная сеть на базе темпорального семантического графа с исполняемыми моделями бизнес-процессов и адаптивным интеллектуальным интерфейсом. Это интерфейс управления деятельностью, объединяющий преимущества трех современных технологий – семантик, DLT и большие языковые модели. Здесь сразу следует сделать такой Такое пояснение, что я буду говорить о DLT-технологии, а не о блокчейне, поскольку блокчейн – это лишь одно из направлений, узкое направление, но наиболее популярное направление DLT-систем, и мы поговорим именно о специфике DLT-систем, а не конкретно о блокчейне, где нужно будет упоминать блокчейн и развлечения я буду говорить. Прежде всего, если коротко характеризовать, что такое Trusted Semantic Network, что за новое поколение. Эта технология, эта архитектура поддерживает все основные функции, все основные достижения DLT-платформ, то есть блокчейнов. Это криптографическая идентификация пользователей, Криптографическая защищенность данных от фальсификации, хранение темпорального и исторически полного массива данных, консенсусная валидация и синхронизация данных, исполнение бизнес-процессов, так называемых смарт-контрактов. Это стандартные функции всех DLT-систем, независимо, являются они блокчейновскими или неблокчейновскими, созданными на хранилищах типа направленного циклического графа. И тут же сначала, потом расшифрую, укажу уникальные функции, которые достигнуты, которые реализуются благодаря использованию дополнительных технологий, в частности использование семантики. Это прежде всего естественное уштабирование сети по семантически выделенным кластерам, предметным областям. То есть все мы понимаем, что в современных DLT-системах, в современных блокчейнах транзакции не специфические относительно содержания, смысла, семантики. Каждый смарт-контракт записывает, посылает транзакции в произвольном формате. И это не позволяет каким-то образом классифицировать, разделять, кластеризировать сеть. Если мы имеем семантическую окраску транзакций, то мы можем разделить сеть естественным образом по предметам и областям. Понятно, что образовательный кластер никак не будет пересекаться с финансовым, гибридчастичным. или, скажем, с кластером недвижимости или земельные реестры. По каждому кластеру, естественно, будет выстраиваться некоторое сообщество узлов, моделей, смарт-контактов, так называемых, и обмен данными будет происходить внутри кластера. Второе, что позволяет нам сделать семантика, именно семантическая окраска транзакций – это управление консенсусом. Сейчас на данный момент в блокчейне консенсус идет по блоку. Или по транзакции, совершенно произвольной транзакции и по группе транзакций. Независимо от того, от кого они пришли, какая тематика данной транзакции. Это может быть совершенно мусорная транзакция от какого-нибудь контракта, который считает котиков каких-то. И может быть транзакция, переводящая токены на миллион долларов. И по всем этим транзакциям одновременно проходит консенсус, по блоку. Что попало в блок, потому и проводится консенсус. Когда мы обладаем семантической окраской транзакций, то мы можем выделить, скажем, отделить мусорные транзакции, служебные транзакции, какие-то комментарии, содержащиеся в транзакциях, от значимых транзакций. и проводить по этим разным классам транзакций различный консенсус. От отсутствия консенсуса по каким-то действиям, которые не требуют сильной валидации, обмен сообщениями, можно бродкастом по кластеру заслать эти транзакции, и до самых серьезных алгоритмов консенсуса для финансовых транзакций. Что у нас позволяет еще делать семантика? Это то, что я демонстрировал на прошлом семинаре. Моделирование бизнес-процессов в семантических моделях и исполняемых моделях. И при этом, что мы выиграем? Наши контракты, наши модели, это же и документы. Практически человекочитаемы. и могут составляться не программистами. могут писаться аналитиками, включая LLM. Именно семантика, различённость, определённость транзакций и семантическая адекватность, однозначность хранимых данных позволяет на хранилище такой сети натравить большие языковые модели. То есть для них, если они обучены словарям, если они обучены стандартам, если они обучены моделям, то для них все хранилище данных, в этот раз от Semantic Network, является читаемым. Да, я еще раз повторю то, что я не буду говорить о блокчейне, буду говорить о Distributed Client Technology как обобщенном понятии, под которое подходят и блокчейны, и другие системы. которые исполняют функции блокчейна. И вот сейчас я коротко хочу пояснить, что же такое все-таки блокчейн и что же такое DLT-технология, почему она нужна и важна, и важна и для семантики, и важна для искусственного интеллекта. Будем тоже употреблять этот термин. Прежде всего, DLT – это технология управления доверием. То есть если мы имеем стандартные обычные айтишные системы, обычные приложения, то доверие к ним определяется внешними условиями. То есть никакое программирование внутри, никакие методы программирования не определяют доверие к данным, которые фигурируют в этих системах, которыми обмениваются элементы между собой этих систем. Если нужно повысить доверие к какому-то программному продукту, можно сервера закрыть на сейковую дверь, либо выдать какие-то лицензии от государства, то есть использовать различные firewalls, различные способы защиты данных, сокрытия данных от нежелательного вмешательства. Технология DLT, блокчейн, она обеспечивает управление доверием технологически. То есть, по сути, у нас имеется биткоин, некая система, айтишная система, и функционирование ее защищено только алгоритмами и протоколами, функционирующими внутри этой системы. Никаких внешних подпорок, никаких внешних защитных средств не требуется, кроме одного, для работы потом я скажу попозже. Ну и достигается чем? управление доверием в DLT. Это прежде всего хранение данных на узлах одноранговой сети, то есть множественное дублирование контента данных, криптографическая защита с помощью подписей, хеширования и синхронное преобразование данных на узлах сети специальными программами модулями, смарт-контрактами. То есть здесь следует подчеркнуть, что DLT-системы, блокчейн – это не база данных, что фигурирует довольно часто в литературе определение, что это некое хранилище данных, база данных, в которую мы можем поместить свои данные, и они будут там сохранены. То есть такого эффекта можно добиться, просто, скажем, подписав некий документ электронной подписью и разослав несколько облачных хранений, мы добьемся полного функционала с защитой от потери и доказательства авторства. Так вот, DLT-системы помимо хранения обеспечивают преобразование данных внутри системы с сохранением валидности данных, чего не может обеспечить никакая другая система, кроме DLT. И даже в криптовалютных блокчейнах, в криптовалютных сетях, в биткоине. Есть некий смарт-контракт, они называются скрипты, которые переводят данные из одного аккаунта на другой аккаунт, то есть изменяют состояние системы с сохранением валидности. То есть мы доверяем блокчейну, доверяем всем данным блокчейном, независимо от того, как они там преобразуются внутри, но только внутри. И вот я возвращаюсь еще к одному фактору, который часто не обращают внимания, который обеспечивает способность DLT-системы – это руль сообщества. Допустим, если я возьму фортну Биткоин. И запущу сеть биткоина у себя в доме среди своих соседей, десятки соседей. Мы скачаем блокчейн биткоина, запустим его и начнем обмениваться транзакциями. Будет это биткоин – не будет это биткоин. И вот это тоже существенное отличие DLT-систем от традиционных. Любая традиционная база данных, любое приложение, если оно будет скопировано вместе с данными с одного компьютера на другой компьютер, то будет то же самое приложение в том же состоянии, в каком мы его взяли во время копирования. Копирование, размножение, fork, DLT-систем не создает ту же самую систему, потому что для того, чтобы она оставалась той же самой, необходимо еще некое сообщество, которое поддерживает работу этой системы, которое вовлечено в работу, участвует в консенсусе и каким-то образом стимулируется участие пользователей в реализации алгоритмов консенсуса и управления системой. DLT нужно характеризовать тремя характеристиками. Это хранение данных с максимальной защитой и потерей от фальсификации, реализация передачи данных с сохранением их достоверности внутри системы и поддержка сообществом. С семантическим технологиями, с семантическим ВИЭВом здесь все проще будет, потому что в системе АГИИ, то есть в сообществе АГИИ среди участников, инженеров искусственного интеллекта, наверное, семантика более-менее знакома. И она представляет из себя набор инструментов и методов, спецификаций для описания данных, чтобы они были понятны для различных приложений. для обеспечения обмена данных между приложениями. То есть стандартные, если это семантик ВЕП, стандартные элементы и методы это RDEF, ORQL и наличие словарей, которые позволяют именно обмениваться данных сохранением их семантики с пониманием семантики между различными приложениями. Схема ORC, дублинское ядро и другие словари специфические. И что мы добиваемся с применением схематической технологии? Это, прежде всего, унификация обмена данными. То есть мы можем обеспечить обмен данными между приложениями, созданными независимыми производителями, независимыми продавистами. То есть если программисты пользуются одной семантикой и одними словарями, они запросто могут обмениваться данными. Я уже упоминал, что в современных блокчейн-системах независимые смарт-контракты никак не могут обмениваться данными, если они специально не напишут API или не обменяются протоколами и значениями своих переменных. Интегрирование данных из нескольких источников – это понятно. Свободное изменение структуры данных – это существенное преимущество относительно других систем хранения, скажем, баз данных SQL, графовых баз данных и хранилищах семантических структур данных. По сути отсутствует как таковая, изменяется буквально внесением новых записей, может измениться структура данных. Значит, обеспечивает сложный тематический поиск. И логический вывод неявных данных я поместил на последнее место, поскольку я считаю, что это не столь существенно выяснить, что Сократ смертен, потому что мы и так это знаем. То есть я не встречал таких задач, где нужно выяснять, что Лебедушка зеленая, а Сократ смертен. Особенно если это касается бизнес-логики, каких-то бизнес-приложений и каких-то юридических и прочих-прочих приложений. Ну и самое короткое описание – это будет больших языковых моделей, поскольку это у нас на слуху, что нам нужно, что мы можем взять из больших языковых моделей для использования на нашей платформе. Прежде всего, анализ генерации текста на человеческом языке. Генерацию программного кода по текстовому описанию, исполнение кода и сценариев, описанных инструкциями, адаптация, персонализация представления данных. то есть под конкретного пользователя, который пользуется системой, и дообучаемость на новых данных. По поводу дообучаемости, конечно, еще много вопросов, но мы представляем, что следующий шаг развития больших языковых моделей или вообще искусственного интеллекта – это будет непосредственно дообучаемость данных на загружаемых пользователях. Теперь будем пытаться скрестить эти технологии чисто теоретически, обсуждая, какие у нас есть проблемы в каждой из этих технологий и как другие две технологии помогают нам решить эти проблемы. Что выигрывает DLT от симпеза с семантикой? Я уже несколько раз упоминал, но это прежде всего модификация отношений пользователей приложений. То есть если мы вводим некую семантику в сеть, в DLT-сеть, в блокчейновскую сеть, то мы можем наладить бесшовный обмен данных между приложениями, посылка данных в сеть и пользование этими данными семантически окрашенными всеми, кто обладает словарями, Так же мы можем строить действительно децентрализованные бизнес-логики, в отличие от современных децентрализованных приложений в современных блокчейнах, которые представляют обычные децентрализованные приложения, но только запускаем их на разных узлах. Но если мы используем семантику, то можем часть приложения запустить в одном кластере, часть приложения запустить в другом кластере. И если мы наладим взаимоотношения между этим, обмен данными между этим приложением, это будет действительно дентализованное приложение, не принадлежащее одному какой-то организации или одному программисту, который пишет. И то, что я упоминал, семантик нам позволяет писать контракт на человекочитаемом языке. То есть эта проблема, предоставлять смарт-контракт в суд как доказательство, написанное там на солидите, она, понятно, существует. Она решаема, но существует. Контракты, написанные на человекочитаемом языке, который может быть представлен сразу в виде текста и, в принципе, с легко анализируемой логикой. И есть, значит, другая часть. семантического значимости. Это семантическая определенность данных, семантическая определенность данных, которая позволяет, как я уже рассказывал, производить кластеризацию сети, то есть разделение сеть. на практически не взаимодействующие кластеры. То есть можно себе представить, что кто-то может создать вообще независимый кластер и не взаимодействовать с сетью, создав свои модели, свои какие-то словари. И это будет как бы приватная часть сети, DLT-сети. Но в любой момент она может присоединиться к общей сети, если начнется взаимодействовать с другими узлами. Это решается проблемой масштабирования. То есть на данный момент масштабирование в сетях решается либо созданием садчейнов, либо пастеризацией. физической кластеризации сети просто ножом разрезается и все, то семантика позволяет кластеризовать сеть осмысленно по деятельностям. То есть если есть некая деятельность, в которой участвует энное количество узлов, которая работает по энному количеству моделей в взаимодействии друг с другом, то эта деятельность может образовываться в кластер, который будет считаться практически независимым, И здесь мы решаем другую же проблему – это оптимизация хранилищ. Что у нас получается? Если некое количество узлов, образовав пластырь, обмениваются по моделям данными, то они могут хранить только эти данные. Нет необходимости каждому полному узлу хранить все данные сети. Каждый узел, работающий по 1 количеству моделей, по 2-3 моделям, хранит только данные, сгенерированные по этим моделям. Он не знает о существовании других моделей и не хранит данные других кластеров и даже этого кластера, созданных по другим моделям этого кластера. Также семантическая окраска данных позволяет нам делить данные на уровни. Я уже упоминал про консенсус, правление консенсусом, здесь он третьим пунктом идет. А еще деление данных по уровню позволяет нам архивировать данные. Ну, скажем, у нас есть некий контракт, есть куча событий подготовительных для заключения контракта, соглашения каких-то условий, какие-то переписки по этому контракту, комментарии, какие-то действия, подтверждающие начальные, и в конечном счете есть транзакция, которая завершает этот контракт. под подписями контрагентов и переводом какой-то суммы. И здесь мы явно видим, что данные разделены на два уровня. Уровень данных значен юридический, который требует сохранения, на который будут ссылаться другие контракты, другие модели, которые будут участвовать в деятельности, и служебные данные, которые не нужны. Так вот, технология позволяет архивировать эти данные, изымать их из графа, первый раз произношу это слово, почему-то не произнес, что хранилище семантическое, естественно, графовое хранилище направленное на циклический граф, изымать целую ветку графа со служебными данными, замыкая итоговое результирующее событие на предшествующие события заключению этого графа. И эту ветку можно сдать в архивные и обращаться к ней по адресации, по идентификатору так, как будто она хранится у тебя, но это будет только дольше. Твое хранилище освобождается от служебных данных. Про управление консенсусом я уже говорил, что в этой модели которое поддерживает заключение некоторого контракта. Есть события, которым нужен очень легкий консенсус. Скажем, соглашение о каком-то пункте. Мы к нему все приходим по консенсусу. Есть события, которым вообще не требуется консенсус, обмен. комментариями. Каждый просто посылает комментарий по этой модели и бродкастом комментарий приходит на каждый узел, участвующий в этом соглашении. И все эти комментарии вообще не нужны. И консенсус максимально BFT полный может быть по конечному результату. Вот старая картинка, демонстрирующая, что у нас есть некий единый граф, он разбит на ветки тематические – Sport, Education, Property. Есть модели. модели, по которым строится этот граф. Никакая транзакция не может попасть в граф вне модели. Обязательно указано, по какой модели, по какому действию или по какому свойствам сущности какой-то заполняется граф. И показано три кластера. Между кластерами показано, что узлы могут участвовать одновременно в нескольких кластерах. Совершенно без разницы. Технологически никак не усложняет сеть. И показано, что один кластер создан на серверных узлах. Один кластер, первый создан на мобильных устройствах и требует маленького консенсуса, или вообще без консенсуса может работать. И сильный консенсус, скажем, в кластере property, он максимальный для заключения каких-то сделок. И теперь, что выигрывает у нас семантика от DLT? Здесь следует прежде всего сказать, почему вообще семантический веб, на который возлагали столько надежд в начале века, где-то Тимбер Бернс Ли, Нам прогнозировал, что ВЕП 3.0 уже к 2010 году будет, но все это заглохло, где-то к 2012-2014 году вообще забыли, что были только семантические ВЕПы и что какая-то была РДФ-разметка страниц. И основная причина была, ну, прежде всего, это ориентация, как идентификатор этой страницы сети, то есть использование URI, домены пропадают, адресация меняется. То есть это вообще гиблое дело, абсолютно как можно было даже подумать о том, что вот на таком ненадежном идентификаторе можно было хоть что-то собрать. Потом ориентация на контент, на данные на веб-страницах, которые делаются людьми, и веб-мастер напишет все, что угодно на этой странице, представит как угодно контент. И плюс еще этот контент будет десятки, а то и тысячи раз дублирован на разных сайтах. абсолютно отсутствие привязки контента к автору. Невозможность вообще определить, кто был автор, то есть автор мог выдать на себя владельца сайта. Ну и абсолютно ненадежность данных даже тех же словарей. То есть каким-то образом можно будет доверять каким-то консорциумом или там специальным организациям, которые выпускали словари, но все равно это обычный РДФ. Дампы или JSON-файлы, которые можно фальсифицировать очень быстро. DLT нам как раз и предлагает. Это, прежде всего, уникальная дисфракция элементов контента по хэшам, а не по каким-то странным райдам. Устранение дублирования контента. Если какой-то текст по хэшу появился в сети, то можно всегда вычислить его дубликаты. Транзакция, посылаемая в DLT-сеть, подписывается актером. который посылает некий контент, то есть это может быть либо просто человек, который отвечает за этот контент, либо автор. Но автор обязательно, если автор посылает контент в сеть, он подписывает своим ключом и указывает, что автор является именно владельцем этого ключа, то мы однозначно фиксируем авторство. Естественно, хранение словарей и данных в детализованном хранилище направленно-циклическом графе исключает их фальсификацию. По сути, именно отсутствие DLT-технологии, отсутствие блокчейн-технологии, криптографии не позволило семантическому вебу хоть как-то Хоть вообще приподнять голову выше плинтуса. Что нужно делать? Искусственный интеллект. Семантика. Чем семантикой делать? Чем выигрывает семантика DLT? И зачем нам семантика DLT для искусственного интеллекта, для больших языковых моделей? Семантикой более-менее понятно. Какие бы хорошие языковые модели ни были, но использовать в бизнесе возможно только однозначно зафиксированные данные. Использовать в бизнесе, в любой деятельности, в политической, экономической, медицинской, образовательной, любой деятельности мы должны ориентироваться не на данные, которые были продуцированы, изгенерированы, языковую модель, а на данные искусственным интеллектом, а на данные зафиксированные в графах. Плюс желательно, чтобы эти данные были подтверждены криптографическими ключами автора и криптографически связаны с предшествующими данными, образуя так называемый event chain сопротив блокчейна. То есть данные транссемантики и network соединяются. Хэшами в графе следующее событие, оно не может появиться в графе, не сославшись на предыдущее событие. А соссылка на предыдущее событие – это ссылка на хэш того события. Поэтому изменить хоть что-то в графе практически становится невозможно. с учетом того, что подписанные новые события подписаны ключами акторов, а не валидаторов, как в DLD-системах в блокчейнах. И нефальсифицируемость словарей и данных, привязка контента к владельцу и ссылка на фиксированные значения, то есть семантический граф обеспечивает нам набор фактических данных, которые должны использоваться языковой моделью для предоставления данных пользователю. По сути, Trusted Semantic Network представляет из себя идеальное совмещение трех технологий. Это тематический подход транспортировки и хранения данных. криптографические методы идентификации и защиты данных, и распределенные их хранения на узлах периметровой сети, и поиск, анализ и представление данных с помощью больших языковых моделей. Эти технологии Взаимодополняют друг друга. Языковая модель обеспечивает доступ к семантическому слову. Экслойда и Алексея. Семантика гарантирует однозначность понимания данными языковой моделью и расширяет проблему. и позволяет масштабировать и повышает производительность DLT. Масштабирование – это образование кластеров, а производительность – это сокращение служебных данных и управление консенсусом. Мы не проводим одинаковый консенсус по всем данным. И DLT обеспечивает идентификацию, валидацию, сохранность данных, без чего невозможно использовать связку языковых моделей и семантиков бизнес-деятельности. То есть мы должны любой бизнес-деятельности, если это не просто мой агент-помощник, а мы имеем некий инструмент для организации бизнес-деятельности юридической значимости, то мы должны использовать не просто языковые модели с семантическими графами, но и с криптографической защитой при помощи DLT-технологии. И сейчас самое интересное, и у нас время. Да, я довольно быстро. Сейчас давайте немножко придержим лошадей и поговорим именно о как это все работает. Общее описание и такие типовые юзкейсы. Как вы уже заметили, и по картинке было видно, что снижение нагрузки на консенсус, снижение нагрузки на хранение всего графа, возможность хранения локально только тех веток графа, по которым работают конкретные пользователи, позволяет запустить Да, и уменьшение объема самого движка семантического, он вот сейчас 600 килобайт занимает, то есть позволяет возможность запустить сам движок, узел и хранилище на устройстве пользователя. То есть мы не имеем внешних каких-то интерфейсов. Каждый пользователь является узлом сети. Он получает свои пару криптографических ключей и загружает клиент. Сначала загружает клиент, генерирует пару ключей, загружает Genesis Graph. Это часть графа, без которой вообще не будет работать семантика. Сейчас это 30 событий. Планируем в полной версии где-то 100-150 событий. Это будет генодизм графа, совершенно незначительный объем, который необходим для начала деятельности. Залужаясь словарей, базовые модели. То есть базовые словари и базовые модели, которые необходимы для запуска движка семантического. И если он начинает действовать, начинает функционировать в сети, он может послать транзакции только по существующей модели. То есть если он поставит заявку куда-то, значит у него должна быть некая общая модель посылки. поиска моделей в каталоге моделей и посылка заявок на приобретение или загрузку этих моделей. Это должна быть модель, базовая модель, одна из моделей, которая для начала пользования. Допустим, что наш пользователь, который вот сейчас только что создал свой узел, загрузил Genetic Graph, загрузил базовые модели и решил создать свой ластер. Он решил создать некую деятельность. Скажем, у меня есть свое сообщество, партия даже, и я хочу создать площадку для взаимодействия членов моей партии. Кстати, во Пле мы делали такую экосистему для партии одной российской, но в связи с тем, что партия была закрыта по известным причинам, так и не был реализован этот проект, но как бы там было все. Сделано по-взрослому, по-блокчейновски, со всеми голосованиями, рейтингами, задачами и прочее. Приступили к тестированию, но вот довести до использования не довелось. Так, это отступление. Для того чтобы начать какую-то деятельность, наш основатель кластера, можно взять термин экосистема, потому что кластер – это понятие техническое, это набор узлов конкретных, энного количества узлов. А когда у нас будет несколько узлов, будут пользователи, будут бизнес-процессы, которые ассоциированы с моделями, загруженными на эти узлы, это будет экосистема. Некая экосистема, в которой будут распределены роли, права, и они будут взаимодействовать. Наш основатель экосистемы регистрирует на платформе главной экосистеме Trusted Semantic Network свою экосистему, дает название и регистрирует в каталоге экосистемы и начинает загрузку моделей. Он подгружает модель голосования, модель рейтинга, модель, скажем, постановки задач, еще какие-то модели. Какие-то модели можно отредактировать. Он не может написать какую-то модель, которая нужна. Для этого не нужен программист. Он может просто в редакторе модели в своем клиенте написать новую модель. Так это я показывал. После этого, когда он тестировал взаимодействие каких-то моделей, выполнение бизнес-процессов, сохранение всех данных в темпоральной истории в своем графе, он приглашает своих пользователей, свои партии, они каждый становятся узлом, загружают к себе те модели, подписываются на те модели, которыми они будут действовать. Скажем, не все модели, которые существуют в этой экосистеме, в этом кластере, нужны каждому пользователю. Кто-то будет только голосовать, кто-то будет выполнять задачи, кто-то будет участвовать в рейтинге, кто-то не будет участвовать в рейтинге, значит, он не загрузит эту модель, и данные по этой модели рейтинга у него не будут фиксироваться. на его устройстве, на его мобильнике. И начав деятельность, все пользователи данной экосистемы обмениваются транзакциями. и выполняют некую бизнес-действие, наполняют свою игру. Здесь мы и закончили. Деятельность заключается в обмен транзакциями событиями по моделям. Каждый согласно своей роли, который ввел основателя экосистемы и назначил эти роли. Роли фиксируются в генодезиграфе и входят в базовые модели управления ролями, поэтому здесь ничего настраивать не нужно, просто нужно выбрать количество ролей и назначить их пользователям. Это я сейчас рассказываю уже, как сейчас работает. То есть, по сути, рассказываю ТЗ на существующую сеть. Сама сеть еще в разработке, но движок существует локально, как это все можно сейчас сделать. И каждый пользователь, имея набор моделей, по этим моделям, согласно своей роли и согласно порядку, предусмотренной этим моделям, шлет транзакции в сеть. Они сохраняются у всех, кто работает по данной модели. И хранение данных состоится внутри кластера, и валидация с помощью узлов кластера. Но может быть предусмотрена модель такая, что если количества узлов недостаточно, мощности узлов недостаточно, возможно подключить внешние узлы из платформенной экосистемы для поддержки деятельности. То есть можно подключить специализированные архивные узлы, в которые сбрасывать архивные служебные данные. Прошло голосование, мы зафиксировали результат, мы можем со всех узлов, ну может быть не со всех, там нужно посмотреть, но по крайней мере с узлов пользователей можем скинуть Ветки на архивирующий узел, они там сохранятся, будут доступны, но не будут отягощать хранилища пользователей. И также можем подключить некие внешние узлы для концентрации. То есть если мы совершаем какие-то действия, в которых нужен серьезный консенсус, и у нас, скажем, малая экосистема, в которой присутствует всем участникам, а нам нужно заключить какую-то сделку между этими участниками, мы можем подключить внешний узел, подключить к нашей модели внешний узел. Тут вопрос о доступах данных. Вполне возможно, данные будут шифрованы, если потребуется. Но и по шифрованным данным можно сделать консенсус. Есть криптографический алгоритм для этого. Во время работы в экосистеме возможно редактирование модели без остановки бизнес-процесса, то есть возможно добавление новых каких-то событий в модель. Можно добавить, а здесь добавим комментарии, а здесь лайк поставим, а здесь добавим еще утверждение данного события администраторам. Все, кто работали по старой модели, будут также работать, но им что-то будет недоступно, они не заметят. Все данные предыдущие будут валидны, они останутся. Любое добавление в модель нового модельного события не отменяет предыдущие данные, физически не может отменить. Для чтения предыдущих данных нужно использовать ту версию модель, которая была на момент предыдущих данных. То есть на год назад я хочу прочитать данные, я должен взять версию модели, которая хранится в графе на тот период, и по этой модели прочитать эти данные. А часть данных, которая была сгенерена после, по новым моделям, она просто по старой модели будет не видна. Просто не видна. Тоже можно прочитать и текущие данные, но новые данные будут не видны, никаких ошибок не возникает, просто возникает некая неполнота. Я нескоро запоминал платформенную экосистему. Экосистема – это кластер узлов, которые запускаются при запуске системы. Эта экосистема нужна, прежде всего, для управления параметрами платформы. В эту экосистему, в этот кластер входят владельцы валидирующих узлов, владельцы архивирующих узлов, инвесторы. И, скажем, пользователи, которые в какое-то сообщество или концерт какой-то, который создает сеть. В общем, какое-то организационное объединение, которое запускает сеть и которое имеет некие права на управление. Основатели экосистем тоже входят в платформные экосистемы, поскольку регистрируют ее. Среди всех этих членов они обладают разными правами, разными ролями, они могут голосование управлять параметрами платформы. Экосистемное строение, платформенное экосистема и управление параметрами было реализовано в Apple, в отличие от других частей. До сих пор это не реализовано ни в одной блокчейн-платформе. Поддержка каталогов моделей и словарей понятна, маркетплей ресурсов, то есть валидирующие узлы и архивирующие узлы регистрируются и могут наниматься, то есть подключаться к некоторым экосистемам для выполнения функций валидации и архивации. И одна из основнейших функций платформенной экосистемы – это эмиссия нативного токена и обеспечение консенсуса по финансовым транзакциям по этому токену. Никто не запрещает каждому кластеру завести свой токен, обмениваться внутри, но он будет функционировать только по моделям внутри данного кластера. некими эквивалентными ценностями между кластерами, между узлами, принадлежащими к классным кластерам, вы должны пользоваться нативным токеном и пользоваться моделью, которая приписана к платформной экосистеме. На платформной экосистеме действует паровая система, состоящие из так называемых смарт-законов. Это тоже изобретение АПЛЫ, то есть это специальные модели, специальные контрольные, семантические модели, скажем, которые предназначены для контроля деятельности каких-то моделей, которые уязвимы, скажем, той же модели финансовых обменов и они созданы для предотвращения угрозы, фиксации угрозы, но самое главное для предотвращения последствий угрозы. Скажем, если на момент Казус «The DAO» в эфире у них был подобный смарт-закон, в котором было прописано, что некие ключи имеют право остановить сеть, провести голосование между заинтересованными ролями, что делать, запускать дальше, либо специальные транзакции, которые уже прописаны в этот смарт-закон, вернуть деньги и пустить дальше сеть. Тогда бы им не пришлось делать форк. То есть наличие Smart закона позволяет урегулировать какие-то известные на момент запуска сети уязвимости и их последствия с помощью специальных транзакций без нарушения валидности всей сети. И теперь самое интересное. Это мы пришли. Как же у нас на нашей сети будет действовать... Большие языковые модели, уровни GPT-4, Клода, Барта. Да, здесь я немножечко предустойчиво скажу, что вообще сама идея подобной сети, семантической, она возникла у меня еще где-то в году 2012. Есть статья на Хабре большая, и по этой статье я делал доклад, в какой-то компании украинской. И тогда в этой компании айтишной украинской мы начали делать эту сеть, тематическую сеть для профессионалов, для работы ученых. И практически даже доделали процентов на 70, на 80, но это был 2013 год, там начались события в Украине, ну и были какие-то проблемы организационные тоже. В общем, этот проект загнулся, но это был сугубо семантический проект без всякого DLT. В те времена я слышал, что уже существует биткоин, но не знал, что такое биткоин. Во время работы в Аппле чуть пораньше, когда я познакомился с DLT-технологией, я уже писал статьи и спецификации для объединения семантики и блокчейна. И где-то в году 2018-19 прочитал какую-то лекцию программистам о том, как может быть использована семантика в блокчейне. И даже пытался в некоторых приложениях использовать событийную семантику. Скажем, действительно было в приложении для аренды коммерческой недвижимости в Дубае. Нет, не в Дубае, в Аджмане, в одном из Эмиратов. которая прошла, и там у нас конкурентами были объемы, у них выиграли, но все-таки до продакшн-то не дошло, ну как ничего не дошло, там же в эмиратах до сих пор связано с DLT. Там пытался делать уже, там был элемент с событийной семантикой. И один из программистов тогда сказал, что я хотел бы это сделать. И вот этот программист уже в 1922 году, в 1921 году, он для меня делал первую версию движка, и потом вторую версию делал движка, но сейчас он по здоровью, по разным причинам выбрал. И была версия Trusted Semantic Network с двумя технологиями, но приписывалось, что есть еще возможность присоединить машинное обучение для анализа графа. Я представлял себе, что сетки, обученные специально на этой семантике, могут выявлять какие-то неоднородности в графе, ошибки в графе только для этого. Естественно, когда появился chat.gpt 3.5, я сразу обучил 3.5 событийной семантики и попробовал, что действительно chat.gpt может генерировать событийные модели. Есть про это статья на Хабре, я делал доклад, предыдущий рассказывал. И, значит, после этого... Двойка технологии была завершена тройкой, и все логично. И вот сейчас давайте посмотрим, как же мы можем использовать языковую модель. Я автор текста, автор статьи, захожу в свой клиент Trusted Semantic Network, загружаю модель загрузки нового контента, новой статьи, и туда загружаю файлы. Прежде всего он попадает в языковую модель. Она проверяет на уникальность. Кто еще, кроме языковой модели, может проверить текст на уникальность, сравнив с тем, что у нее существует и в распределенном виде векторного пространства, и сравнив быстро по саму графу. Прежде всего, даже не по самому графу, а по, скажем, суммаризе по статьям или по каким-то только известным самой языковой модели отпечаткам вот этих текстов, по которым она может определить их идентичность. Здесь не о наших идет речь, потому что не о изменении одного знака, а именно их близости именно семантической. То есть языковая модель может определить близость и сказать, что да, этот текст похож на десяток статей, либо он уникален. Действительно, в нем есть какие-то уникальные идеи, которые не встречались нигде. Я знаю, что когда загружаю, просто проводил эксперименты, загружал в сеть, более-менее известные такие тексты, типа реферативные, она говорила, да, это типа реферат по таким-то статьям, таким-то, таким-то. Я загружаю свою оригинальную статью с оригинальными идеями, мы покрасим статью. данной сети. Она говорит, да, это оригинальная идея, нигде такого не было, вот очень оригинально это то, здесь правда вот сложности такие-то, вот здесь сложно реализовать это, здесь проблемы такие-то, вот все понимает. Значит, проверка уникальности. И классификация этой статьи Классификация проводит тоже языковую модель и вставляет некий тематический кластер, тематического графика, то есть определяет место. Она относится либо к DLT, либо к семантике, либо к философии, либо узко, скажем, к неокантианству, а не просто к джаканту, а к неокантианству. Также сеть при загрузке делает предварительное ранжирование контента по оригинальной качеству и в дальнейшем корректирует это ранжирование по использованию этого контента, по этой статье. Сколько раз загрузили, сколько раз цитировали, сколько раз Некое аранжирование текста делает языковая модель специально для этого предназначена. Естественно, поиск контента и предварительное определение по своему представлению векторному, конкретизация ветки графа и поиск по ветке графа конкретных данных. Сначала генерация ответа, скажем, предварительного, без фиксации, Потом загрузка в соответствующие ветки графа, корректировка по фактам и выдача ответа со ссылкой на факты и с конкретными ссылками на цитаты авторов и все прочее. Не нужно забывать, что у нас весь контент хранится в графе. Он сохраняется, кроме того, что он модифицирует саму модель. Сейчас нынешнего уровня нет, но мы предполагаем, что будет следующий шаг, когда каждый новый универсальный контент будет модифицировать языковую модель. Также языковая модель по запросу может провести анализ графа, классификацию каких-то понятий в графе, выведение закономерности данных, то есть провести то, что мы называем анализ данных. И дальше, естественно, то, что мы называем именно адаптивный интерфейс под конкретно меня, под конкретно известные мной запросы, под мои предпочтения в визуализации. Языковая модель строит интерфейс. мой конкретно мне интерфейс. Никакого другого интерфейса, который был сгенерирован под конкретный запрос, либо это аналитика. Я могу сказать, а мне вот, пожалуйста, представь еще вот этот график в виде диаграммы, добавь еще одну ось, и тут же перерисовывается. Это сейчас уже реализовано. То есть, пожалуйста, код интерпретатора сейчас GPT вот так работает. Также, что делает модель, это анализирует действия пользователя для улучшения персонализации. Иногда я говорю, что нужно делать в интерфейсе, но в основном она следит за моими действиями, что я загружал, от чего отказывался, что лайкал. создает некий профиль, мой профиль, по которому она будет ориентироваться и при поиске, и при анализе, и особенно при генерации адаптивного интерфейса. Вот это работа с пользователем. Переходим к следующему уровню – это функционал, который может привнести языковая модель. Естественно, создание нового контента. Стандартно то, что сейчас мы работаем в ChargePT. Ответы на вопросы, классификации различные, параметры различные. оценка каких-то уже записанных текстов, то есть стандартная работа по генерации нового контента. А здесь следует интересно еще отметить, что я не подразумевал, но не отмечал, но потом в конце это еще специально отмечу, что имеется в виду, что это одна модель. Не на каждом узле свой маленький агент, который работает только на конкретного пользователя. Естественно, что если я создаю контент и отсылаю этот контент преподавателю своему, И эта же сеть анализирует этот контент, то есть обнуляется вообще вся проблема генерации фейкового контента и генерации контента с помощью языковых моделей, которые не созданы самостоятельно. Сама же сеть генерирует контент, сама же знает со стороны уже работы преподавателя, который получил это курсовую, просто скажет, что там написано, что это я только что сама сгенерила. Или я генерила вот это, а вот эти записи сделал он сам. Его запрос «выдай-ка мне оригинальный текст, созданный этим учеником из этого текста» – это элементарное действие для языковой модели. Дальше. То, что тестировано – это создание бизнес-логики по текстовому описанию. Сейчас я бизнес ноутбук создаю руками в редакторе, ввожу новые модельные события, приписываем еще модельные события, создаю ограничивающие свойства. Уже проверено, что это все можно сделать просто по текстовому написанию. Создаю мне модель документа. Создаем модель подписания документа тремя акторами с программой таким-то, таким-то. Предварительно они должны заполнить такие-то параметры, предложить суммы по этому контракту, дальше провести какой-то анализ каких-то данных и в конце подписать, еще плюс подписать админам, а консенсус должен быть вот такой. И модель может создать, языковая модель может создать Естественно, ее нужно тестировать и подправлять, чтобы добавь это, добавь это. И можно иметь модель тестирования моделей, по которой будет другая ветка этой модели, то есть другая сессия будет тестировать эти модели. То есть это возможный контроль. И возможно исполнение моделей. В принципе тоже проверено. Если есть модель некого бизнес-процесса, то сама модель может его выполнить. Насколько это надежно – это вопрос проверки. Но в принципе это возможно. Насколько она будет глючить? Поскольку она выполнять будет по готовым схемам, то вряд ли она будет так же глючить, как при свободных вопросах какие-то прокотиков, кто кем является, исторических событий. Здесь довольно четко она выполняет, тем более, что всегда можно проверить. Но если сама она выполняет эти модели не очень надежно, то она всегда может генерить код, который будет исполнять эти модели. Это последний пункт – генерация кода. Естественно, Смотрите, что мы имеем. То есть сама модель, встроенная в сеть, выполняет огромное количество функций и приложений, которые раньше выполняли отдельные приложения отдельных производителей. То есть перевод текста, проверка текста, суммаризация текста, классификация текста, генерация картинок различных. То есть все больше и больше функционала, который требовал специализированных То есть, понимаешь, у нас были целые программы для распознавания текста по картинке. Сейчас ты, пожалуйста, идешь в тот же... Барт, тоже часть GPT, загружаешь картинку, он тебе пишет, что на этой картинке. Огромное количество специализированных программ, которые писались и переписывались программистами, они все заменяются одной единственной интеллектуальной моделью. И генерация программного кода, она может, скажем, если я создал движок под новую функцию, может дописать этот движок и сама оттестировать, я оттестирую и запустит движок уже на исполнение бизнес-логики. И если нужны какие-то мелкие приложения, конкретно, она тоже сейчас прекрасно это делает. Я могу написать, мне нужно из этого текста выделить вот такие-то предложения, сосчитать, сколько в таких-то вхождений слов, и указать, какое количество отношений между этими словами есть, и использовать их, записать файл для использования в такой-то модели. То есть это все он создаст тут же. Либо сам сделает, либо создаст код, если я прошу на Питоне, и выполнит, обработает этот текст и выдаст результат. И у нас мы заходим уже на финальную прямую. Чего нам не хватает? для вот этой всей фантазии, которую сейчас я изложил. Как я уже упоминал, не хватает перманентного долучения на новом контенте. В сессии, когда я работаю с Клодом, он помнит все, что я загрузил, все тексты, все, что я говорил, и я могу задавать вопросы по этим текстам. У Клода вообще большое окно, он треть книги может загрузить на русском языке. Но в сессии мы ждем. Ждем, когда загруженный текст, большую языковую модель, будет проиндексирован внутренне. Она будет знать его так, как она его знает в сессии. Она классифицирует этот текст и загрузит в определенную ветку хранилища. Классифицирована в эту ветку графа. Плюс разметит его как-то и как-то ассоциирует идентификаторы этого текста с моей внутренней структурой, то есть еще отдельно какой-то идентификатор текста сохранит. Как заголовок может помнить текст, так и может идентификатор помнить в графе. И при частой ссылке на какие-то цитаты, какие-то фрагменты текста тоже может производить… уникальную идентификацию этих цитат и привязку их к образу этой статьи внутри своего векторного пространства. Этого не хватает, чтобы перманентное обучение графа на новом контенте и не обучение его на том контенте, который он считает мусорным. То есть если он получает контент, который Он уже множество раз получал, он помечает его как мусорный и сбрасывает куда-то там. В граф он может его записать, конечно, но пассивировав его значимость, его рейтинг. Чего нам не хватает? Децентрализованной архитектуры больших языковых моделей и искусственного интеллекта вообще, чтобы он работал не на дата-центре одном, а на совокупности устройств сети. Как это? Я плохо себе представляю. Где-то есть какие-то наметки, как это может работать, но в принципе это идеальный вариант. что вся языковая модель, она работает на... о ресурсах участников сети. То есть в этих ресурсах есть и большие, конечно, дата-центры, которые поддерживают валидирующие узлы или архивирующие узлы, и есть узлы-пользователи. И вот на всех этих узлах на действия пользователей, запускающих что-то, выполняющих какие-то модели, вот прямо на этих действиях должна функционировать большая языковая модель. Условно так называемая. И, конечно, с распределением по узлам мы должны потерять собственника. У нас не должно быть собственника этой модели. Она должна функционировать как протокол интернета. нового интернета. То есть, по сути, вот все, что я сейчас говорил, я начал с того, что это вот некая локальная, ну не локальная, то есть некий продукт, изготавливаемый там несколькими программистами сейчас, там несколькими программистскими конторами, называемый Trusted Semantic Network, то по сути я описал будущее интернета. То есть когда есть контент, хранящийся распределенно, есть доступ к этому контенту, есть система управления этого контента, система предоставления, адаптивного предоставления этого контента каждому пользователю, есть система загрузки этого контента, контроля этого контента, аналитики контента, то есть и она единая. по всему контенту, но разделена, жестко разделена по кластерам, что позволяет сильно снизить нагрузку на общую сеть и на конкретные узлы. И Trusted Semantic Network, условное название, оно, конечно, будет другим, а может быть и останется. Это новая версия интернета. Она представляет из себя симбиоз криптозащитного децентрализованного хранилища, семантический граф фактов знаний, создаваемый по исполняемым моделям и моделям сущностей, недвижимости, объектов недвижимости, и моделям действий, заключения контрактов, различной бизнес-логики. И адаптивного интеллектуального интерфейса, который обеспечивает что-то типа будущих версий языковых моделей. Ну и остался один вопрос, ну не один вопрос такой, а сайт перетер? На сайтах нет. Сайт как пережиток. того, что кто-то, дядя Вася, собрал какой-то контент и имел доступ, скажем, в конце прошлого века, имел возможность где-то арендовать пространство на каком-то серваке, и создал сайт, и раздавал этот контент всем желающим. современном мире, современными скоростями интернета, современными объемами устройств пользователей, никакой необходимости концентрации, свалки контента на каком-то месте, оформленном кем-то, не мной, не для меня, нет. Нет необходимости. То есть это интернет юзероцентрический. Сайта никакого нет, есть только узлы пользователя, которые через свой семантический браузер или адаптивный AI-интерфейс, как можно по-разному назвать, получают контент со всей сети, адаптированный под мой образ, мою семантику, под мою антологию индивидуальную, и принимающие от меня данные в той области, в которой работаю, и классифицируя эти данные, проверяя эти данные. И вместо сайтов у нас имеется адаптивный интеллектуальный интерфейс. Состояние проекта – на сентябрь. Семантический движок бизнес-логики работает, сейчас он модернизируется, там добавляются функции какие-то, сейчас API пишется к нему, проверен в нескольких профаконцептах на различных бизнес-логиках, в бизнес-логике анализа безопасности предприятия. Сейчас разработано и начинается уже программирование сетевого слоя на базе библиотеки LibP2P. Я рассказываю, что подтверждена возможность обучения языковой модели для генерации семантических модели, исполнение модели. На данном этапе это будет как сбоку бантик. Не интегрировано, конечно. Можно интегрировать, если взять некую открытую модель и ее научить. Но пока такой необходимости нет. Пока это все планируется и держится в уме. Но генерация моделей непосредственно сейчас уже возможна. И до конца года ожидается MVP в личной сети. Он будет включать в себя один кластер из пользовательских устройств, мобильников, на одном из которых или на компьютере можно будет прямо на глазах публики создать модель. какую-нибудь, скажем, модель чата создать, запустить эту модель в сеть. Все подключающиеся по этой сети смогут обмениваться сообщением по этой модели. Прямо на глазах публики можно будет добавить, скажем, лайки к сообщениям в этом чате, и сразу они появятся на всех устройствах, и все будут лайкать. Можно добавить, скажем, многоуровневое комментирование или еще чего-то. Все это вживую. Вот такой образ MVP, который должен появиться в ближайшее время. Тут несколько картинок. Вот так выглядит модель. Я уже показывал. Это модель перевода словных единиц. Реально работающая она так выглядит. Эту модель может составить и языковая модель. Это служебный интерфейс, нет пользовательского интерфейса, который сейчас создается, генератор пользовательского интерфейса, который будет создаваться также с помощью семантических моделей. То есть я семантически описываю что-то, индивид неосущной страницы, семантически описываю содержание этой страницы с запросами графа, и контроллер строит интерфейс конкретно под эту модель, под конкретного пользователя. пользователя ссылок под роль. А сейчас это вот в таком пользовательском виде, в админском виде работает модель перевода условных единиц. И ранее там вел получателей суммы, и они получили. И, значит, как обещал, здесь 12 ссылок. Здесь нет статей по блокчейну, но можно на фортлог зайти. Это основной русскоязычный журнал по блокчейну, по технологиям DLT. С ним у меня хорошее отношение, поскольку именно благодаря первой моей статье, опубликованной на страницах этого издания, я нашел основателя проекта, который потом назывался АПЛА. Я столько лет работал и приобрел некую специализацию, некие компетенции в области блокчейна. И в ближайшее время выйдет мой курс по DLT-технологиям. Такой большой, под 100 страниц, большой курс, популярный не технологически, не для программистов, но именно для DLT, но описывающий на том уровне, на котором я начинал описывать геотехнологию, Все. На этом мы уложились час двадцать. Я думаю, что вполне хорошо для такой темы довольно обширной. Спасибо за внимание. Давайте вопросы. 

А. КОЛОНИН [01:12:21]  : Александр, спасибо. Во-первых, несколько комментариев. Во-первых, от меня, да, тема очень близка. Многие вещи, которые вы говорите, наверное, кроме дилемм практически все. Я там, можно сказать, уже не первый десяток лет. Тоже развиваю. Очень близко и понятно. Сейчас будет дальше в вопросах некоторая критика с моей стороны. Следующий комментарий. Два комментария от Виктора Сенкевича. Я их зачитаю. У меня есть конкретная задача, которая пересекается с изложенным. В нашей постановке задача становится более простой и вполне реализуемой. Мы делаем конструктор документов, в частности, контрактов. Любой контракт состоит из контекстов, параграфов. Приложение позволяет манипулировать такими контекстами. Добавить возможность предлагать через LLM варианты таких контекстов будет очень неплохо. Этот комментарий прокомментируете? 

А. БОЛДАЧЕВ [01:13:23]  : Да, да, да. То есть подобную систему я делал для Дубая, конструктор контрактов на блокчейне, опла. И таких решений довольно много. Я не знаю, как конкурировать в этой области. И ближайшее применение сети, которая сейчас проектируется, это как раз платформа контрактования. Но платформа контратования, которая позволяет заключать контракты по моделям, в отличие от составления из фрагментов готовых текстовых, главный элемент – это прежде всего юридическая значимость. То есть это подписывание, криптографическая защищенность контрактов, криптографическая защищенность всех этапов их заключения и результатов. И плюс финансовая система, введение на платформе контрактования собственного токена, в котором происходят взаиморасчеты внутри. И вот вывод через финансовый институт ФИАТ. По сути, такую систему вывода евро мы делали в Люксембурге. Был большой заказ, но с COVID и с закрытием АПЛА он не был реализован. были уже контракты заключенные с финансовым институтом и проверены возможности перевода евро в оплу и заплыв евро. Самым главным была лицензия. Но лицензия прогорела, поскольку Все, что вы сказали, это все реализуемо. Если обобщить, то данная технология соединения трех технологий, выполненная либо в виде приватной сети нескольких узлов, либо публичной сети, либо вообще в виде облачного сервиса, реализует, перекрывает все возможные применения, связанные с бизнес-логикой, контрактованием, организации каких-то деятельностей внутри сообществ. В общем, написать можно все. То есть это как бы универсальный инструмент, который не требует программирования. Ноу-коды, полностью ноу-коды. 

А. КОЛОНИН [01:15:46]  : Спасибо. Это был, кстати, комментарий Виктора Сенкевича. Следующий комментарий у нас был от Александра Летуновского в телеграмме. Как водится, у великих людей мысли сходятся. Он прислал картинку, которая иллюстрирует, что у него такие же, как у вас, идеи в части тематических токенов и сообществ. В общем, тоже все очень близко. 

А. БОЛДАЧЕВ [01:16:17]  : Да, конечно. Довольно часто встречаются, мне пишут, я обсуждаю. Эти идеи летают. И вот именно экосистемное устройство в АПЛЕ было реализовано. Там единственная разница была в АПЛЕ. В АПЛЕ была единая сеть. Она не была кластеризована. Вот сеть не была кластеризована. Там были фиксированы узлы, которые, эвализирующие узлы, как и EOS, совершали консенсус сохранения данных блокчейна. А вот организация деятельности была разбита на экосистемы. Каждый человек может войти в одну экосистему, где реализуется партийная деятельность. В другой он мог заниматься бизнесом. В третьей он мог заключать контракты на недвижимость экосистемы. В каждой экосистеме можно было делать эмиссию токена, помимо того, что был токен общий по всей системе нативной. 

А. КОЛОНИН [01:17:10]  : Спасибо. Следующий вопрос от Виктора Носко. Откуда информация о том, что модель сгенерировала некоторый текст? Сможет сама в следующей сессии определить, что именно она же, эта версия этой компании ТП является автором этого текста? А видимо здесь что? Слово пропущено. 

А. БОЛДАЧЕВ [01:17:34]  : Да-да-да, это понятно. То есть дело в том, что я же указывал, что есть какие-то моменты, которые сейчас еще не реализованы. Но в принципе себе представить, что, скажем, когда генерит текст GPT, языковая модель, она может этот текст воспринимать как внешне загруженный же ей. То есть она его выгружает и тут же подгружает как будто от пользователя. И его уже классифицирует как вторичный относительно каких-то текстов, которыми она пользовалась. То есть это один из вариантов. Можно предусмотреть варианты, чтобы сеть, генерирующая текст и записывающая его в граф, каким-то образом в своем пространстве вектором нам фиксировала идентификатор этого текста. Это фантастика сейчас. Но в принципе, я представляю, что технологически это реализуемо. Скорее всего, технологически это будет реализовано при помощи уровневой структуры языковых моделей. То есть должен быть уровень, текущий уровень GPT или CLOD, который обеспечивает сугубо языковые компетенции, Вот языковая компетенция, которая понимает, а над этим уровнем должен быть уровень специальной компетенции, который уже ориентирован на отдельные кластеры, то есть он пользуется для чтения текста языковым уровнем. Какой-то выход идет, некая суммаризация, или в каком-то даже векторном виде этот текст представляется уже на следующий уровень, на котором происходит уже идентификация с какими-то конкретными текстами, сравнения. Скорее всего, так будет. Чтобы не трогать никогда языковый уровень, чтобы он был зафиксирован, нам достаточно, чтобы вот эта система понимает текст как человека, лучше человека мы фиксируем. Дальнейший рост компетенции идет на уровнях специфических по областям, так я себе представляю. А тогда, Виктор, действительно, это сейчас невозможно сделать, но я думаю, что это задача решаемая. 

А. КОЛОНИН [01:19:50]  : Спасибо. Дальше. Следующий вопрос. Вот мои уже, видимо, вопросы пошли. Смотрите. Вы когда говорили про то, что в конечном итоге сайтов не будет, а будут просто устройства пользователей, я думаю, что несколько десятков лет назад Симбернер Сли бы говорил вот чуть ли не буквально те же самые слова с таким же огнем в глазах и с такой же мотивацией по поводу сначала ну по поводу семантического веба еще раньше то же самое говорилось про интернет да что мы просто подключаем устройство компьютер это сеть У каждого пользователя на устройстве лежат файлики. Любой пользователь может зайти к любому пользователю по FTP протоколу и эти файлики получить. А потом выяснилось, что не обязательно ходить по FTP протоколу. Можно у каждого поднять HTTP и качать файлики уже в виде HTML разметки с каждого пользователя на каждый пользователь. В свое время, в 1982 году, я еще не знал о существовании HTTP, поскольку интернета тогда не было, и прочитать про то, что люди за рубежом придумали HTTP, было негде. Я сам такую систему тоже придумывал, где у каждого пользователя на компьютере лежат файлики. а любой пользователь может к любому пользователю зайти. Тут же было придумано то, что сейчас называется ДНС, для того, чтобы пользователи могли находить друг друга. То есть эти идеи хорошие, я к чему подвожу. Я подвожу к вопросу, все идеи прекрасны, но мне кажется, да, я на эту тему даже у меня статейка есть, вот и может быть даже поговорим об этом на следующем семинаре, кстати, у нас 5 октября, по-моему, будет семинар как раз посвященный круглый стол, посвященный искусственному интеллекту и блокчейну. Вот там будет несколько спикеров, мы будем тоже об этом говорить, о чем сегодня Александр более развернуто на своем собственном семинаре рассказывает. Значит, я к чему подвожу? К тому, что мне кажется, что то почему не полетел интернет, в том виде, в котором он был задуман, что сайтик у каждого свой, и любой может зайти к любому на сайтик. И почему тем более не полетел семантик веб, что каждый хранит свою RDF-схему, и каждый ходит на сайтики со своими схемами и выкачивает оттуда RDF-графы в определенных персональных схемах, Это не полетело по причине вычислительной неэффективности. Потому что каждый раз, когда люди рассказывают про... У нас есть время, поэтому я позволю себе развернутый комментарий. Надеюсь, Александр, вы сможете дать развернутый ответ. Каждый раз, когда рассказывают про то, что долой мировые банки и у нас все будет на блокчейне и будем друг другу платить из криптов кошельков, я думаю о том, что возьмем эфир Да, казалось бы, ну, ничтожное количество людей в мире пользуется эфиром, да, и транзакции там проходят с чудовищной скоростью. Но если ты хочешь разместить у себя эфировскую ноду, то еще лет 5 назад, для этого нужно было по-моему как минимум 64 гига. Для того, чтобы только загрузить ее. Сколько нужно сейчас, мне страшно подумать. Для того, чтобы просто ноду держать. И при этом я работал в банке. И я знаю, с какой скоростью обрабатываются банковские транзакции. Я не могу воспринимать серьезно, чтобы какой-нибудь блокчейн мог использоваться в качестве принятого платежного средства. Дальше мы переходим к разговору как раз о сегментированных распределенных реестрах, хедж графах, дагах. с неполной репликацией. Поэтому, Александр, большое вам спасибо, что вы начали разговор с утверждения о том, что надо говорить о распределенном реестре, а не о блокчейне. А блокчейн можно использовать просто как этикетку. Говорим нейронные сети, подразумеваем искусственный интеллект, мы говорим блокчейн, подразумеваем распределенный реестр. Только так к словам блокчейн можно относиться всерьез. Но я хотел бы сделать утверждение, что ВВВ не полетел, потому что оказалось гораздо эффективнее весь этот граф. зафигачить в один Knowledge Graph, или SIG, или DBpedia, или что угодно специфическое для онтологической модели знаний конкретной организации, и исполнять на одном локальном компьютере, или хотя бы на одном усилительном кластере. и что поэтому все полетело в централизованном, а не децентрализованном виде. И я плохо, честно говоря, понимаю перспективы децентрализации, хотя бы потому, что тот же самый ИИ. То есть какой ИИ полетел? Полетел ИИ, который сидит на кластере вычислительного OpenAI, И все платят ему денежку за подписку. И никакого там edge-компьютинга нету, потому что весь edge-компьютинг сводится к тому, чтобы загонять по API за денежку промпты. Я подвожу к своему следующему, второму и последнему вопросу. Но вот хотелось бы развернутый комментарий, Александр, от вас. 

А. БОЛДАЧЕВ [01:25:50]  : Хорошо. Практически на все я ответил еще по ходу, но сейчас просто акцентирую внимание. Конечно, обсуждать, что почему не полетело в прошлом веке – бессмысленно, просто по ресурсам. Сейчас рядовой пользователь носит в кармане дата центра того времени. по сути. Скорости обмена данными – это же то, что было, когда я подключался к интернету, это же какой-то кошмар был. Там несколько килобайт у меня иногда, килобит в секунду было, у меня там плохая сеть была, в общем, тоже по сравнению с сегодня. Поэтому плюс следует еще учитывать, что По тем временам доступ вообще к этой технологии имели единицы и могли создать, загрузить контент единицы. То есть имели сканеры, компьютеры, вообще текст вести. Для остальных это было вообще тёмной лесой. Когда только появились компьютеры, стали подключаться к интернету, помыслить можно было только то, чтобы пойти по ссылке и посмотреть что-то. Поэтому ресурсы сейчас позволяют. То есть я, скажем, посчитал, что если пользователь будет генерировать события по каким-то моделям, если это какие-то не файлы он там загружает, а просто какие-то опционные события, если есть какой-то размер события, транзакции в сеть, через каждые где-то 5 минут, через несколько минут ему хватит где-то гигабайт. на 30 лет вроде, чтобы заполнить гигабайт своего хранилища, а это в принципе нормально для мобильника сейчас, что там гигабайт в мобильнике. То есть ресурсы отменяются. Второй момент. Все-таки семантик взлетел не из-за того, что там скачивали-не скачивали к себе дампы, вот эти Хардеевовские графы, а потому что сама технология подразумевала или подразумевала, Разметка будут сайты, размещать будут сайты и размещать они будут пользователи, не пользователи сайтов, а владельцы сайтов. И URL была, то есть URI была ориентирована на домены. То есть это было все сшито такими нитками, что срастись не могло. А самое главное, что по тем временам контент был абсолютно не идентифицирован. То есть я могу на своем сайте назначить на него вот такой идентификатор. На другом сайте, на этот же контент, будет назначен другой идентификатор. Тем более еще все это злобно будет переврано владельцам сайта. И никакой гарантии уникальности контента не было. То есть контент дублировался. в тысячу экземплярах, и поэтому действительно загрузить в себе оригинальный контент по какой-то тематике можно было, только если ты развернул его на своем сервере, ты сам владеешь этим сервером и сам отобрал, то есть ты гравзна не создаешь. То есть создание графа знаний – это был именно ответ на то, что по тем временам децентрализованное хранение без идентификации, без валидации и подтверждения неизменности данных было невозможным. И вот технология DLT нам дала возможность прежде всего криминографической идентификации, гарантии неизменности данных, А развитие технологий, ресурсов позволили нам хранить у себя огромное количество данных на своем устройстве, и скорость обмена данными тоже велика. А семантика, чего не было в DLT, она как раз и позволяет разбить на кластеры. То есть нет необходимости мне хранить все транзакции по какому-то контракту, по которому подписываются договора по недвижимости. Нафиг мне это хранить у себя. Система именно касторизации позволяет в каждом пластыре на каждом устройстве хранить только то, с чем ты работаешь, и то даже не все с архивацией. Поэтому я даже не могу себе представить в данной схеме, где может быть этот сайт. Вот что он должен быть? Это новостной сайт может быть? По сути, есть некая модель, по которой все постят новости в сеть. Они просто постят в граф. У меня настроено, я подписался на 3-4 модели. Издательство новостное запустило в граф свои данные, просто они в графе, они распределены где-то. Я, имея эту модель, подписавшись на эту модель, просто как в сети это реализовано, подпись на допике, получаю эти данные. Если мне не нравится, я ставлю фильтры какие-то. Если совсем не нравится, подписываюсь на другой. Кто-то создал модель интеграции, сравнения разных новостных источников и выдачи совпадающего. Я подписываюсь на это. То есть того места, откуда я должен скачивать, это место есть в этот грамм. А его хранить где-то на одном каком-то серваке, размеченный с HTML, ну, смысл какой? Смысла нет никакого. Естественно, может быть, это не завтра. Точно не завтра. 

А. КОЛОНИН [01:31:38]  : Где лежит этот граф? Этот же граф должен где-то лежать? 

А. БОЛДАЧЕВ [01:31:42]  : Да, конечно. Он лежит везде, правильно? Не везде. Он лежит... на тех устройствах, пользователей на тех узлах, которые подписались на модель, по которой он формируется. Скажем, есть новостное агентство ТАСС. Тысяча человек подписалось или 10 тысяч подписалось. И все данные лежат на всех этих узлах, они идентифицированы. У них уникальный идентификатор, но при этом есть возможность, скажем, этот же ТАСС арендует десяток архивных узлов, на которых постоянно будет храниться после консенсуса, а я могу свои удалять, чистить, мне нет необходимости хранить все. Но, в принципе, кто-то будет хранить, кто-то нет. 

А. КОЛОНИН [01:32:39]  : Спасибо. И второй вопрос по поводу экономики. Как, предполагается, работает ваша система с точки зрения экономики вообще? и в частности откуда берутся деньги на оплату подписки чат gpt ну причем в деталях то есть как происходят транзакции которые конвертируют ваши токены в доллары которые платятся OpenAI 

А. БОЛДАЧЕВ [01:33:08]  : Значит, смотрите, здесь нужно, конечно, различить идеальную систему. Идеальную, скажем, я его писал. Идеальная, когда уже у нас не чаджи пяти, а то, что я писал, требования какие у нас нужны, что это у нас… Нет собственника и централизованная архитектура. Допустим, мы реализовали тогда сам факт существования вот этого распределенного искусственного интеллекта, я оплачиваю его, используя свои ресурсы – память и процессор своего устройства. То есть он работает на процессоре на моем. То есть каждый, кто пользуется этой сетью, он оплачивает своими ресурсами. То есть если я хочу что-то большой, запрос какой-то, изгенерить большой текст, я должен побольше потратить своих ресурсов и больше скачать к себе каких-то моделей и больше потратить время процессора. Но при этом этим же процессором пользуются мои и другие. Вот это та часть, которая касается искусственного интеллекта, активного интерфейса. Если сейчас говорить, то я упоминал, что скорее всего с первой версии сети будет использоваться не GPT-4, не клод, за который нужно платить, а вполне возможно обучить создавать интерфейсы и генерировать открытую модель. Лама-2 – это следующая какая-то версия, которая при специализированном тюнинге под конкретные модели, создание моделей семантических, под конкретные алгоритмы вполне возможным будет работать. Она тогда будет принадлежать основателям сети. Теперь следующий вопрос – как что там оплачивается? Возможности заработка и ресурсы, то есть ресурсы валидирующих узлов и ресурсы архивирующих узлов, они оплачиваются экосистемой, если используется. Обычно нормально – это комиссия. Комиссия, подписка стандартная. Как сейчас любой валидирующий узел блокчейна сети, он берет комиссию по реализации консенсуса. Если, скажем, та же партия, о которой я говорил, в кластере, в экосистеме работает внутри себя и не использует никаких других ресурсов, они никому ничего не платят. Они сами пользуются своими узлами, используют свои сети. Если потребуется и покажется ненадежным хранение внутри кластера, можно действительно заплатить и нанять три архивирующих узла, на которые подписать на все модели кластера, и там они будут сохраняться. Но это деньги, нужно будет за это платить. И еще один явный источник доходов или источник финансирования или взаиморасчетов – это использование модели подписки на модели. То есть в модель можно прописать модельное событие, которое при пользовании этой модели, если эта модель подразумевает некие финансовые расчеты. Скажем, в бизнесе модель какая-то там того же соплачейная модель, Она подразумевает расчеты между участниками цепочки поставок, и вот в эту модель можно зашить отчет каких-то копеек от транзакций, которые совершаются по этой модели. То есть человек, который написал модель, он может получать деньги за ее использование. И на marketplace модели может разместить, где они могут полностью покупаться. Возможности монетизации есть. Вопрос про доллары и евро – это отдельный совершенно, но он ничем не отличается от стандартного вопроса DLT – вот и вывод валюты битков и эфириума. Биржи или еще что-то. То есть в данном случае, говорим, меня больше всего интересует, скажем так, как все-таки идеолога и философа, а не атишника, общая схема и применение конкретным бизнес-процессом, где этих проблем не стоит. То есть просто реализация самой технологии, как она дальше будет развернута в личной сети. Это уже отдельный вопрос, там есть маркетологи, я в этом вообще не разбираюсь. Честно говоря. 

А. КОЛОНИН [01:37:47]  : Александр, спасибо. Есть еще два вопроса от Ильгизара Талипова. Первый вопрос по поводу устранения собственника. Означает ли это, что должны устраниться собственники мнений, знаний, специфики локальных контекстов, аспектов предметных областей? 

А. БОЛДАЧЕВ [01:38:05]  : Так они сейчас никому не принадлежат. Разве именно предмет на область кому-то принадлежит? Прежде всего, по собственнику именно искусственного интеллекта. Мы понимаем, что интернет никому сейчас не принадлежит, и протоколы не принадлежат. Но вот это как бы протокол будет такой, допустим. Хотелось бы, чтобы языковая модель стала базовым протоколом сети нового поколения. Если говорить шире, футурологически, говорить о сингулярности технологической, переходе к новому уровню эволюционному, то нечто явленное сейчас, как языковая модель, должна стать базовой платформой этого уровня и работать как экономика. У нас экономика никого не принадлежит, она работает. Мы туда вкладываем, получаем, покупаем, экономика работает. Вот с таким слоем следующий революционный уровень должна быть языковой модели. А про собственность контента, если ты подписал своим приватным ключом текст, который вот только что написал, никто никогда тебе не отнимет его, этот контекст. Если вы работаете в некотором экосистеме, в некотором кластере предметном и создаете что-то коллективное, то в принципе, если вы шифруете данные и не делаете их общедоступными, вы можете продавать эту собственность кому-то. Можно себе представить, что был создан какой-то расчетный кластер с какими-то знаниями, с какими-то умениями, и внешние пользователи через языковую модель, будут обращаться к нему и получать какие-то знания за оплату. То есть это возможно, защитить знания в графе возможно шифрованием, но принадлежать будет всем пользователям этого кластера. То есть такое возможно, но так, чтобы вообще предметная область принадлежала, я с трудом себе представляю. 

А. КОЛОНИН [01:40:13]  : Спасибо. И еще один вопрос от Альгизара Талипова. На сегодня ЛЛМ не признается как удовлетворяющий элементарным базовым требованиям информационной безопасности. Насколько в этой ситуации можно говорить о доверии к этой технологии? 

А. БОЛДАЧЕВ [01:40:30]  : Я не знаю, почему. В общем, никакой безопасности нет. А какой безопасности? В смысле, есть два типа безопасности. Она взломает что-то, но это проблема не того, кто взломает, а того, кто защищается. Поставь себе языковую модель, чтобы она защитила от взлома. То есть против лома всегда находится лом этого же уровня. По безопасности. Если говорить о социальных вопросах, то что она там глючит, фантазирует, то это не проблема языковой модели. Она, кстати, решается в рамках такой сети. Любой текст, любой контент, Он принадлежит тому, кто подписал его. То есть не может появиться в сети текст, не подписанный Ивановым, Сидоровым, Петровым. Не может, в принципе не может. То есть модель не пропустит, если ты каким-то приватным ключом не подпишешь его текст. А с кем сгенерена этот текст? Может бабушки там что-то наплелите, может музыка навеяла, может чарджепетит. Это никого не волнует, ты отвечаешь за тот контент, который ты производишь, подписал своим ключом. Потому что, скажем, галлюцинаций всяких и фантазийных текстов у людей полно. Мы им верим, не верим, в зависимости от того мы узнаем, что вот это Вася написал, вот Вася такое сочинил. А то, что Вася там, его языкомодель напела это, но это другая проблема. Тут безопасности нет. Безопасность именно... Проблема безопасности пропадет тогда, когда каждый текст будет идентифицирован тем, кто его предложил. Он же будет его автором. Если он уникальный, этот текст. Также новостные модели. Если подписал Сидоров новость фейковую, конкретно подписал, никого не волнует, он сам придумал, ему заплатили за это или это ошибка языковой модели. Отвечаешь ты за то, что ты фейковую новость, ты конкретно отвечаешь по суду. И проблема решается. Я не вижу проблем, которые связаны именно с генерацией текста. Ну а плюс соединенная языковая модель с семантическим графом, в которой все факты зафиксированы, скажем, допустим, совсем идеальную историю будущего, когда вся деятельность реализована на семантических моделях, на моделях событийных, и они все записываются в граф. Тогда самой новости, которая пришла от человека, не будет. То есть если что-то произошло, это будет зафиксировано датчиками, камерами, расшифровано, записано текстом. И как таковой новости, вот там что-то взорвалось, человек, который про это будет писать, не нужен. Потому что все будет зафиксировано в сети, в определенном месте графа, идентифицировано, классифицировано, и уже ты будешь запрашивать LLM по конкретным фактам, а не через Васю, который тебе перепоет это. Так что здесь проблема небольшая, не вижу, она решается технологически. И криптографически. И семантически. 

А. КОЛОНИН [01:43:58]  : Хорошо. Александр, спасибо. Коллеги, ещё есть вопросы к докладчику сегодняшнему? Отлично, уложились? Да, мы уложились. Александр, большое Вам спасибо, тема большая, тема интересная. Мы продолжим говорить об этой теме в октябре месяце. 5 октября будет семинар по искусственному интеллекту и блокчейну. Вам, Александр, желаю успехов в вашем проекте и большое спасибо. Здесь спрашивают книгу к прочтению по вашей теме. Я так понимаю, вы можете сейчас еще раз показать слайд со ссылками. Да. И, может быть, рекомендуете какую-то базовую литературу по этой тематике? 

А. БОЛДАЧЕВ [01:44:47]  : Нет, я не знаю. Куча статей вот этих, а чтобы хоть кто-то соединял вместе три технологии, нет. Это нужно смотреть, значит, вот эти тексты и плюс за последние полгода мой фейсбук. Там много на эту тему. Именно вот и Лизаров ответ про безопасность есть, и про соглашение логики, языковые модели, то есть все это. Да, я еще не сказал, что это в работе, и в принципе это поддержано блокчейн-сообществом, людьми конкретными, аналитиками, профессионалами. Многие знают этот проект, и он поддержанный. Вполне возможно, мы скоро его увидим в реализации. 

А. КОЛОНИН [01:45:42]  : Хорошо. Успехов вам и большое спасибо. Спасибо огромное. Спасибо всем, кто слышал. Всем спасибо. До свидания. Всего доброго. 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
