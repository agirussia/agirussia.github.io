## 14 сентября 2023 - Как мультимодальные модели учатся понимать текст, картинки, видео и аудио - Андрей Кузнецов — Семинар AGI
[![Watch the video](https://i.ytimg.com/vi/hH8dcl62XsY/hqdefault.jpg)](https://www.youtube.com/watch?v=hH8dcl62XsY)




А. КОЛОНИН  [00:00:04]  : Коллеги, всем добрый вечер. Сегодня у нас в гостях снова представитель СБЕР и АИРИ Андрей Кузнецов, исполнительный директор по исследованию данных СБЕР ИАИ и научный консультант АИРИ. И он нам расскажет, как мультимодальные модели учатся понимать текст, картинки, видео и аудио. Андрей, пожалуйста, вам слово. 

А. КУЗНЕЦОВ  [00:00:28]  : Всем добрый вечер. Сегодня у нас достаточно интересная тема, потому что в целом в последнее время все инфополе очень сильно занято различными новостями про постоянно появляющиеся новые языковые модели, про разные алгоритмы, которые позволяют работать с текстом и картинками, связывать их как-то друг с дружкой. И позволяют работать в одной такой, скажем так, парадигме с точки зрения задач. То есть можно работать и с текстом, и с изображениями так, как будто бы одна архитектура может уметь или учиться понимать разные типы данных. И это действительно важно, потому что если говорить про стремление к созданию сильного искусственного интеллекта, его сила должна быть в том, чтобы максимально быть близкой к человеку, близкой к его основному пользователю этого сильного искусственного интеллекта и уметь воспринимать разную информацию, которая может приходить из внешнего мира. И на самом деле, если посмотреть на то, как работает мозг у человека и, в принципе, все его органы чувств и все его рецепторные функции. Мозг-человек умеет эту информацию из разных источников собирать воедино и работать с этой информацией одновременно. То есть мы можем ехать за рулем автомобиля то есть управлять автомобилем одновременно анализировать состояние на дороге и превращать это в реакции свои физические то есть там действия по движению руля, торможению, ускорению. Мы можем слушать, что вокруг происходит. Это тоже может давать нам дополнительную информацию. И тем самым человек, собирая из разных источников разные типы данных, он ими оперирует внутри себя, в голове, просто в виде импульсов. И с точки зрения импульсов, они уже для мозга абсолютно одинаковые, то есть не важно, что пришло извне, важно, какая обратная связь, обратная реакция возникает у человека. Ну и понятно, что с точки зрения искусственного интеллекта развитие алгоритмов, оно развивалось тоже, начиная от простых, от унимодальных задач, от задач, связанных с одним типом данных, с текстом, с картинками и так далее. И первые решения всегда были направлены на естественные и понятные задачи, например, распознать что-то на изображении, классифицировать изображение. выявлять какие-нибудь дефекты на картинках. С точки зрения речи, это задачи понимания, предсказания следующего слова в предложении, какие-то задачи, связанные с суммаризацией и так далее. То есть все то, что является наиболее часто встречающимися задачами среди инспекторов, тех задач, которые решает человек. И, конечно, если вот это все объединять, то, наверное, возникает обязательно вопрос, а как же с такими схемами, с таким обилием информации можно научить работать компьютер, потому что понятно, что для компьютера это все не так просто и однозначно, как для человеческого мозга. Ну и такое базовое видение, как это может работать, это когда у нас есть некоторая фундаментальная модель. И эта фундаментальная модель, она на вход принимает абсолютно разные типы данных. Это могут быть тексты, изображения, аудио, табличные данные, биометрическая информация двухмерная, то есть абсолютно из разных источников. каким-то образом внутри себя эту информацию переваривает и далее превращает эту информацию в конечные задачи, то есть в решение конечных задач, например, найти какую-то информацию, это information retrieval, отдельное поле для исследований. Это предсказательные задачи, регрессионный анализ, распознавание объектов, ну и конечно же синтез, то есть создание каких-то новых образцов из тех модальностей, с которыми умеет работать система. То есть очень важно научить модель не только принимать на вход какие-то типы данных, но и уметь их, собственно, воспроизводить. То есть человек абсолютно так же, он имеет возможность как что-то принять из внешней среды, так и это, в принципе, создать. Ну и, конечно, вот в настоящее время под фундаментальными моделями, то, что находится в центре этого графика, принимают большие языковые модели. Языковые модели, которые способны работать с текстом, способны работать с текстовой информацией и которые до определенного момента воспринимались именно как механизм создания какой-то естественной коммуникации с человеком, в режиме чат-бота. Ну и не секрет, что сейчас это и чат GPT известное приложение, и приложение GigaChat, которое мы не так давно анонсировали нашу модель, точнее ансамбль моделей. И, конечно, сегодня хочется поговорить и сделать упор на то, как работают эти языковые модели, в каких режимах они могут работать. И доклад я решил построить следующим образом. Сначала расскажу про то, в каком режиме чаще всего сейчас используются языковые модели с точки зрения работы с разными типами данных, то есть не только текст, но и картинки, аудио, видео. Потом поговорим про то, как можно перейти к естественной мультимодальности, то есть другой подход к работе с разными типами данных. И в заключении, скажем так, в последней части будем говорить про то, как именно синтезировать данные. Сделаю небольшой такой уклон в сторону нашей модели Кандинской. который умеет синтезировать по тексту визуальную информацию вот будет такая сборная солянка из разных из набора ссылок из набора какой-то информации по вот этим проблемам ну и наверное даст такое общее представление о том как в настоящее время развивается направление мультимодальных исследований и куда мы все движемся. Начнем с такого направления, как использование языковых моделей, как оркестратор. Что такое оркестратор? Это по сути механизм, который позволяет отдавать инструкции во внешнюю среду, а в этой внешней среде у нас могут находиться разные какие-то подсистемы, которые на вход имеют определенный интерфейс, ну и на выходе выдают то, что предназначено с точки зрения цели вот этой внешней системы. То есть, что она должна решить какую-то задачу или ответить на выдать output на входящий input. И в данном случае языковая модель она выступает в роли такого реального администратора, то есть она умеет очень хорошо оперировать текстом и создавать из этого текста, из простого запроса пользователя, правильный формат для выходных систем. Ну вот начнем с краткого обзора того, как работает один из самых известных, наверное, в настоящее время таких фреймворков, который разработан компанией Microsoft. Он построен на, собственно, модели ChatGPT, то есть построен вокруг языковой модели. И задача этого фреймворка – это научить работать языковую модель с разными модальностями, при этом не наделяя языковую модель пониманием явным этих модельностей. Это значит, что вокруг языковой модели возникает ряд каких-то экспертов, или экспертных подсистем, которые, как я сказал, могут принимать на вход определенную информацию и давать на выход соответствующие решения по этой задаче. Ну вот здесь написан пример список решаемых задач. Например, это ответы на различные математические и текстовые запросы. с использованием графической информации для того, чтобы ответ был аргументирован. Понимание пространственного расположения объектов. Формирование каких-то аргументаций, если у нас… Все, вот так сделаем. Формирование различных аргументаций, если у нас есть несколько изображений. Визуальное планирование предсказания – это попытка описать, по сути, изображение текстом и из этого описания делать какие-то выводы. Ну и так далее. И все вот эти подсистемы, они умеют на вход принять какую-то информацию и на выходе предоставить информацию в текстовом виде так, чтобы модель текстовая умела работать с этими данными так, как будто бы она не видит, но по описанию может понять, что это такое. В естественной среде это выглядит так. Например, человек увидел какую-то ситуацию, он ее запомнил, И никакой футурологической информации у него нет, то есть фотографию он не сделал. Он просто звонит, допустим, или пишет своему другу и описывает просто текстом, что произошло. И у человека начинает в голове складываться из этого текстового описания некоторое представление о сцене, о том, что произошло, о событиях просто из общих своих знаний и пониманий. Ну и задача как раз научить языковые модели правильно промтить или создавать правильные текстовые входы для вот этих систем, точнее, правильные входы для этих систем для того, чтобы получать правильные текстовые выходы и наполнять контекст диалога этой информацией. Ну вот здесь на примере показано, как можно добавить в языковую модель понимание картинки. Вот у нас есть, соответственно, картинка, которая передается в несколько подсистем, и эти подсистемы выдают на выходе текст. Ну то есть вот подсистема Image Captioning, которая описывает изображение в целом. Что здесь есть на изображении определенного разрешения, баскетбольный игрок баскетбольной команды в желтой одежде с мечом в его руке. Дальше происходит обнаружение объектов на изображении, и эта информация превращается в описание в виде координата. Дальше мы навешиваем различные теги на изображение, то есть это работает отдельная подсистема, которая по картинке учится точнее, выдаёт информацию о том, что это за картинка, что это спорт, что это какой-то вид игры, что это баскетбол и так далее. Далее мы распознаём лица на изображении, если можем, описываем их локейшн, их координаты на картинке. Дальше мы можем, соответственно, эти изображения распознать, распознать известных людей на них и так далее. И вот эта информация формирует контекст, И в итоге Чаджик Дим может выдавать информацию, что на изображении находятся два известных человека, это Коби Брайн и Пол Пирс, и они оба баскетбольные игроки, и у них, собственно, вот так описывается сцена. Далее, как только эта информация для chat.gpt превращается уже в текстовое описание, пользователь может задавать вопросы так, как будто бы он задает вопрос картинки. То есть он может спрашивать, сколько чемпионатов прошло так, чтобы один из игроков, например, чтобы игрок слева, сколько чемпионатов выиграл игрок слева, и так далее. И модель, уже получив информацию о картинке из контекста, она начинает уметь с ней работать и отвечать на пользовательский вопрос. Ну и так далее. Так диалог строится очень долго, и чем больше у нас таких подсистем, тем более обширной становится описание. В целом, мы по такому же принципу попробовали присоединить к языковой модели GigaChat, в частности, нашу модель Кандинский, которая имеет по текстовым описаниям генерировать картинку. Это сделано для того, чтобы, например, в ходе общения с пользователем в контексте диалога в какой-то момент он может попросить нарисовать картинку, и модель отправит этот запрос, формулирует из этого запроса. сформирует из этого запроса правильный вход для генеративной модели, и генеративная модель, соответственно, нарисует картинку, вернет ее в чат. Таким образом, модель научилась работать в одну сторону. Она умеет генерировать картинки. То есть в данном случае она пока не понимает, что на изображении, но она может из контекста, из текстового диалога понять, что нужно пользователю для того, чтобы нарисовать. И в этом смысле для пользователя это выглядит достаточно просто. Идет диалог, и в какой-то момент пользователь просит нарисовать ситуацию, которая описывается здесь. Например, опиши специальную теорию относительности по-пацански. Значит, она описывает это все, и далее пользователь просит нарисовать эту ситуацию. Ну и, соответственно, Гигачат, понимая, что слово «эту» относится к той ситуации, которую до этого он описывал в контексте диалога, он рисует соответствующую картинку. Казалось бы, в целом выглядит все очень органично, но не очень понятно, для системы не очень понятно все-таки, что такое изображение. То есть, можно научить модель считывать вот этот текст, получать эту информацию из внешних систем, делать разные механизмы, там, кэпшинг, распознать объекты, детекшн какой-то сделать сверху, лица распознать, узнать людей, писать максимально детально, но все равно это не нативно. То есть, все равно модели в этом смысле не умеют и не могут понимать картинки так, как это умеет делать человек. Для человека картинки – это уже естественная среда, это практически свой язык. Даже в некоторых статьях по мультимодальности картинки называют таким словом имглиш. То есть имидж и англиш – это как от английского оставшаяся часть. Получается, что это как будто новый язык для языковой модели. И сложности, которые возникают, когда мы работаем в режиме оркестратора, выглядят следующим образом. Мы имеем достаточно ограниченный список решаемых задач. и превращая там картинку, видео, аудио, любую неизвестную модальность в текст, мы ограничиваем модель только вот теми внешними системами, которые выполняют задачи описания в текстовой форме тех или иных данных. И понятно, что чем более обширной мы хотим создавать описание, тем больше у нас должно быть вот этих внешних систем, тем тяжелее и дольше будет становиться inference, то есть вычислительные затраты на ответы пользователя, потому что придется все больше и больше алгоритмов запускать в параллели. И, конечно, это все утяжеляет весь процесс, при этом не сильно гарантируя нам бесконечное количество возможных задач для работы с изображением и работы с таким контекстом. Ну и, в принципе, если представить, то это не супер естественная форма коммуникации. Ну вот здесь показан пример. двух диалогов. Система пишет, что... Точнее, пользователь пишет, что он гулял сегодня со своей собакой, присылает фотографию. И пользователь пишет, что мне нравится цвет ее хвоста, и система отвечает. Да, коричневый выглядит реально, ей подходит. но на самом деле он белый. И в мультимодальном варианте это выглядит иначе, то есть мы начинаем картинку понимать, и в мультимодальном варианте мы уже видим, что система понимает, что цвет белый, и, конечно же, это более естественно и понятно для человека, то есть возможность совершить ошибки куда меньше для таких систем. Естественно, говоря про нативную мультимодальность, я сделал небольшой овервью систем, алгоритмов и архитектур, которые вышли только в этом году и которые направлены на то, чтобы объединять несколько модальностей и работать с ними одновременно. Я по ним кратко сейчас пройдусь. Просто покажу, какие подходы в целом сейчас присутствуют и как работают разные актуальные решения с такого рода задачами. С задачами объединения в единый контекст модальностей, разных модальностей и текстной модальности. Одно из первых решений, которые хотелось бы показать, это архитектура Fromage. Здесь представлена тоже ссылка на статью. Смысл заключается в том, что у нас есть некоторая замороженная языковая модель. Это значит, что ее веса никаким образом не обучаются, когда мы добавляем в нее новую модальность и мы встраиваем туда новый визуальный энкодер который позволяет нам входящую картинку вот например как здесь на схеме входящую картинку превращать просто вектор чисел или так называемый embedding, или вектор признаков, по-русски говоря, и встраивать этот вектор признаков в контекст, который попадает на вход языковой модели. То есть мы начинаем пробовать через специальные токены, вот здесь возникают специальные токены, имиджей и какой-то индекс. Встраиваем туда информацию об этом изображении. И запуская обучение в режиме Funtune, мы получаем, что ответы, которые формирует модель, она уже начинает воспринимать и понимать с учетом вот этой визуальной информации. Для того, чтобы научить модель работать с картинками, понятно, нужно всегда выбрать какую-то базовую задачу, в которой объединяются картинки и текст. Если мы работаем обычно с текстом, то там, как правило, возникают понятные задачи. Это предсказание следующего токена или следующей части слова или слова в предложении, когда идет какое-то длинное предложение, и трансформер предсказывает следующее. наиболее вероятное слово для продолжения контекста. И вторая языковая задача классическая – это когда нужно найти пропуски и заполнить пропуски в предложении. То есть пропущены какие-то слова или части слов, и нужно их восполнить. То есть модель учится на таких задачах. Что делать, когда у нас есть картинки и текст? Вот в этом случае, как правило, используются две базовые задачи, это Image Captioning, описание изображений, и Image Text Retrieval, когда мы хотим получить, используя там описание и картинку, какой-то текст, который может нам дать общую информацию об этой паре информации, об этой паре входных данных. Соответственно, здесь возникает два таких важных момента. Это, соответственно, маппинг визуальных вот этих векторов, признаков от картинок в текстовое пространство и специальный токен для изображений, который позволяет вот этот эмбеттинг через текстовое описание превращать в визуальный эмбеттинг для того, чтобы найти, насколько они близки, сколько текст, близок к описанию или, наоборот, далек. Это так называемый контрастив подход, который позволяет нам, собственно, текст и картинку связывать в одном векторном пространстве, по сути, и смотреть, насколько они близки или далеки. Здесь есть специальный токен RED. И вот когда мы подаем на вход такие данные в обучение MuchText Retrieval, у нас здесь есть описание силуэт самолета на фоне восхода, заката, и картинка, соответствующая этому тексту. И, соответственно, здесь контрастивый подход должен дать нам близкое расположение этих. этой картинке текста, и тем самым модель начинает не просто учиться там продолжать диалог и описывать изображение как вот в первой задаче, но еще и находить близость между картинкой и текстом. Ну и, собственно, вот по такому принципу работает эта модель. она выложена в открытый доступ, по ней есть вся информация о том, сколько она обучалась, то есть у нее не сильно большое количество итераций на обучение, она всего лишь обучалась один день на одной карте А6000. Ну и параметры модели тоже не сильно наворочены, но при этом по различным показателям качества модель занимает в принципе неплохие строчки по ряду задач. Есть метрика как раз по реколу для топ-1, топ-5 и топ-10 ответов. Интересная модель, которая расширяет возможности языковых моделей, добавляя туда возможность генерации, называется GIL. Это интересно над тем, что это одно из немногих решений, которое позволяет работать с моделью языковой, так чтобы она могла все-таки еще и генерировать наружу картинку, потому что в основном все модели сейчас строятся по принципу понимания картинки на вход, то есть учитывать ее в контексте текстово-картиночного диалога, но не генерировать. И вот здесь в качестве генератора используется модель Stable Diffusion 1.5, на им. Авторы придумали механизм, который позволяет превращать из текстовых описаний формировать эмбеддинг картинки в пространстве признаков стейбл диффьюжн. То есть стейбл диффьюжн, если вдруг кто не знает или не интересовался, то это модель, построенная на архитектуре латентной диффузии. И латентная диффузия означает, что мы картинки восстанавливаем и работаем с картинками в пространстве латентном, в пространстве картиночных эмбэдингов, в специальном векторном пространстве, которое для диффузии является естественным. И задача как раз генерировать вот эти спецтокены так, чтобы можно было из них потом восстанавливать через диффузию нужную картину. то есть эмбэдинги кодируются трансформер-энкодером, а трансформер-декодер уже потом выдает через линейный слой некоторые предсказанные эмбэдинги, и вот эти эмбэдинги уже через диффузионную модель могут превращаться в картинку. тем самым модель научилась работать с картинками и текстом, и картинки просто хранятся в определенном пространстве признаков, с которым умеет работать Stable Diffusion. Тем самым мы научились генерировать картинки. Это действительно важный и интересный подход, и он позволяет расширить модель Fromage, которая была на предыдущем слайде, до возможности генерировать изображение. Компания Google представила в этом году модель на основе своей большой модели Palm. При этом модель закрыта, веса ее закрыты. Понятно, 540 миллиардов параметров – это достаточно большой объем, чтобы его можно было где-то проверить, извести. Они добавили туда возможность работать с псевдонепрерывными данными сенсоров. Это означает, что языковая модель уже начинает уметь оперировать в таком контексте какими-то физическими устройствами. То есть можно, например, просить ее управлять роботом, что-нибудь открывать, выполнять какую-то операцию на конвейере. Можно учить ее поднимать кубики определенного цвета, то есть работать с манипулятором. И помимо текста изображения у нас получается, что добавляется еще одна модальность. То есть до этого мы говорили только про две модальности, теперь у нас появляется третья модальность. Это некоторый такой секвенс, последовательность каких-то событий, последовательность состояния среды. с учетом трехмерной информации о сцене, для того чтобы можно было управлять и давать правильные инструкции для манипуляторов, например. И, конечно, здесь каждая сцена она представляется в виде набор векторов которые описывают каждое состояние этой сцены ну и вот контролирующий там управляющего воздействия который возникает из как выход языковой модели вот она здесь показано это достаточно там сложно интересная задача но вот при этом к языковой модели для работы с двумя новыми типами данных получается это изображение и вот эти состояния среды которые по сути тоже просто набор изображений понадобился всего лишь один визуальный трансформер там на 22 миллиарда параметров и при этом суммарно все равно языковая модель занимает больше существенно большее количество параметров То есть это говорит о том, что текст значительно важнее научиться понимать, нежели добавлять туда что-то новое. То есть если у тебя сильная языковая модель, которая хорошо понимает текст, то научить ее в пространстве признаков оперировать, скажем так, новым, незнакомым ей подпространством или языком с точки зрения модели, в принципе, не должно быть сильно сложной операции. Ну и вот здесь классические задачи VQA – это visual question answering, ответы на вопросы по картинкам и описание изображения. Это все то, что нужно как раз для того, чтобы в том числе оперировать состоянием внешней среды. Ну и вот последовательное выполнение или планирование операций для роботизированных разных устройств. Ну и вот в центре можно посмотреть на картинке, что если у нас есть на вход вот такое состояние среды в виде вот эмбейдинга зеленого с вот такой вот картинкой, которая вот таким образом кодируется, как взять синий блок. вот ну и дальше начинается там как это выполнять как это операцию нужно сначала взять желтый блок который сверху лежит на синим и потом там положить его сбоку потом взять и не так далее ну и вот это все превращается в определенной инструкции которым уже работает который выполняет собственно манипулятор Космос-1 еще одно решение, которое работает не только с картинками и текстом, но еще и с аудио информацией. В принципе, это тоже расширение архитектуры Fromage. Здесь достаточно схожий механизм работы с новыми модальностями модель тоже не сильно большая всего лишь 1.6 миллиардов параметров и опять-таки для кодирования визуальной информации используется визуальный трансформер вот для обучающей выборки тоже используется там не гигантское количество токенов всего лишь 360 миллиардов ну вот для ряда задач эта архитектура получила гораздо более высокие SOTA, можно сказать, результаты. Это Image Captioning, это Visual Question Answering, ответы на вопросы по картинкам. Reasoning – это объяснение ситуации по изображению. И мультимодальный Chain of Thoughts – это когда модель должна ответить на вопрос используя какую-то скрытую информацию и построив какую-то цепочку логических выводов для того, чтобы ответить на финальный вопрос. вопрос не всегда содержится напрямую в картинке. здесь есть несколько примеров. самое интересное из них это то, что модель начинает учиться решать тесты на логику, но тесты IQ имеют определенный алгоритм решений. Здесь надо аккуратно смотреть и исследовать, не использовались ли эти данные где-то в ходе обучения, чтобы она не перемучилась и не научилась вот этим вот механизмам подбора логических каких-то систем и дополнений. Но так или иначе, картинка умеет понимать вход, понимать на вход разные ответы и сопоставлять картинки между собой для того, чтобы получать предсказания о том, что же должно быть заполнено в той или иной области. В данном случае у нас опять-таки в эмбеддингах получаются разные типы данных. Здесь у нас есть и язык, как я сказал, и картинки, и какая-то аудиоинформация. Модель Lama, которая была выпущена в открытый доступ и Lama 2 сейчас, которая еще и получила возможность использования для коммерческих целей, вокруг нее тоже построилось очень много интересных расширений, которые научились работать с картинками. Они являются просто такими надстройками над языковой моделью и, по сути, механизм скажем так работы с этими моделями он построен просто на разных вариациях сгенерированных существующими моделями каких-то описательных инструкций в каком-то случае это там чат GPT, в каком-то случае это GPT-4, и разные модели используются для того, чтобы создавать правильные синтезированные выборки для инструктивного взаимодействия картинка-текст. Но с точки зрения архитектуры это все тот же самый подход, используя дополнительные специальные токены, учить модели через инструктивные описания, учить модели работать с изображением. Ну и здесь много разных вариаций. Здесь вот только часть из них я перечислил, которые были активны в информационной среде. Это лава, альпака, виккуна, разные варианты надстроек над ламой. еще один вариант работы на основе открытой модели окун фламинго это научить модель опять-таки работать изображение здесь все то же самое тот же самый подход ничего не меняется здесь единственное достаточно существенно больший объем обучающей информации, и некоторая интересная архитектурная операция, которая позволяет ускорять обучение – это то, как превращать картинки в текст. Перед попаданием в языковой блок здесь используется специальный гейт от X-Attention, который как раз из архитектуры фламинго позаимствован, это собственно ее расширение, и здесь есть ряд cross-attention слоев и self-attention, которые создают правильное векторное описание для языковых моделей, то есть просто другой механизм создания картиночных эмбеддингов, ну или там векторов признаков о новой модальности. Здесь, соответственно, в архитектуре визуального энкодера 5 от Клипа есть информация о пространственном расположении объектов или пространственные признаки. это персивер и семплер, ну и соответственно лор это low rank адаптеры которые добавляются как раз в self-attention для того чтобы это вот эти вот штуки которые позволяют снизить пространство признаков и ускорить обучение собственно плюс одно решение не так давно вышла модель Falcon Она вышла в двух версиях, и вот буквально недавно вышла еще одна версия на 180 миллиардов параметров. Обучена на достаточно хорошем и вычищенном датасете, который называется RefinedWeb. Это датасет, который содержит смесь картинок и текстов из интернета, просто обкачанные сайты. Их еще называют interleaved данные, то есть перемешанные. обучена на ряде языков в основном это английский немецкий испанский французский но что интересно мы проверяли ее работоспособности на задачах на русском языке и она в принципе на ряде задач достаточно успешно справляется это тоже еще одна особенность мультимодальных моделей мультиязычных моделей которые возникает в том что вот какие-то слова и какие-то токены они все-таки для разных языков попадают в одном векторном пространстве, оказываются в одинаковых местах, и тем самым даже новые языки, возникающие, они в принципе становятся понятны для модели, для определенных задач. Ну и в принципе это тоже для человека характерно, если представить, что ты знаешь какие-то общие сведения, допустим, из английского языка, то некоторые слова на испанском, на итальянском, они могут быть тебе знакомы просто потому, что это все про образы каких-то латинских групп. И в целом в этом есть определенные корреляции с тем, как человек воспринимает информацию. модель показывает очень хорошие результаты на открытых лидербордах и вот там новая версия модели которая там на 180 миллиардов конечно существенно тяжелее и в современном мире когда там максимум модели выходит в открытый доступ от 70 миллиардов 180 выглядит как на такой спалин но он там получает какой-то выигрыш в соответствии там с оценками на разных задачах, получает определенный прирост в качестве. Есть небольшое сравнение от авторов. Они показывают, насколько хороша их модель, и что у нее обязательно уже есть возможность коммерческого использования, и сколько токенов используется для обучения, и насколько она хорошо сжато, там есть специальный механизм кэширования вокруг attention, который позволяет существенно уменьшать размер модели для inference. Ну вот можно видеть, что там размеры прям отличаются в сотни раз за счет этого механизма. Например, 40-миллиардная модель весит всего лишь 240 мегабайт, что позволяет, собственно, ее запустить практически на доступных Google коллаб. Интересная модель, которая научилась работать уже с видео. Здесь появляются специальные токены времени, то есть понятно, что как только у нас появляется новая модальность и новое пространство, новое измерение, надо научиться это измерение правильно кодировать. Ну вот здесь появляется помимо спешл энкодера, то есть пространственного еще и временной энкодер, который кадры из видео начинает связывать в единую последовательность. То есть получается такая последовательность вот этих эмбейтингов, которые формируют контекст всего видео. И далее задача возникает в том, чтобы восстанавливать, например, эти кадры и генерировать описание. Ну вот, две задачи на pre-training, как я уже начал говорить, это генерация описаний к входному видео и denoising, то есть заполнение дырок в описании для пар видео и соответствующий текст в описании. То есть у нас есть видео, у нас есть соответствующий текст, и мы из этого текста какие-то слова выдергиваем для того, чтобы научить модель связывать видео и В принципе, среди базовых задач не очень понятно, почему не стали пробовать делать контрастив обучение, что в принципе тоже дало возможность получить модель в таком контексте, в контексте понимания языка и картинки. Но в целом тоже подход достаточно интересный и не сильно много обучаемых параметров, всего лишь 314 миллионов. Если смотреть на задачи, на которых мерилось качество, то описание событий во временном разрезе и описание сцен, описание видео, состоящих из различных сцен, на этих бенчмарках модель уже достигла соты результатов, что тоже интересно. Вообще, на самом деле, очень большое внимание в обучении мультимодальных архитектуры, мультимодальных моделей всегда уделяется данным. И недаром ребята из команды Falcon, они говорили здесь про свой датасет RefinedWeb, и большое внимание уделили тому, как долго они его фильтровали. То есть это действительно непростая процедура, и частота данных очень сильно влияет на качество обучения. Про это, наверное, можно делать отдельный доклад и рассказывать, какие механизмы есть фильтрации данных и какие способы есть для того, чтобы очищать данные и снижать их объем. Но я думаю, что об этом можно как-нибудь в другой раз поговорить. Еще одна архитектура, которая называется Composable Diffusion. Эта архитектура позволяет объединять и картинки, и текст, и видео через текстовую модельность, по сути. И, похоже, архитектура, ее здесь не представлено, Про нее сейчас тоже не так давно выходило много интересных новостей. Называется ImageBind. Это архитектура, которая через текстовую модальность позволяет связывать другие модальности. И это, на самом деле, тоже интересный подход. Коль мы хотим научить языковую модель работать с картинками, то почему бы нам не попробовать научить понимать ее таким образом, чтобы в рамках векторного пространства мы могли находить соответствие между картиночным вектором и текстом описания. И, по сути, это как будто бы задача Image Captioning, но с точки зрения векторного пространства мы можем емкость данных лучше контролировать в ходе обучения. Мы можем, собственно, получить информацию об изображении с точки зрения вектора более содержательно, чем просто описание из нескольких токенов. Ну и вот здесь показан, как модель работает из аудио, и через текст она переходит в описание картинки и так далее. Что самое интересное, все механизмы работы с разными модальностями построены по принципу диффузии. И здесь четыре модальности в рамках вот этих диффузионных архитектур. связываются и здесь есть специальный такой bridging alignment подход который позволяет связывать значит кодированное представление изображений аудио либо видео с текстом энкодером И потом этот текстовый энкодер становится таким центром переноса информации из одной модальности в другую. И возникает вторая стадия joint generation, когда мы можем через вот этот текстовый энкодер переходить в разные другие модальности. Например, в визуальную, в аудио модальности и так далее. При этом вот этот многоэтапная процедура обучения она как раз позволяет обучаться на линейном наборе каком-то задач и потом комбинации из этих задач или там разные новые задачи возникающие на пересечения описания этих задач и для таких же входных и выходных данных, эта архитектура может вполне справляться. То есть обучая вот эти энкодеры для пар модальностей A, B и B, C, то есть когда у нас есть задачи для модальности A, B и B, C, мы тем самым получаем связь между модальностями A и C. И соответствующие задачи, которые возникают между модальностями A и C, мы получаем более естественным путем. это очень хорошо когда нам нужно работать например с картинками и аудио потому что создать такой датасет очень тяжело так чтобы связать аудио и картинку если это просто не видео да какой-то естественно где обычно там речь и какие-нибудь звуки но достаточно большое количество информации среди открытых датасетов, оно как раз, если искать, то можно найти между текстом и аудио, это понятно, там расшифровка и описание, и между текстами и картинками линиеративных моделей. И таких датасетов реально очень много. И для того, чтобы получить датасет между картинками и аудио, нам достаточно связать вот эти две задачи через какой-то промежуточный энкодер и получить информацию между новыми двумя модальностями – картинка и аудио. Мы в этом направлении тоже работаем и в настоящее время исследуем архитектуру GPT для того, чтобы можно было работать с картиночной информацией, с визуальной. Формируем диалоги мультимодальные, которые состоят из описаний текстовых и соответствующих картинок. И учимся воспринимать визуальную информацию через специальные линейные слои. Проводим ряд экспериментов для того, чтобы научить модель на русском языке понимать картинки, понимать текст и вести диалог в такой естественной среде. В принципе, к этому идет все развитие чат-ботов и мультимодальных чат-моделей. С точки зрения данных для обучения пару примеров приведу. Пользователь и среда, они общаются с использованием фотографий для того, чтобы увеличить собственный контекст. Вот, например, там внизу, где живут медведи. какой вид медведя интересует, в качестве ответа пользователь отправляет в серединке какую-то картинку, ну и дальше уже модель отвечает, где обитают эти медведи, что это за вид и так далее. То есть, картинка становится частью контекста. Либо мы подаем на вход изображение и пытаемся вести диалог, не связанный с этим изображением, И проверяем, насколько контекст влияет на то, чтобы модель не спутала, не осталась вот в том контексте, а при запросе об изображении смогла ответить правильно, не учитывая, что мы до этого говорили вообще про другого человека. Ну и понятная задача – это когда у нас есть изображение, и для него есть вполне конкретный диалог, писательного характера, то есть такой visual question answering диалог, когда мы задаем вопрос по картинке, и в то же самое время подключаем какую-то фактологическую информацию, которую знает модель о месте, о том, какая погода, где это находится и так далее. Как правило, для оценки качества используется метрика BLUE. Она взята из задач машинного перевода для того, чтобы оценить, насколько текст в переводе и исходный правильный ground true соответствуют друг другу. И в зависимости от того, сколько N-грамм кандидата и вот этого референтного или ground-true текста мы сравниваем, мы по этой метрике можем определять, насколько ответы модели близки к тем, которые мы ожидаем в рамках мультимодального диалога. Мы сравнивали в качестве первых экспериментов результаты нашего подхода, с которым мы сейчас эксперименты проводим, по этой метрике, используя в рамках GPT архитектуру GPT-J. сравнивали с архитектурой Fromage, это базовая архитектура на английском языке, и получили существенно большее качество на датасете Kuko. Это датасет описаний и картинок. соответствующих этим описаниям. И на русской части этого датасета, на переведенной, мы попробовали провести эксперименты на тоже ряде наших моделей русскоязычных, и модели Saiga тоже получили достаточно хороший результат с точки зрения метрики. В целом это направление сейчас мы продолжаем развивать и делаем ряд экспериментов, которые в целом свидетельствует о том, что языковую модель научить работать с новыми модальностями можно, и это рабочая схема, и это направление надо развивать, потому что оркестрация, как я уже сказал, имеет ряд ограничений, которые не позволят использовать мультимодальные диалоги в каком-то простом и понятном интерфейсе. вот примеры работы которые сейчас есть по этому направлению. здесь представлен ряд диалогов с использованием изображений. ну и как я обещал хочется немного рассказать и про генеративную составляющую мультимодальных моделей, потому что если мы говорим, отвечаем, пытаемся ответить на вопрос, как мультимодальные модели учатся понимать, очень важно понять, может ли модель еще и сгенерировать то, что она понимает на вход. Несколько примеров таких архитектур было, в частности, ГИЛ. Я покажу наши исследования, наши эксперименты, которые получаются в ходе исследования генеративного искусственного интеллекта в части мультимедийных данных. В этом году у нас вышло две модели. Это Kandinsky 2.1 и 2.2. Они отличаются по сути стилями и некоторыми архитектурными особенностями, плюс датасетами, которые использовались для обучения. При этом для конечного пользователя разница, как правило, заключается именно в том визуале, который создает модель. Если мы говорим про модель 2.1, то она ориентирована больше на создание картинок в таких художественных красивых стилях. Тут могут быть акварели и просто прорисовки такие художественные, портретные, ну и какие-то такие игрушечные стили, которые не супер фотореалистичны. Если говорить про модель 2.2, то она как раз наоборот больше направлена на то, чтобы генерировать фотореалистичное изображение, и при правильных описаниях она может создавать действительно очень близкие к фотографиям картинки. Здесь вот можно увидеть несколько примеров таких генераций. Модель работает в нескольких режимах. Она может генерировать изображение по тексту на русском и английском языке. она умеет смешивать картинки между собой для того, чтобы формировать новое изображение, и это достигается за счет работы в латентном пространстве, в пространстве признаков, и тем самым можно заметить, как, смешивая в векторном пространстве два вектора, можно при декодировании этого вектора получать вполне себе близкие к обоим картинкам картинкам изображения. Режим смешивания картинки и текста – это практически тот же самый механизм, только здесь кодируются текст и картинка в одном векторном пространстве, и при смешивании возникает картинка, создается картинка, которая находится на перекресте, на суперпозиции векторов текста и картинки. режим вариации изображений это возможность создания похожих картинок когда у нас есть некоторый вектор мы добавляем к нему определенную долю шума небольшую и восстанавливая картинку получаем вариацию того же самого изображения полезный для людей, кто занимается фотографией и редактированием фотографий режим InPainting. Мы можем взять или создать изображение, какую-то часть, которую на этом изображении мы нашли, мы хотим выделить и заменить на что-то. Мы можем выделить эту часть, мы можем ее закрасить пластиком и, собственно, через описание изменить эту область, добавить туда какую-то нарисованную новую часть, тем самым это получается такое редактирование через текстовое описание. Этот режим, если развернуть и выставить в обратную сторону, то это будет механизм outpainting, когда мы хотим картинку снаружи дорисовывать, мы хотим все время расширять полотно, например сгенерировали картинку и хотим эту сцену дорисовать, дорисовать, дорисовать. И с помощью такого механизма можно дорисовать картинку снаружи. Это тоже интересное и полезное свойство генеративных моделей. Создавать и генерировать конкретных людей тоже одна из фишек генеративных моделей. Заключается в том, что не всегда обязательно и нужно обучать модель на определенном человеке. Можно использовать другие подходы. У нас есть еще модель, которая позволяет переносить лицо с фотографии на фотографию либо видео. и в принципе создать такой тип фейк до картинки в данном случае у нас для человека есть некоторое описание его внешнего вида и через это описание мы можем создать условно его двумерный аватар на картинке соответствующий нам телосложение формы головы, цвета глаз, цвета волос, прически, стили прически и так далее. И, используя механизм переноса лица с фотографии на фото или видео, перенести уже портрет этого человека, лицо на сгенерированную картинку. Ну и вот механизм фотореалистичности, он позволяет как раз добиваться качественного переноса, потому что сгенерированные таким стилем изображения, они очень хорошо подвергаются переносу лица, потому что если у нас здесь были бы какие-то мультяшные изображения или что-то такое из художественных стилей графики, то процесс переноса лица, он бы был не всегда идеален и это бы бросалось в глаза. Но с учетом фотореализма и фотореалистичных промптов этот механизм работает очень хорошо. Игрушечный режим чисто для каких-то маркетинговых вещей – это просто генерация стикер-паков в Телеграме. Ими можно обмениваться, генерировать, их делиться и так далее. Интересный режим генеративных моделей, который называется control net. До этого я показывал механизмы, где картинка изменяется полностью, когда мы смешиваем с другим изображением или когда меняем ее по тексту. В данном случае мы можем сохранять структуру сцены. То есть, сохранятся все контура, в данном случае изображения, и изменится его стиль и его детали. То есть, все объекты будут находиться в тех же самых местах, сцена и структура сцены сохранится, но в соответствии с текстовым описанием это изображение может полностью измениться. Ну и вот можно видеть, что человек после режима этого изменения, он сохраняет свое положение, взгляд, все, кроме каких-то деталей, которые уже описываются текстовым описанием, текстовым промптом. Здесь смысл заключается в том, что в рамках векторного промежуточного латентного пространства у нас появляются некоторые барьеры в виде карты контуров, которые не позволяют при генерации эти области изменять, тем самым мы сохраняем возможность этим частям векторного описания оставаться неизменными в нужных местах, это такой уже подход или попытка перейти уже в некоторую интерпретацию является сейчас очень интересной и важной задачей для мультимодальных исследований, это как заставить модель понимать пространство, и векторное пространство было бы интересно каким-то образом семантически интерпретировать. ну поиграться с моделями можно в двух местах даже больше использующий пространство салюты сайту удали точка ру много есть источников где можно использовать модель с точки зрения применения этих моделей генеративный и сейчас это наверное самое популярное направление и для него очень много прикладных различных вещей и решений, которые можно в разных, скажем так, приложениях находить применение этим моделям. Это генеративные какие-то визуалы, это различные маркетинговые кампании, это какие-то открытки, игры карточные, все что угодно. С точки зрения дизайна, это может быть и дизайн вещей, футболок, бижутерии, объектов дизайна, интерьера. Все, что можно, в принципе, генеративные модели сейчас имеют и могут покрывать. Интересно, что если выходить за рамки простой генерации картинок, то у нас есть интересная коллаборация с лабораторией робототехники, где сделали дополнительный модуль, который умеет векторизовать картинку. превращать ее в svg и соответственно робот может отрисовывать эту картинку потому что он работает в векторном режиме и для него возможность отрисовать возникает только когда есть векторная картинка Ну и, соответственно, механизм превращения растро-векторной картинку, этот модуль, он позволяет построить такой симбиоз генеративной модели и визуализации сразу в реальном времени через механизм физического манипулятора. Еще несколько примеров. Это, понятно, наружная реклама, потому что здесь нужны сочные, красивые, интересные картинки. Это различные иллюстрации для каталогов могут быть. Ну и направление синтеза картинок по текстному описанию, оно, конечно, уже дошло до достаточно высокого качества и сейчас идет борьба за определенные стили, за какие-то моменты связанные с пальцами, с отрисовкой текста и так далее. То есть это все сложные еще задачи, которые остаются, но в таком дженерал домене изображение генерируется достаточно хорошо и возникают даже вопросы и задачи того, как отличить синтезированный контент от естественного. Есть несколько механизмов и направлений исследований в области синтеза видео, которыми мы сейчас тоже занимаемся. Это создание анимации по текстовому описанию и создание видео по текстовому описанию. Здесь у нас уже текст превращается в некоторое пространственно протяженный набор связанных кадров. Это сложная задача. и на данный момент нет ни одной модели, которая бы визуально создавала что-то действительно качественное, но в принципе с картинки мы начинали точно так же. первые модели, которые генерировали изображение по тексту, они тоже были далеки от идеалы и генерации были в большинстве своем достаточно мягко говоря, некрасивый. Собственно, анимация – это механизм, который позволяет на основе какой-то картинки сделать ее изменения во времени, да, через разные механизмы анимирования и создания визуальных эффектов движения, приближения, отдаления и так далее. Как раз используют механизмы пейнтинга и аутпейнтинга, про которые я там чуть выше говорил, показывал слайды. И, конечно, это все более такой простой вариант синтеза видео, но если мы хотим синтезировать видео целиком, то это нужно создавать отдельные архитектуры, которые будут генерировать сначала какие-то ключевые кадры и потом заполнять промежутки между этими ключевыми кадрами, используя дополнительные модели интерполяции. ну вот с точки зрения анимации это может выглядеть следующим образом пока это тоже еще есть куда развивать и качество изменений качество визуализации она еще имеет достаточно много степеней для улучшения но с точки зрения видео здесь вот я решил показать несколько примеров существующих моделей как они умеют оперировать с этим здесь есть варианты и синтеза видео по тексту и превращение видео в другое видео то есть такое видео стилизация по тексту видно что при стилизации у нас возникает сохранение контуров и в принципе схема превращения картинки в видео либо видео видео она более стабильно работает чем если бы мы говорили про синтез видео по текстному описанию, но разработки в этом направлении тоже ведем и я думаю что в следующий раз обязательно расскажем с точки зрения AGI еще и про архитектуру и разработку в направлении генеративных моделей абсолютно разных модальностей, не только видео. В целом, наверное, все. Здесь есть ссылки на меня, почта. Я буду рад ответить на вопросы. А сейчас, потом... Спасибо большое за внимание. 

А. КОЛОНИН  [01:01:59]  : Андрей, спасибо большое. Но раз Вы упомянули AGI, я тогда начну с последнего вопроса, поскольку у нас все-таки сообщество AGI. Вот, кстати, Иван Вас на наш семинар рекомендовал пригласить Сергея Маркова, когда он делал у нас доклад летом. Ему был задан вопрос о сознании, в частности, и феномене сознания, и его роли во всей вот этой тематике, которой мы занимаемся. И он как раз отослал к вам, поскольку вы занимаетесь мультиязыковыми моделями. А у нас несколько лет назад здесь был семинар, где Большинство участников высказались, что сознание – это такая штука, которая возникает в результате как раз мультимодальности. Как вы, во-первых, относитесь к перспективе AGI в контексте мультимодальных моделей и вообще? Это вот первый вопрос. И второй вопрос – как вы относитесь к тому, что называют сознанием? Есть ли оно вообще? Нужно ли оно о нем говорить в контексте машинного обучения и искусственного интеллекта? Связано ли это с мультимодальностью? И что вы по этому поводу думаете? 

А. КУЗНЕЦОВ  [01:03:24]  : Отвечая на первый вопрос, я считаю, что сильный искусственный интеллект, он неразрывно связан с мультимодальностью, потому что если мы говорим и оперируем определением, что сильный искусственный интеллект должен максимально близко приближать к расширению способностей человека, это значит, что он должен уметь работать с разными типами данных хотя бы как человек. Это значит, что для него не должно быть барьеров в части понимания новых модальностей. То есть так же, как и человек. Он, когда растет, взрослеет, учится, он начинает изучать новые типы данных. И для него совершенно несложно оперировать разными модальностями в естественной среде. Это значит, что мультимодальные архитектуры и их развитие – это как раз будущее AGI. И они должны, в моем понимании, они должны находиться в основе как раз принципов создания сильного искусственного интеллекта. По-другому просто никак, потому что одной модальности, очевидно, недостаточно. И развивая эти методы, мы можем говорить о том, что мы стремимся к усилению архитектур и моделей искусственного интеллекта в этом направлении. Если говорить про сознание, наверное, это интересный вопрос, потому что в моем понимании, если мы утвердительно скажем, что да, у искусственного интеллекта появилось сознание, это значит, что мы в определенных задачах готовы доверительно отдать ему образы правления для решения каких-то задач. Это значит, что мы можем быть уверенными в том, что все действия, которые совершит искусственный интеллект, они будут сознательно безопасны и сознательно не нести вред человеку. Собственно, принципы создания алгоритмов, алгоритмов машинного обучения и программ. Самое главное, чтобы при появлении сознания мы были уверены в том, что это сознание не будет нести вред. А для того, чтобы научить модель лучше понимать среду, потому что сознание возникает из… анализы информации из разных источников возбуждения в естественной среде, нужно уметь оперировать разными источниками и правильно формировать output или decision, или действие какое-то решение, принимать решение в рамках этой же естественной среды. Это значит, что нужно уметь прогнозировать те результаты, которые возникнут в случае создания вот этого действия как ответа на возмущение, возбуждение в естественной среде. Я думаю, что мы стремимся к этому, мы идем к этому, но языковые модели пока показывают достаточно большой объем галлюцинирования. И галлюцинирование – это, наверное, с одной стороны можно называть, с точки зрения психологии, философии, Но с другой стороны, в моем понимании, это все-таки нарушает принципы безопасности, потому что мы не можем, давая согласие на ответ, есть ли это сознание, и это действительно сознание у языковой модели, давать ей возможность жить в естественном безуправлении процессе и принимать какие-то ответственные решения. 

А. КОЛОНИН  [01:07:15]  : Спасибо. Еще вопрос. У нас появляется все больше вопросов. У меня еще был самый первый вопрос. Смотрите, вы употребляли широко распространенный в среде специалистов в области машинного обучения термин «эмбеддинги». А в среде людей, которые являются где-то, может быть, традиционными символщиками, если можно так выразиться, и в частности недавно вышла статья совсем на днях Питера Восса, одного из основателей терминологии и идеологии AGI. Есть понятие концепта. Система должна понимать концепты. А вопрос, а вообще не является имбединг по сути концептом? То есть, когда мы говорим о концептах, не говорим ли мы об имбедингах с точки зрения нейросетевого подхода и вообще не является ли одно просто другим, просто в разных системах ценностей и системах понятий? 

А. КУЗНЕЦОВ  [01:08:22]  : А что мы понимаем под концептом? 

А. КОЛОНИН  [01:08:30]  : Под концептом я понимаю некоторую абстрактную сущность, которая закреплена в некоторой понятийной сетке и имеет некоторое энтологическое отношение с другими концептами. 

А. КУЗНЕЦОВ  [01:08:43]  : Ну, наверное, да. Наверное, можно связать эти вещи, потому что если мы стремимся к тому, чтобы объединить эмбейдинги или вектора признаков в едином пространстве признаков, где картинка и текст соответствующие будут близки друг к другу, Это значит, что мы можем этим эмбэйзингом описать состояние среды или концепта в цифровом или векторном мире. Если мы возьмем картинку, оцифруем ее, превратим в вектор и найдем ее место в пространстве, то в целом, умея интерпретировать это пространство, мы сможем понимать, что все вектора, например, мы взяли собачку, фотографию собаки векторизовали и посмотрели в пространстве признаков все точки, лежащие рядом с этим вектором, значит там везде плюс-минус где-то должны находиться собаки или близкие к собаке какие-то сущности. Если это так, и мы придем к этому, то, мне кажется, мы в этом направлении стараемся двигаться, чтобы разные модальности, они для нас внутри, как и для человека в голове, превращались в определенный, назовем это словом, концепт или сущность, которая описывается каким-то состоянием в этом промежуточном пространстве. что эмбэдинг, что у человека в памяти. Мы можем не запомнить точно детали, но если мы увидим какую-то ситуацию, то она у нас внутри превратится в определенную сущность, которую мы сможем передать другому человеку. в разной форме. Если мы умеем рисовать, мы можем ее заскетчить, отрисовать. Если мы сможем ее описать, так как мы ее запомнили, мы ее передадим человеку, и у человека из этого описания, либо из этого скетча, сложится абсолютно тот же самый концепт в голове, который есть у нас. То есть у нас будет, получается, единое такое пространство информационное. Наверное, да. 

А. КОЛОНИН  [01:10:40]  : Спасибо. Еще практический вопрос. Здесь один из участников поинтересовался, будет ли доступна презентация, потому что там много ссылок. 

А. КУЗНЕЦОВ  [01:10:51]  : Я специально ссылки оставил для того, чтобы можно было потом презентацию передать и посмотреть, потому что статьи действительно такие флагманские этого года. 

А. КОЛОНИН  [01:11:01]  : Да, будет здорово. А у меня-то вопрос вот какой возник. С Вашей точки зрения, насколько мы сейчас далеки от того, чтобы некоторой доступной, без особых усилий, языковой модели мультимодальной можно было сказать? Модель. Вытащи, пожалуйста, из этого видео все ссылки в виде файлика. 

А. КУЗНЕЦОВ  [01:11:26]  : Думаю, сейчас пока еще мы до такого не дошли. Но в целом, если разложить видео просто на кадры и задетектить просто детектором все ссылки, это будет проще, чем решать это языковой моделью. То есть, есть задачи, которые можно решить куда проще, чем придумывать какие-то вот... Это как микроскопом гвозди забивать. Есть задачи, которые можно, очевидно, и надо решать более простым способом. 

А. КОЛОНИН  [01:11:58]  : Спасибо. Еще два вопроса есть у Владимира Смолина. И у меня потом еще будет один. Владимир, пожалуйста. 

В. СМОЛИН  [01:12:07]  : Добрый день. Я хотел поблагодарить за интересный рассказ, все было замечательно, но хотелось такую вещь прояснить. Сейчас очень популярная идея зеро-кодинга, то есть то, о чем вы рассказывали, можно себе представить так, что скачали откуда-то модель, одну-вторую соединили и, собственно, проверили, работает, хорошо, не работает, давайте по-другому из комбинировать. Есть второй подход, чтобы все-таки понимать, как работает трансформер, как работает диффузионка, вот насколько вот это понимание вы считаете необходимы для вашей работы, и если у вас есть специалисты, которые это понимают, могли бы они нам все-таки рассказать не про опыт зеро-кодинга и замечательный результат, который таким путем достигается, а все-таки про то, на каких принципах эти модели работают, соответственно как там где это работают как диффузионность там что дает вот этот вопрос возможно обрисовать вашему коллективу да так как я просто не был 

А. КУЗНЕЦОВ  [01:13:12]  : заранее уверен в том, что какая аудитория может быть у доклада, я его сделал таким комбинированным, чуть технически. Вам никаких претензий. 

В. СМОЛИН  [01:13:20]  : Я к тому, что да. 

А. КУЗНЕЦОВ  [01:13:22]  : Это вопрос про отдельный доклад. Можно сделать отдельный доклад и рассказать более подробно в деталях про диффузии, про то, как вероятностно там все внутри завязано. Прямо погрузиться в детали. Можно рассказать и про мультимодальные точно так же. Посмотреть, как вот, например, там архитектуру Коди можно про нее там отдельно, про это Composable Diffusion рассказывать очень долго. Да, можно, обращайтесь, можно будет устроить какой-нибудь интересный вебинар, более такой, технически подробный. 

В. СМОЛИН  [01:13:52]  : Ну, вот к Антону Германовичу на просьбу согласовать этот вопрос, будет очень приятно послушать. Это первый вопрос. А второе, собственно, вот мое отношение к мультимодальным и вообще к языковым моделям, что их замечательная успехи основаны на том, что наш язык, неважно какой английский, немецкий или китайский, он соответственно структурирует наше пространство, то есть выделяет из него простые понятия, и это упрощает описание этого окружающего мира. И, собственно, успех этих моделей, больших языковых моделей, связан с тем, что человеческое умение разделить пространство на какие-то простые компоненты, оно используется и очень эффективно, что позволяет достичь эффективного описания окружающих пространств. Согласны ли вы с этим утверждением, или вы считаете, что какие-то другие веспрективы основные? 

А. КУЗНЕЦОВ  [01:14:49]  : да пожалуй соглашусь это действительно так и в принципе структура предложений она как раз построена на там понятных подлежащие сказуемое дополнение и так далее и за счет этого мы и сами воспринимаем информацию потому что мы разделяем предложение на части и у нас оно связывается в концепты которые мы просто грамматически потом верно споставляем чтобы не было ошибок С языковой моделью то же самое. То есть, у нее есть в голове запомненные, выученные на основе огромного объема текстов, информации, разные понятия и концепты, и она ими точно так же просто оперирует, сопоставляя, выставляя их в правильном порядке и дополняя в ходе притроина, научившись заполнять дырки, пропуски и предсказывать следующее слово. То есть, она учится структуре действительно. 

А. КОЛОНИН  [01:15:43]  : Спасибо. Андрей, у меня еще есть вопрос. Может быть, где-то немножечко провокационный, но не могу его не задать с учетом вашего погружения в генеративный AI. И в Кандинске, в частности. Я, кстати, в Кандинске использовал пару раз для генерации заставок для научно-популярных статей. Смотрите, к языковым моделям большим предъявляют две претензии, в частности. Первая претензия – это то, что они галлюцинируют, да, и вы сами это повторили. А вторая претензия – что да, они могут очень хорошо воспроизводить то, чему их научили на тренировочных корпусах, но у них нет креативности. Вот у меня есть такая мысль, что эти два утверждения в каком-то смысле противоречат друг другу. Потому что если попытаться проанализировать, что такое креативность, способность генерировать неожиданные сочетания или комбинации фактов, концептов, решений, схем поведения в неизвестных ситуациях. То есть, в этом смысле, когда нейросеть, большая языковая модель, сталкивается с вопросом, на который у нее нет ответа, то есть, она что-то знает на эту тему, но четкого понимания ситуации нет. Она пытается домыслить ситуацию. И точно так же люди, когда проявляют креативность, они сталкиваются с незнакомой ситуацией. пытаются подобрать что-то напоминающее, отдаленное решение, которое могло бы с разных точек зрения к этой ситуации подойти. В большинстве случаев, естественно, это получается ерундой, но в каких-то случаях это получается та самая эврика, которую потом патентуют, записывают в изобретение. Не являются ли галлюцинации на самом деле креативностью? Или, может быть, просто нужно еще немножечко поднимать градус галлюцинирования для того, чтобы это можно было считать креативностью? Или все-таки креативность – это другое? 

А. КУЗНЕЦОВ  [01:18:05]  : Я отвечу на ваш вопрос. В моем понимании креативность и эволюционирование – это как черное и белое. Это две плоскости, по сути, одной способности модели придумать что-то новое. Разница в чем? Креативность – это создание действительно новых концептов и каких-то описаний визуалов, стилей, чего бы то ни было, неважно какой генеративный ИИ. При этом это воспринимается и обществом, и пользователями как действительно новая какая-то крутая штука, которая несет определенную стилевую составляющую. Если мы говорим о галлюцинировании, то, как правило, в галлюцинировании обвиняют модели, которые грубо нарушают какую-то фактологическую информацию. Это значит, что если она начинает, ну как вот студент при ответе на вопрос, когда он приходит к тебе на экзамен, берет билет и начинает как-то юлить, ходить вокруг темы, не зная правильного ответа, но какими-то там набрасывая термины и слова, которые имеют отношение к вопросу, он начинает из них складывать какие-то интересные сюжеты. Вот мы же в этой ситуации не оценим его как креатину. Это как в фильме идею пятерка, а за билет двойка. Вот так и здесь. То есть мы можем модель оценить с точки зрения того, что она выкручивается и увиливает как типа молодец ты креативно, но с другой точки зрения мы как человек, как пользователь, знающий правильный ответ, понимаем, что модель врет. Тем самым для нас это негативно. Я бы рассматривал креатив как нечто не несущее фактологических ошибок в себе. Тогда да, это креатив, это позитивная какая-то составляющая, белая. А черная – это когда галлюцинирование приводит к ответу пользователя на вопрос с нарушением фактологической информации. Не дай бог пользователь спросит что-нибудь там, какое лекарство выпить при таких-то, таких-то симптомах, и модель ему напридумывает, что там условно парацетамол приложи к болячке, она у тебя там заживет. Или там все лечи этим углем. И, наверное, вот в этой ситуации не хотелось бы, чтобы галлюцинирование нарушило какую-то футурологические нарушения привели к неправильному восприятию и неправильному ведению диалога в целом. Поэтому, пока мы от этого не избавимся, доверять искусственному интеллекту всецелую и вслепую, конечно, нельзя. Поэтому, отвечая на ваш вопрос, они связаны, но для меня галлюцинирование – это креатив с нарушением фактов, грубым нарушением, которое может привести к чему-то нехорошему с точки зрения безопасности, как бы не понимать слово «безопасность». 

А. КОЛОНИН  [01:21:11]  : Андрей, спасибо. Есть еще один вопрос у Владимира про галлюцинирование. Владимир, пожалуйста. Да, и сразу же... Да, давайте, Владимир, а потом другой еще вопрос. 

В. СМОЛИН  [01:21:24]  : Ну, собственно, я, наверное, мало сам беседовал с ЖПТ, но, тем не менее, вот существует такое мнение, что, значит, если диалог идет на тему, в которой человек не является специалистом, то воспринимается хорошо. А если специалист начинает слушать по своей теме, что отличает ЖПТ, это, в общем, находит много элементов эволюционирования. Насколько прогресс есть в этой области и какие на ваш взгляд перспективы? 

А. КУЗНЕЦОВ  [01:21:55]  : Прогресс в этой области есть. Он связан с увеличением базы данных фактологическими ответами. Есть дополнительные исследования, которые ведут с использованием графа знаний, когда к языковой модели подключается фактологическая информация на основе ретривала какого-то, то есть на основе извлечения знаний из какой-то системы, из того же интернета. Тот же Bing поиск работает по такому принципу, когда ты задаешь вопрос в чате, он предварительно делает декомпозицию твоего запроса на части. поиска ответы и оперируя ссылками и информацией в этих ответах, он формирует тебе запрос. Тем самым у тебя в запросе возникает ответ, где к каждому утверждению есть определенная ссылка. Другой вопрос, являются ли эти ссылки достоверны. Но использование графа знаний, википедии, графа знаний, фактологической информации какой-то мировой в разных бизнес, медицина, все что угодно. Отовсюду можно извлечь какое-то графзнание. И делая из него ограничения для языковой модели, сейчас такие исследования ведутся, языковые модели можно контролировать на предмет Галлюцинирование. Вот. 

В. СМОЛИН  [01:23:24]  : Можно еще небольшой ответ. Значит, вот была очень замечательная и успешная расцвете этого альфа-го, альфа-зеро, и он был основан на том, что не первый результат выдается, пусть основанный на большой фаркелогической базе, там и со различными ограничениями, а исследуется, собственно, последовательность ходов. То есть вот этот факт, допустим, который достается откуда-то из веки, можно проверить, насколько там все выводы, которые из него можно сделать, насколько они будут соответствовать. Вот насколько, собственно, идет работа в направлении не просто выдачи первого самого хорошего ответа, а анализа того, что получается. То есть какие из этого следует продолжение и насколько они хороши. 

А. КУЗНЕЦОВ  [01:24:14]  : Если про это говорить, то в принципе даже в chat.jpg же есть механизм reinforcement learning with human feedback, который как раз направлен на то, чтобы регулировать вот эти ответы и выбирать из близких ответов, то есть аранжировщик или оценщик, который сидит и размечает такого типа данные, выставляет их в правильном порядке, он как раз ориентирован на то, чтобы соблюдать вот эти механизмы устранения галлюцинации. То есть, когда есть несколько ответов, и надо выбрать из него тот, который действительно лучше других, хотя они все вроде бы об одном, они все близкие, но но лучший ответ ранжируется и выделяется как наиболее правильный среди всех, которые может сгенерировать модель, тем самым у нас есть несколько генераций и такой механизм он так и учится, то есть у нас большая языковая модель она генерирует несколько ответов и потом отдельная reward модель она оценивает эти ответы для того, чтобы выбрать лучший из них в соответствии с принципами меньших рисков по безопасности, по галлюцинациям, по фактологической точности и так далее. 

В. СМОЛИН  [01:25:28]  : Я не совсем про это спрашивал, но спасибо. 

А. КОЛОНИН  [01:25:32]  : Спасибо, Андрей. Еще здесь был вопрос от Виктора Носко. Есть ли у Сбера наработки по решению проблемы галлюцинации? Я так понял, вы по сути уже ответили на него, да? И еще один вопрос из YouTube. Можно ли изменять ракурс точки съемки в генеративных моделях с сохранением самих объектов в сцене? 

А. КУЗНЕЦОВ  [01:25:56]  : Это как раз к вопросу об интерпретируемости векторного пространства. Мы сейчас делаем небольшие исследования в этом направлении, и это действительно важная и интересная и очень крутая задача, но пока каких-то практических результатов еще мы в этих исследованиях не получили. суперкрутая штука, и прям этим направлением хочется заниматься больше. И интерпретируемость латентного пространства – это, я думаю, одно из следующих важных достижений, которое произойдет, если это случится. 

А. КОЛОНИН  [01:26:37]  : Можно я тогда уточню еще. То есть, я так понимаю, что вы позитивно относитесь к развитию сторону построения интерпретируемых моделей и интерпретируемого искусственного интеллекта? Да. Спасибо. И еще есть вопрос от Эльгизара Талипова. Слышно? 

Э. ТАЛИПОВ  [01:27:06]  : Хорошо. Андрей, спасибо за содержательную презентацию. Вот я правильно понял, что на данный момент ведущим является все-таки языковая модальность. И у меня возникла такая вот мысль. Если, допустим, мы разрабатываем автопилот для автомобиля, то не должна ли быть там модальность, ведущая не языковое, а такая вот из позиции, из картины самого движения автомобиля. И есть ли такие подходы? как бы в этом направлении движения. И ведь там потом получается, что языковые конструкции там несколько излишни для самого автомобиля являются. И в то же время нечто у самого автомобиля будет такое, что нельзя выразить языком. 

А. КУЗНЕЦОВ  [01:28:12]  : Да, язык можно считать это просто некоторым кодированным представлением слов, которые мы используем. То есть язык, как таковой, это просто опять-таки вектор. Если мы говорим про языковые модели в автопилотах, то здесь, во-первых, надо понять, надо ли нам человеку через язык управлять движением автомобиля. Мол, там, довези меня дотуда, проедь через там-там-там или еще что-то. Это, казалось бы, вполне понятные задачи. Тут сильно языковая модель не нужна. Языковая модель хороша тем, что она Научившись работать с языком, она умеет правильно работать с последовательностью. А любая система, которая работает с псевдонепрерывным каким-то набором событий, это тоже последовательность. И мы можем придумать такой энкодер, такой кодировщик, который вот эти состояния из внешнего мира, из состояния автомобиля, там, с коншины, с лидаров, еще с чего-то, с проективного отображения сверху. положение автомобиля, маппер такой, да, из координат пространственных в мировые, допустим, мы можем собрать несколько модальностей, которые можно попытаться объединить, закодировать и посмотреть, насколько модель сможет вот с этим длинным контекстом таких событий работать. Потому что языковая модель – это ведь на самом деле что? Это на самом деле механизм, который, имея на вход определенной длины цепочку каких-то токенов, назовем их просто абстрактно токенами или эмбэддингов, может предсказать следующий наиболее вероятный эмбэддинг. Это значит, что Чем длиннее вот этот контекст сможет получить и научиться понимать модель, тем больше информации она из него сможет извлечь, чтобы предсказать следующее состояние среды. Поэтому модели, они ведь почему и увеличены. Есть модели 7 миллиардов параметров, есть 40, есть 180, 500 и так далее. Чем больше модель, тем, как правило, у нее длиннее контекст, который она может одновременно проанализировать и с использованием механизма внимания выделить оттуда какие-то наиболее важные сущности, наиболее важные токены или информацию из этих токенов для того, чтобы предсказать следующий. Поэтому как только мы перейдем из вот этого физического представления мира камеры, состояние автомобиля, его угол наклона рейки, повороты, еще что-то, тормозной усилий и так далее. Мы перейдем в некоторый цифровой вид, то мы сможем с этим работать просто как с последовательностью событий. И должны понимать, что мы с этой последовательностью событий должны делать следующим образом. Остановиться, повернуть, резко затормозить, дать газу, какое-то действие совершить. К вот этому вопросу, про который вы говорите, очень близко как раз модель Палмье, где есть некоторое описание и некоторая последовательность каких-то состояний среды, и мы из этой последовательности пытаемся понять, что нам нужно сделать дальше. Надеюсь, я ответил на ваш вопрос. 

А. КОЛОНИН  [01:31:29]  : Спасибо. Коллеги, есть ли еще вопросы к докладчику? 

А. КУЗНЕЦОВ  [01:31:36]  : Вот тут написал Владимир Фролов, было бы интересно обучать на огромных данных с видеорегистратором. Данные собрать можно. Вопрос в том, какую конечную задачу мы будем решать, то есть эти же данные должны быть каким-то образом размечены, то есть мы не можем взять просто данные. Мы можем пробовать восстанавливать пропуски между ними, можем пробовать предсказывать следующие кадры, но нам все равно нужно размечать саму среду вокруг, то есть не просто кадры, а именно то, какую ситуацию эти кадры описывают, то есть там опасная ситуация, сближение, опасное сближение, там пешеход выскочил, откуда-то еще что-то. то есть если вот эти данные еще и постоянно с помощью краудсорса например размещать и постоянно наполнять то это будет супер ценный конечно объем данных для обучения таких беспилотных разных средств 

А. КОЛОНИН  [01:32:27]  : А, кстати, Вы никак не пересекаетесь по Вашим работам с тем, что Ольга Ускова делает. Я так понимаю, она как-то же с Асбером взаимодействует сейчас. И концепция так называемого Deep Fusion объединения разнородной сенсорной информации используется 

А. КУЗНЕЦОВ  [01:32:48]  : Нет, с ней нет. Внутри мы с подразделениями СБЕР по роботехническим всяким задачам сотрудничаем, а вот на внешку у меня, по крайней мере, в окружении, в команде нет направлений. 

А. КОЛОНИН  [01:33:05]  : Хорошо. Коллеги, есть ли еще какие-то вопросы к докладчику? Нет, но тогда, Андрей, спасибо Вам огромное. Очень насыщенный, информационный, качественный и интересный доклад. Я думаю, мы еще тогда свяжемся, посмотрим, можно ли раскрыть эту тему поглубже. И тогда обсудим время, место и тему. Спасибо огромное Вам за доклад, за ответы на вопросы и спасибо всем участникам. 

А. КУЗНЕЦОВ  [01:33:35]  : Вам спасибо, участникам спасибо за интересные вопросы, за обсуждения. 

А. КОЛОНИН  [01:33:39]  : Всем до свидания. Спасибо. Всего доброго. До свидания. 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
