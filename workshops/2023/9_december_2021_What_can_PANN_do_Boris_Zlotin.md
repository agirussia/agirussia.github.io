## 9 декабря - Что может PANN? - Борис Злотин — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/6COFVQcMThU/hqdefault.jpg)](https://youtu.be/6COFVQcMThU)

Суммаризация семинара:

Семинар был посвящен теме Progressive Artificial Neural Network (PANN) и его применению в области искусственного интеллекта. В ходе семинара были затронуты вопросы реализации ИИ с использованием PANN, а также обсуждались сложности, с которыми сталкиваются разработчики в процессе создания и оптимизации нейронных сетей.

Борис Злотин, поделился своими знаниями и опытом в области PANN, подчеркнув важность практического применения и взаимодействия с реальными задачами. Он также отметил, что команда, работающая над PANN, включает в себя как технических специалистов, так и консультантов, связанных с нейронными сетями и бизнесом.

В ходе обсуждения были затронуты темы обучения нейронных сетей, их точности и способности к распознаванию образов. Борис Злотин также упомянул, что его команда успешно применяла методы, позволяющие улучшать точность PANN, и что они планируют дальнейшие исследования в этой области.

Кроме того, Борис Злотин поделился своими мыслями о будущем нейронных сетей, указывая на важность гибридного подхода, сочетающего математические и статистические методы, для улучшения их функциональности. Он также обсудил потенциал PANN в создании малых, но эффективных продуктов, созданные отдельными людьми или малыми группами.

Семинар завершился обсуждением планов на будущее, включая проверку схемы эмуляции PANN с помощью PANN клеточного автомата и планирование новых исследований и разработок.







S08 [00:00:07]  : Итак, коллеги, добрый вечер. У нас сегодня в гостях Борис Злотин. На прошлом семинаре мы витали в облаках квантовых запутанностей, и нас овевали волны вселенского разума, проходящие сквозь черные дыры. А сегодня мы спускаемся к жесткой реализации искусственного интеллекта с помощью Progressive Artificial Neural Network, и нам в этом поможет Борис Злотин. Борис, пожалуйста. 

S03 [00:00:43]  : Добрый день, господа! Рад с вами снова встретиться. Около четырех месяцев назад мы с вами встречались. Я много-много говорил, много-много показывал, многих утомил, даже немножко троллили меня. Но я к этому спокойно отношусь, это пожалуйста. Совсем не напрягает. Я тогда пообещал показать реальность. Я заводчанин, всю жизнь держусь за заводскую трубу, считаю, что имеет смысл то, что можно продать, произвести и так далее. И сейчас я вам буду показывать конкретно реальные вещи. Только до этого два слова. Вот здесь вот девять человек, которые принимали наиболее активное участие. В центре, в желтой каемочке, непосредственные работники, те, кто программы делали, писали и работали. Слева, скажем так, теоретики, которые руками водят, руководят, то есть, И слева сильные консультанты, как в нейронных сетях, так и связанных с ними бизнесе. Все эти три человека тоже принесли очень серьезный вклад в наши работы. 

S02 [00:02:08]  : – Борис, один вопрос. Можно демонстрацию сделать, не эдит, а демонстрацию распахнуть? 

S03 [00:02:16]  : – Я разве не показываю? Я share экран включил. Вы не видите мой экран? 

S02 [00:02:23]  : Мы видим, но дело в том, что это режим редактирования слайдов, а надо распахнуть сам слайд. 

S03 [00:02:30]  : Я могу это сделать, но это всегда очень сильно замедляет работу. Я убедился, что потом будут проблемы с включением реальной сети. Лучше давайте в этом оставим. Ладно, все, оставили. Спасибо. Да, потому что это, так сказать, обоюдоострое оружие. Он жрет массу компьютерных ресурсов, а у меня лаптопчик не очень сильный. Итак, вот эти вот люди, которые принимали участие в разработке софтверов. И следующая очень важная вещь. На 20 число этого месяца назначен полный релиз, то есть на вот этих четырех интернет-сайтах, порталах будут выложены возможности загрузить Ту сеть, которую я сегодня показываю, и точно будет показывать Иван Иванович Негрешный, ознакомиться с этим, использовать, тестировать. Можно практически начать продукт делать. Это уже вполне допускается. И теперь я не буду дальше особо продолжать. Последний слайд в этой части презентации – описание самой программы. Программа состоит из рабочего модуля и ее хелпов. Вот сейчас включаю. Вот это вы видите рабочий модуль. И сейчас я сразу включу хелпы, чтобы было понятно, что здесь мы имеем. Хелпы называются вот они. Я так сдвину, чтобы можно было и рабочий модуль видеть, и читать хелпы. Итак, я начинаю работу. Я для начала хочу вам показать разные режимы загрузки и обучения. Начнем с самого простого. Я нажимаю «Загрузка классов» и захожу в свою демонстрационную директорию DEMO. Здесь написано FACES. Эти FACES – это Ельский университет разработал специальную серию для тестирования, и я говорю FACETRAIN. Чтобы было понятно, что такое Facetrain, я сейчас включу просто обычный браузер и покажу, что у меня в этом браузере. Вот «Faces», «Face Train». Здесь 15 поддиректорий. Каждый из поддиректорий включает 8 лиц разных. Все черно-белые, все одного формата. Это, я повторил, ельский тест. Вот я говорю «FaceTrain», говорю «Select Folder», и у меня здесь загрузились все вот эти вот классы. Я могу посмотреть любой класс, например, и картинки в нем присутствующие, либо вот там какой-то другой класс, картинки присутствующие. И теперь я попросту говорю, обучайся, тренируй этот класс. За 0,36 секунды класс полностью оттренирован. Теперь я показываю два режима распознавания. Да, здесь всего 15 по 8, это 120 картинок. Показываю режим распознавания. Для начала я распознаю любую штучку из того, что я уже тренировал, из тренированных. Ну, допустим, я зайду вот в этот первый класс и говорю вот этого типа. Она мне говорит, первый класс 0,4485 – это нейронная сумма, представляющая собой вероятность того, что это именно данный образ. Обратите внимание, не 100%, потому что это обобщенный класс сделан. Как происходит обобщение? Все восемь штук Примерно как в обычной нейронной сети тренируются. Прелесть здесь только в одном. Обратите внимание, эпох-каунт. Две эпохи тренировалось. В самом деле одно. Первая эпоха – это предтренировочная, выставляются все нули. Во второй эпохе полная тренировка на всю величину ошибки. Обобщение и тренировка. Это вот я выбрал из тех, кого уже обучал. А теперь я возьму из того же Ельского университета. Они задали не только трейн, они задали фейстесты. Вот этот же первый класс, вот фейстест. Она мне говорит, 0,1. 0.34 по сравнению со следующим 0.17. Очень хорошее, в принципе, распознавание. И с этим пока все. Теперь давайте я сделаю следующий тест. Этот я обнуляю и говорю следующий тест. Я покажу вам, как я тестирую на отдельных имиджах. Я захожу в имиджи и Вот здесь вот у меня сделана из этого же Ельского университета сборная солянка. Здесь все 120 имиджей вместе. Я говорю Ctrl A, открываю все 120 имиджей и тренирую. Теперь тренинг будет несколько дольше. Нормально с этими 120 имиджами это примерно от 19 до 22 секунд, но при включенном зуме он может до минуты доходить. Зум жрет сильно оперативку. Я не знаю, сколько времени это в самом деле протренируется, и поэтому пока могу немножко показать то что у меня вот он уже слушайте как быстро 22 секунды этого я никак не ожидал вот теперь очень интересная штука я начинаю распознавать ну для начала распознают что-нибудь точно здесь уже на тренировано ну вот этого же или любого другого и вот теперь обратите внимание на крайне важную вещь Она мне дает полное распознание всего, по всем 120 этим. Причем первые тренированные всегда распознаются как единица. Какое там переобучение? Всегда и абсолютно все тренированные распознаются как единица. Но теперь обратите внимание следующее. В вот этом они все понумерованы. номером класса. Вот это вот все относится к первому классу, а вот это уже пошел вот здесь, видите, subject 02, второй класс и так далее. И вот здесь вот первый, второй, третий, четвертый, пятый – это все из этого класса. Первый, другой вот из Вот этот поглушился, но здесь уже 0,34 по сравнению с единицей, как вы понимаете, не очень много. Потом опять 0,1 идет, потом 0,9. Это уже пошел разброс. Он узнает уже лица как таковые, а не конкретных деятелей. А вот теперь я могу сделать еще один очень интересный опыт. Я снова очищаю все. Это уже у меня третий тест. Здесь идет счет тестов. И я загружу и то, и другое. Я загружу вот там, где у меня были классы, ну, допустим, вот все all face images, я классы гружу вот face train я загрузил все классы и я еще загружаю отдельно все имиджи и снова тренирую Надо сказать, что теоретически и проверено, мы можем просто дотренировать, загрузить одно, потом дотренировать другое. Но поскольку тренинг очень быстрый, мы сейчас не реализовали доучивание, мы каждый раз переучиваем. Но это, так сказать, не проблема. Сейчас еще одну важную вещь покажу, когда она обучится. Вот пошла дольше учиться. 31 секунду она училась. Обратите внимание вот на что. Опять только две эпохи обучения. А вот нейрон-каунт, количество нейронов 135, 15 обобщенных нейронов, на каждом из которых по классу, и на каждый образ тоже отдельный нейрон. То есть каждый образ как бы сам в себе представляет класс. И вот теперь я снова говорю распознавать. И беру, например, вот какое-то вот это лицо. Мне без разницы, что брать. Обратите внимание, вот здесь распознание по классам 0,459 и 0,17. Очень характерное распознание. А здесь первый, второй, третий, четвертый, пятый – это все эта же личность. И вот это практически каждый раз. Зачем мне это надо? А вот теперь я показываю другой крайне важный и интересный момент. Я нажимаю кнопочку, которая называется New Class. И вот здесь вот, передо мной, вообще говоря, все распознаны с их величинами распознания. Вот я говорю, что вот этот вот Плюс этот. Плюс этот. Плюс, например, вот этот. У меня будут новый класс. Ну, как вы понимаете, я могу что угодно выделять в новый класс. Я им говорю новый класс. Захожу, например, вот сюда, вот в Daemon. Здесь у меня Faces. И говорю здесь, например, создать класс New Folder. Назову его Вася. Вот у меня появился фолдер Вася. 

S04 [00:14:03]  : Я говорю Select этого Васю. 

S03 [00:14:06]  : И все. А вот теперь очень важный момент. Вот у меня это новый класс. Понимаете, какая штука? Проверена такая вещь. Я взял фолдер, в который напихал несколько сотен самых разных картинок, но так, чтобы было десяток лиц, десяток башен, десяток кошек разных. как мусор набросанный. Дальше я включаю эту, и я строю автоматическую классификацию, давая ей название. То есть идет автоматическая кластеризация. Но один из самых забавных элементов этой кластеризации еще вот какой. А вот я возьму и рассмотрю самые неподходящие, самые удаленные. где 006 распознание. 

S04 [00:15:06]  : Вот видите, вот этот, вот этот, вот этот, вот этот, вот этот. 

S03 [00:15:15]  : Самым странным образом я отобрал негров и китайцев. Мы в свое время ужасно смеялись. У нас было в наборке там четыре Китайца? Так вот, когда распознаешь одного китайца, он сперва дает тех, кто к нему относится, а потом дает остальных китайцев. Такая российская, понимаете, программа. Ох, черных вылавливает на один момент. Что еще существенно об этой программе? Давайте Я сейчас покажу следующие несколько моментов. Во-первых, я провел все эти тесты. Я хочу записать эти тесты. Я нажимаю рекорд тест и говорю, хватит 10 имиджей только. Вот у меня отформирован полный рекорд. Здесь результаты тренинга, результаты распознавания. И все, кто вошли в распознание, это вот по классам и отдельно по имиджам. То есть дальше я это могу сохранить. Это будет, как он называется, FP3. неудобный формат, но если я хочу, то я его сохраняю в PDF-формате или в HTML или в RTF. Правда, заранее скажу, RTF как-то так криво сохраняет. Очень похоже, что это какой-то из предыдущей версии RTF Microsoft, которая сейчас уже... Не рекомендую RTF сохранять, а вот Acrobat запросто. PDF-овские файлы очень хорошо сохраняются. Что еще здесь существенно? Давайте покажу еще парочку развлечений. Развлечения очень забавные. Допустим, я порекомендаю, здесь вот у меня faces corrupted, кривые всяческие морды. Как считаете, вот это распознает картинку? Еще как прекрасно распознает. Вот такую вот сильно испорченную. Но это более-менее регулярная порча, с этим проще. Но, пожалуйста, вот глаз я ему выбил, распознаёт. Вот сильно я его потолстел, морду такую отъел большую. Распознаём. Интересно, что вот это распознает наклонную и увеличенную, а вот это не распознает. То есть, не все абсолютно распознает. Нужен определенный уровень сходства. Еще не включен в программу, но уже разработан тестер наклонов, и с тестером наклонов распознает играючи. Просто здесь пока он распознает в пределах плюс-минус примерно 10 градусов нормально. Дальше нужно включать специальную систему, связанную с наклоном. Точно так же распознает Я хотел другое показать. Эти тоже практически все распознаются. Вот смотрите какой. Распознал? Сейчас еще проверю. Есть важный момент в распознании, который мы проверяли. Фейс-шифт с двигем. Распознает вот такую вот? распознает ой нет здесь как раз нет распознал вот не не распознал это не то лицо вот здесь тоже не распознал а ну-ка она одном из предыдущих тестов распознавал давай-ка проверим вот этот не ага похоже что на последних мы улучшали кое-что Ладно, сразу говорю, далеко не все распознает. Бывают ошибки и все бывает. Я вам показывал хорошие варианты, но среди 120 лиц есть порядка 5-6 лиц, которые всегда распознаются неправильно. А почему? А фокус вот в чем. Мы пока не полностью использовали все возможности. Мы создаем одну спектральную сумму и распознаем по ней. Но в самом деле спектральных сумм столько, сколько у нас уровней и весов. Помните, я рассказывал про уровни. Вот здесь мы использовали 8 уровней и весов. Сейчас готовится система, где распознавать будет каждый спектральный уровень и отношения между ними. Это должно дать на порядок более высокую степень распознавания. Теперь я немножко расскажу о других вещах, связанных с этой системой. Я выведу на первый план ХЛП. Слишком большой получился. Давайте я вот так вот сделаю. Моя рекомендация, когда вы загрузите, если будете загружать эту систему, сразу же начинайте с того, что открываете Help. Нажав на вот эту кнопочку Help, легко найти, сразу открываете Help. В Help как бы два основных раздела. Первый, самый главный, рабочий раздел. Описание программы. Здесь вот все иконы и рабочие поля. Здесь перечисление всех икон и рабочих полей. Рекомендую один раз прочесть, ну и когда понадобится, к этому можно будет обращаться. Дальше. Выбор параметров нейронной сети. Вот это очень важный и интересный момент. Я хочу показать, что с параметрами. Пикселы 32, число весов 8. 32 на 32 – очень маленькая картинка. Вы знаете, для нас самих было потрясающее открытие. Маленькие картинки с малым числом весов. Ведь 8 уровней весов – это означает 8 цветов всего. Очень бедная картинка. Как мы опознаем друг друга? Я смотрю на лицо присутствующего здесь Лёши Захарова, которого очень хорошо знаю, или Саши Смолкина, или Володи. Я на них смотрю, и я вижу, что я вижу. Вот если вдруг Лёша Захаров себе роскошные кавказские усы отрастит, я его не узнаю, но узнаю заведомо. И Колонина узнаю, если даже он пострижется наголо и оденет женский парик. Почему и как я узнаю? Я ловлю паттерны. Но чем больше картинка, тем больше теоретически бесконечное множество паттернов. Огрубление картинки снижает необходимость паттернов, улучшает распознавание. Мало того, господа, нас не интересуют очень прихотливые паттерны, что у меня, например, левый глаз после аварии чуть-чуть больше, чем правый. Ну, разные размеры. Это не столь существенно, а вот то, что у меня нос поломанный картошкой, более существенно, чтобы распознать. Нас интересуют паттерны, которые имеют вероятность хотя бы одну десятую. Это значит, на 50, на 60 картинках мы с железной неизбежностью эти паттерны выявим. И нафиг нам не надо, как пишет Хинтон, 50 тысяч картинок, чтобы надежно распознать. Я вам сейчас покажу Хохму. На мой взгляд, это Хохма. Я сделаю вот что. Давайте так. Новый тест. Открою. У меня где-то есть то, что я назвал сборная солянка. Вот сборная солянка. здесь у меня есть там куча я беру вот обучу одному лимону но одному роялю но одной морде ну чему еще одним часам и одной планете вот так обучаю 15 сотых секунды на обучение. А теперь, заметьте, у меня по одной штуке только обучено. По одной. А теперь я беру и, ну вот, допустим, другой фрукт, перец. Он его распознает как лимон, потому что это ближе всего из того, что есть. Перца я не обучал. А теперь я распознаю физиономию, совсем не ту, которую я учил. Ясное дело, он распознает это как физиономию, он зацепил закономерности физиономии. А теперь я обучу... не обучу в смысле ой я случайно заново обучил но это ничему не мешает я возьму ну что ну вот вот у меня есть другое пианино вот это вот совсем хреновенькая для распознания она его распознал по цветовой гамме как планету ошибочка вышла но это всего по одному образу а вот это вот и это по цветовой гамме Кстати, сразу замечу, основное распознавание здесь по цветовому спектру, цветовой спектр, но уже проверена возможность распознавания по пространственному спектру, то есть нормальное разложение фурье, первая гармоника, третья, пятая. очень даже спокойно распознается с записью координат. Это очень хорошо для ловли отдельных элементов. Реализовано распознавание, например, носов. Реализовано распознавание правого глаза, который сильно отличается от левого. но все-таки мне не хочется завершать на таком неудачном варианте распознания давайте что-нибудь все-таки поищем чтобы хорошо распознала вот часики я по каким я вот по этим обучал а сейчас давайте поэтому посмотрим да что за дьявольщина факир был пьяный фокус не проходит ну-ка еще раз я Считайте, что этот кусок по одной штуке, вообще говоря, и не очень должен был получиться. По одной всё-таки плохо, надо делать обобщение и хотя бы десяток картинок. Но теоретически это абсолютно реально работает. Нам не нужно тысячи образов, чтобы хорошо научить. Десять, двадцать, пятьдесят образов за глаза и за уши обучают. Вот это проверено. Теперь давайте посмотрим еще. Число весов точно так же. Здесь интересная штука. Можно начинать с двух. Меньше двух не бывает. Это специфика сети. Можно начинать с двух весов. Доходили где-то до 256. Забавно, для 32 оптимально 8 весов. Для 64 на 64 где-то 10-12 весов становится оптимальнее. Увеличивается время обучения. Там такая типичная у-образная кривая по времени обучения получается. Ошибка. Здесь у нас стоит исход на 1 сотая, то есть 1% ошибка. В самом минимальном мы можем сделать 0,01%. Чистой ноль нельзя поставить, но 0,01%. К нашему изумлению, это не увеличивает практически время обучения. Странно, но это так. Ингибирование. Вот это вещь в себе. То есть натуральная вещь в себе. Мы придумали очень интересный способ повышения точности распознавания, когда у меня один нейрон вычитает себя из другого нейрона. Они расходятся и у них очень большая разница в распознавании получается. В некоторых случаях работает совершенно великолепно, в некоторых случаях так себе. Пару случаев было, когда ухудшался результат. Но вот интереснейшая штука. Здесь присутствует мой старый друг и коллега три специалист, Александр Смолкин. Вот, Саша, помаши ручкой. Тебя многие видят. Да. Я ему послал для тестирования программу. А он мне написал, что он стал тестировать почерки. Я был и собрался написать «Бог с тобой». Это совершенно не предназначено для тестирования почерков. Это не может тестировать почерки. А после этого он мне пишет, он различает почерки его и жены, и на русском, и на английском. Кстати, довольно похожие почерки, как ни смешно. Вот, различает. в режиме ингибирования. Честно говоря, ингибирование – это, знаете, как дикая карта. Мы до сих пор не очень хорошо представляем его свойства. Если будете тестировать, от души рекомендую поиграться с этим ингибированием. Может, чего неожиданное выйдет. Кстати, основная идея ингибирования принадлежит присутствующему здесь по машей рукой Володе Владимиру Маценко, который делал всю эту программу и интерфейсы, и наполнение, и все в этом. Кстати, для профессионалов. Презирайте нас изо всех сил. Никакого здесь нету питона, никаких здесь нету сложностей. Все сделано на C++. Но никаких проблем перевести в питон, сделать пригодной для видеокарты. Все это впереди. Ну и хэлпы пока все на русском, но сейчас переводится на ангельский язык, безусловно. По-видимому, на японский будут переводиться. Они уже проявили определенный к этому интерес. Еще существенно обратить внимание колор, грей и грей приведенный. Вот я сейчас просто покажу, чем они будут отличаться. Если я возьму Gray и проведу заново обучение, то у меня пианино будет выглядеть вот так вот. И при распознавании, сейчас проверим, они выглядят вот так. есть еще возможность gray приведенной это мы сделали так чтобы выровнять световые гаммы если я сейчас оттренирую на этом на приведенном то у меня например тот же юпитер будет выглядеть вот таким вот образом а ну-ка интересно давайте распознаем recognize какой-нибудь другой юпитер где они у нас там кстати юпитер наверняка лучше распознается именно цветным нет все-таки распознается вот то есть они сейчас от грей приведенная в одной световой гамме подогнанная да и Это я вам все показал, как работает. Осталось только вот это. Но здесь нечего особо показывать. Я могу сохранить любой проект и потом его открыть и вызвать. Никаких особенностей это не вызывает, не приводит к никаким особенностям. выбор параметров сети, следующий выбор параметров файлов для обучения. Здесь показаны все методы загрузки, как отбирать файлы, что рекомендуется, какие режимы лучше использовать, и приведены конкретные материалы. Здесь подробно Примерно то, что я вам показывал про распознавание образов, но только на распознавание другой группы могу показать. А, вот что я еще не сказал. Здесь очень интересный момент. Вот я здесь распознавал лаптопы, пиано и часы. Вот у меня первые все опознались как лаптопы. И я подсчитываю, ну в самом деле сейчас я примитивно с сложением подсчитываю, в самом деле надо подсчитывать по байесу. Это будет реализовано в следующей версии. Я по байесу подсчитываю реальную вероятность того, что это лаптоп. Здесь получается вероятность лаптопа где-то... 3,9 против 0,04. Ну, скажем так, неплохая вероятность. Распознавание образов не входящих в выборки. Распознавание при минимальном обучении. Вот то, что я вам сейчас показывал. А, кошечку я не показал, она всегда очень хорошо распознается. Забыл. Распознавание поврежденных образов. неудачное распознавание. что делать при неудачном распознавании? очень интересно, при неудачном распознавании можно выделить новый класс. вот то, что я здесь говорил, классификация образов. я вам показывал эту классификацию на разных классах. Мы создаем новые классы. Вот этот класс Вася я создавал. И выделяем новые классы. Вот здесь все это показано подробно. Попросту, точно не укладывается, можно выделить в особый класс. Чисто предварительно, без реального врача. Я загрузил целую серию рентгенограмм желудка. И было написано, что это рентгенограмма абсолютно здорового желудка. Из примерно трех десятков подобралось штук десять, которые ближе всего к здоровому, а внизу подобрались те, которые явно нездоровые. Моих знаний не достаточно, чтобы определить, это язва желудка или просто пережрал товарищ. Но это живой возможный путь. Что еще здесь существенного? Распознавание образов, протоколирование работы, возможность использования программы. Вот это большой кусок. Общее коммерческое использование. Оно просто перечислено. Гибридизация. классических сетей с Паном. Это следующий кусок презентации, который я вам покажу через несколько минут, сделанный Иваном Ивановичем Негрешным. Он предпочитает показать фильм, а не самому участвовать. Ну, у каждого человека свои привычки. Он провел огромную работу вместе с Дмитрием Карпенко по сравнению, вот Колонин у меня спрашивал, есть ли сравнение. Да, Иван Иванович сделал мощнейшее, великолепное сравнение с множеством результатов классических нейронных сетей. Это вы увидите через буквально пару минут, а сейчас я закончу вот с этим. Мы считаем ПАН Нисколько не отвергает классические нейронные сети. Не отвергает. 70 лет люди работали. Что мы говорим? Ребята, вы хорошо делали, вы хорошо работали, вы сделали массу прекрасной работы. Ну и массу чепухи тоже, нет сомнений. Так всегда в науке. 1% серьезного и 99% чепухи. Но серьезного много. И это никуда не пропало. Просто в ваш старый автомобиль, в котором стоял пыхтящий и глохнущий двигатель внутреннего сгорания, мы поставим мощный электромотор. А дальше ехайте. Шасси не надо менять, кабину не надо менять, сиденья не надо менять, рулевое устройство не надо менять. Просто поменяли сердце. Мы сегодня уже знаем, как сделать на базе ПАН свёрточную сеть. Она будет намного проще, чем существующая сегодня свёрточная сеть. Как сделать все глубинные слои вместо машин, ограниченных машин Больсмана, как сделать их из ПАН. Работать они будут так же, только примерно пару тысяч раз быстрее. Но сверточная сеть будет сверточной сетью, и глубокая сеть будет глубокой сетью. Если вам нужна сеть Коханина, welcome, на базе ПАН она ничуть не хуже работает, чем, в самом деле, лучше. Что еще существенно? Это вы сами прочтете. Гибридизация существующих программных продуктов и ПАН. В частности, возрождение экспертных систем, развитие объектно-ориентированных языков. Любой кусок ПАН годится вам как объект. Объектно-ориентированный объект распознавания носов или распознавания гнилых лимонов. Система умной адаптации софтверов. И мы об этом в тот раз чуть-чуть говорили. Одна из самых любимых моих тем – это умная система управления базами данных, которая ползает по вашей сети, ловит закономерности, выводит вас куда надо и так далее. Возможности эмуляции. Но самое главное, понимаете, Об этом я дальше скажу. Не буду сейчас на этом останавливаться. Хочу быстрее перейти к части Ивана Ивановича. Заканчиваю с вот этим. Возможности использования программы здесь масса разных вариантов. А, вот одно еще не показал. Типичный проект. Проект-система не реализована еще, но, видимо, будет. Или не будет, тут уж кто его знает. Пока все в процессе. Но вот система распознавания – это не просто только сетка. Одна сеть ни черта вам не даст. Ну да, давайте продавать усовершенствованный цилиндр автомобиля. Шофера здесь, наверное, немало. А вы купите отдельный цилиндр? Куда вы его деваете? На выставку? Вот проблема. Идет сплошной поток стеклопакетов, примерно 30-40 разных вариантов. И в них могут быть трещины в стекле, могут быть сколы, может быть повреждение фурнитуры, еще что-то. Черт-те что может быть. Стоят люди, надзирают, глаза у них болят, пропускают половину брака, потом возвраты, скандалы, падение авторитета, мы вас больше не будем покупать, ну и так далее. Вот разработанная конкретно линия. Ральганг, здесь-то не все едут по ральгангу такому, перемещаются в вертикальном положении, могут в горизонтальном. Ральганг две камеры, лампы подсветки, вот камера нарисована подробно, лампы подсветки обнаруживает ошибку, моментально сообщает, передвигается на второй рельганг, бракованная продукция на доработку и возврат, либо выход на упаковку. Вот это точно мы называем проект и точно мы предлагаем людям разрабатывать. На нашем сервере будет не только программа «Работайте как хотите», но и тризовские инструменты и наша помощь, консультация тризовской группы для тех, кто хочет создать систему распознавания брака или систему распознавания магазина хворишек или систему распознавания заполненности полок в универмаге и миллион с хвостиком других систем вот это так сказать по поводу применения и что я очень хотел сказать вот здесь вот Общая эволюция продуктов и технологий показывает жутко любопытную вещь. В начале XIX века пошла паровая машина, и началась энергизация всей промышленности. Если считать начало XIX века, то более 50% всей энергии, которую люди использовали, была наша собственная мышечная сила. Вы знаете, какой процент собственной мышечной силы в той энергии, которую использует человечество сегодня? Менее двухтысячных, при самом щедром подсчете. Менее двухтысячных. Все остальное – энергия. До начала 20-го века энергия в основном промышленности. Начало 20-го века полностью сменило наше окружение. Вот в Форд-музее есть четыре комнатки. Комнатка 17-го века, 18-го века, 19-го века и 20-го века. Не комнатки – кухни. Первые три почти неотличимы. Ну, чуть красивше печка выглядит. Ухваты чуть более ухватистые. Двадцатый век совершенно другое. И те, и стиральная машина, и холодильник, и посудомоечная машина, и кран, по которому течет вода. Все энергетическое. Мы живем в энергетическом окружении, либо Вот это вот не энергия, это просто бутылка, но она сделана через энергию. Как иначе полиэтилен заполимеризуешь? Как воду очистишь? Короче, 20 век – полное изменение нашего окружения в связи с тем, что это 20 век, тем, что энергия пошла в наше окружение. абсолютно достоверное предсказание. В следующие 30-50 лет будет полная смена нашего окружения на умные продукты, либо продукты, сделанные по-умному. И вот это гигантская область для будущего гигантского маркетингового торнадо. И вот здесь очень интересная вещь. Эта торнадо не может развернуться на базе тех нейронных сетей, которые делают толстопузые компании типа Амазона, Гугла, Майкрософта, Фейсбука и же с ним. Они бюрократизированы, они не могут делать маленькие продукты. Маркетинга и торнадо возникает всегда только тогда, когда к делу подключается огромное количество дилетантов, любителей. На базе ПАН небольшую систему для распознавания, например, почему чешется левая рука. Хороший врач Кожник в этом разбирающийся, сделает в течение нескольких месяцев. Вот это крайне важно. Мы предполагаем огромную вспышку маркетингового торнадо на базе малых продуктов, которые будут продаваться на всяческих сетевых магазинах, которые будут делать отдельные люди, которые будут делать небольшие группы, кафедры, которые будут делать стартапы и так далее. Нет, это не исключает большие компании тоже. В любом случае маленькая группа не сделает огромную языковую систему, и мы ее будем использовать. И вот сейчас я перехожу к чему. 

S08 [00:46:36]  : Еще я очень быстро... Борис, извиняюсь, я просто чуть-чуть стрянул. У нас уже 50 минут прошло, то есть у нас тут есть еще вопросов. То есть у нас по докладу, точнее у вас, сколько примерно планируется? 

S03 [00:46:50]  : Я планирую кинофильм сейчас минут на 20 и ответы на вопросы. 

S08 [00:46:56]  : Ага, хорошо. 

S03 [00:46:57]  : Просто вы еще сказали, что Владимир профессиональный бюджетник. Сейчас я просто закончу чем? Вот в самом деле здесь есть необязательный кусок, который называется «Справочные материалы». Что такое ПАН? Здесь подробное описание. Главные особенности – конструкция, обучение, преимущества – это, вообще говоря, выдержки из пишущейся книги. Просто тем, кто захочет познакомиться. В самом деле, для работы с сетью это совершенно не обязательно. Вот попросту не нужно, это можно без этого делать. Но для тех, кто захочет познакомиться, это пожалуйста. Здесь сравнение разных нейронных сетей. Вот сравнение биологической, классической и пановой сети. Красное, желтое – это хорошо, серое – это плохо. И здесь набор статей для более глубокого ознакомления. Видеоматериалы, включая прошлую нашу конференцию, и материалы по защите инвентарной собственности. Доступ к этим патентам, пожалуйста. Если кому-нибудь интересно, они у меня есть и на русском языке, но на английском они сильно изменены, потому что в процессе переписки многое менялось, в русском этого нет. Всё, я это сейчас закрываю, ну или просто свёртываю, и я хочу запустить теперь кинофильм. Негрешный Иван Иванович. руководитель программной группы, который сделал платформу Омега-сервер. Вот это здесь платформа. Он предоставляет вот это вот примеры применения PAN для самых разных вещей. Они все обучены. И он предоставляет нам возможность посмотреть его кинофильм. 

S00 [00:49:08]  : Здравствуйте всем. Очень приятно присутствовать на конференции. Я прошу прощения, Борис. Видео я прислал в скайпе, свежую ссылочку. Пожалуйста, откройте. Вот сейчас, сегодня буквально. 

S03 [00:49:19]  : Понял, понял, Иван. Спасибо, что вмешался. Открыть в скайпе, да? Сейчас, сейчас, сейчас. Вот, Иван Иванович, не грешный. Вот она. Я открою на весь экран, окей? 

S08 [00:49:43]  : Да, конечно. Так, почему-то на весь экран не показывается. 

S03 [00:49:55]  : Сейчас она еще не загрузилась. Пэйдж. Чего-то она... 

S00 [00:50:03]  : Для демонстрации принципиальных возможностей применения нейросетей типа PAN для решения практических задач. Платформа включает расширяемый набор плагинов, каждый из которых предназначен для генерации определенного типа нейросетевых веб-приложений, PVA, для классификации изображений, семантики. Кроме новейших моделей нейросетей типа PAN, для предобработки данных могут использоваться компоненты других библиотек машинного обучения, таких как TensorFlow и других. В этом видео будет продемонстрировано создание приложений типа Chatbot, приложений для классификации эмоций, распознавания лиц голосовых команд. Для экономии времени презентации в некоторых местах воспроизведение видео ускорено. Следующий пример демонстрирует создание приложения типа Chatbot. В выпадающем меню плагинов выбираем плагин для генерации приложения типа Chatbot и вводим имя нового приложения. На панели меню находятся настройки параметров нейронной сети в зависимости от конфигурации текущего плагина. Все компоненты приложения отображаются в древовидном списке и на панели закладок в окне приложения. Примеры файлов, содержащих тематические подборки текста в виде вопрос-ответ в формате, который используется для обучения нейронных сетей. Для обучения нейронных сетей могут быть использованы любые пользовательские данные, переписка техподдержки и так далее, приведенные в данный формат. Запускаем процесс обучения. Для каждой тематической подборки создается отдельный пакет нейронных сетей. После окончания обучения публикуем приложение, содержащее в себе обученные нейронные сети. опубликованное приложение. 

S08 [00:52:08]  : Борис, извиняюсь, а нельзя все-таки сделать так, чтобы было видно, потому что четверть буквально полуэкрана видна. 

S05 [00:52:18]  : Вот здесь. Я первый ребенок. 

S00 [00:52:31]  : Доступ к опубликованному приложению... Да, вот сюда, сюда. 

S08 [00:52:34]  : Вот на квадратик нажать, Борис. Правее. 

S04 [00:52:39]  : Нет. 

S08 [00:52:40]  : Нет, правее надо было. 

S03 [00:52:44]  : Что-то непонятно. Так. 

S02 [00:52:49]  : Антон, я прошу прощения, Иван скинул ссылку на это видео. Я понимаю, но нам тогда нужно расходиться и смотреть ссылку, то есть мы же все-таки… Нет, это же можно параллельно делать, я это делаю параллельно вот сам просто. 

S00 [00:53:00]  : …приложение для классификации человеческих эмоций. В выпадающем меню плагина выбираем соответствующий плагин и вводим имя нового приложения. В качестве обучающей выборки должны быть представлены изображения лица человека, на котором наблюдается определенная эмоция. Примеры классовых эмоций, предопределённых в шаблоне по умолчанию. Нейтральность удивления. 

S08 [00:53:32]  : Да, вот, коллеги, мне тут предложили, просто действительно ничего не видно. Может быть, давайте я просто тогда это видео запущу. 

S03 [00:53:45]  : Иван Иванович, подожди секундочку, здесь народ плохо видит. 

S08 [00:53:50]  : Да, давайте я попробую запустить эту ссылку. Я тогда у себя это закрываю? Да, пока закрывайте, а я попробую тогда открыть ссылку. Так, ссылку, ссылку, ссылку. В нашем чате сейчас. Да-да-да, секундочку, я пытаюсь попасть в чат. А, да, я сначала должен стоп-ше сделать. Да, теперь я должен запустить вот эту вот ссылку. 

S00 [00:54:22]  : Так, сделать ее... Перед вами альфа-версия платформы для демонстрации принципиальных возможностей применения нейросетей... Сейчас, секундочку, теперь я скриншей должен сделать. 

S08 [00:54:32]  : Так. Теперь я должен сделать скриншер. Так, все, мой экран видно и все, поехали, да? 

S00 [00:54:41]  : Для решения практических задач. Платформа включает расширяемый набор плагинов, каждый из которых предназначен для генерации определенного типа нейросетевых веб-приложений, PVA, для классификации изображений, семантических данных, аудио и так далее. Кроме новейших моделей нейросетей типа PAN, для предобработки данных могут использоваться компоненты других библиотек машинного обучения, таких как TensorFlow и других. В этом видео будет продемонстрировано создание приложений типа Chatbot, приложений для классификации эмоций, распознавания лиц и голосовых команд. Для экономии времени презентации в некоторых местах воспроизведение видео ускорено. Следующий пример демонстрирует создание приложения типа Chatbot. В выпадающем меню плагинов выбираем плагин для генерации приложения типа Chatbot и вводим имя нового приложения. На панели меню находятся настройки параметров нейронных сетей в зависимости от конфигурации текущего плагина. Все компоненты приложения отображаются в древовидном списке и на панели закладок в окне приложения. Примеры файлов, содержащих тематические подборки текста в виде вопрос-ответ в формате, который используется для обучения нейронных сетей. Для обучения нейронных сетей могут быть использованы любые пользовательские данные, переписка техподдержки и так далее, приведенные в данный формат. Запускаем процесс обучения. Для каждой тематической подборки создается отдельный пакет нейронных сетей. После окончания обучения публикуем приложение, содержащее в себе обученные нейронные сети, на веб-сервере для общего доступа. Опубликованные приложения могут работать на платформах Windows, iOS, Android, Linux, на десктопах, смартфонах, планшетах, терминалах и т.д. Приложения имеют открытый исходный код, есть возможность работы оффлайн, размещения в App Store и Google Play. Открываем опубликованное приложение по ссылке в браузере. для доступа к опубликованному приложению других пользователей. Достаточно поделиться ссылкой на него. Следующий пример демонстрирует создание приложения для классификации человеческих эмоций. Выпадающим в меню плагина выбираем соответствующий плагин и вводим имя нового приложения. В качестве обучающей выборки должны быть представлены изображения лица человека, на котором наблюдается определенная эмоция. Примеры классовой эмоции, предопределённых в шаблоне по умолчанию. Нейтральность удивления. Добавляем класс «Злость». Добавляем изображение для созданного класса из демонстрационного набора. Изображения для классов по умолчанию уже добавлены. Запускаем процесс обучения. Для изображения классов определяются координаты ключевых точек лица с помощью предобученной модели FaceMesh в составе TensorFlow.js. Эти данные используются для обучения нейросети Kpopan. Публикуем приложение. На данном этапе большинство приложений, которые позволяют генерировать платформы, являются эскизными, то есть обладают ограниченным функционалом. Однако открытый исходный код позволяет расширять функционал, как с нашей помощью, так и привлекая сторонних разработчиков. Открываем опубликованное приложение по ссылке. Данное приложение позволяет выполнять классификацию в двух режимах. Классификация изображений, загруженных из хранилища, и классификация изображений, полученных через веб-камеру устройства, анализируя кадры с заданной периодичностью. Перед классификацией с помощью PAN происходит построение маски по ключевым точкам лица. Следующий пример демонстрирует создание приложения для распознавания лиц. В качестве набора данных для обучения используются изображения, которые содержат лица людей в произвольном окружении. Каждый класс соответствует конкретной персоне. Классы по умолчанию «Анна Холлоуэй» и «Уилл Смит», каждый класс по умолчанию уже содержит набор фотографий персоны. Пользователь может удалять существующие классы и создавать новые. Создаем пользовательский класс ArnoldSchwarzenegger и добавляем изображение из демонстрационного набора. В процессе обучения для каждого изображения предобученная модель Blazor Face в составе TensorFlow.js определяет контуры прямоугольника лица Bounding Box, после чего обрезанное по этим контурам изображение используется для обучения. Публикуем и открываем приложение в браузере. Перед классификацией происходит определение bounding box лица на исходном изображении. Полученное обрезанное изображение используется для распознавания. Следующий пример демонстрирует создание приложения для распознавания звуковых команд. В качестве набора для обучения используются звуковые команды, определяемые пользователем в зависимости от решаемой задачи. Каждый класс должен содержать набор звуковых файлов человеческой речи определенной голосовой команды. По умолчанию созданы классы для команд Down, Go, Left, No, Right, Stop, Up и Yes. Перед обучением набор звуковых файлов проходит сложную цепь преобразования в конечный тензор для подачи в модели нейронной сети PAN. Основным узлом преобразования является использование алгоритма краткосрочного преобразования Фурии, который заключается в конвертации звуковой волны исходного звукового файла в его спектрограмму. Изображение спектрограммы проходит процесс нормализации и подается в модель нейросети. Для классификации звукового файла необходимо открыть iOS-хранилище. Также в приложении есть возможность записи и анализа звука. 

S08 [01:04:46]  : Так, коллеги, что-то произошло, меня слышно, есть звук или нет? 

S00 [01:04:50]  : Вас слышно. Определяет контуры прямоугольника лица, bounding box, после чего обрезанное по этим контурам изображение используется для обучения. 

S06 [01:05:07]  : Фильм отсадился на несколько секунд или даже минут назад. 

S00 [01:05:10]  : Отбиваем приложение в браузере. Немножко вперед перемотайте, пожалуйста. Это уже было у нас. В качестве набора для обучения используются звуковые команды, определяемые пользователем. В зависимости от решаемых класс должен содержать набор звуковых файлов человеческой речи определенной голосовой команды. По умолчанию созданы классы для команд Down, Go, Left, Up, Pop, Up и Yes. Перед обучением в набор звуковых файлов проходит сложный цепь преобразования в конечный тензор для подачи в модели нейронной сети PAN. Основным узлом преобразования является использование алгоритма краткосрочного преобразования Fourier, который заключается в конвертации звуковой волны исходного звукового файла в его спектрограмму. Изображение спектрограммы проходит процесс нормализации и подается в модель нейросети. Для классификации звукового файла необходимо открыть его из хранилища. Также в приложении есть возможность записи и анализа звука с микрофона устройства. 

S03 [01:06:57]  : почему-то повторяются, а нет. 

S00 [01:07:23]  : На данном этапе разработки платформа позволяет генерировать приложение следующих типов. Яблоко. Приложение для классификации изображений. Приложение для классификации эмоций человека. Приложение для распознавания лиц. Приложение для классификации пост руки человека. Приложение для классификации текстов и заголовков. Определение токсичности. Приложение для классификации пост тела человека. Приложение для классификации голоса человека. Определение коммуникации. Приложение для распознавания языков ума. 

S01 [01:08:11]  : Спасибо за внимание. 

S08 [01:08:23]  : Спасибо, коллеги. Так, у кого-то, кому-то надо сделать мьют, по-моему. 

S03 [01:08:29]  : Снова демонстрацию, господа, да? 

S08 [01:08:32]  : Да, Борис, пожалуйста, да. 

S03 [01:08:35]  : Вот я хочу подчеркнуть крайне важную вещь. Ваня, на мой взгляд, великое дело. Это не то, что я говорил, что всё классическое используется, что мы сами будем делать все заново, все эти огромные наработанные материалы. Нет, он просто использовал ПАН с уже наработанными материалами, и это создает огромное ускорение всего. Вот здесь вот Я забегу чуть-чуть вперед. Антон Германович задал целый ряд вопросов, и в частности была вот серия второй и третьего вопроса. Каковы результаты сопоставления результатов ПАН с результатами State of the Art в двух этих самых? Вот это вот точно сделал негрешный с Дмитрием Карпенко. Это наборы данных по дилибету индейцев из пятой конкретной данной. Скорость работы Керос и ПАН 0.84, 0.0.41, 0.00.41. Обратите внимание, вот это вот уже конкретные данные испытания и всех тоже можно будет найти. 

S08 [01:09:59]  : Ларис, извините, вот здесь вот как раз самое интересное начинается. Вот нельзя в этом месте все-таки попытаться запустить слайд-шоу. Вот вверху есть кнопочка слайд-шоу. 

S04 [01:10:13]  : Какая? 

S08 [01:10:13]  : Влево в меню, в меню влево. 

S03 [01:10:15]  : Вот сейчас проверим. Сейчас должно получиться. 

S08 [01:10:19]  : Влево, влево. Да, вот, прекрасно. Вот, да, так хотя бы лучше. Да, пожалуйста. Да, да, все, все, все. Теперь вот, пожалуйста, продолжайте. Ага. 

S03 [01:10:29]  : обратите внимание что здесь попросту вот сравнение по скорости работы то что для нас сейчас наиболее важная так мелочь разница чисто керос и пан разница в 203 раза по скорости ну 23 это совсем мало для этого приложение 96 здесь 166 честно говоря Господа, это еще и не совсем правильно. Во-первых, у них же все прописано на очень высоком уровне, у нас существенно меньше. Мы находили разницы в тысячи раз, когда обучали по матричной системе, но разве этого недостаточно? Посмотрите, а точности примерно соответствующие. 72, 78, 97, 77. У нас здесь хуже. Панк здесь зачастую хуже по точности, но критически очень ненамного. А вот это мы можем и улучшить. Мы сегодня прекрасно знаем, как улучшить точность. Мы не испочерпали своих ресурсов. Вот это вот, на мой взгляд, крайне важная вещь. Антон Германович, я ответил на ваш вопрос? 

S08 [01:11:53]  : Да, да, да. То есть у меня и дальше, естественно, остается вопрос, а как теперь можно улучшать ПАН, но это, если вы знаете, то это... Я расскажу. 

S03 [01:12:03]  : Сейчас вернемся к следующему. Я перейду к специальному материалу, который я хочу очень коротко здесь озвучить. о светлом будущем классических нейронных сетей большими черными буквами. Я не издеваюсь. Я издеваюсь, в самом деле. Развлекаюсь. Имею я право развлечься. Произошло два, на мой взгляд, исторически страшных события. В октябре вышла номер журнала спектрум я не знаю вы его видите на экране да вы видите а он у меня просто перекрыт нажим неважно вы видите и и спектрум господа 50 лет назад молодым инженером на электросиле я молился на и и или а и и и Это ассоциация американских инженеров, электриков и электронщиков. Самая большая научная ассоциация в мире. И в отличие от большинства других профсоюзов, там ни одного бюрократа у руководства, там реальные люди у руководства. Их журналам можно было всегда верить свято. Мы на «Электросиле» дрались, когда получался очередной выпуск, кто первый будет читать. И вот там специальный репорт. 9, а почему-то здесь только 8, ну может быть 8, я мог ошибиться, 8 статей о глубинных сетях. Бурное прошлое, неопределенное будущее. Как работает глубокое обучение. Дипмайнт, переизобретение робота. 7 причин, по которым ИИ терпит неудачи. Это люди из MIT, это люди из высшего уровня. Что они в целом говорят? Ну вот, это цитата прямо из этого. «Экстраполируя достижения последних лет, можно предположить, что к 2025 году уровень ошибок лучших системы глубокого обучения, предназначенных для распознавания объектов, должен быть снизен до 5%. Мы, вообще говоря, это легко можем сделать на своих сетях. У них вычислительные ресурсы энергии, необходимые для тренировки таких систем, приведут к выбросу углекислого газа, который Нью-Йорк производит за месяц. Это, господа, это что? Это сотни мегаватт, мегаватт. Я электрик, я понимаю, что это такое. А я это все сделаю на лаптопе. Ну, не на лаптопе, скажем, на обычном дешевом мейнфрейме. Вот это график, который я сделал лично 9 лет назад. Кто только надо мной не смеялся. Откуда ты взял эти данные? Их графики, там просто много графиков, трудно было все это копировать, воспроизводить, но они повторяют все это. Это именно вот периоды выступления и точно сейчас предсказывают Нил Томпсон, профессор MIT. говорит о том, что кронты нейронным сетям. Все, следующее падение впереди. Мы утверждаем – нет. Вот это из их статьи. Я не буду сейчас все это пересказывать, но отмечу парочку проблем. Мы как-то никогда особо не задумывались. Нейронные сети в 80-х, 90-х не могли пойти просто потому, что не было столько информации. Интернет дал достаточное количество информации, выяснилось, нет столько техники, чтобы это все обработать. Вот самый большой в мире чип больше двух миллионов раз стоит. Ребята, этот чип 7 на 7 сантиметров выделяет 20 киловатт тепловых потерь. У меня трёхэтажный дом, у меня 12 киловатт расход на весь дом. Вражёные уродства персептрона, об этом я рассказывал. Ошибка Дейла и ошибка Хэбба, которые привели к тому, что произошло. Там, где начинается нейронная сеть, кончается наука. Это прекрасный цитат, он мне сказал Антон Беляков, его профессор, который обучал нейронным сетям. А вот Джеффри Хинтон говорит то же самое. Мы смутно представляем, как работают. Мы абсолютно точно сегодня представляем, как работает ПАН. Она совершенно прозрачна. Ну, 60 лет в струю. Была тогда такая книжка Игнатьева. Господа, чем сегодня занимаются нейронные сети? Играми в основном. Ваши игры в сильный ИИ, они мне нравятся. Я с симпатией отношусь к тому, что вы делаете, просматривая ваши материалы. Как это самое? Будет собственных Платонов и быстрых разумов Нейхтонов, сеть электронная рожать, сказал Ломоносов. Но будем ли мы в это время жить? Это очень хорошо, но это как рассуждение о будущем развитии техники у Роджерса Бекона в Новой Атлантиде. Далеко. А я практик, мне 75 лет, и я хочу увидеть результаты своих трудов. А вот что касается остальных, вы знаете, из примерно 200 миллионов ссылок на сети нейронные, больше 100 миллионов включают слово «этика». Это же идиотизм невероятный. Конференции по этике. Нейронные сети и гомосексуалисты. Нейронные сети и расизм против черных, против белых, против зеленых, против голубых. Сдохнуть можно. Нейронные сети и целая конференция, политическая корректность. Я когда слышу эти пакостные слова, злоба охватывает. Проблема сингулярности. Вспомните, еще несколько лет назад сколько криков. Ну, не могу я поверить, что умница, блестящий изобретатель и исследователь Рэй Курцвелл настолько был глуп, описав эту проблему. У меня такое впечатление, что он всех троллил. Ну, тролл это, а не проблема. А вот свеженькое, просто для общности. В августе 32 преподавателя, 117 ученых Стэнтонского университета заявили, что произошел радиальный двиг парадигма. Я не поленился, открыл, почитал. Знаете, в чем радиальный двиг парадигма? Они придумали новое слово, модели фонда. Ну нихрена, ничем они не отличаются от обычных моделей. Ну вот модели фонда и всё. 

S08 [01:19:30]  : И это... Извините, Борис, я поправлюсь всё-таки, извините. Это всё-таки речь идёт о базовых моделях. То есть это foundation models, это корректный перевод и уже это пошло в русскоязычной литературе как базовые модели. 

S03 [01:19:47]  : Знаете, я посмотрел на английском ссылке. Честно говоря, только наискосок и только со злобой. 

S08 [01:19:55]  : Ну там речь идет просто об глубоких моделях, которые осуществляют мультимодальное обучение. То есть обучение в различных модальностях. 

S03 [01:20:08]  : Ничего специального нет. Хинтон об этом раньше написал и гораздо интереснее. Я посылал, по-моему, большой статью, анализ статьи Хинтона. но вокруг этого крики сопли вопли публикации понимаете но я понимаю надо писать диссертации я понимаю надо получать гранты но почему за нас счет ладно это я уже просто Я не буду это сейчас комментировать. Получите материалы, посмотрите про катастрофическое забывание. Очень интересная вещь из семи факторов, которые перечисляются в этом спецотчете. Шесть факторов. Я сразу сказал, она нас не влияет, а мы мимо. Один точно так же. Слабы нейронные сети в математике. Я с этим ушел и стал думать, а почему они слабы в математике. И я обалдел. А мы не слабы. Фокус в чем? Математика очень хороша в гибридном режиме. нейронная сеть, ты уже начал учить глубокую сеть backpropagation, ты не можешь остановиться до окончания. Мы идем вперед, мы обучили один слой, остановились, обработали его на статистике, на той же MATLAB, на чем угодно, получили результаты, пошли дальше, опять остановились. P-сеть сама по себе также слаба в математике, как любая другая нейронная сеть. Но она позволяет остановиться и поработать математически, а потом пойти дальше, не прерываясь. И еще одна вещь. Я не показывал это, но я проверил интересный эксперимент. Я взял, нарисовал ломаную кривую где-то из 30-40 точек. Такая ломаная кривая идет. Произвольно нарисовал. Я взял ее, поместил в серединку по центру веса координат и провел прямые, которые ее сверху и снизу как бы охватывают полностью. И я обучил сеть нескольким десяткам наклонных кривых из этого центра. И так получается, я прихожу к аппроксимации, но я пришел 5 градусов между ними. В самом деле я могу сделать в будущем и 0,01 градуса. Я аппроксимирую чисто зрительно. И точно так же можно делать зрительную аппроксимацию по квадратурам, по кубам, я уже не говорю про синусоиды, там все вообще просто. То есть и математика тоже работает. В создании «Панк как науки», в ее прозрачности, хочу отметить и буквально поклониться памяти Вадима Давыдовича Глезера. Его книга «Зрение как мышление» Блестящая совершенно книга. Я ее подробнейше проработал и написал по ней конспект. Она дает очень много для построения полной ПАН-сети. Сегодня это на мази. Я думаю, что день через полгодика я попрошу у Антона Германовича еще раз рассказать уже теорию. И второе событие, которое меня потрясло за это время. Бюрократ там правит бал. Вот красавчик Фейсбук. Что они сделали? Не просто закрыли сервис, ведь больше миллиона людей им пользовалась, больше миллиона профилей исчезло. А вы знаете, что прямо вопль стоит? Масса слабовидящих людей, слепых, пользовались этим сервисом. Почему не закрыли? Никаких технических проблем. Они, извиняюсь, облажались на чисто бюрократическом руководстве. Они на себя накликали миллиардные суды, потому что не могут контролировать. А почему не могут? Потому что пытаются контролировать все. Ребята, абсолютно та же ситуация, как когда-то в Старом Советском Союзе министерству пытались контролировать каждый чих заводов, в результате возникла абсолютно липовая система так называемой корректировки планов. Я еще помню, как каждый квартал я брал портфельчики на стрелях, учался в Москву корректировать планы по своей группе ФСА. Это же сдохнуть можно было. Кретинская бюрократия. И вот это все то, что ПАН даст возможность простому человеку, грубо говоря, делать что-то, не зависеть от гигантов, которые, к тому же, всю нашу информацию в свою пользу крадут и используют. Я считаю очень важной вещью. Ну и Это я вам уже говорил. Рынок интеллектуальных продуктов. Не будем касаться. Вот это, я считаю, реальное будущее. Не будет вот этого загиба. Пойдет вперед. И во след дилижансам, что были новинками транспорта, отработав свое, паровозы уйдут на покой. Это цитата из Евгения Сазонова. Помните такого душеведа и душелюб? Людовед и душелюб Евгений Сазонов в литературной газете. Классические нейронные сети вымрут. Точнее не вымрут, а приспособятся к пансердцу. Ну и вот теперь Я ответил на третий и четвертый вопрос Антона Колонина. Первый вопрос был. Является ли математическая модель достойной нейросети ПАН эквивалентной двуслойной классической сети? Очень трудный для меня вопрос. Я не знаком с двуслойными классическими сетями и, честно говоря, не имею времени с ними знакомиться. Мы не проводили математического сравнения, мы проводили практическое сравнение. Я утверждаю, не эквивалентно, но не по соображениям математического анализа. Если вы хотите, сделайте такой анализ, мы представим все данные. А по совсем другим соображениям. Обучение классической сети идет медным градиентного спуска. Вам нужны десятки, сотни и тысячи а может десятки тысяч эпох. А у меня, вы видели, все учится в две эпохи, из которых одна – холостой ход. Это не может быть эквивалентным. И еще на этой базе практически что мы сделали? ПАН – это персептрон. Это просто персептрон, продвинутый перцептрон нового типа. И на базе этого нового перцептрона мы можем сделать и глубинную сеть, и сверточную сеть, и все прочие сети. Это я тоже уже говорил. Буквально последние два дня назад мне Антон Беляков подсказал обратить внимание на карты признаков. Они прямо ложатся у нас. трудом делал как распознавать детали образа выяснилось через карту признаков щелки готово все распознает очень сильно очень хорошо ну вот по моему в целом да это уже все было а вот еще четвертый вопрос был очень важный четвертый вопрос Можно ли использовать ПАН для построения интерпретируемого и объяснимого искусственного интеллекта? И как именно? С примерами. Здесь, в самом деле, два ответа, связанные с двумя разными пониманиями, что такое интерпретируемый искусственный интеллект. Первое понимание – прозрачность работы сети. понимание, как она обучается, как формируется обобщенный образ, как распознаётся, как идёт классификация и кластеризация. Это абсолютно понятно, абсолютно прозрачно, легко объяснимо. А вот второй, точно самое важное для, например, экспертных систем, для систем диагностики, ну не один врач, получив диагноз от машины, которая говорит, дай ему 37 грамм цианистого калия, не решится это сделать, потому что он не понимает, как цианистый калий может помочь при чесотке в промежности, понимаете. Нужно объяснить, почему именно это решение. сложнее и теоретически должно быть объяснимо на практике мы проверили несколько простых случаев ну вот типа там распознание кривых я например делал такую вещь обучил систему и системе набору сеток Система 32х32, 16 черных, 16 белых полос вертикально и горизонтально объяснила, как работает. Кстати, великолепная линейность. Линия под углом 45 градусов дает абсолютно одинаковое распознание по всем наборам сетки. 14, 12, 10, 8 линий и так далее. Абсолютно линейно. Но это еще не все. Вот Антон Беляков сделал схему возврата образа. То есть мы сделали обобщенный образ, А теперь можем вернуться, увидеть его картинку и сравнить два обобщенных образа и выявить, чем они отличаются. Более того, их можно формально, я их могу вычесть один из другого. В нашей сети это позволяется. Но пока не сделано. И тогда понять, в чем главное отличие. Это уже ход к пониманию паттерна. А паттерн – это и есть объяснение. Разработана пока теоретически схема вербализации выявленных сетью паттернов. И вот здесь вот очень интересная штука. В нашем софтвере, который называется Directed Evolution, управление эволюцией, я о нем чуть-чуть рассказывал в прошлой встрече, порядка 600 паттернов развития технических систем. Эти паттерны выявлялись, естественно, вручную, просто анализируя историю развития. Но есть паттерны выявления паттернов. Мы знаем, как это делать. В частности, для этого очень хорошо работает наша система Problem Formulator. Еще не сделано, но вполне может быть сделано, я думаю, в следующем году мы постараемся это сделать, систему, которая будет гибридизовать сеть PAN с системой проблем-формулятора для формального выявления и вербализации паттернов. Мысли о том, как это сделать, есть, но это еще только общие мысли. 

S06 [01:32:31]  : Борис, прости, пожалуйста, а вот вопросы в чате зума ты видишь? Может быть, ты хотя бы быстро включишь чат зума и пробежишь по ним? Люди задают очень много вопросов. 

S08 [01:32:40]  : Анатолий, я предлагаю, давайте мы просто дадим Борису закончить, а потом я по вопросам в зуме... Я уже закончил. 

S03 [01:32:48]  : Самое смешное, что я уже закончил. Последнее, что я еще не сказал, вот это последнюю строчку. В будущем году мы будем проверять схему эмуляции ПАН с помощью ПАН клеточного автомата. А клеточный автомат само по себе тоже объяснительный механизм. Он прекрасно... Помните эту гарднеровскую игру «Жизнь»? Он прекрасно объясняет, как происходит развитие. Я не могу сказать, что точно Антон Германович спросил, что я прямо могу сказать, да, это готово. Нет, это не готово. Но я знаю, как к этому подойти, и мы на это пойдем. Может быть, не так быстро это получится, понимаете. Мы очень многого не знаем. Мы очень часто открываем новые вещи. Мы делаем очередную версию ПАН, начинаем тестировать. Упа! А вот как она? Представляете, вот только вчера мне Саша Смолкин прислал этот анализ почерков. По всем моим предположениям, если бы меня попросили поставить пари, я бы сказал сто против одного, что это не будет работать. Заработало. я еще не понимаю почему разберемся все я закончил эту часть и я сейчас тогда включаю чат вот где он у меня вот чат 

S08 [01:34:13]  : Борис, там лучше с конца смотреть. В начале там в основном вопросы от меня, но давайте мы начнем с вопросов от уважаемых слушателей. И вот в первую очередь я бы дал слово Сергею Терехову. Сергей, вы можете задать свой вопрос? 

S04 [01:34:30]  : Давайте. 

S07 [01:34:32]  : Спасибо большое, Борис. Я очень благодарен вам за сообщение, за такую огромную работу, за такую целостную картину, о которой вы рассказали. Это очень здорово и, конечно, приятно. Это существенно отличается от большого потока информации, которая сейчас идет, такого хайпового, так сказать, плана вот в отношении этих глубоких сетей там и так далее. У вас совершенно, в неком смысле, такая классика такая получается. Но у меня вот вопрос на понимание. если я сразу неправильно понимаю, то вы мне прям сразу поправьте, и тогда вопрос исчезнет. вопрос вот какой. когда шло видео и то, что вы рассказывали, система обучается таким образом, что она берет некоторый класс, и работает с этим классом. она строит представление этого класса. она модальностями, многовходовыми нейронами, разными способами нашла некоторое представление этого класса. после этого она работу с этим классом закончила, и она берет теперь образы следующего класса и строит представление теперь для вот этого следующего класса. и так она делает для каждого из классов. после этого они по байосу сравниваются. естественно, тот, чья постелируемая вероятность раскрытия выше, тот и объявляется результатом классификации, а вероятности остальных классов тоже известны, вы нам их показываете. Вот это правильно я сейчас качественно говорю, да? Правильно. 

S03 [01:35:56]  : Хорошо. Только одну важную вещь. Мы проверяли два варианта формирования классов. Один вариант, один класс, один нейрон. Один вариант. Мы формируем класс, как в обычной нейронной сети. То есть берем этот нейрон и обучаем, допустим, 20 образов. Это запросто. Получается 100 эпох, полтораста, 200 эпох. Зависит от количества образов и так далее. Но у нас есть другой способ обучения. Честно говоря, очень смешной. Мы берем картинки, делаем их весовое представление каждой картинки и складываем типа среднего арифметического. 

S07 [01:36:47]  : Я понимаю, Борис. Это просто способ вычисления модели класса. Давайте будем считать, что в вашем распоряжении есть модель класса. теперь у вас есть несколько моделей для разных классов вы можете по байсу с ними работать так как мы всю жизнь работали в течение там всех 40 там каждый срок да теперь вот вопрос в этот момент когда он теперь начинается поскольку сеть не знает о том, какие будущие классы появятся, вот она когда работает с первым классом, она не знает, что появится второй, когда она работает со вторым, не знает, что появятся десятые, сколько их будет и будут ли они вообще, но вот другие классы. Скорее всего, если задача устроена так, что образы довольно хорошо разнесены в каком-то смысле, в каком-то представлении, то вам повезет. То есть новый класс, который появится, он будет достаточно в каком-то смысле дифференцироваться просто таким байосовым способом. Но что делать, если новый класс, который вы будете добавлять, в том представлении, которое не знает о том, какой будет этот класс, ведь первый класс вы закончили уже с ним работу полностью, а второй оказался вот таким, что вот в этом новом представлении они просто ну либо наехали друг на друга, либо вот Байсова граница значит слабо выражена. как быть, если не повезет, если выяснится, что следующий класс окажется в этом представлении, наложится на предыдущий. я вопрос этот мог бы задать и 40 лет назад на самом деле, потому что он стартует со старинной теории эстетического обучения образов и так далее, где говорилось о том, что нужно построить хорошую качественную статистическую модель каждого класса, и тогда границы байсов между этими моделями класса и есть оптимальная байсовая классификация. вот это классика, которая тогда была. она никуда не делась, эта классика, за исключением той ситуации, что делать, если в том представлении, которое выработалось, классы наложатся. Спасибо большое. И еще раз, чтобы второй раз не возвращаться, спасибо еще раз огромное вам за прекрасный доклад и очень приятно было его слушать. Спасибо большое. 

S03 [01:38:58]  : Ответ, в самом деле, два ответа. И первый из них очень смешной. настолько линейный коррелятор, что мне все, что относится к одному классу на одном нейроне, к второму на втором, я могу назвать класс первый и обучить его чему-то и класс второй и обучить снова тому же абсолютно он просто у меня будет распознавать первый класс 0 578 второй класс 0 578 они друг другу не мешают я могу его стереть отдельно один класс могу добавить стереть у меня первая нейронная сеть с первым классом включает один нейрон но все тысячи входов там сколько есть Второй класс, я добавляю второй нейрон. 128 класс, у меня будет 128 нейронов. То есть они просто друг другу не мешают. Но это первый ответ. А вот если я хочу существенно повысить развлечение, то я включаю режим ингибирования. И тогда происходит очень интересная вещь. Один класс убивает другой класс. то есть вычеркивает себя из другого класса. И если два одинаковых класса, то они попросту убьют друг друга. И начнет это тык-тык-тык-тык-тык бесконечный процесс, один будет убивать другой. Не начнет. Они просто сравниваются с самого начала. Если оказывается разница меньше какой-то величины, то вместо того, чтобы формировать второй класс, 

S07 [01:40:34]  : добавляется в первый класс. Для этого вам нужно иметь представление, в котором вы можете такую алгебру устроить. Чем сложнее будет задача распознавания... На чистой статистике. Если сходство больше 95 процентов, включаем и все, не разбираясь. такие уровни подходы байсовые они были, например, обучающие векторное квантование. модели Кахона, которые для карты Кахона хорошо известны, а вот learning vector quantization они менее известны, но они именно на этом принципе тоже основаны. но у них, как вы знаете, есть тоже генетические проблемы, связанные с тем, что на самом деле классам, поскольку вы понимаете, как вы сжать фазовый объем класса не можете, к вас классам становится тесно. то есть они просто не находят в том представлении для себя объема, чтобы там разместиться. в результате 128 классов вы запомнили, а для 129 класса у вас просто нету места в этом фазовом объеме. 

S03 [01:41:42]  : Прошу прощения. У меня же сеть наращиваемая. Я не начинаю с того, что у меня, допустим, 1000 нейронов или 25 нейронов или 85. Я могу наращивать постоянно сколько угодно нейронов. Более того, теоретически сейчас реализована простая схема. Один образ – один нейрон. Один класс – один нейрон. проверена, работает. Схема, которую мы назвали 3 на 2. Ну, грубо говоря, Когда один класс, один нейрон, у меня все нули на выходе и одна единица. 

S07 [01:42:20]  : Понятно, вам надо две единицы из трех, чтобы был класс. Я понимаю. Тоже такие схемы кодирования тоже были. Занимался именно рендером. Есть, на самом деле, очень интересные работы 80-х еще годов. Но все они упирались в то, чтобы Новую существенно сложную задачу решить нужно изменять качественно представление. Не просто инкрементально увеличивать длину кода, а нужно менять внутреннюю направленность, менять внутренние фильтры, которые внутри этого нейрона. А в этом случае единственный путь, который остается, это градиентное обучение. Потому что вы не можете, не зная, какие будут будущие классы, не зная заранее, какие они будут, вы не можете выбрать оптимальный фильтр, который вам обеспечит какую-то селективность. Но это у нас в дискуссии и с 80-х годов идет. Извините за то, что я на 40 лет назад отмотал. 

S03 [01:43:19]  : ну давайте я получил ответы на самом деле спасибо большое огромное еще раз я думаю что мы уже так много времени заняли мои вопросы спасибо понимаете в чем дело поскольку мы прекрасно понимаем что внутри дойдет до проблемы решим мы же специалисты по три мы решаем изобретательские задачи на раз пока это никогда не возникло у меня ощущение что ты не должно возникнуть возникнет решим Потому что мы прекрасно понимаем, как это работает внутри, нет больших проблем. Понадобится наращивать, понадобится, ну была такая идея, П-сеть внутри П-сети. Реализуемо. Короче, грубо говоря, или мягко выражаясь, вполне возможна иерархическая схема, когда каждый нейрон это собственная отдельная П-сеть. Небольшая там, допустим, но внутри этого нейрона еще пять нейронов внутренних. Ведь одна из важнейших вещей, вы знаете, в классических нейронных сетях есть одна страшная штука. Вы не можете обойтись без функции активации синусоидная, экспонентная, логистическая, та или другая. У нас нет функции активации вообще. Поэтому у нас абсолютно линейность. Вот точно на одном из прошлых занятий спросили, это линейный коррелятор? Да, это линейный коррелятор. Очень простой. Только если мы включаем ингибирование, он становится не совсем линейным. Но большая часть линейной всё равно. Другие вопросы, давайте посмотрим. 

S08 [01:45:03]  : Борис, есть вопрос у Юрия Бабурова, который на самом деле я тоже поддерживаю. Вот смотрите. Из опыта использования обычных нейросетей известно, что для того, чтобы повысить точность, нам нужно нелинейно увеличивать время. и вычислительные затраты. То есть такой феномен, что, к примеру, у нас сеть, нейрообычная нейросеть, для того, чтобы производительность улучшилась на несколько процентов, нам нужно снизить скорость и вычислительные затраты в разы. В принципе, такой же эффект, как у вас показано в табличке, может быть достигнут, к примеру, заменой PAN на деревья решения или наивный BIOS. Скорость увеличится в разы, если не в десятки раз, но пострадает точность. Вот нет ли здесь такого же эффекта? То есть, нет ли такого, что мы такие же результаты, как у вас, можем получить на обычной нейросети просто изменением параметров, уменьшением числа нейронов в этой сети или слоев, за счет чего мы получим резкое увеличение скорости и снижение точности до вашего уровня? 

S03 [01:46:29]  : Значит, ситуация. Точность у нас не хуже, в основном выше. Мы можем повысить точность и знаем, как это сделать, практически не увеличивая время. Но есть одна пока для меня не очень понятная вещь, слегка таинственная. То есть, ну нет, математически понятно, но с ней трудно сжиться. Вы видели, у нас в сети есть показатель точности. Поставили 1%. Я меняю на 10%. У меня скорость обучения не меняется. Я меняю на одну десятую процента, на одну сотую процента. А скорость обучения плавает в пределах 5-10%, которая вообще все время плавает. Практически почему то, ну нет, почему, я знаю, потому что мы учим сразу на всю величину. Мы сразу на всю величину учим. У нас не случайно распознавание уже того элемента, который был в выборке обучения, полностью единица. Один, ноль, ноль, ноль, ноль, ноль. При ингибировании бывает, например, меньше или больше на несколько сотых. По факту, вот AlexNet была в свое время сделана на двух видеокартах в 2012 год. Через шесть лет ее точность подняли в два раза с где-то… если не ошибаюсь, 15%-16% до примерно 8% ошибки. И стоило это не в разы, а в тысячу раз больше. То есть двойное увеличение точности потребовало тысячекратного увеличения железа и энергии. Мы пока абсолютно этого не заметили. то есть меняем точность, а почти ничего не меняется. надо с этим еще глубже разбираться. при ингибировании немножко меняется. 

S08 [01:49:00]  : еще здесь куча вопросов я вообще вижу сейчас давайте я я здесь слежу за ними давайте я буду отбирать значит есть еще вопрос или комментарий от николая рабчевского следующий что общая беда как для обычных нейросетей так и представляется для пнн Это размазанное или распределенное представление знаний и информации. И есть предположение, что это является общей и принципиальной проблемой для всех системы искусственного интеллекта, как по НН, так и нейросетей, и решение именно этой проблемы должно привести к каким-то новым прорывам. То есть, что требуется не просто там переход от одноканальных нейронов к многоканальным нейронам с дискретными значениями нулей или единиц, А отказ от размазанного представления знаний в пользу локализованного? 

S03 [01:50:04]  : Уф! Все правильно сказано, но с точностью до наоборот. Я считаю одним из гигантских преимуществ размазанность знаний по всей сети. Объясняю, что это такое. Вот у меня один из старых моих компьютеров, скопустился начисто, потому что в бут-секторе сломалось буквально несколько ячеек, и я вообще потерял географию компьютера. Ну, все потерял. В классическом компьютере критичной может быть потеря одного бита памяти, одной ячейки памяти. В нашей сети Мы взяли, обучили, распознает все хорошо. Потом взяли и нахально стерли 10% всех ячеек. Не стерли, заменили случайными. Распознавание ухудшилось на примерно 0,02%. Потом 20% заменили. В общем, мы это почувствовали только когда у нас заменилось где-то порядка 30-35%. На разных картинках это по-разному было. То есть мы совершенно нечувствительны к потере 1-2 десятка ячеек памяти. Это первое преимущество. Второе. В этой сети не может жить вирус. В матрице он не живет, потому что у меня есть нейронная сумма матрицы. Мгновенно ловится любая попытка вируса собраться. В нейронной сети не может произойти самосборка вируса. И он не запишется туда. Более того, хакер проникает в мой компьютер как? через прерывание. Почему он может проникнуть? Потому что все адреса известны. Да, я защищаю память, не даю войти, если что повеснет компьютер, но не допустит в память. У нас информация хранится в виде матрицы в безадресной области, у нас нету конкретных адресов, у нас любая конкретная информация, любой конкретный кусок. У меня написано, вот это морда на экране, это Борис Львович Злотин. И это слово Борис, оно размазано между тысячей пикселов, тысячей весов. Это защищает от вторжений, это защищает от вирусов, это защищает от разрушения элементов. И еще это колоссально повышает общую емкость сети с небольшими повреждениями. Повреждения при активировании обязательно, но очень небольшие. Мы можем записать на одном нейроне, чертову кучу, когда делаем обобщенный образ, обобщить очень многое. Например, проведем такой эксперимент. На одном нейроне записывается прямой вид зеркала кверх ногами, так, под этим углом, под этим, под этим. Но обобщенный образ выглядит, конечно, совершенно ужасно, просто как кусок серого поля. 

S07 [01:53:35]  : Борис, извините, я не могу не вмешаться, но вы, простите, пожалуйста, но вы рассматриваете возмущение, которое ваша матрица, как и любая синопическая матрица, она компенсирует. Да, но для таких матриц существуют очень маленькие возмущения, которые переводят все образы в ее ноль пространства. Вы не видите этих изменений весов, они практически там ничтожны, но система перестает работать полностью. Вы просто для вашего класса задач не нашли тех возмущений, они, еще раз подчеркиваю, маленькие по норме, которые просто будут убивать ваше байосовое представление. Поверьте, это свойство матриц. То есть есть операторы, которые маленькие, которые сильно искажают матричные операции. Поэтому вот вы приводите примеры, которые в выигрышном плане с вашей стороны, а алгебраисты быстро или не, они бы здесь подсказали. Спасибо, извините. 

S05 [01:54:23]  : Я хотел бы очень короткий, пояснительный аргумент к тому, что было написано. Распределённое представление очень полезно с точки зрения надёжности хранения уже имеющейся информации. но ровно в такой же степени оно является препятствием для системы, которая должна самообучаться непрерывно. Потому что самообучение непрерывное как раз означает перманентное изменение хранимой информации. Поэтому требования надёжности хранения и требования возможности самообучаться перманентно, они противоположны по сути. Вот всё, что я хотел сказать. 

S03 [01:55:23]  : Понятно. Я отвечу. Это справедливо абсолютно, и то, что предыдущее сказано, справедливо абсолютно для классических сетей. А у нас очень высокая линейность. В линейной системе малое возмущение приводит к малым результатам. Вот то, что вы говорите, вот в этой сборнике из восьми статей из спецрепорта ИИЕ, они об этом пишут. Сеть уверенно распознает лошадь. Изменили один пиксел, она ее распознает как лягушку. почему? а это критическая как бы обвальная ситуация. она начинает распознавать, у нее появляется первый паттерн и она уже дальше валит по этому паттерну. это результаты нелинейности. а у нас система линейная. линейную систему малыми изменениями не обрушишь. Это на вот предыдущие. А теперь насчет того, что… Борис, ну это не точно. 

S07 [01:56:22]  : Ну вы понимаете, но рядом, понимаете, всегда при большой размерности матрицы, всегда рядом с ней находится сингулярная матрица. Вот то, что вы говорите, это… Еще раз. Извините, но я не могу с этим согласиться, но это большая дискуссия. Простите, пожалуйста. 

S03 [01:56:39]  : 50 лет назад моя первая диссертация, которую я так и не защитил, потому что увлекся изобретательством, была математическая. И я строил матричные системы для расчёта набора линейных уравнений. Так вот, линейная матрица, именно линейная, без нелинейных элементов, всегда устойчива. Её нельзя опрокинуть никакими малыми изменениями. только если есть нелинейные результаты, порождающие лавины. И второе, вот то, что я начал говорить по поводу противоречия между удобством хранения и удобством изменения. Дело в том, что у нас другая схема изменений. Вот, например, я фиксирую подряд постоянное обучение. Например, каждую секунду я фиксирую картинку. Я ее просто фиксирую на следующем нейроне, на следующем нейроне, на следующем нейроне, а пока я фиксирую то, у меня предыдущие схлопывают и создают новый вариант сети. То есть мне не мешает, я точно так же средним арифметическим их схлопываю, и у меня не происходит никаких проблем. То есть у меня появляется промежуточный этап. Вот я новую картинку сохранил, первую, потом сохраняю вторую новую, а в это время старое добавляется к тому, и к тому, и к тому, и у меня появляется, так сказать, обобщенная картинка временного изменения. Еще раз, здесь вопрос самообучения не сводится к стихийным процессам выявление статистики в обычной сети. Вы все время смотрите с точки зрения обычной сети. У меня простая линейная арифметика. Матричная арифметика даже не калькулюс, не супер что-то. Примитивно как валенок. В этом вот вся убийственная штука. Знаете, я ведь профессиональный изобретатель, я всю жизнь занимаюсь изобретательством. И труднее всего клиенту прийти и сказать, «О, парень, у тебя здесь охрененно наверченная такая вот супер-дупер-система, а я тебе предлагаю». Вот у меня здесь где-то было… А, вот. Это мое первое большое изобретение в Америке. Вот, смотрите, это педаль. Обычная педаль. Надо было сделать педаль перемещаемая. В этом блоке перемещаемой педали был мотор, было специальное фиксирующее крепление, вращающийся микрометрический винт, перемещая деталь, фиксатор, чтобы не сломался винт. Порядка 50 в общем деталей. Я к ним пришел вот с этой дрянью. Вот одна деталь. Вот такая вот, видите, из оргстекла. Вот вторая деталь, здесь два пина торчит, и вот это вся педаль. Я закрепляю вот эту штуку, она здесь специальная дырочка подвешена, и вот мотор перемещает вверх или вниз эту штуку, и педаль выдвигается на 15 сантиметров ближе или дальше. Это спасает маленьких женщин, чтобы их не убивало при вождении. Ребята, когда я пришел к специалистам, которые спроектировали этот стоящий 2-3 сотни долларов ящик по перемещению педали, если бы вы знали, какие они мне матерные слова говорили, как они в присутствии собственного директора, ну как, СТО, Chief Technology Officer, как они меня чуть не матом поливали, я слушал, слушал, слушал, А я сделал видеофильм, я сделал вот... Все это чепуха, все это подделка. А потом я вынул эту модельку из портфеля, закрепил на стене и показал. Вот теперь орал этот самый чифт-технологи-офицер. Три дня пошли вон отсюда, чтобы немедленно все было сделано. Крайне трудно принять простой вариант. Простой всегда намного сложнее, чем сложный. Примите, эта чертова пансеть неожиданно для нас оказалась примитивно как валенок. 

S07 [02:01:03]  : Борис, да мы любим линейные системы, мы многие применяем линейные системы. Вы напрасно думаете, что мы сидим в каких-то тензорфлоуах и гоняем питоны. Мы прекрасно работаем с линейными системами или слабо-слабо линейными системами, хорошо знаем их свойства. Не убеждайте нас. 

S03 [02:01:18]  : Мы союзники с вами. 

S08 [02:01:20]  : Борис, а можно я озвучу несколько комментариев? Во-первых, Юрий Бабуров здесь высказывает пожелание в следующий раз сделать результаты на цифар и имиджа нет. Причем без супервайзер или ансупервайзер сетей под капотом, решающих 99% задач. И считаю, что если у вас так быстро обучение проходит, то результаты можно будет получить достаточно быстро. 

S03 [02:01:49]  : Но я хочу сказать одну маленькую вещь. Мы стартап-компания с пока очень небольшими поднятыми фондами. У нас мало работающих. И мы работаем на то, чтобы сделать продажный продукт, И меньше всего нас вообще говоря интересует признательность ученого сообщества. Мы хотим продать в первую очередь. Я понимаю, что это я неприлично говорю, что я не ученый. Ну вот, примите, что я такой вот нехороший человек. Да, можно это сделать на Цифаре. Скачан у меня Цифар, проверяли мы на нем. Но потратить массу времени, господа, Вот то, что я вам показал. Help я закончил писать только вчера. Тесты идут вовсю. Да, дойдем до Цифара. На нем человек, который сядет и пройдет полностью по Цифару. А хотите, сделайте вы. Опубликуйте. Мы вам дадим материалы. Предложим. У вас есть аспиранты? У вас есть студенты? Подключайте смело. Мы во всем поможем. У нас просто не хватает сил, и у нас есть приоритеты по первичности того, что нам надо. И первично для нас убедить инвесторов. А инвесторов, им говоришь Сефар, они говорят, а что это такое, и нафиг это нам надо. А вот у меня есть тут альбом, фотокарточек любимой жены, давай покажи, как ты его распознаешь. Это совершенно другой подход и, к сожалению, как коммерческая организация, мы ему даем полное предпочтение. Я прошу прощения. А мне лично перед вами гораздо интереснее выступать, чем перед инвесторами. Но я член Совета директоров, я ответственен за коммерческий успех. 

S08 [02:03:43]  : Борис, спасибо. Вопрос по поводу данных. Игорь Романко задает. Те данные, которые использовались для обучения, лежат где-то в открытом доступе? 

S03 [02:03:57]  : Да. 20 числа мы назначили время релиза. Как всегда, надо всегда успеть занести хвосты. Я вчера успел закончить хелп. Честно сказать, вы там найдете кучу грамматических ошибок и уже запятые расставлены по щучьему велению, по моему хотенью. Есть картинки, которые не получаются. Есть то, что, например, мне совершенно непонятно, некоторые тесты не проходят. Все в процессе работает мы предполагаем что к 20 числу мы сделаем достаточно грамотный релиз и вот на эти сайты которые я вам с самого начала написал вот они здесь находятся будут поставлены и возможность выйти на данные и экзешка вот та которая я вам сегодня показывал То есть, вот это вот… Чего она у меня не уходит? А, вот. Сейчас. Я не могу выйти из режима этого, демонстрации. Ну, ладно. Я хотел… А, вот, вышел, наконец. Я хотел показать вот это вот. Это будет поставлено на сеть. И вы сможете сами поэкспериментировать, увидеть, как она учится, увидеть, как создавать новые классы на ней, записать все, что хотите. Ну, попробуйте на Цифаре, почему нет. 

S08 [02:05:36]  : Очень даже хорошо может быть. Борис, спасибо. У меня еще одна просьба. Вас или кого-то из ваших коллег, вот те ссылки, которые были на этом слайде, прислать, чтобы их можно было разместить вместе с докладом. Ссылки, ссылки, которые вы показали на слайде, прислать ссылки, чтобы их можно было... Я всю вот эту вот слайдовую презентацию пришлю. хорошо вот и есть еще вопрос вы говорили про умное управление базой данных вот умное управление базой данных и как-то вот в том что вы сейчас показывали можно задействовать потому что я не увидел я увидел там есть я расскажу это очень интересная мысль 

S03 [02:06:28]  : Субэдэ – главная дырка во всех базах данных. Мне пришлось поработать в основном с реляционными базами. Ну, прям, скажем, большого удовольствия мне это не доставило, хотя они были совсем не такими большими. Но возня и трудности. Представьте себе, вы покупаете небольшой продукт, По сути дела, вот это вот фигня. И она, подсоединяющаясь к вашей базе данных, бегает по ней и копирует все, что есть в заданном формате, например, 32. Самое, так сказать, по мелочи. Но теперь, когда она это откопировала, она вот этим методом, который я показал, среди обученных, определяет их сходство, собирает классы. Кстати, здесь одно из преимуществ. Кластеризация пересекающаяся. Здесь совершенно не нужно линейная классифицированность, линейное разделение. Как это? Линейная... Linear Separability. Оно не нужно. Я вполне могу сделать, что у меня вот эта морда Бориса Злотина войдет в группы Старые Хрены, в группу Тризовцы, в группу Белые Люди и в группу Фамилия Злотиных и так далее. Пусть все пересекаются классы. Главное, что вот эта база данных с этими 32 маленькими картинками, она позволит выявить в закономерности связи между картинками. Она будет иметь гипертекстовую отсылку к любому элементу полной базы данных. Она будет бегать по сети и искать то, что надо для этой базы данных дополнять. Сама примет такое решение. и она сможет работать в гибридно-человеческо-машинном режиме, что я считаю одной из самых важных вещей. Вот я бы остановился, два слова по поводу гибридного режима. Знаете, вот у меня на стене, я покажу. Вячеслав Владимирович Митрофанов. Это не просто мой учитель, это гуру, который по жизни сделал для меня невероятно много, он меня научил к человечности. И он крупнейший специалист Патрис, я его ученик. Так вот, у меня есть два-три десятка его фотографий. Вот эту картинку сделал художник на улице за 15 минут. И она гораздо сильнее выражает суть Вячеслава Владимировича, чем все портреты. Представьте себе, у меня картина, я отобрал, допустим, у меня 100 портретов. Я отобрал из них первые 10 наиболее похожих. Я дал грамотному художнику. Он нарисовал обобщенный образ. Я этот обобщенный образ как отдельный класс дам. И он будет ловить у меня образы лучше, чем точно делает классическая нейронная сеть. Причем я использую и данные обычной классической нейронной сети. Более того, после того, как он нарисует эту картинку, я ее включу, я отберу и скажу, вот смотри, вот это вот не попало, а ну-ка, дополни картинку. Режим работы хорошего художника с нейронной сетью может реализовать абсолютно новый уровень понимания и распознавания. Я не сказал, что у нас в сети есть возможность, пока в этом релизе не сделано, выделять контуры. Но это в нейронных сетях известная вещь. Мы знаем, как выделять контуры здесь. Короче, я утверждаю, что следующий этап развития – это в очень значительной степени человека-нейронная сеть. Вот я вам показывал схему, как распознавать брак этих стеклопакетов. Вы что думаете, мы будем собирать тысячу фотографий стеклопакетов и учить? Да ни хрена подобного. Мы поставим эти камеры, поставим двух человек, которые будут смотреть на эти стеклопакеты. И говорить, этот хороший, этот хороший. А наши камеры будут делать его снимки, запоминать, вот этот хороший, этот хороший. А вот здесь вот брак. Проведет там, обведет, например, фломастером. И здесь брак, и здесь брак. В течение нескольких дней он полностью обучит эту сетку. Человекомашинная работа. Точно так же. Не обучать, как сейчас обучают невероятно долгое время вождению автомобиля, а просто сядет какой-нибудь хороший драйвер и проедется. Камера будет видеть все, что вокруг, и хватать движение рычагов. Один проедется, второй проедется, сто двадцать пятый проедется, тысячный. Все это соберется вместе. Будет супер водитель. А как объединять ПАН-сети, это мы умеем. Вот такие вот общие вещи, как мы себе представляем. Что из этого получится? Ну, пока система управления базами данных только в голове. Мы даже не пытались это сделать. Вы видели, нас девять человек, из которых только три пишут реально программы. Пока девять. Получим инверт, будет больше. 

S08 [02:12:31]  : Борис, спасибо. Еще есть пара вопросов. Во-первых, по поводу интервальных весов. Если я правильно понимаю, у вас стандартный восьмиканальный нейрон. То есть, есть синапс, у которого есть 8 весов на различные интервалы значений. 

S03 [02:12:50]  : Пожалуйста, я здесь поставлю 2 и будет 2 веса. 

S08 [02:12:54]  : Хорошо. Я тогда уточню вопрос. Есть некоторое количество интервалов, для которых определяются веса. Эти веса задаются статически при настройке или они могут определяться динамически? 

S03 [02:13:11]  : простейшем варианте. что значит динамически? я изменил, переучил за 5 секунд или за 20 секунд, и у меня уже новый, нам не нужно менять. Теоретически мы предполагаем, что придется делать сети с разными секторами, с разным числом весов. Например, для распознания одновременной изображения и звуков. Там могут оказаться разные. 8 оказалось оптимальным для 32. При 64 нужно больше немножко. Что еще существенно? В патенте у нас записано равномерное распределение, неравномерное распределение, логарифмическое, логонормальное, гауссово, по гауссиане распределение. Мы это все еще не пробовали, потому что равномерное полностью устраивает. Есть одно исключение очень интересное. Мы не 256 уровней делим. Исходно мы перебрали 256 цветную. Но представьте себе картину «Руанский собор в тумане» Мане. В ней все цвета примерно от 150 до 180. очень одногразная. Поэтому мы можем взять, например, 99% занято в этом диапазоне и этот диапазон разбить на 8 кусков. Зачем нам все остальное? Но это все не реализовано. 

S08 [02:14:48]  : А вот тогда вопрос, собственно, по теме нашего сообщества, поскольку у нас сообщество посвящено общему искусственному интеллекту. Как мы можем построить систему на основе ПНН, которая заранее не будет знать, какие изображения будут 32х32, 16х16 или 1024х768? решать проблему автоматического определения количества интервалов и их равномерности. 

S03 [02:15:19]  : Учитывая быстроту обучения, сделаем простенькую автоматическую систему, которая будет начинать всегда с установленного. Вот здесь у нас всегда установлено 32,8. потратит несколько минут или минуту на обучение по этому, потом возьмет поднимет до 64 до 128 или пробежится по диапазону весов и подберет оптимальную. Big deal. 

S08 [02:15:47]  : Спасибо. И последний вопрос по поводу того, что фигурировал анализ Fourier и фигурировал TensorFlow. Каким образом PAN сочетается с анализом Fourier? То ли это отдельная предобработка идет, то ли в PAN сам неявно Fourier вычисляет, и почему нужно взаимодействие с TensorFlow и как оно осуществляется? 

S03 [02:16:15]  : В этой системе, которая сейчас на экране, никакого TensorFlow даже не пахнет. Мы не взаимодействуем с TensorFlow. Все, что есть, есть в этой экзешке. Вот взаимодействие с TensorFlow очень для нас благодатное делает Иван Иванович. И почему это благодатно? Ну не будем же мы все готовые материалы заново делать. Поэтому очень важна возможность передачи. Но а какое отношение имеет анализ Fourier к этой картинке? Обратите внимание. что первый шаг делается без нас автоматически, точно так же, как в наших глазах. Дискретное представление картинки, вот эти 32 на 32 или 64 или там 1000 на 1000, что угодно, это уже дискретное преобразование Fourier. А когда мы раскладываем, например, на 8 или на 10 весов, мы создаем спектр. Фактически сумма каждого из набора весов – это величина спектра. Дальше все это в первом же цикле обучения мы нормализуем. Мы перемножаем простое матричное умножение, матрица входов, на матрицу весов, которая в первой варианте нулевая. Естественно, получаем нули. Потом берем выходной результат, который мы всегда хотим иметь на данном нейроне, единицу. В формировании результата у нас участвовало 1024 входа, это значит 1024 веса. Берем единицу, грубо делим на 1024 и заполняем все веса. Все, она у нас обучена. Теперь второй, теперь третий и так далее образ. То есть просто на уровне валенка. Но что мы получаем? Мы получаем сумму, всех спектров. Вот то, что я назвал нейронная сумма, которую мы трактуем как вероятность правильного распознавания. Эта нейронная сумма, она просто сумма всех спектров. Представляете анализ Фурье. Мы имеем синусоиду, а в отображении мы имеем один столбик. По другой частоте мы имеем второй столбик, по третьей частоте третий столбик. Мы соединяем все эти столбики и получаем конечную цифру. И по ней распознаем. Проблема. Если мы распознаем с точностью, например, до двух знаков, то у нас до сотой будет случайных совпадений. До трех знаков одна тысячная, но все равно плохо. Следующий шаг, который пока не реализован, но реализуется. Мы будем распознавать не общую сумму, а все, например, восемь сумм по спектру. Давай между ними отношения. Вот теперь вероятность ошибки будет там минус восьмые, минус десятые. Я очень, так сказать, не готов ввелся об этом рассказывать. Основная суть очень простая. Фурье преобразования заключается сперва в пиксельном разбиении, а потом в собирании спектральных сумм. Пока спектральные суммы только по яркости. Поэтому вы видели ошибки, когда пианино в тонах, похожих на планету Юпитер, распознается как планета Юпитер. следующий шаг, который мы готовимся выполнить в следующем релизе. у нас будет спектральное разбиение не по яркостям, а сохраняя яркости, будет разбиение еще по пространственным частотам. точно это реально сделать? абсолютно никаких сомнений. Точно быстрое преобразование Фурье используется хоть вот в этом, понимаете, телефоне, хоть в лаптопе, хоть где, никаких проблем с ним нет. Кстати, я должен отдать честь, практически первый, кто заговорил о возможности спектрального преобразования такого типа, был прибран в своей книге по мышлению. А кто это очень подробно разработал, это Вадим Давидович Глезер. Вот книжку, которую я показывал. Там очень хорошо это все раскрыто. 

S08 [02:21:20]  : Борис, я тогда задам последний, уточняющий вопрос. Я правильно понимаю, что у вас распознавание в итоге строится на основе преобразования Фурье, а не на основе определения характеристик объектов, не на основе неявного выделения каких-то элементов изображения? 

S03 [02:21:41]  : Знаете что, это интересный вопрос. В самом деле, вот в этих штучках мы распознаем только, только и только всю картинку целиком. Здесь я не имею возможности ее выделить, но уже сделаны, дайте я сейчас покажу, 

S08 [02:22:03]  : Да, я просто поясню вопрос. Я не хотел отложить это на следующий раз, но мы к нему перешли. Когда мы говорим про объяснимость, вот мы, к примеру, распознаем кошек и собак. И опознали собаку как кошку. А мы хотим спросить, а почему... Система, скажи, почему ты считаешь, что это кошка? И она показывает, что вот действительно треугольные уши, вот там наша самая круглая мордочка, что на самом деле это был мопс, а он распознался как кошка. То есть, мы когда распознаем, если мы распознаем в спектральном составе, то мы не можем объяснить. что тут у нас уши треугольные, а вот здесь у нас мордочка круглая. Мы просто с частотами работаем, с пространственными. Объясни, тогда, откуда берется? 

S03 [02:22:53]  : Понятно. светлое будущее и не такое же далекое. Дело в том, что мы уже проверили, сделали возможность распознавания отдельных частей. Например, носа, глаз, рта. Обучили, распознали. И мы можем распознать те координатные привязки, которые тоже существенны. А вот дальше забавная вещь. Я беру и запускаю на эту обученную штуку, то есть распознаю лаптоп. И на лаптопе я обнаружил нос, например. В самом деле это карта признаков. Вот классическая, как в сверточных сетях, карта признаков. я кстати не знал название карта признаков пока мне это не рассказали буквально пару дней назад антон белюков показал мне что мы делаем я сперва это сделали мы с володей маценко потом узнали что это карта признаков какой удар со стороны классиков очень приятный удар то есть нам не надо самим это делать то есть например На картинке лаптопа мы обнаружили нос, но глаз мы там не обнаружили и рта тоже нет. Значит, можно считать, что это неправильно. В самом деле, реализуемо, проверено, что это можно сделать, но не реализовано еще. Это достаточно много программной работы. Надо делать следующую версию софта. Надо сказать, что последние два месяца мы в основном занимались интерфейсами, потому что, ну, демонстрировать надо. А у нас вначале был классический программный интерфейс, как Бог послал. Здесь одно, здесь другое, здесь третье. Вот сейчас это уже более-менее нормально, и с Нового года мы приступим уже к следующему, к подготовке следующего релиза, где все это должно войти. То есть, Не забывайте, мы еще младенцы. Вы к нам часто пытаетесь предъявить, ну не вы, а скажем инвесторы и так далее, требования. Вот вы сделайте, покажите мне, прям как я на гугле имею. Ребята, они там миллиард затратили, а мы свои карманные деньги. 

S08 [02:25:22]  : Хорошо, Борис, коллеги, огромное вам спасибо. Тогда будем ждать новых решений в области объяснимости, интерпретируемости. 

S03 [02:25:34]  : Я скопировал все вопросы и в спокойной обстановке посмотрю. Не гарантирую, что на все буду отвечать. Я обычно не отвечаю, если не понимаю вопрос или если это просто троллинг. Но если будут интересные вопросы, я на них постараюсь ответить, подготовлю это. И вот, как я понимаю, Антон Германович подготовит материал для постановки на сеть, чтобы можно было это посмотреть, скачивать и так далее. И мы тогда на него поставим отсылки на всех вот этих сайтах, о которых я говорил. И там будет, в частности, ответы на все вопросы. А эту презентацию я могу послать сразу сегодня, если надо, никаких проблем. Хочу заметить, что условием скачивания будет записать какие-то свои данные, потому что вы это пошлёте, и мы вам пошлём все необходимые линки. Вот этот материал, он будет в виде зипа. ну вот он у меня здесь где-то лежит zip файл предыдущего обычный zip файл и файл redmi будет вместе вот этот matrix rar не такой большой 100 мега причем этот zip будет что включать он будет включать Точно здесь есть экзешка, директория хелпа. Здесь вот этой директории. Постарайтесь ее не калечить, потому что покалечите, не будет работать. Лучше сюда вообще не заходите. Демон – это демонстрационная директория. Здесь картинки вот кошек, например, разных. хорошо распознающихся. Сборная солянка – это такие вот общие разные образы. Что еще здесь? Лица, коровы для распознавания, пианино, часы, лаптопы. Вот такие вот наборы. Это все скачано с интернета, это все стандартные тестовые наборы. Когда будете работать, если вы хотите сохранить всю работу, я показал там, как нажал и сохранил. Здесь мы поставили пустую директорию WORK и внутри нее протокол. Вы можете все работы сохранять в протоколы, это вот протоколы, а в Work мы обычно сохраняем в директории Work эти акробатические этюды, акробатические копии тестов. Когда тестируем, мы это сбрасываем в эту директорию. Но это как вам будет удобно. Когда откроете у себя, где-нибудь поставите, можете открывать свои директории. вообще все, что угодно делать. ваш компьютер, ваши дела. добавлю, что мы под очень приличными антивирусными программами американскими. у меня сильный и очень противный, очень мешающий, как он называется, firewall, не знаю как он по-русски называется но вроде бы вирусов с нами вы не должны получить хорошо спасибо борис коллеги огромное спасибо возникнут реально серьезные вопросы только не философского плана я терпеть не могу философские вопросы обсуждать но реальные конкретные вопросы мой адрес будет на той же самой штуке я сделаю специальный аккаунт чтобы мне посылать запросы по по сетям там же будет посылайте запросы если очень надо можно будет связаться и по скайпу ну это уже следующие вопросы 

S08 [02:30:16]  : Хорошо. Хорошо, Борис. И, кстати, я, пользуясь случаем, хочу сказать, что у нас 27 января будет семинар, где как раз Юрий Бабуров, который задавал некоторые вопросы сегодня, через меня будет как раз рассказывать о интерпретируемости современных глубоких нейронных сетей и насколько они являются или не являются черным ящиком мне кажется будет любопытно поучаствовать и может быть со своей стороны задать какие-то вопросы и узнать что-то новое я прошу прощения я заведомо не буду участвовать 

S03 [02:30:58]  : Я глухой. Я сходу почти не понимаю, что говорят. Особенно, если у человека чуть-чуть высокий голос. Я сегодня дико перенапрягся. У меня тут динамики орущие, чтобы что-нибудь понять. Но если у вас будет запись, я ее обязательно просмотрю. Я не успеваю в режиме реального времени понять. Мне иногда надо по 2-3 раза прослушать. Я в молодости занимался испытанием электрогенераторов 6 лет. 140 дБ шум. 

S08 [02:31:32]  : Борис, огромное Вам спасибо за сегодняшнюю презентацию и всем спасибо за вопросы. Поздравляю всех с наступающим Новым годом, если кого-то в этом году мы не увидим. До встречи на следующих семинарах и с новыми результатами в Новом году. 

S03 [02:31:52]  : Всем спасибо и до свидания. 

S08 [02:31:55]  : Борис, спасибо еще раз и всем всего самого доброго. Спасибо, до свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
