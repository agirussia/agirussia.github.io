## 09 ноября 2023 - О переносе знаний в нейросеть с иерархическим многозадачным обучением — Иван Бондаренко —Семинар AGI
[![Watch the video](https://img.youtube.com/vi/aVI79jQ8r3c/hqdefault.jpg)](https://www.youtube.com/watch?v=aVI79jQ8r3c)


А. КОЛОНИН  [00:00:04]  : Коллеги, всем добрый вечер! Сегодня у нас в гостях Иван Бондаренко и у нас будет интересный разговор. Мы здесь в нашем сообществе много говорим часто о достоинствах и недостатках с одной стороны символьного или символического искусственного интеллекта, с другой стороны символического или ассоциативного или нейросетевого искусственного интеллекта. Возможно, сегодня мы узнаем о том, как в одну сторону это можно совместить, перенося в нейросетевые модели структурированные знания из предметной области. А у меня сразу заранее вопрос. Может быть, Иван, по ходу дела сможете пробросить мостик на тему того, как можно обратную процедуру делать, как извлекать из глубоких сетей знания с помощью иерархического многозадачного знания извлечения. Спасибо. Онлайн-пространство ваше. 

И. БОНДАРЕНКО  [00:00:58]  : Спасибо, Антон. Спасибо, коллеги. Спасибо, слушатели. Меня Антон уже представил. Моя тема доклада о переносе знаний эксперимента области в глубокую нейросеть с помощью нейросетческого многозначного обучения. Меня зовут Иван Бондаренко, я буквально пару слов о себе сделаю. Я интересуюсь темой машинного обучения с 2005 года примерно. В 2006 году я закончил магистратуру Донецкого национального технического университета, в 2009 году закончил аспиратуру этого же университета в Донецком Донбассе. Там я работал до 2013 года преподавателем, преподавал... предметы, связанные с нейронными сетями и с искусственным интеллектом, в том числе с символьным искусственным интеллектом на основе продукционных систем, семантических сетей и так далее. С 2013 года я перешел на некоторое время в IT-индустрию. Работал программистом на C++, работал специалистом по анализу данных для компании Дубль ГИС, известная сибирская компания в целом в масштабах России. Занимался, прежде всего, задачами, связанными с компьютерной лингвистикой, с анализом текстов на естественном языке. Два года, с 2018 по 2020, работал удаленно в московском физтехе и в индустриальном партнере физтеха компании Data Monsters. Я работал в группе Бурцева в лаборатории нейронных систем и глубинного обучения. на проекте Deepavlov. Фестиваль занимался разработкой самого проекта Deepavlov, разговорного искусственного директора Adreta Monster, занимался продажей решений на базе Deepavlov для внутренних и зарубежных заказчиков. Потом я с 2020 года тоже пару лет работал в новосибирском подразделении компании Huawei, Huawei Russian Research Institute. Был техником команды FTV для больших данных. И наконец в 22-го года я окончательно перешел в НГУ, с которым начал сотрудничать в 15-м году, и сейчас я целиком полностью являюсь сотрудником Новосибирского государственного университета, научным сотрудником, старшим преподавателем. на мехмате, у лидеристов, у экономистов и даже на фите. Примерно одним и тем же занимаюсь дипленингом. Веду курсы, связанные с дипленингом, веду, руковожу неокрами для индустриальных заказчиков и так далее. И меня, как человека, как и практика в области ML индустрии и человека, связанного с исследованиями, всегда интересовала устойчивость моделей в ходе эксплуатации. А устойчивость моделей в ходе эксплуатации тесно связана с проблемой сдвига данных. В англоязычной литературе это называется distributional shift, сдвиг распределения дословно, но обычно говорят о сдвиге данных. Он наблюдается и в компьютерном зрении, и в анализе текстов на естественном языке, например, когда мы обучаем нашу модель на текстах одного периода и одного жанра, например, тексты Википедии за период с 15 по 20 год. а эксплуатируем в другой временной период, например, в 23-м году для анализа соцсетей, к примеру, Реддита. Это издвиг жанра, издвиг по времени. С ходом времени меняется лексика, жанр вносит свои коррективы в синтаксическую структуру высказываний, и это все приводит к деградации системы, которая, казалось бы, была хорошо обучена и хорошо показала себя на тестовой выборке. В обработке табличных данных во многих задачах практического машинного обучения проблема снижения качества моделей в ходе эксплуатации, вызванная именно проблемой сдвига данных, что распределение признаков, тех же самых признаков, которые были в обучающей выборке, меняется. Получается, что тестовые примеры, по сути дела, берутся из другой генеральной совокупности, нежели обучающие примеры для обучающей выборки, это острая проблема. Эта проблема как-то интуитивно понималась и решалась практиками МЭТ-МЭД достаточно давно, но в теории машинного обучения она начала осмысливаться недавно, где-то в 18-19-х годах, основное начало такой массовой исследований. В качестве такой забавной иллюстрации могу привести пример из одной из моих любимых статей про инвариантность, про построение инвариантности в машинном обучении. Invariant Risk Minimization. В введении к этой статье как раз была такая забавная иллюстрация того, как мы сталкиваемся с двигом данных в компьютерном зрении. Например, мы обучаем систему компьютерного зрения, которая учится задачи бинарной классификации. отличить корову от верблюда по фотографии. Вот это у нас корова, вы ее видите слева, такое животное с рогами, говорит му, пасется на зеленой травке, вот справа у нас верблюд, гордый корабль пустыни с двумя горбами, который бредет сквозь бархат песчаный. Казалось бы, все просто. Мы обучили систему, она демонстрирует высокое качество классификации, порядка 98%, и мы ее тестируем, отдаем ей фотографии коровы на пляже. Система говорит уверенно – это верблюд, безусловный верблюд. Смотрите, корова – это то, что на траве, а если это на песке, то верблюд. Яркий пример – нахождение ложных корреляций между признаковым описанием и целевой переменной. Казалось бы, мы должны были найти ключевые, сущностные свойства объектов нашего класса коровы, что у них есть рога, что у них… 4 ноги, но они невысокого роста, ноги сравнительно короткие, туловище массивное, то есть вот это вот все. Она решила, что раз на траве, то корова, раз на песке, то верблюд. Все. Точка. хотя это проблема. Безусловно, эту проблему можно решать по-разному. Некоторые могут сказать, ну а в чем проблема? Ну давайте мы добавим фотографии коровы на песке и все, и будем учить систему, что коровы на песке тоже коровы, а не верблюды. Это, конечно, простое решение, но, к сожалению, оно в общем случае невозможно. Мы не можем никогда предусмотреть все ситуации, с которыми столкнется система в ходе эксплуатации. Соответственно, лучше решить эту проблему кардинальным образом, сделать такой алгоритм, такую модель, которая могла бы либо моделировать собственную неуверенность в ответе, уметь говорить нет. Как Сократ говорил, я знаю, что ничего не знаю, просыл мудрейшими из Афинят, так и система должна уметь понимать, что что-то с нашими входными данными не то, они являются антипичными и лучше отказаться от принятия решения, от распознавания, сигнализировать об этом оператору, чем уверенно принять ложное решение. Такая система будет являться более доверительным искусственным интеллектом. либо же сделать систему более инвариантную, которая действительно учится находить инвариантные корреляции между признаковым описанием и целевой переменной, а не ложные. Соответственно, как я уже говорил, эта проблема начала активно осмысляться и дискутироваться в научном мире где-то в конце 2010-х годов. И специалисты по машинному обучению любят соревноваться. Научные соревнования – это всегда очень полезный источник анализа разных алгоритмов, источник сравнения разных алгоритмов, когда мы не на каких-то данных заказчика пониок и делали на собственных данных, а есть открытые данные, одинаковые для всех, есть хорошо описанная методика тестирования, одинаковая для всех, и в этом условии каждая исследовательская команда пытается представить наиболее эффективный алгоритм, решающий научную проблему. И вот в 2021 году при конференции NEARIPS, крупнейшей конференции в мире машинного обучения, прежде всего, крупнейшей конференции по нейронным сетям, которая существует с 1988 года, было организовано соревнование, Shift Challenge называлось. Вообще при NEARIPS каждый год проводится несколько соревнований на разные проблемы машинного обучения. Меня тогда привлекло именно это соревнование, поскольку Оно было посвящено реально больной теме – проблеме с двигателем. Организаторами этого соревнования были ребята из исследовательского подразделения компании Яндекс.Ресерч, из группы теоретического кругового машинного обучения Оксфордского университета и из лаборатории машинного интеллекта Кембриджского университета. собственно, в чем там был смысл? Основная цель соревнований исследовательских это исследовать проблему оценки неопределенности в моделях машинного обучения, наиболее эффективной оценки неопределенности в ситуации сдвига данных. Это внесение априорных знаний в модели для повышения устойчивости этой модели в ситуации сдвига данных и собственно непосредственно сама робастность устойчивости сдвига данных, способность принимать адекватные решения либо отказываться от их принятия, если ситуация слишком аномальная, слишком атипичная. И соответственно для того, чтобы рассмотреть Все аспекты проблемы с двигателем, вне зависимости от конкретной задачи, организаторам было предложено три дорожки, три трека. Это Weather Prediction, это Machine Translation и это прогнозирование поведения участников дорожного движения. Задача регрессионная, на табличных данных необходимо было прогнозировать температуру на основе более чем 100 признаков, описывающих объекты. Машинный перевод – это обычный машинный перевод на основе, например, sequence-to-sequence модели нейросетевой. Это может быть трансформер, это может быть что угодно, не обязательно трансформер. В 21-м году это, как правило, трансформер какой-то типа BART. При этом источником сдвига данных для машинного перевода был жанр и стиль, то есть, условно говоря, тексты из параллельных корпусов ООН, это могли быть текстами для обучения, а, например, комментарии в социальных сетях, в Твиттере или в Реддике, это тексты для тестирования. Вот, пожалуйста, сдвиг данных. Прогнозирование поведения участников дорожного движения – еще одна проблема, связанная со сдвигом данных. Во-первых, сама задача прогнозирования актуальна в том смысле, что… автопилотам, беспилотным автомобилям мало видеть то, что происходит на дороге, им нужно понимать, что будет через несколько секунд. например, вот эта машина в левом ряду, она движется или она остановилась? Вот эта бабуля, которая стоит на краю тротуара, она сейчас будет переходить дорогу или нет? Соответственно прогнозирование важно, но там могут быть проблемы. Изменения правил. Например, раньше на кольцо объезжали по одному, сейчас по другому. Это изменения условий эксплуатации. Климат Краснодара, климат Мурманска. Это какие-то ремонтные работы на типичных узловых точках дороги и тому подобные вещи. Но мне тогда заинтересовала больше всего задача, связанная с прогнозированием погоды. Во-первых, это была максимально общая задача. Если в машинном переводе, в прогнозировании поведения железнодорожных движений есть своя специфика, связанная с особенностью задачи, это тексты или прогнозирование траектории, то задача на табличных данных максимально общая, без всяких априорных предположений о структуре данных в задаче, тем она интереснее мне показалась. С другой стороны, задача анализа табличных данных была связана с моей тогдашней работой в CoWare. Мы делали автоэмэль на больших табличных данных для… облако ML-решений у AWS, соответственно, я мог это согласовать с основной работой. Сама задача описывалась как регрессия обычных данных, у нас была целевая переменная, это температура, необходимо было прогнозировать температуру на основе как некоторых первичных признаков данных метеостанций, так и вторичных признаков результатов прогнозирования другими прогнозными системами, такими как Global Forecasting System, Канадский метеоцентр и другими. А сдвиг был здесь в следующем, две причины сдвига, это темпоральная и географическая. Собственно организаторы подготовились обычное разбиение данных на трейд. трейн использовался для обучения, для некоторой валидации, чтобы, например, ранние основы нейронных сеток реализовать, а evaluation – это финальное тестирование. И здесь, вот вы видите, зеленые области – это обучение, оранжевые – это внутреннее тестирование, evaluation – это финальное тестирование. Вы видите здесь, что данные были взяты из одной генеральной совокупности, то есть данные теплых климатических поясов и за один и тот же период времени, с 1 сентября 2018 года до апреля 2019 года. Казалось бы, вот типичная задача, но вот эта вот область в правом нижнем углу уже является нетипичной, она моделирует сдвиг данных. В внутреннем тестовом множестве Development и в финальном тестовом множестве Evaluation Set были представлены данные не только из той же самой генеральной совокупности, но и из другой генеральной совокупности. Во-первых, не с тёплых, а с холодных климатических поясов. Во-вторых, данные не за 2019 год, не за конец 2018-2019 года, а данные за конец 2019 года. Климат меняется со временем, это является источником издвига данных. Закономерности изменения климата для тёплых и для холодных климатических поясов разные. Второй источник издвига данных. А как это все совместно оценить, вот вопрос, как оценить умение модели давать максимально точный прогноз для вот этих блоков, developed in и evaluation in, то есть для тестовых данных, которые являются знакомыми с той же генеральной совокупностью и одновременно с этим модель должна либо попытаться каким-то образом дать качественный прогноз для вот этих out данных. Либо же отказаться от принятия решений на этих аутданах. Как померить качество одновременно точного прогноза и умение отказываться от прогноза. Авторы предложили следующую любопытную методику. Точно так же мы оцениваем. качество бинарной классификации как площадь под рок кривой по оси x это у нас не специфичность по оси y у нас чувствительность и мы ставим разные пороги срабатывания вероятностного классификатора и ставим точки на этом графике площадь под рок кривой. как и здесь авторы предложили построить кривую удержания ошибок и считать площадь под ней. чем меньше площадь Тем более эффективным является алгоритм. Тем выше у него совместная способность принимать точные решения и отказываться от плохих, неточных решений. Строится кривая удержания ошибок следующим образом. мы просим модель не только прогнозировать целевую переменную, но также прогнозировать собственную неопределенность в прогнозе целевой переменной. И все прогнозы, все решения моделей на тестовой выборке можно отранжировать по этой неуверенности. Вверху самые уверенные решения и постепенно по убыванию уверенности в конце самые неуверенные решения. Соответственно, мы можем взять все решения модели, как уверенные, так и неуверенные. Естественно, на всех тестовых примерах среднегвадратичная ошибка у модели будет самой большой. мы можем отказаться от принятия любого решения. Как там есть шутка? Не ошибается тот, кто ничего не делает. Действительно, если мы ни одного решения не приняли, у нас нулевая ошибка. PyX – это доля удержанных примеров, на которых уверенность модели самая большая. Ноль удержанных примеров, нулевая ошибка, все примеры удержанные. все 100% примеров удержаны, ни от одного не отказались. Самая большая ошибка. 20% примеров удержали, на которых модель наибольшую уверенность демонстрирует. Ошибка меньше, чем на 100%, но больше, чем 0. Таким образом, мы для 20, для 30, для 40% удержанных примеров строим оценки, строим кривую. И вот мы видим, площадь под кривой удержания ошибок кем? Чем меньше, тем лучше модель. Здесь мы видим для двух моделей, одиночной модели и для ансамбля. Для ансамбля всегда лучше, потому что ценой не увеличения смещения ошибок, он снижает разброс ошибок, поэтому для него, как правило, кривые такие будут лучше, очень кривые меньше. Это авторская методика, и помимо авторской методики авторы предложили бейзлайн, базовый вариант решения. Базовое решение было основано на десяти алгоритмах градиентного бустинга. Поскольку в числе организаторов соревнования были ребята из Яндекса, соответственно, на них предложили авторский подход на основе CardBoost, авторский градиентный бустинг исследовательского коллектива из Яндекса, который является дальнейшим развитием их алгоритма MatrixNet. Сам гранитный бустер – это ансамбль деревьев решений, где каждое следующее дерево, как мы все знаем, усиливает ансамбль, минимизируя невязку предыдущего состава ансамбля. То есть сам грандиентный бустик – это ансамбль, а здесь по сути дела ансамбль ансамбля предлагался. Десять таких ансамблей, построенных по методике усиления. При этом каждый из алгоритмов CatBoost включал в себя 20 тысяч деревьев решений, очень большого количества, и специальная функция потерь RMSI withArsoidActive. Это было необходимо для того, чтобы моделировать оба аспекта неопределенности. Эпистемиологическую неопределенность, неопределенность знаний. моделировалась по настроениям, моделировалась ансамблем, коллективом из десяти алгоритмов, алеаторная неподеленность учитывалась с помощью RME-Arithmetic, то есть это специфическая функция потери, основанная на том, что наша модель возвращает два параметра, это прогнозируемую цельвую увеличенную и оценку неуверенности в собственном прогнозе. На основе этого мы можем построить распределение ошибок модели. можем максимизировать функцию правдоподобия описания распределения ошибок модели, тогда у нас неопределенность может посматриваться как среднее квадратическое отклонение в распределении ошибок, которые мы моделируем, и мы там вводим правдоподобие, соответственно, максимизируем функцию правдоподобия, либо минимизируем кросс-антропию, таким образом у нас получается… специальная функция потерь для учета алеаторной неопределенности. Неопределенность в данных. То есть это солевая переменная могла бы измерена с погрешностью, это приборы, которые измеряют признаковые компоненты могли работать с погрешностью и тому подобные вещи. соответственно решения которые заняли призовые места 2 и 3 были основаны на подходах похожих на бейслайн похожих на базовое решение только Отличия были скорее количественные, а не качественные, то есть большего размера ансамбль, больше деревьев в грандиентном бустинге, глубже сами деревья. В частности третье место заняло решение японских коллег из компании KDDA и Research, они предложили огромное количество грандиентных бустингов на базе LightGBM. второе решение заняла команда из Томска они предложили решение на основе 80 отбустов для моделирования целевой величины и еще 20 код бустов для моделирования неуверенности, при этом там был очень развитый фич инжиниринг, они специальную предобработку признаков делали, всякие Evereast кто-то добавляли на основе задачи, на основе представления о погоде и тому подобные вещи. Ну а первое место заняло решение, основанное на сравнительно небольшом ансамбле глубоких нейронных сетей с крайне простой предобработкой признаков без всякого навороченного фича инжиниринга. И три варианта этого решения оказались в итоге лучшими в турнирной таблице. Они продемонстрировали самую меньшую площадь кривой удержания ошибок. И, соответственно, стало интересно. А почему так? Чем обусловлено то, что стандартная небольшой ансамбль глубоких нейронных сетей, там 20 нейронных сетей было 18-слойных, оказался эффективнее, чем очень сложные решения на базе градиентных бустингов. Традиционно считалось, что нейронные сети сильны в области компьютерного зрения. в области распознавания речи, то есть везде, где данные являются весьма специфическими, имеют специфическую природу, то есть там наблюдается свойство локальной упорядоченности. То есть, яркость из соседних пикселей на матрице, растровой матрице изображения изменяется плавно. Спектральные огибающие с ходом времени на спектрограмме изменяются тоже плавно. Вот в таких задачах можно использовать нейронные сети и не обычные многослойные персептроны, а сверточные нейронные сети, у которых в архитектуре моделирование локальной упрядоченности данных зашито. в ядрах свёртки, и вот поэтому свёрточные нейронные сети работают. А обычные универсальные модели типа многокристаллинных перцептрон на универсальных задачах типа табличных данных работать не будут. Потому что порядок столбцов не играет никакой роли в таких данных, а алгоритмы деревянные на базе деревянных решений наиболее эффективными являются. Это было мнение, которое разделялось многими. Многими, но не мной. Я имел глубокое убеждение, что глубокие нейронные сети, прощу простить за ортологию, глубокое убеждение, глубокие нейронные сети являются наиболее эффективным инструментом решения любых задач машинного обучения. в том числе и задач табличных данных, потому что ключевые преимущества нейронных сетей не в том, что они учитывают свой статокальную порядочность, а в другом. И подтверждением моему убеждению был успех моего решения на этом соревновании. Итак, давайте поговорим, что является залогом успеха многих нейронных сетей. Во-первых, глубокая нейронная сеть хороша именно потому, что она глубокая, то есть многослойная. Нейронная сеть – это обучаемая иерархия представлений. Известна иллюстрация того, что иерархия представлений строится при решении задачи для сверточной нейронной сети. Мы видим, что когда глубокая сверточная нейронная сеть решает задачу распознавания изображений, Первые слои нейронной сети выделяют достаточно простые низкоуровневые признаки, какие-то графические примитивы, черточки разных, под разным углом наклона, какие-то светлые пятна, темные пятна и тому подобные вещи. Следующие слои. компонуют вот эти простые графические признаки в более сложные признаковые представления, которые можно сопоставить уже с конструктивными элементами распознаваемого объекта. И, наконец, последние слои являются наиболее задачно специфичными, наиболее высокоуровневыми. Они решают максимально конкретную задачу распознавания марки автомобиля. Но эта картина, я просто привел примеры из компьютерного зрения, потому что примеры из компьютерного зрения наиболее иллюстрабельны. Но на самом деле то, что нейронная сеть, любая, хоть сверточная, хоть полносвязная, строит иерархию представлений, это известный факт в теории нейронных щитей. Именно то, что мы строим, обучаем в иерархию представлений, избавляет нас от необходимости делать сложный, разветвленный фича-инженеризм ручной. Мы делаем максимально простое признаковое представление наших объектов, а дальше нейронная сеть, которая является иерархическим фича-экстрактором, строит иерархию модуля излучения признаков, скрытых слоев своих, и классификатора последнего слоя. Эти все модули извлечения признаков и классификатор обучаются совместно с самым эффективным способом градиентным. Например, для сравнения, дерево решений строится жадным алгоритмом. По умолчанию, наиболее не таким эффективным. Если мы делаем какой-то компонентный подход, то у нас там модуль, например, сокращение размерности. имеет свою функцию потери, минимизируем невязки, какой-то модуль выделения тем тематического анализа, свою функцию потерь и тому подобные вещи. То есть это целый каскад модулей со собственными функциями потери, которые не связаны с решаемой задачей, только потом финальный классификатор в пространстве специальных признаков. обучается решать непосредственно задачи, то с нейронными сетями и с иерархией представлений все совершенно не так. Это каскад совместно обучаемых фич, экстрактов и классификаторов, обучаемых решать задачи, минимизируя функции потери, максимально приближенную к критерию качества решаемой задачи. Именно поэтому многослойность для нейронной сети – это ключевое преимущество. Это раз. Так, я прошу простить, тут, похоже, в чате вопросы возникают. 

А. КОЛОНИН  [00:26:30]  : Смотрите, по вашему вопросу, на ваше усмотрение, если удобно по ходу отвечать, можно по ходу, но у нас обычно ждем конца, потом идем по вопросам. 

И. БОНДАРЕНКО  [00:26:41]  : Ну я сразу походу, мне на самом деле проще походу. Табличные данные были для погоды и температура по дням, там была плавность, но опять таки фактор времени был скрыт. Мы не могли учитывать фактор времени, то есть это не временные ряды. Это просто некоторое признаковое описание какого-то объекта. Данных с метеостанций первичных и вторичных прогнозов систем фактора времени там не было, мы не могли его учитывать и мы его не знали. Если бы там мы знали фактора времени, то мы могли бы учитывать его как-то в принятии решения в ситуации, чтобы правильно сдвигать данные. Можно ли рассмотреть тему с учетом примера алгоритма деления в арифметике? Ну я не до конца понял вопрос, возможно мы можем обсудить вопрос более развернуто после моего доклада. Первое преимущество, почему нейронная сеть победила, почему она лучше, чем классический вид машинного обучения, такие как деревья решения, это потому, что она многослойная, это ее первое преимущество. Во-вторых, очень интересное свойство нейронной сети, чем нейронная сеть больше, чем больше параметров, которые она имеет, тем выше ее обобщающая способность. Это парадоксальное свойство, которое противоречит опыту классического машинного обучения. Есть такая дилемма – смещение разброса. в машинном обучении, которая говорит, что в сложности модели машинного обучения, то есть числа настроенных параметров, в случае, допустим, нейронной сети, глубины дерева, в случае деревьев решений и тому подобных вещей, у нас снижается смещение ошибки и увеличивается разброс ошибки. Смещение ошибки, смещение разбросов, можно их интерпретировать, если простыми словами, можно интерпретировать Смещение – это как ошибка модели на обучающей обыборке. А разброс – это разница в ошибках на обучающей и на тестовой выборке. Известно, что обычно мы обучаем модель машинного обучения с помощью концепции минимизации эмпирического риска, то есть мы берем обучающую выборку из генеральной совокупности, решаем задачу минимизации на этой выборке в надежде на то, что модель будет хороша и на генеральной совокупности в целом. Но так бывает не всегда, когда мы решаем задачу минимизации, мы снижаем смещение, снижаем ошибку на обучающей выборке, но при этом расхождение в поведении модели на обучающей выборке и на тестовой выборке, выборке из примеров, которые модель не видела в ходе обучения, все больше и больше растет. И модель в конечном итоге, чем более сложная модель, чем больше у нее параметров. тем легче такой модели заучить примеры обучающей выборки и минимизировать смещение, но при этом разброс растет. Модель слишком оверфитится, слишком переобучается. Две стороны искусство специалиста машинного обучения было в том, чтобы найти модель такой архитектуры, такого числа параметров, чтобы у нас смещение было еще низким, а разброс был. все еще низким. То есть, с одной стороны, не слишком высокое смещение. модель не является недообученной, в другом случае пока еще невысокий разброс, разброс растет с ходом времени, модель становится все более-более переобученной. Это классическая интерпретация, которая с начала 90-х доминировала, но выяснилось, когда в 2010-х годах глубокие нейронные сети демонстрировали впечатляющие успехи сначала в задачах компьютерного зрения потом в задачах анализа речи и текстов начали пытаться понять почему почему нейросети с ростом сложности с ростом количества слоев ширины слоев смысле количества нейронов в слоях оказывается все лучше и лучше хотя казалось бы они должны быть все хуже и хуже у них обучающий персонал должна падать Выяснилось, что вся соль в процедуре обучения. Глубокие нейронные сети, как правило, обучаются именно стокастическим градиентным спуском. Обратно, распространение ошибок для многокислотной нейронной сети – это модификация градиентного спуска с учетом циклочечного правила, мы это все знаем, но при этом градиентный спуск может быть пакетным, может быть стокастическим. Пакетный гардиентный спуск означает, что мы берем обучающую выборку, каждая итерация или эпоха обучения заключается в том, что мы считаем гардиенты на всех примерах обучающей выборки, получаем некий усредненный или суммарный гардиент и делаем шаг в направлении суммарного антигардиента. Модель обучения пакетный гардиентный спуск теоретически обоснована, она позволяет добавлять всякие подходы связанные с поиском оптимального ленингрейда, оптимального шага в направлении гардиента, можно дополнительно использовать методы сопряженных гардиентов, это квази-ньютоновские методы, если нейросеть не слишком большая. Соответственно, кажется. что это хорошо, но проблема в том, что это к сожалению не слишком хорошо, когда у нас нейросеть слишком большая, соответственно оптимизируемая функция потерь имеет очень высокую размерность. очень высокое пространство параметров. В такой ситуации наша функция потери или функция ошибок имеет очень сложную поверхность. С многочисленными локальными минимумами, с плато. Если модель попадает на плато функции ошибок, то она крайне трудно с этого плато сдвигается. В ситуации, когда мы используем пакетный гардинет на спуск. Мы инициализируем модель случайным образом, соответственно, эта случайная инициализация может разместить нашу модель в не самую лучшую точку в пространстве параметров, и из нее наш путь, если мы используем пакетный градиентный спуск, казалось бы, более технически обоснованный и более надежный, приводит нас к надежному застреванию в плохом локальном минимуме, в неглубоком, хотя наша цель найти хороший локальный минимум. В 90-е годы хороший локальный минимум считался максимально близким к глобальному минимуму, а еще лучше найти глобальный минимум. С этим связано было много негардиентных методов, Методы имитации отжига, генетические алгоритмы, считалось, что они являются алгоритмами квазиглобальной оптимизации и позволяют нам найти эффективное решение. Потом исследователи поняли, что на самом деле глобальный минимум в функции потери нервной сети искать необязательно, достаточно хороший локальный минимум найти. И опять градиентные методы вернулись, стали популярными. Соответственно, пакетный градиентный спуск застает в локальных минимумах, плохо сидит на платок. А когда мы используем стокгластический градиентный спуск или онлайн градиентный спуск, то есть мы не считаем суммарный градиент на всех примерах обучающей выборки, и мы разделяем понятие. Эпоха – это проход по всем примерам обучающих выборки, а итерация – это шаг корректировки весов на основе антигардиент. Так вот, итерация – это один пример. Один зенбудистский философ писал, как в капле расы отражается вселенная, так и здесь в одном примере в градиенте, рассчитанном на одном примере отражается общий градиент по всей обучающей выборке. Ну естественно, отражаются они точно. Поэтому, когда мы делаем шаг в направлении антикорридента, посчитанного по одному случайным образом выбранному примеру, мы, безусловно, делаем шаг в похожем направлении, как если бы мы делали шаг в направлении суммарного антикорридента. Но каждый раз, так как мы пример симплируем случайный из выборки, соответственно, шаг тоже в немножечко случайном направлении, он гуляет. за счет такой стахастики. мы можем проскакивать какие-то плохие узкие локальные минимумы либо соскакивать с плато именно за счет стахастичности поведения стахастического регионоспуска по примеру вот и это оказалось залогом успеха чем более сложная модель тем легче она оптимизируется стокастическом приятном спуске соответственно сложные поверхности для нее не проблема потому что узкие плохие узкие локальные минимумы она проскакивает а в широких локальных минимумах она застряет кстати говоря об узких широких локальных минимумов Наша же цель не просто найти локальный минимум на поверхности функции ошибок на обучающие выбрать, мы же не просто решаем оптимизационную задачу, мы пытаемся придать нашей нейронной сети максимум обобщающей способности и здесь оказывается, что широкие локальные минимумы функции ошибок на обучении соответствуют более высокой общающей способности, чем узкие локальные минимумы, хотя значение этих локальных минимумов может быть примерно то есть ошибка обобщения может быть одной и той же, но ошибка обобщения в зависимости от природы этих локальных минимумов может быть разная. И вот еще один плюс тахотического верного спуска в том, что он проскакивает такие вот плохие локальные минимумы и застает в хороших широких локальных минимумах. Соответственно... так ли это исследователи в девятнадцатом году брэдди нил и его коллеги решили проверить что они сделали вот это вот разброс который ведет себя нестандартно они декомпозировали на два типа разброса разброс по данным и разброс по оптимизации разброс по данным моделировался сэмплированием разных подвыбор из обучающие выборки и семейства и целые набор нейронных сетей обучали на разных подвыборках, одно и то же обучающие выборки, и эти все нейросети были с одними и теми же стартовыми весами, то есть с одного и того же рандом всегда начиналось обучение. Таким образом моделировался разброс по данным, и разброс по данным вел себя в общем традиционно, как и положено было разброс. Он возрастал с ростом сложности модели и потом до какого-то пика. дорастал а вот разброс по оптимизации другой тип разброса к тому делился по-другому один и тот же набор данных Но разные рандом седы, то есть разные начальные значения весов нейронной сети выяснялось, что разброс по данным как раз вел себя колокообразно. Сначала он вел себя, как и любой порядочный разброс с ростом сложности. Он взрастал, достигал какого-то пика, потом резко убывал и выходил на плато, очень маленьким становился. колоколобразное поведение разброса по оптимизации обусловлено именно стокастичностью процесса оптимизации приводил к тому, что разброс по оптимизации доминировал в общем разбросе, в суммарном разбросе, и приводил к тому, что общий разброс тоже вел себя колоколобразно. Очень интересная статья, которую я советую почитать, и она Объясняешь, что очень сложные раскидистые деревья – это плохо, очень сложные модели на основе, например, каа ближайших соседей – это плохо, а вот очень сложные нейросети – это хорошо. И, наконец, третий фактор успеха, почему нейронная сеть победила, наиболее интересен с точки зрения сегодняшнего доклада. Эти первые два фактора были известны и до меня, они теоретически обсуждались, иерархичность нейронной сети, ее эффективность была доказана еще в нулевых годах. Проблема в том, что большая нейронная сеть не подвластна традиционному пониманию дерева смещения-разброса, нейронный разброс не увеличивается, и это связано с природой обучения глубоких нейронных сетей, стало сознаваться в конце десятых годов, а вот еще один момент, нейросеть знала о структуре понятий предметной области. Смотрите, коллеги, когда мы решаем задачу прогнозирования, задачу оценки точной температуры, например, минус 5,5 градусов, эта задача может рассматриваться не сама по себе, а с точки зрения таксономии понятий предметной области. Температура может рассматриваться как некое уточнение задачи прогнозирования обобщенных температурных категорий. например, холодно, тепло, жарко. Эти категории, как я уже сказал, образуют таксономию, некую иерархию понятий. А поскольку сама по себе нейронная сеть является иерархией обучаемых представлений, мы можем попробовать помочь этой иерархии обучаемых представлений быть более правильной с точки зрения таксономии категории предметной области. В частности, мы можем обучить модель одновременно прогнозировать и температуру и решать задачу классификации в целом холодно тепло или жарко. И поскольку эти понятия — точная температура и обобщенный температурный класс — образуют иерархию, точная температура – это более высокая, более специфичная категория, чем более простая, абстрактная категория температурная – холодно, жарко, тепло, то мы эти задачи можем вывести из разных скрытых слоев нейронной сети. То есть финальный классификатор строится на последнем скрытом слое. которая моделирует наиболее высокоуровневое представление объекта, а вспомогательная задача более простая, более низкоуровневая, своего рода гипоним, наоборот, гипероним, если мы будем говорить в терминах гипоним-гипероним из тезаусов. То есть гиперониум более общее, гиперониум более частное. То есть финальная задача более частная, температурная категория это более общая. Она выводится из какого-то промежуточного слоя. Например, если тут 18 слоев, то из 18 скрытого слоя строится основной классифицирующий слой для температуры, основной регрессионный слой для температуры. А если, например, 12-го спиртового слоя строится в гладиолипосфенцирующий слой для температурной категории. Почему это работает? Почему мы вообще говоря рассуждаем об иерархии категорий, иерархии понятий предмета области? Именно потому, что сам по себе человек мыслит иерархиями. Его семантика, семантическая память обладает свойствами иерархии. На эту тему достаточно много исследований было в 50-е и 70-е годы. Специалисты по когнитивным наукам занимались тем, что пытались понять, как мыслит человек. Как вы помните, в 60-е годы это был период, когда нейронные сети бурно развивались, там работа Розенблата, других специалистов по нейронным сетям. Потом пришел Марвин Минский и показал, что персонплоды Розенблата обладают принципиальными недостатками. Напоминаю, персонплоды Розенблата – это была трехслойная нейронная сеть, но в которой обучался лишь последний слой. Первые два слоя инициализировали случайным образом. Такой персептрон обладает принципиально ограниченной представляемостью, он способен решать задачи линейно-неразделимые. Марвин Аминский пришел, доказал и сделал выводы, что нет никакого основания предполагать, что если мы будем обучать и другие слои, не только последние, эти недостатки не будут преодолены. Проклятие Марвина Аминского привело к первой зиме в области теории нейронных сетей. И специалисты начали искать альтерактивные походы, связанные с символьным искусственным интеллектом. Прежде всего, с семантическими сетями. А семантические сети родились как раз из исследований людей, близких к биологии, в частности, вот из знаменитой работы Колинса Квиллиана 1969 года об иерархическом устройстве семантической памяти человека. понятия выстраивают некую иерархию соответственно ну например да доберман овчарка испаниель это частные случаи понятия собака гипонима гиперонима собака собака кошка и корова это частные случаи для понятия млекопитающая соответственно млекопитающая присмыкающаяся земноводная частные случаи понятия животное и тому подобные штуки и вот они проводили эксперименты предлагали людям проверить истинность или ложность некоторых утверждений на естественном языке И замеряли время реакции человека, насколько человек быстро подтверждает истинное и сложное высказывание. И вот выяснилось, и они предположили, что если у нас понятия образуют разные уровни иерархии, то для понятий близких по уровню иерархии, например. доберман и спаниель это все собаки они близки уровню иерархии в этом дереве человек быстрее отреагирует быстрее истинная сложность высказываний а если у нас будет высказывание фигурировать например спаниель и ящерица человек чуточку дольше будет отвечать И действительно, так оно и получилось, в ходе целой серии экспериментов выяснилось, что человек чуть дольше реагирует на высказывания, в которых используются понятия очень далекие друг от друга, которые с точки зрения гипотезы об иерархии имеют очень далекого общего предка, очень далекого общего гиперонима. В то же время, когда у нас понятия имеют английского гиперонима, то человек быстрее не их встречает. Это исследование было весьма фундаментальным. Дальнейшая работа в этом направлении уточняли какие-то аспекты этой теории, кибернетической теории, но в целом до сих пор эта теория считается доминирующей и принципиальных опровержений ее не найдено. На основе этого были построены модели символов интеллекта такие как семантические сети в частности. Но кажется, тезаурсы, тот же вернет, это отражает собственно работы по вернету как раз стартовали в начале 70-х годов, как раз после вот этих работ, связанных с семантической памятью человека, все взаимосвязано было. Но кажется, что у нас есть две иерархии. одна иерархия естественная, иерархия симметрических понятий в сознании человека, и другая иерархия искусственная, иерархия слоев глубокой нейронной сети. Давайте мы поможем этой иерархии искусственной наиболее эффективным образом моделировать естественную иерархию сознания человека, которая представлена формальном виде. Например, в виде тезауруса верднет подобного, или в виде антологии, или в виде просто таксономии понятия предметной обрасти, выраженной какой-то форме. У нас глубокая нейронная сеть образует каскад обучаемых представлений и последним слоем является классификатор. Универсальный классификатор в пространстве последнего обучаемого представления, последнего скрытого слоя. Мы можем эту модель обучать насквозь. сохраняя основное преимущество нейронной сети, мы можем регуляризировать процесс обучения, внося сильную априорную информацию о пророде задачи на основе существующей таксономии для этой задачи. То есть мы вкладываем нашу антологическую иерархию в нейро-сетевую иерархию, делаем некий knowledge embedding в нейронную сеть, и получается весьма эффективно. Оказывается, что такой подход позволяет эффективно решить ряд проблем, которые наблюдались в обучении обычных глубоких нейронных сетей. В целом, если мы говорим о практической технологии глубокого обучения, основанной на вот этих трех преимуществах нейронных сетей, то это сводится к следующему. У нас есть ряд проблем, которые мы решаем. Это устойчивость к двиганию данных, это оценка неопределенности, это преодоление дилеммы смещения-разброса. Это проблема затухания градиентов, специфическая именно для нейросетевых алгоритмов. У нас глубокая нейронная сеть, но мы используем рекуррентные формулы распространения градиента, локальных градиентов, локальных ошибок обратно. И поскольку там цепочка умножения малых величин, градиент может затухнуть, и первые слои могут неэффективно обучаться. Соответственно, глубокое обучение самого по себе концепция. может повышать эффективность решения проблемы устойчивости к сдвигу данных и оценки неопределенности именно из-за того, что мы выстраиваем иерархию слоев. Специальные функции потери могут эффективно моделировать один из аспектов неопределенности – алеаторную неопределенность. Ансамблирование – это известная техника, которая повышает устойчивость к сдвигу данных и эффективно моделирует эпистомиологическую неопределенность. А вот иерархическое многозначное обучение позволяет более эффективно решать задачи устойчивости к сдвигу данных за счет того, что иерархическое многозначное обучение подсказывает модели, какие корреляции между целевой переменной и входными признаками являются наиболее правильными, инвариантными. а какие являются ложными, именно за счет того, что простые задачи, возвращаемся назад, простые задачи, наиболее абстрактные, которые связаны с наиболее высокоуровневыми гиперонимами. Они назначаются младшим первым слоям нашей нейронной сети, самым первым обучаемым представлением, и они наиболее общие не только для конкретной задачи, а и для целого класса задач вне зависимости от входных условий, вне зависимости от распределения признаков, и они являются наиболее инвариантными к сдвигу данных. А последние задачи, связанные с гипонимами и с тезаурусом, соответственно, с последними слоями нашей нейронной сети, являются наиболее задачно-специфичными и наиболее чувствительными к сдвигу данных. Но поскольку мы показываем модели правильную иерархию, смотри модель, вот это выделение категории гипонимов, решение целевой задачи, финальной, специфической. основана на какой-то системе гиперонимов для них, а эта система гиперонимов основана на системе гиперонимов еще более высокоуровневых, то мы тем самым заставляем модель учить более устойчиво обучаемые представления на промежуточных слоях. То есть мы тем самым решаем проблему устойчивости в виде данных, одновременно с этим иерархическое многодатчное обучение позволяет еще более эффективно решать дилеммы смещения и разброса. Напоминаю, что разброс декомпозируется на разброс по данным и разброс по оптимизации. Именно разброс по оптимизации обеспечивает колоколобразную форму общего разброса, а разброс по данным ведет себя традиционно. Но, опять-таки, внося... Знания антологические о решаемой задачи в виде иерархии понятия в модели мы можем снизить и разброс по данным, таким образом еще более эффективно решать дилемму смещения разброса. И наконец, затухание градиентов тоже позволяет, мы снижаем остроту проблемы затухания градиентов благодаря тому, что задачи образуют иерархию. вспомогательные задачи выводятся из промежуточных слоев глубокой нейронной сети, таким образом, когда мы распространяем градиент обратно, например, к этому слою он уже может подзатухнуть, а тут у нас, хоп, вспомогательная задача. фактически новый импульс придаем распространению градиента и дальше идут тоже опять-таки большие значения. Таким образом, мы можем более эффективно обучать сеть глубокую на всю глубину без всяких дополнительных трюков типа батч нормализации и тому подобных вещей, которые плохо работают, которые хорошо работают для сверточных моделей, но плохо работают, например, для многослойных персифтеров. Это общая практическая методология глубокого обучения, которую я предложил на основе своего опыта участия в соревновании Pre-NeurIPS 2021, а потом я попробовал апробировать эту концепцию на другие задачи. Возможно, мне просто повезло, возможно, специфика конкретной табличной задачи регрессии была такая, что это сработало. Может быть, на других задачах не будет работать. Давайте посмотрим. Мне было интересно, насколько эта методология является общей. Распространяется, например, на задачи анализа речи, задачи выделения семантики в текстах и так далее. В 1923 году было проведено другое научное соревнование уже при конференции AI Journeys, с которой многие из слушателей знакомы или, наверное, все даже знакомы. Там было четыре соревнования, одно из них было посвящено проблеме распознавания речи для языков малых народов России. Называлось AI Fatalk. Необходимо было решить задачу преобразования звукового сигнала в цепочку фонем международного фонетического алфавита. для языков малых народов России, для которых речевые корпуса являются весьма небольшими. Такие языки, как луговоморийский, эвенкийский, камасинский являются, по-моему, мертвым уже языком, вымершим. Но, тем не менее, речевые корпуса и с разметкой лингвистической, фонетической для него существуют. И вот... Какой алгоритм будет наиболее эффективным для этой задачи преобразования звукового сигнала в цепочку фонем? Мы взяли базовое решение на основе глубокой нейронной сети типа трансформера, гибридной на самом деле, сверточная нейронная сеть плюс слои с механизмом многоголовочного внимания, трансформер-кодировщик это Wave2Vec 2.0. которая предварительно обучалась на большом объеме неразмеченной речи на разных языках, более 50000 часов речи на разных языках, задачи восстановления пропусков в речевом сигнале. Вообще говоря, поскольку нейронная сеть строит иерархию обучаемых представлений. При этом младшие представления являются наиболее общими, наиболее абстрактными и общими для целого класса задач, а последние представления являются наиболее достаточно специфичными, то возникает соблазн такую иерархию обучаемых представлений переиспользовать. То есть обучить на одной задаче, а переиспользовать на других. На этом основана концепция переноса обучения или «трансферлёни». knowledge transfer, переноса знаний в нейронных сетях и, соответственно, Перенос знаний – это еще одна из причин успеха нейронных сетей в целом. Ни один другой алгоритм машинного обучения не способен к эффективному транстер-ленингу, а нейронные сети, именно благодаря тому, что они являются иерархией обучающего представления, способны. И вот здесь как раз концепция транстер-ленинга была реализована. Модель предварительно обучалась на большом объеме неразмеченных данных в режиме self-supervised learning. То есть источником разметки являлся сам… сигнал звуковой, мы просили модель восстановить пропуск и сигналь. А потом мы брали эту модель предобученную, дообучали на малых речевых корпусах, размеченных отечественными фонетистами для определения цепочки фонем. Это решение дало 32% фонетических ошибок. Но, хотеть хочется больше. И мы подумали, были привлечены фонетисты, лингвисты-фонетисты, а фонетике известно, что фонемы встраиваются в некоторую таксономию фонетическую. Сами по себе фонемы это наиболее дробный, наиболее такой высокоуровневый класс, это система каких-то гипонимов. которые встраиваются в некую антологию предметной области. Фонемы бывают гласные и согласные. Гласные фонемы бывают ударными и безударными. Согласные фонемы тоже делятся по виду и способу образования. Фрикативные, неогубленные и тому подобное. Естественно, мы можем вот это вот... иерархию классов политических использовать для синтеза вспомогательных задач в рамках концепции иерархического многозначного обучения. Когда мы синтезировали низкоуровневые задачи, то есть высокоуровневые задачи, это распознавание фонем целевых. а низкоуровневая задача – это распознавание обобщенных фонетических классов, и эта задача не требует никакой дополнительной разметки. Разметка для низкоуровневой задачи синтезируется автоматически на основе высокоуровневой разметки и имеющейся таксономии. И использование такой вот… вкладывание такой вот антологии простой фонетической в нашу акустическую неростивую модель позволило снизить уровень фонетических ошибок с 32 до 27 процентов и мы тогда на этом соревновании взяли серебро что называется другой пример чисто текстовая задача задача анализа текстов эта задача распознавания именованных сущностей, при этом в языко-независимой постановке. Каждый год при известном воркшопе Semaval Semantic Evaluation который проговорится, по-моему, больше 10 лет ставится несколько соревнований на тему анализа текстов, семантического анализа текстов, прежде всего, на это намекает название workshop, semantic collation. И вот высокоуровневая задача, это задача анализа и распознавания именованных сущностей, названий компаний, имен людей, каких-то продуктов, каких-то геолокаций и тому подобных вещей. но при этом для любого из 12 языков, языки от португальского до фарси, разные языки. У лингвистов тоже есть представление об уровне знаний о языке, об уровне сложности представления языка. Самый простой уровень, низкоуровневый, считается морфологический уровень. Морфология – это наука о словах, о способе словообразования и словоизменения, и о падежах слов, о морфемном анализе слова. Следующий уровень понимания языка синтаксический синтаксист это наука о взаимосвязях между словами в рамках предложения, например, или чего-то более общего, например, синтаксического единства. Наконец, самая высокоуровневая задача это задача семантики. Семантика и наука о смысле слов. Есть еще были высокоуровневые задачи, например, прагматика. Прагматика – это задача о контексте семантики. Но, тем не менее... мы выше не поднимались, потому что сама по себе задача распознания минного сущности скорее семантическая, а не прагматическая. Соответственно, это высокоуровневая задача. Мы синтезируем среднеуровневую задачу, синтаксическую и низкоуровневую задачу, морфологическую, но здесь уже не на основе разметки и онтологии, а на основе разметки, онтологии и вспомогательных систем. которые уже существовали для данных языков, таким образом мы синтезировали иерархию понятий из приметной области, из понимания языка на иерархию слоев глубокой нейронной сети типа трансформер-кодировщик BERT и получали повышение качества. Здесь уже не снижение уровня ошибок, а повышение качества, потому что использовалась сбалансированная F-мера с 59% до 64%. Но это было не первое место для этого соревнования. Первое место занял исследовательский коллектив из китайской компании Alibaba, Alibaba Group. они предложили очень большую нейросетевую модель, которая получила F1-скор примерно 80%. Мы тогда были ограничены вычислительно, то есть мы использовали небольшую модель, это XLM Roberto в качестве базового решения, XLM Roberto Base на 270 миллионов параметров, мы не могли оперировать миллиардными нейронными сетями. Для нас основной задачей было проверить, как для одной и той же архитектуры, для одной и той же базовой модели Изменение способа дообучения, внесения, вложения таксономии предмета области в процесс дообучения моделей изменяет качество. И оно действительно существенным образом изменяется. С 9% до 54%. Это тогда позволило нам занять, по-моему, 23 место из 28. Там команды из разных коллективов, университет Ля-Рошели, Али-Баба-Групп, университет Хельсинки совместно с финляндским ПВМ, университет Нидерландов и тому подобное. Там наши коллеги из института информатики Суран тоже участвовали, одно из их решений, по-моему, заняло. последнее место, а другое решение совместно с университетом Иннополиса и Школой экономики заняло решение чуть выше моего. Вспомнил, не 23 место, 26 место мое решение заняло, но тем не менее использование, вкладывание антологии, лингвистической антологии в модель машинного обучения позволило существенным образом увеличить качество решения задачи. Таким образом, можно подходить к концу и делать общие выводы. Глубокая нейронная сеть действительно является наиболее эффективной моделью машинного обучения, независимо от природы данных. Это не только речь, картинки и тексты, это в том числе наиболее универсальные табличные данные, наиболее дисспецифичные. С другой стороны, внесение априорных знаний о задачи из антологии, из какой-то формальной модели, из модели формальной семантики, регуляризирует обучение модели. В то же время, если мы эти априорные знания вносим не просто так, а в виде. таксономии в виде таксономии наиболее соответствующей естественной иерархии понятий в сознании человека, то такие знания оказывают наиболее эффективный вклад обобщающую способность глубокой нейронной сети. Это является естественным способом регуляризации глубокой нейронной сети, который обеспечивает эффективное решение целого класса задач из разных доменов, из разных областей, будь то анализ табличных данных, будь то распознавание речи, будь то анализ текстов и тому подобных. Соответственно, можно считать, что такой фреймворк вкладывание антологии в виде иерархии задач глубокой нейронной сети является достаточно универсальным и эффективным. В настоящий момент я работаю над статьей, которая Я надеюсь, будет подготовлена к зиме к конференции ICML дедлайн от подачи сабмитов открывается 9 января, закрывается 1 февраля и я полагаю, я надеюсь, что работа на эту тему будет подготовлена. Пока что есть припринт выложен на архиве на тему моего участия в соревнованиях 2021 года. На этом я хочу закончить. Спасибо за внимание. Надеюсь, что мой доклад был вам интересен и полезен. Здесь мои контакты есть. Пользуясь случаем, немножко прорекламирую. Есть канал мой, который мы с коллегами ведем на тему искусственного интеллекта. Милости просим. Спасибо за внимание. Буду рад вашим вопросам. И, наверное, начну с ответа на вопрос про хронозирование без времени. Но это не прогнозирование в строгом смысле, это регрессионная задача, то есть какая температура будет завтра при условии, что сегодня У нас есть признаки климатические, погодные признаки, вот такие вот данные метеостанции, какие-то вторичные прогнозные системы. То есть неважно, есть ли упорядоченность в наших данных или нету, важно, что мы решаем задачу прогнозировать температуру на завтра. То есть прогнозирование в этом смысле. Это просто регрессионная задача. Нужно определить значение целевой переменной. которая является вещественно-значной в пику классификационной задачи, когда у нас оба значений целевой и переменной – это, конечно, множество классов. Сразу второй вопрос – каким методом априорные знания закладываются в нейронную сеть? Либо вручную, скажем так… Либо полностью автоматически, например, в примере для прогнозирования мы иерархию синтезировали полностью автоматически. Мы рассматривали целевую величину вещественно-значенную температуру как задачу регрессии, которая является уточняющей для решения задачи классификации, а обобщенные температурные классы синтезировали на основе либо кейминг с кластеризацией целевой переменной, либо разбиение с целевой переменной на квантире. Таким образом, мы выделяли температурные классы и автоматически синтезировали иерархию задач. В случае с распознаванием речи, с синтезом или с анализом текстов, с синтезом иерархии для классификационных задач, там полуавтоматический способ использовался. То есть мы вручную строили таксономию фонетическую где описывали отношение гипонием-гиперониям для фонем для все более общественных фонетических классов и уже из-за таксономии автоматически синтезировали вспомогательную задачу для фонетических классов. Наконец, для задачи анализа текстов Мы использовали вспомогательные системы машинного обучения, решавшие общие задачи синтетического морфологического анализа, и их результаты использовали для построения таксономии и синтеза множества задач, проецируемых на разные скрытые слои нейронной сети. Надеюсь, что я на вопрос на ответил. Дальше вопрос из телеграмма. Почему более общее понятие «холодное животное» представлено в более раннем слое сети, а более частное понятие на последнем слое? Почему не наоборот? Это связано именно с тем, что, как я показывал в самом начале, что для самой нейронной сети Иерархии, обучаемые иерархии выстраиваются от простого к сложному. Самые первые слои моделируют наиболее общие, наиболее абстрактные иерархии, наиболее абстрактные понятия, которые соответствуют каким-то высокоуровневым гиперонимам. А чем ближе слой к последнему классифицирующему слою, решающему конечную задачу. классификации либо регрессии, тем более специфичными являются вот эти иерархические представления, и они, получается, соответствуют каким-то гипонимам в некотором тезаурусе. То есть мы проецируем на нейронную сеть перевернутый тезаурус. Именно поэтому, не наоборот, надеюсь, что на вопрос ответил. Как все-таки извлекать иерархические знания предметообласти с предобученных сетей? О, это хороший вопрос, это обратная задача. Я пока что не могу сказать точно как. У меня точного ответа нету, это вопрос открытый. Могу предположить, что... Для некоторых частных неразливых архитектур, например, для архитур типа трансформер. Я наблюдал в результате анализа матриц внимания между состояниями, я наблюдал то, что сигналы, контекстные связи между отдельными элементами последовательности, которые наблюдаются в результате анализа матриц внимания соответствуют разным уровням понимания высказывания. То, что я говорил, морфологические и синтаксические. То есть вполне возможно, что используя какие-то графовые методы для анализа матрицы внимания, рассматривая матрицу как некую матрицу для графа, мы можем строить какие-то связи и рассматривать как некие деревья зависимости синтаксические, либо смартфонологические знания о связях между морфемами отдельных слов. Естественно, таким образом, да, мы можем извлекать иерархию понятий, но иерархию понятий лингвистических. Но в общем случае, я пока эту проблему не решал. И вот еще вопрос, можно ли извлекшить эти знания, обнаружить правительство с двиг-ручками на уровне онтологии и загрузить обратно? Хороший вопрос, хороший вопрос. в явном виде, когда мы строим вспомогательные задачи и модель учится их прогнозировать, кажется, да. Более того, если модель учится распознавать, решать вспомогательные задачи, то мы можем использовать рассинхрон в решении вспомогательных задач и основной. для того, чтобы еще дополнительно оценивать какую-то эпистемиологическую непригодность модели. Например, модель спрогнозировала температуру плюс 23 градуса, но вспомогательную задачу она говорит холодно. Очевидно, что модель столкнулась с какой-то аномалией, ситуация сдвига данных и решение оконечной задачи не бьется с решением более низкоуровневой вспомогательной задачи. Вот такой ситуации мы можем к минимуму. Делать explainability говорит, что модели доверять нельзя, это еще один дополнительный источник эффективного моделирования неопределенности. Так, тут Борис Новиков поднял руку. 

А. КОЛОНИН  [01:09:35]  : Да, Борис, пожалуйста. 

Б. НОВИКОВ  [01:09:37]  : Здравствуйте, добрый вечер, спасибо за интересный доклад, но мне кажется, все-таки... Основной вопрос о методе. Как вы вносите априорные знания в нейронную сеть? Я вижу два основных пути. Либо вручную изначально задавать веса какие-то, либо менять разметку обучающих ног, более мелкую или более крупную. Или у вас какой-то третий путь, или какой-то из этих, или оба, вот как вы вносите априорные знания от структуры предметной области, как вы говорили, об энтологии, в нейронную сеть? 

И. БОНДАРЕНКО  [01:10:19]  : Ну вот смотрите, спасибо за вопрос, это хороший вопрос. У нас есть разметка в данных, мы решаем задачи с супервайзерами, размечения на размеченных лунах. У нас есть разметка в данных? И мы эту разметку категории классов, например, можем рассматривать как гипонимы для некоторого тизауса или понятия высокоспецифичные в некоторой таксономии. Соответственно, зная разметку. Тезаурус мы можем пройти от гипонимов к гиперонимам и синтезировать разметку новую для вспомогательной задачи автоматически из гиперонимов для данного тезауруса. Еще можем пройти по тезаурусу выше, ближе к еще более высокому уровню гиперонимов. Еще один уровень разметки синтезировать, то есть имея разметку и тизаурус, мы можем синтезировать разметку для вспомогательных задач более абстрактных из гиперонимов этого тизауруса в общем случае. Как мы строим этот тизаурус или самую простую токсономию, это вопрос отдельно. Либо мы используем сами классы это какие-то понятия на естественном языке мы можем использовать вернет подобные тезауксы для того чтобы определять место вот этих понятий целевой разметки и дальше идти оттуда по цепочке гиперонимов, синтезируя более абстрактные понятия для вспомогательных задач, либо можем строить таксономию вручную, например, когда я приводил пример по распознаванию речи, мы простую таксономию фонетическую построили сами, сделали тезаурус, есть гипонимы, это конкретные фонемы, есть гиперонимы разного уровня, это разноуровневые фонетические классы, и тот же самый прием, имея целевую разметку, И тезаурус мы синтезируем в разметку в помогательной задаче. Вручную мы ничего не доразмечаем. Это все производится автоматически, если у нас есть целевая разметка целевой задачи и тезаурус предметы области. Я ответил на ваш вопрос? 

Б. НОВИКОВ  [01:12:34]  : Да, спасибо, но небольшое уточнение. Это все делается под конкретную задачу, поскольку все зависит от контекста. Нам нужны охраняющие животные, это могут быть не только собаки, но и гуси и так далее. Если нам нужно повышать удойность коров, то это одно, если мясной животноводства, то это другое. Каждый раз под конкретную задачу нужен свой тезаурус. 

И. БОНДАРЕНКО  [01:13:07]  : Мы можем использовать предметы специфичной онтологии, безусловно, для конкретных задач, а можем использовать какие-то общие тезаурусы типа вернетта на самом деле. 

Б. НОВИКОВ  [01:13:16]  : Но только для задачи распознавания образа. 

И. БОНДАРЕНКО  [01:13:20]  : Ну, для задач анализа текстов на естественном языке, например, или для каких-то задач распознавания образов тоже есть ли у нас сами классы этих образов, описанные понятием на естественном языке, да. 

Б. НОВИКОВ  [01:13:30]  : Скажем, за расчёты балки вы не можете ввести сопромат, и чтобы, используя уравнение сопромата и какие-то граничные условия, рассчитать балку в строительстве. 

И. БОНДАРЕНКО  [01:13:44]  : Здесь необходимо строить антологию именно на основе понятия сапрамата, соответственно, граф, симметический граф? 

Б. НОВИКОВ  [01:13:51]  : Антология – это качественное соотношение, включение, подможность, множество, классы. А формулы – это не антология, насколько я понимаю. Поэтому я спросил, спрашивал первый мой вопрос, можно ли заложить... как заложить в сеть алгоритм выделения в столбе. Это возможно сделать? 

И. БОНДАРЕНКО  [01:14:17]  : Это хороший вопрос и, скорее всего, Скорее всего, нет, если мы говорим об обычных нейронных сетчатках. Можно, но это будет неэффективно, потому что нейронная сеть это прежде всего статистический метод, она основана на приближенном решении. Ситуация, вот есть деление задач, решаемых задач на три типа. Это формализуемые задачи, трудноформализуемые задачи и неформализуемые задачи. Это не я придумал, это известная система разделения задач Александра Ивановича Гаушкина, одного из основателей советской нейроинформатики. Он в 1973 году написал известную работу про многослойные системы распознавания образов, где впервые, еще до Румельхарда Эйнхиттена, предложил обратное состояние ошибки. Так вот, он делит задачи на... Формализуемые, трудноформализуемые и неформализуемые. Неформализуемые задачи – это задачи, где сама функция задана в виде набора примеров. Это классические задачи распознавания образов. Трудноформализуемые задачи – это задачи, для которых возможна формализация, аналитическое решение теоретически возможно, но в практике оно малоэффективно из-за высокой сложности задачи и используют численные методы решения задачи. И, наконец, формализуемые задачи – это задачи, которые легко решаются аналитически. Соответственно, задача деления в столбик, мне кажется, что относится к формализуемой задаче и здесь гораздо проще использовать точные методы, чем приближенные, связанные с нейронными сетями. Тем не менее, способность... нейронных сетей к математическим рассуждения представляет определенный интерес ввиду того что сейчас развиваются большие языковые модели это модели нейронной сети которые возникли из задач анализа генерации текстов но при этом выяснилось что они не просто умеют манипулировать словами а способны кажется на какие-то рассуждения на основе текстов на естественном языке. И такие модели могут проявлять определенные математические способности. Они могут складывать, делить. Они могут какие-то находить алгоритмы, типа, не знаю, решета эротосфена и тому подобные вещи. Но это все-таки выходит за ту тему, которую я хотел вам представить сегодня на своем докладе. 

Б. НОВИКОВ  [01:16:52]  : То есть, включить априорноформализованную информацию как постпрограмму работы сети – это задача, не та задача, которую вы рассматривали. Естественно, вы оба взяли градиент за неделю. Если температура росла неделю, то с большой вероятностью она и дальше будет расти, например. А для этого надо поделить... суточные значения одно на другое, и чтобы поделить, есть алгоритм простой, можно к этой программе обратиться, или это технология нейронных сетей пока не очень позволяет. 

И. БОНДАРЕНКО  [01:17:34]  : Я рассматриваю общие модели нейронных сетей типа многослойный пресептрон и другие модели, такие как трансформерные модели, сверточные, и я пытаюсь решить проблему, я пытаюсь решить проблему вкладывания онтологии в архитектуру нейронной сети. научить модели манипулировать какими-то понятиями такого уровня, это такое возможно, когда мы используем авторегрессионные языковые модели, большие языковые модели, типа GPT-4, например, которая встроена в GPT, это какие-то модели типа Lama или Vicuña, опенсорсные аналоги GPT, то есть это… авторегрессионные трансформерные нейронные сети, декодировщики. Там, да, можно научить модель использовать инструменты в ситуации, когда модель сама понимает, что не способна решить задачу. Например, можно научить модель, такую большую языковую модель, не пытаться ответить непосредственно на вопрос, связанный с вычислением производной. а использовать Вольфрам для того, чтобы передать задачу, сформулировать задачу в терминах этого языка для Вольфрам или для Матлаба, например. То есть да, такое делают, но опять-таки это немножечко про другое, про другую тему, вне пределов темы моего доклада. Вне пределов темы моего доклада. 

Б. НОВИКОВ  [01:19:15]  : Пожалуйста. 

А. КОЛОНИН  [01:19:16]  : Иван, спасибо. Иван, спасибо. Еще вопрос из Telegram. Ну, например, в сверточных сетях обработки изображения более ранние слои представляют более частный элемент изображения линия, а более глубокие представляют более сложную часть машины. 

И. БОНДАРЕНКО  [01:19:33]  : Спасибо, но я по-другому бы это же самое сформулировал, более ранние слои представляют более простые элементы изображения, такие как линия, и они являются не более частыми, а наоборот более общими, а более глубокие представляют более сложные, в смысле более задачно специфичные часть машины. Часть машины это уже не часть и не часть дома. Линии как бы все, и крыло птицы, и колесо автомобиля, и подъезд дома, они в общем-то состоят из линий, а более глубокие слои представляют более специфичные задачи, специфичные объекты для конкретной задачи. а не для целого класса задач. То есть тут не частные элементы общие, а наоборот, более простые являются более общими, а более сложные являются более специфичными, соответственно, более частными. Я так это интерпретирую. И на основе этого делаю проекцию онтологии в виражах слоев. 

А. КОЛОНИН  [01:20:37]  : Спасибо. Еще вопрос от Петра Матюкова. Предположим, мы решаем задачу АССР в колл-центре. Какая, например, иерархия может быть там? Что может привнести из предметной области как вспомогательная задача? 

И. БОНДАРЕНКО  [01:20:50]  : Отличный вопрос. 

А. КОЛОНИН  [01:20:51]  : Что может привнести из предметной области как вспомогательная задача? 

И. БОНДАРЕНКО  [01:20:54]  : Отличный вопрос, да, я уже приводил даже пример. Если мы решаем задачу ISR, то неважно, это в колл-центре или в голосовом командном управлении, это задача автоматического распознавания речи. И в общем случае, это задача превращения сигнала в цепочку каких-то лексем. Соответственно, мы можем рассмотреть эту задачу как наиболее высокоуровневую. конкретная лексика и конкретная синтаксическая структура порождается на основе некоторого общего тематического представления, понимания общего домена. Это, например, речь медицинская, или это речь бытовая, или это речь официально деловая на каком-то семинаре. Соответственно, вспомогательная задача – это задача общей тематической классификации речевого сигнала и еще более вспомогательная задача низкоуровневая – это задача фонетического анализа, т.е. фонемы, фонетика – это самый низкий уровень в преде речевого сигнала, общий тематический уровень, такой общий семантический – это более высокий уровень. То есть, это речь из какого домена, медицина, бытовая речь, лексика, официальное выступление и тому подобные вещи. И, наконец, финальная задача – это конкретный синтез текста, высказывания в текстовом виде с учетом его лексико-синтоксической структуры. Соответственно, когда мы используем вспомогательную задачу общетематическую, там еще дополнительный полезный бонус появляется, мы можем манипулировать разными языковыми моделями специфичными, то есть выход системы распознавания речи, нейросетевой, может быть заглажен какой-то энграм на эстетической модели языка, и мы, имея вспомогательную задачу типа тематического анализа высказывания, Можем не использовать большую, гигантскую, универсальную энграмную модель, а можем взять несколько доменно-специфичных энграмных моделей, в зависимости от того, что нам показало решение вспомогательной задачи, подключать ту или иную энграмную модель для сглаживания решения финальной задачи, построения лексики синтетической структуры высказывания. Вот пример того, как это можно применяться для распознавания речи. Более того, это мы применяем в своих подходах к распознаванию речи. 

А. КОЛОНИН  [01:23:18]  : спасибо коллеги есть еще вопросы к докладчику сейчас посмотрим все тихо Хорошо, коллеги. Ну тогда, Иван, большое вам спасибо. Было очень интересно. Вы, я как понимаю, ведете же семинары по пятницам, но эти семинары, они не онлайн, да, то есть там на них присоединиться нельзя по разбору. 

И. БОНДАРЕНКО  [01:23:48]  : Есть онлайн. Как выяснилось, я сразу об этом не знал, а мои коллеги инженеры подсказали, у нас этим семинарам есть посвящен канал на Рутубе. Есть онлайн, можно через зум подключаться. То есть да, можно подключаться, задавать вопросы и так далее. Но это семинары, они скорее учебные. То есть ребята, магистранты наши, новая магистратура Мехмата по машинному обучению и большим данным, они учатся, изучают курс по машинному обучению, а параллельно идет научный семинар, где я предлагаю ребятам для чтения разные статьи научные. по классическим мерам одномашинного обучения без нервных сетей, потому что в первом семестре они учат классический МЛ без нервных сетей. И соответственно тема этих статей определенным образом синхронизируется с темами лекций на основном предмете. И ребята учатся читать научные статьи, понимать... опонировать, спорить друг с другом и тому подобные вещи. Ну и мы приглашаем индустриальных партнеров, специалистов в области практического машинного обучения, которые бы могли взглянуть на вот эти разбираемые, анализируемые статьи. под практическим углом. Насколько на самом деле описанная методология работает, насколько она помогает в решении практических проблем или наоборот она не работает и тому подобные вещи. Семинар разбит на две части. Первая половина семинара выступают ребята, два докладчика. Один докладчик рассказывает про статью, другой докладчик рассказывает про научный контекст статьи, как она вписана. как она связана с другими работами в этой области, а потом задаются вопросы, идут дискуссии, а потом выступает приглашенный эксперт, представитель индустриального партнера допустим какого-то и рассказывает, что все на самом деле не так или там все на самом деле так, как надо, вот примерно так вот в этом заключается наш семинар магистрский. 

А. КОЛОНИН  [01:25:40]  : Ага. Ну, я думаю, что будет интересно, если вы поделитесь информацией об этом семинаре, как в нем можно поучаствовать друг к кому-то и слушать, или… С огромным удовольствием. С огромным удовольствием поделюсь. Да, или участникам сообщества будет любопытно. А потом, поскольку вы меня приглашали в этом семинаре поучаствовать, то вот как раз конкретно по тематике… Извлечение структурированных знаний, скажем так, в широком смысле и энтологии в частности, или формальных грамматик в другой частности из предопущенных моделей – это вот как раз тема, которая мне близка, и если вдруг по этой тематике у вас будет какой-то разбор, то информируйте, я с удовольствием постараюсь поучаствовать, хотя время с точки зрения рабочего графика не очень удобное, но что-нибудь придумаем. 

И. БОНДАРЕНКО  [01:26:27]  : Спасибо большое, Антон. Спасибо. 

А. КОЛОНИН  [01:26:30]  : Коллеги, вопросы к докладчику не появились? 

И. БОНДАРЕНКО  [01:26:33]  : По-моему, Борис Ловиков поднял руку, или это давно было? 

А. КОЛОНИН  [01:26:36]  : Это была рука. Все, понял. Да, тогда огромное спасибо, Иван, вам за вопрос. Всем спасибо, кто пришли в Zoom и на YouTube. У нас следующий семинар будет доклад немножечко с другой точки зрения, больше с точки зрения формальной математики и семантики. Андрей Нечесов будет рассказывать про задачный подход в искусственном интеллекте, теорию обучения и иерархия знаний. Тоже про иерархию знаний. Поэтому до новых встреч. Иван, спасибо и жду от вас информации о вашем семинаре. Всем всего доброго. Хорошо. Спасибо. 

И. БОНДАРЕНКО  [01:27:13]  : До свидания. Всего доброго, коллеги. До свидания. 


https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
