## 14 декабря 2023 - Result GPT: мультимодальные трансформеры в задачах управления - Сергей Довгань — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/gas4OWpvZcQ/hqdefault.jpg)](https://youtu.be/gas4OWpvZcQ)


А. КОЛОНИН [00:00:07]  : Коллеги, всем добрый вечер. Сегодня у нас в гостях Сергей Довгань, и он нам расскажет о своих работах по резал ГПТ и всему с этим связанному. Сергей, пожалуйста. 

С. ДОВГАНЬ [00:00:21]  : Здравствуйте, коллеги. Меня зовут Сергей Довгань. Я занимаюсь рекомендательными системами на основе машинного обучения. Я делал такие системы для сети Mobile, для Яндекс, для ряда ритейл-сетей. И сегодня я расскажу про свою разработку, которая является системой управления на базе трансформера. Но я зайду немного издалека, зайду с точки зрения того, как это все связано с общим искусственным интеллектом. Вот что такое общий искусственный интеллект в моей концепции? Это общий решатель задач. То есть можно решить любую задачу в любой среде, если это физически возможно. Такая терминология используется, так сказать, школой интеллектостроения, представителями которой являются Ютковский, Потапов, Бостром, Хаттер, Мюльхаузен, Колмогоров и прочие люди из этой когорты. Почему мы используем именно такую терминологию? Потому что обычно, когда кому-то нужен искусственный интеллект, ему нужен просто решатель его проблем, которые он сам решить не может. Но это не всегда так. Некоторым людям нужно что-то другое, но в нашем случае так. Согласно этой терминологии, интеллект делится на общий и узкий. Общий – это такой, про который известно, что он может решить любую задачу в любой среде, если задача не является нерешаемой. Ну, например, про человека мы, например, точно не можем сказать, является ли он общим интеллектом. Потому что у нас нет доказательств, что человек любую задачу прям решит. Он точно какие-то задачи не может решить, но может быть дело в том, что ему просто не хватило времени на них. Примером узкого интеллекта, вот такой прям яркий пример узкого интеллекта – это Например, шахматный робот. То есть существует среда, и он в ней умеет решать задачу. И выйдя за пределы среды, он не обучится решать какие-то новые задачи. Или в той же самой среде его нельзя заставить решать какую-то другую задачу. Теперь вообще, что такое задача? Вот это слово нужно определить в такой форме, чтобы свойство решать задачи можно было измерить, чтобы это было что-то математическое, а не философское, чтобы все задачи это было понятно, как проверить. Определим так, что агент включен в среду, у среды есть состояние среды S, это вот исчерпывающее описание всех переменных среды. Агент каждый кадр получает наблюдение Z, которое является частью состояния S, то есть он через сенсоры воспринимает часть переменных среды, и он выдает управляющий сигнал A. который как-то влияет на эту среду. И кроме того, у агента есть функция R от S, либо R от Z, это функция вознаграждения. И именно в ней записано, какова цель данного агента. То есть именно в ней записано, что от агента хочет оператор, какова вот задача. Эта функция подается извне обычно, например, оператором агента и описывает хотелки этого оператора. Агент действует так, чтобы максимизировать сумму вот этой функции вознаграждения по траектории, то есть чтобы исполнить желание оператора. То есть мы сводим задачу интеллекта к задаче оптимизации, если более строго, то нелинейного, нестационарного математического программирования. Чем хорош такой подход к определению того, что такое все задачи? Во-первых, он очень математический. Во-вторых, он позволяет описать широкий спектр задач. То есть мы реально многие задачи можем либо прямо на практике описать в такой форме, либо у нас есть теоретическая возможность это сделать. И он позволяет направлять агента на такие задачи, которые оператор решать не умеет. То есть, например, в таком подходе можно корректно поставить задачу сделать, чтобы у меня на столе был килограмм золота до конца недели. Ну и я слышал, что некоторые говорят, что AGI должен быть ученым, инженером, программистом и еще много кем, но в той формулировке, что я писал, он и будет ученым, инженером, программистом, если задача такова, что ее может решить только ученый, инженер или программист. Сейчас перейдем к тому, какие сейчас есть продукты, подходящие под описание общего искусственного интеллекта. Во-первых, это нейроэволюция. То есть агент включен в среду, поведение агента описывает некоторая стратегия, которая принимает на вход наблюдение, выдает на выходе действие. И у нас есть некоторый способ, как измерять качество этой стратегии. То есть мы, опять же, знаем, чего мы хотим от агента, знаем, как он должен повлиять на среду, во что он ее должен превратить. То есть та самая функция вознаграждения у нас есть. Мы просто прогоняем разные стратегии через среду и смотрим, какая стратегия приносит больше вознаграждения. Каким-то образом скрещиваем эти стратегии, в общем, ищем таким образом стратегию, которое приводит к наибольшему суммарному профиту по траектории. Второй вариант – это гол-агенты, так называемые. То есть агент включен в среду, из его траектории нарезается датасет, где агент начал, куда он пришел, какие действия он для этого сделал. И дальше мы обучаем модель, которая по сочетанию из текущего состояния среды и целевого предсказывает, какие действия могли бы привести в это целевое состояние. И в-третьих, системы обучения с подкреплением. Их много разных, в большинстве случаев они построены вокруг машинного обучения, то есть система машинного обучения предсказывает некие факты о будущем, и на основании этих фактов выбирается действие. Например, предсказываются долгосрочные последствия различных действий и выбирается то действие, для которого эти последствия самые лучшие. Либо же предсказывается сразу то самое действие, которое наилучшее с точки зрения долгосрочных последствий. Ну и так как машинное обучение уже 5-10 лет с прогнозированием справляется в среднем лучше человека, то я считаю, что это зрелая технология. Выглядит, что использовать ее для управления и для построения вокруг нее общего интеллекта – это вполне логично. Но по факту в обучении с подкреплением до недавнего времени обычно применялись только очень маленькие модели и процесс такого обучения очень долго сходился. То есть да, интеллект формально общий. Да, он может решать все задачи, это можно математически доказать. Но у него быстродействие очень плохое. И если использовать модель серьезного размера, то быстродействие падает еще сильнее. А если использовать модель маленького размера, то для маленьких моделей все задачи решить не получится. Например, триммер V2, это некоторое время назад была сота по быстродействию, то есть это самый быстродействующий агент был. Он требует 100 000 кадров взаимодействия с игрой Pong, чтобы просто начать действовать лучше рандома. А быстрее дримера особо ничего и нет в обучении с подкреплением. Поэтому задача построения общего интеллекта сводится к тому, чтобы не просто сделать систему обучения с подкреплением, а сделать ее быстрой и с ощутимо сильной выразительной моделью. То есть мы, например, умеем обучать модели размером с GPT-3. Вот если бы такую модель приделать к обучению с подкреплением и сделать, чтобы она училась быстро, то вот это и будет то правильное направление развития общего интеллекта, вот как оно мне видится. обозначу свое целеполагание. Моя программа минимум была следующая. Обучить модель, которая может заниматься управлением в широком домене. То есть, например, Pacman, Pong, что-нибудь такое. И не одну из этих игр, а вот несколько сразу. Часть или все домены должны быть трехмерными средами, и качество управления должно быть значимо выше рандома. В общем, нужно было обойти обычные системы обучения с подкреплением по универсальности, получившейся после обучения системы. То есть она должна быть не просто потенциально способна обучиться решать любую задачу, она должна обучиться так, чтобы уметь решать сразу несколько задач без дальнейшего дообучения. Поговорим о том, как решить проблемы обучения с подкреплением. Во-первых, сигнал подкрепления обычно разреженный, а взаимодействие со средой происходит каждый кадр. Так что нужно, чтобы модель предсказывала и сигнал подкрепления, и наблюдение среды. Тогда модель внутри себя будет вынуждена создавать такие представления, которые полезны и для прогноза будущих кадров, и для прогноза наград. Это значит, что правильная модель среды получится намного быстрее, чем в обычном обучении с подкреплением. И, например, в Dreamer оно ровно так и сделано. А второе – нужно сделать, чтобы агент мог обучаться на чужом опыте, а не только на своем. То есть, например, если мы хотим выкатить какую-то систему обучения с подкреплением в продакшен, то мы хотим, чтобы до нее какие-то решения принимались, мы хотим, чтобы все эти логи можно было обучить в систему обучения с подкреплением, и она могла воспринять их как свой собственный опыт, поделить их на хороший опыт и плохой, и сделать из него выводы. В-третьих, хорошо бы, чтобы модель умела параллельно решать задачи, какие-нибудь вроде распознавания объектов на картинке. Это будет дополнительно склонять внутренние представления этой модели в сторону таких, какие они у человека. А человеческие представления – это очень неплохой бейслайн. В-четвертых, хорошо бы использовать модель, которая полна по тьюрингу. Такая модель сможет выделять инварианты как минимум не хуже людей. То есть она сможет улавливать любые закономерности, вот сколько их есть в природе. И так как любые наши инварианты, любые наши закономерности выразимы через тьюринг-полную модель, выразимы через алгоритмы. В-пятых, нужно иметь возможность как-то описывать цель в явном виде. В случае обучения с подкреплением функция наград обычно задается эмпирически. То есть, чтобы узнать функцию наград в заданном состоянии, надо в это состояние попасть. Вот там, хотим мы сообщить агенту, что он в Марио должен дойти до конца уровня. Мы ему выдаем сигнал награды в случае, если он прошел до конца уровня. Агент все это время, пока он не пройдет до конца уровня, он не знает, чего мы от него хотим, и он вынужден это выяснять экспериментальным путем. Хотелось бы как-то этого избежать, потому что вот эту информацию мы с агентом могли бы и поделиться. То есть надо ему каким-то образом уметь это сообщать. Все эти идеи можно относительно легко провернуть на GPT. Я использовал GPT в качестве модели, то есть в датасете в начале каждой нашей строки было наблюдение в конце план действия и его оценка. Чем выше оценка, тем в большей мере этот план является примером, а чем оценка меньше, тем в большей степени он считается антипримером. Чтобы подать кадр в текстовую модель, я использовал vacube, который переводит картинку в набор токенов. В моем случае он переводил картинку в 81 токен, то есть он ее разбивал на 81 квадратик, и каждый из этих квадратиков каким-то образом кодировал. Как же были воплощены все эти вышеперечисленные идеи? Первое, чтобы прогноз был не только наград, но и состояние среды, в датасет были подмешаны строки вида, что видел агент, какие действия выполнил агент и что агент увидел после этого. То есть в чистом виде задача прогноза решена. Потом, чтобы агент мог учиться на чужом опыте, использована разновидность Actor-Critic подхода, при котором используются все сэмплы за всю историю. То есть мы сохраняем строки, в каком состоянии был агент, что он сделал, что получилось в итоге, сколько это подкрепление. И я переделал стандартную функцию обучения трансформера так, чтобы вот этот сигнал подкрепления можно было из этой строки вычленить и его использовать для того, чтобы домножить его на функцию ошибки. Такой подход будет немного хуже обучаться на экспериментальных данных, зато он будет лучше обучаться на чужих примерах. То есть модель будет обучаться и на чужих примерах, и на собственных экспериментах. Но она будет и с тем, и с другим справляться более-менее нормально. И чтобы у модели сформировались более правильные представления, в датасет были добавлены строки вида кадр-текстовое описание. И чтобы модель полную по тьюрингу я использовать не смог в данной ситуации, я использовал трансформер. Он не совсем полон по тьюрингу, но из мейнстримных моделей это самое-самое, самое близкое к полноте по тьюрингу. И чтобы задать цель в явном виде, в строках управления пишется не только текущее состояние, но еще и описание задачи. Эту функциональность я толком не применил, но я ее сделал. Планирую применять ее в дальнейшем. Поговорим о среде. Вообще, в каких средах обычно тестируют вышеперечисленные системы около общего интеллекта? Самый первый вариант – это среды с простыми задачками из теории управления, то есть обратный маятник, например. Второй уровень – это задачки игры Atari. Третий уровень – это трехмерные среды, ну, с входными данными в виде картинки. То есть в основном это либо WizDoom, это порт к игре Doom, либо это Minecraft. Ну, Minecraft скорее следующий уровень, он все-таки посложнее. И я выбрал порт к игре Doom. какие есть бейзлайны. То есть, что сделали люди до меня и насколько хорошо быстро они справились. Вот был первый бейзлайн, там агент решал задачу, первая задача это стоять в комнате, стрелять в монстров, которые появляются напротив и выворачиваться от их снарядов. Он обучился до состояния лучшего рандома примерно за 100 тысяч кадров. Это было сделано в 2016 году. Второй бейзлайн. Агент бегал по кислоте и собирал аптечки, пытался прожить как можно дольше. Результат лучшего рандома достигнут за 150 тысяч кадров. Опять же, 2016 год. Потом относительно общее решение с кучей предобучения. То есть агент обучался решать множество игровых задач. Сколько там было этого предобучения, я не знаю, и насколько оно было. Ну, оно явно человеческое было. То есть на человеческих примерах учился. Но агент после этого дообучался в среде за примерно 180 тысяч кадров. И после этого там выдавал какие-то результаты хорошие. Это 2017 год. В 2019 году были сделаны узкие решения, то есть какие-нибудь решающие одну задачу, условно умеющие играть лучше рандома на одной карте. И они сходились до состояния лучше рандома. То есть не до своего максимума, а до состояния просто лучше рандома. Они сходились за 50 тысяч-100 тысяч кадров. Вот. Соответственно, моя задача была сделать лучше, чем у них. Там, чтобы или сходимость была лучше, или чтобы универсальность была лучше. В идеале, чтобы и то, и другое. Вот. Устройство датасета, какое у меня было. Я использовал GPT, который используется для текстов, поэтому я все задачи конвертировал в текстовый формат. Я использовал GPT в смысле я брал трансформер и обучал его сам. Первый тип данных, который был в датасете, это задача аннотирования картинок, то есть в ней есть последовательность токенов. От VecuGAN ключевое слово description и после этого Описано, что находится на картинке, желательно с разбивкой, вроде вот этот предмет слева, вот этот справа, этот подальше, этот поближе. Таких строк было порядка миллиона. Второй тип, это задача продолжения текста. Я надергал тексты с википедии английской, тексты про игру Doom, приключенческая литература, всякие разные справочники. Там 500 тысяч строк. Задача прогноза картинок – это третий тип. В начале подавалась картинка в виде токенов, потом ключевое слово «план», дальше описывался план, последовательность действий, а потом было написано, что надо прогнозировать. прогнозирую в данном случае на эту картинку. Говорил, спрогнозируй мне картинку через такое-то количество шагов. И дальше шла картинка. Таких данных было около 100 тысяч строк. Потом задача прогноза переменных – это то же самое, но прогнозируют внутриигровые переменные. Там около 400 тысяч строк. И дальше примеры управления. То есть в виде токенов картинка, потом описание этой картинки самой же GPT, там выше описано, что GPT у нас умеет описывать картинку. Затем задача, что нужно делать, в данной ситуации kill all, убить всех монстров. Дальше слово план и дальше последовательность действий и сигнал подкрепления. То есть для игры применялся только последний режим, подавалась картинка, затем GPT аннотировала эту картинку, затем запрашивался план действий и выполнялось первое действие из этого плана. Соответственно, вот эта последняя строчка, где сигнал подкрепления есть, когда она попадала в алгоритм обучения GPT, GPT на этой строчке обучался нестандартным способом, то есть он не просто пытался прогнозировать. строку, а он домножал сигнал ошибки на вот этот вот сигнал подкрепления. Там было немножко сложнее, было как в системе Advanced Detector Critic, то есть в качестве коэффициента брался не сам сигнал подкрепления, а то, насколько он отличается от ожидаемого. То есть условно, если мы находимся в комнате, залитой кислотой, то Ожидаемый сигнал подкрепления, допустим, минус 10. И если фактически сигнал подкрепления минус 9, то это значит, что отлично, это хороший пример, так и надо делать. Потому что мы превзошли ожидания. Дальше. Я протестировал модель в среде. Здесь показано видео, как агент играет в среде. Использовались как человеческие демонстрации, но их там относительно мало было, так и очень много экспериментов самого агента. ну не то чтобы очень много, там меньше миллиона строк получилось у всех этих данных. Из 33 карт удалось на 13 сыграть лучше рандома, то есть чуть больше трети. Потом я взял другой набор карт. Вообще-то это действие, которое в обучении с подкреплением никто не делает. Взять другой набор сред, на которых агент не учился, и без дообучения запустить его на этих средах. У меня получилось, я 28 карт запустил, на них на 11 получился результат лучше рандома. То есть, опять же, чуть больше трети. А потом я протестировал, как модель ведет себя при аннотировании картинок. Ну вот она как-то описывает их, она там периодически замечает, что это игра или ну не пишет об этом, пишет каких монстров видит, пишет где она их видит, пишет какие предметы видит. не всегда правильно, не всегда точно. Я потом расскажу. Я последними своими улучшениями немножко испортил вот это свойство системы, ее свойство аннотировать картинки, поэтому она стала работать в этом отношении хуже. Прошлая версия, я чуть позже расскажу, как именно я это испортил и почему так надо было. Сейчас я покажу, как она действовала до этого вынотирования картинок. Здесь я красным подсветил, где она ошиблась. То есть, например, красным прямоугольным обвел предметы, которые система не разметила. Красным текстом подсветил те слова, которые она сказала, но это неправда по факту. Мы здесь видим, что система достаточно неплохо обнаруживает Многие из тех объектов, которые она называет, они действительно есть на картинке. Еще одно аннотирование, это уже за пределами основного домена. Я взял просто какие-то картинки, на которых система не обучалась. Тут, естественно, качество намного хуже, но она все равно как-то предсказывает в большинстве случаев лучше рандома. Потом задача прогнозирования картинок. Мы подавали картинку, мы просили систему сказать, вот если мы сделаем действия такие-то, то что она предсказывает, какую картинку она ожидает увидеть. Здесь, наверное, наиболее интересно вот эта вторая картинка. где use-use это агент подошел к двери кнопка use открывает дверь и видно что предсказывается какой коридор агент увидит там за дверью Задача продолжения текстов. Вот с ней агент справился очень плохо. Тут совершенно не похоже на GPT-2. То есть, например, вот его спрашиваешь, как дела? Он говорит, я не уверен, я не уверен. Ее спрашиваешь, кто такой Уинстон Черчилль? Он говорит, это великий человек, но я не уверен. Она то есть что-то говорит, но не особенно умно. Это я еще хорошие варианты подобрал. Обычно, в общем, эта задача скорее провалена насчет продолжения текстов. Память. Как мы помним, в каком виде агент решает задачу управления, мы помним, что агент получает на вход картинку и запрашивает по Модель получает на вход картинку, и модель мы запрашиваем, чтобы она выдала план действий. Но этот подход не позволяет учитывать прошлое. Поэтому я сделал следующим образом. Я попытался обучить модель делать суммаризацию. Нет, то есть были три подхода. Три подхода. Первый – подать на вход в GPT просто последние несколько картинок. Первый подход. Второй подход – вместо множества картинок использовать множество их описаний текстовых. Но второй подход он так себе, потому что текстовое описание, они достаточно большие, емкие. Первый подход тоже так себе, потому что каждая картинка достаточно большая. И третий подход – обучить GPT суммаризировать сочетания из картинки и текста. То есть GPT получает картинку, я его прошу сделать summary, он описывает эту картинку, потом он получает на вход summary прошлое и картинку нынешнюю, я его прошу сделать summary, некое описание, он делает суммарное описание из текста и картинки, и при следующем запуске он получает на вход опять же прошлое summary из двух картинок, и нынешнюю картинку. Такой подход хорошо работал для текстов, и я его пытался применить для управления. Этот подход получилось обучить так, чтобы он действовал лучше рандома, чтобы он как-то действовал, но все равно получилось хуже, чем если памятью не пользоваться. И этот подход очень требователен к данным. Мне пришлось потратить намного больше экспериментов, чтобы добиться от него какого-то профита, чем при просто обучении на последнем кадре. Кроме того, когда я обучал вот этот подход, то я испортил задачу аннотирования картинки. Поэтому в дальнейших моих экспериментах с ResultGPT я буду применять какой-нибудь другой подход к обработке прошлого. То есть какие у нас выводы по всему этому? Система в целом работает, надо ее развивать до чего-то лучшего, выкатывать в прот, но надо что-то сделать с памятью. И если мы хотим, чтобы система умела понимать тексты и конвертировать их в навыки, то нам надо уделить больше внимания текстам. надо как-то добиться, чтобы система нормально умела их продолжать, ну или отвечать на вопросы, в общем, делать то, что делает обычная GPT. И сейчас я пытаюсь сделать то же самое, но для робота в среде WeBots. Оно как-то получается, но пока что очень средненько. Всем спасибо за внимание. Мой канал в Телеграме. И вот код моей модели. 

А. КОЛОНИН [00:27:21]  : Сергей, спасибо. У нас есть ряд вопросов. Давайте, наверное, начну с конца. Вопросов немного. Вопрос по поводу лучше, чем рандом. Мне казалось, что вещь достаточно стандартная. Я даже пошел писать Владимиру, что это стандартная вещь, можно погуглить. Пошел гуглить и обнаружил, что такой стандартной вещи. Поэтому, видимо, она не для всех стандартная, далеко. Можете пояснить, как вы определяете, что такое лучше, чем рандом? 

С. ДОВГАНЬ [00:27:58]  : Ну, стандартный бейзлайн, стандартная проверка агента на то, что он адекватен – это сделать агента, который делает случайные действия. Ну, там есть две стандартных проверки. Первая – это сделать агента, который делает случайные действия, посмотреть, сколько очков он наберет, и взять нашего агента. Там это первая стандартная проверка. Вторая – если мы делаем не агента, а прогнозную систему, прогнозировать либо средним, либо прогнозировать наиболее частотным классом и сравнивать свою систему с вот этим вот. то есть это базовый такой тест на адекватность. 

В. СМОЛИН [00:28:39]  : можно уточнить? 

С. ДОВГАНЬ [00:28:41]  : пожалуйста. 

В. СМОЛИН [00:28:44]  : если мы делаем рандомные действия, потому что они случайные, результат будет каждый раз разный. С каким из этих разных результатов мы сравниваем действия, что они лучше рандомного? 

С. ДОВГАНЬ [00:28:56]  : Мы делаем много тестов, например, 20 раз запускаем одну и ту же среду со случайными действиями и результаты усредняем. 

В. СМОЛИН [00:29:07]  : Хорошо, но половина из этих результатов лучше вот этого среднего, а половина хуже, я правильно понимаю? 

С. ДОВГАНЬ [00:29:12]  : Нас интересует только среднее. 

В. СМОЛИН [00:29:14]  : Нет, нас интересует среднее, но вот из этих 20, которые вы только что сказали, половина лучше этого среднего. Они относятся к тому, что вы говорите, лучше рандомного, или это нет? 

С. ДОВГАНЬ [00:29:27]  : Они относятся к этому лучше рандомного, но моя модель дает систематически лучше рандомного, а не один раз за 20 попыток. 

В. СМОЛИН [00:29:35]  : Как формально вы определяете, что значит систематически лучше рандомного? 

С. ДОВГАНЬ [00:29:38]  : Но я ее повторяю, запускаю один раз, два раза. Я ее также многократно запускаю, и она воспроизводимо дает результат лучше рандома. То есть я все те же несколько раз ее запускаю, и в среднем она дает результат лучше, чем рандом. 

В. СМОЛИН [00:29:52]  : Но это 1%, 5%, 10%. Насколько она должна быть лучше рандома, чтобы вы считали, что это статистически достоверно? 

С. ДОВГАНЬ [00:30:06]  : Такой проверки не проводится, она должна быть просто хоть насколько-то лучше рандома. В данной ситуации, в данной среде все было достаточно очевидно. там лучше рандома это лучше хотя бы на там ну в данной проверке лучше рандома это лучше хотя бы на два попадания на два попадания в цель а всего сколько попаданий то есть два попадания на фоне с какого числа зависит от среды в большинстве задач рандом дает 0 попаданий а агент ну дает сколько-то ну допустим там 5-10 попаданий Есть среды, где целей много, и там попадания могут измеряться десятками, но этих сред, ну скажем, одна-две из тридцати. 

А. КОЛОНИН [00:30:56]  : Я поясню вопрос Владимира. То есть, грубо говоря, если в среднем рандомный агент получает 0 попаданий, а ваш агент получает 5, то это вообще ого-го. А если в среднем рандомный агент получает 100 попаданий, а ваш 101, при этом разброс плюс-минус 50, то это ни о чем. Понимаете? 

С. ДОВГАНЬ [00:31:19]  : Я понимаю, я согласен, вот таких сред было абсолютное меньшинство. В большинстве случаев было вот не так, в большинстве случаев рандом это значит ни одного попадания. 

А. КОЛОНИН [00:31:29]  : Я к чему, думаю, Владимир меня поддерживает, что в этом плане хорошо бы иметь какие-то статистические оценки с доверительными интервалами, если всерьез эту историю показывать, рассказывать. 

В. СМОЛИН [00:31:41]  : Понятно, о чем речь, да. 

С. ДОВГАНЬ [00:31:45]  : Я полностью согласен с этим. Да, надо это лучше прорабатывать. Я сейчас вложил столько ресурсов и смог сделать такие оценки. 

А. КОЛОНИН [00:31:56]  : Спасибо. И второй вопрос от Владимира. Суммаризированное описание. В какой форме это делается? 

С. ДОВГАНЬ [00:32:09]  : модель получает на вход картинку в виде последовательности токенов, затем она получает слово summary, двоеточие, и после этого она должна выдать некий текст, который описывает картинку. То есть в базовом варианте должно быть как-то так. Но в принципе я согласен с тем, чтобы модель выдавала там что угодно, любую последовательность токенов, если она потом готова это воспринять как summary, то есть если она потом готова это использовать как некое описание истории, которое увеличивает качество управления. И действительно иногда агент скатывался в какой-то свой язык, состоящий из знаков препинаний, которые он как-то понимал. Но это все недостаточно проработано, оно слишком оказалось требовательным к данным. Я решил, что этот подход не надо использовать. 

А. КОЛОНИН [00:33:05]  : Спасибо. У меня несколько вопросов насчет как раз затратах в данных, ресурсах и так далее. Во-первых, то, что вы сами считали, на каком железе вы это считали и какие были затраты по времени. и второй вопрос тоже с этим связанный значит вот то что вы показывали об счет на разных картах это вы в реальном времени считали или это с каким-то временным фактором грубо говоря там день час за день или там или год за или или год за день вот и какой этот фактор был если это так ну в целом у меня 

С. ДОВГАНЬ [00:33:47]  : Так, я использовал две видеокарты. То есть у меня был локальный компьютер и был сервер арендованный. На локальном компьютере происходил инференс, то есть там была симуляция, там был агент принимающий решения. На сервере происходило обучение, и я где-то месяц запускал агента в локальной среде, и два раза в сутки я его синхронизировал, то есть я скидывал полученные агентом данные туда на сервер и перезапускал обучение модели там на сервере, а с сервера брал последнюю версию модели и выдавал ее агенту. Насколько это реалтайм, та версия модели, которая вот здесь описывается, это не совсем реалтайм, она у меня, ну каждый инференс модели, это было где-то 0,4 секунды, то есть она немного не дотягивала до реалтайма. 0,4 секунды. 

А. КОЛОНИН [00:34:57]  : То есть, 0,4 секунды – это за насколько? То есть, грубо говоря, для того, чтобы секунду вообще считать в реал-тайме, сколько у вас уходило? 

С. ДОВГАНЬ [00:35:06]  : в среде, ну предполагалось, что агент будет в среде делать три действия в секунду, по факту он делал ну где-то полтора-два действия в секунду, то есть до real-time он не дотягивал. 

А. КОЛОНИН [00:35:21]  : Ну то есть 0,7 получается, да? Да. Угу, окей. Так, значит еще здесь какие-то глубокие вопросы. Во-первых, уточнить что от Бориса Новикова. Во-первых, что называется задачей вашей концепции? 

С. ДОВГАНЬ [00:35:47]  : что называется, задачи в конкретно этом агенте, я так понимаю. Можно было выдать ему разные правила выдачи подкреплений. Так как это игра Doom, то я сделал четыре варианта подкрепления. Первое – это за убийство либо ранение монстров, второе – это за подбор предметов, третье – за пополнение здоровья. Четвертое – за разведку, то есть за то, чтобы агент оказался как можно дальше от той точки, где он стартовал. И, соответственно, у него было четыре варианта вот этих задачей. GPT получала на вход описание задачи, то есть одно из вот этих четырех ключевых слов. Ну, не слов, а последовательности слов. Ну, например, там kill all, или scouting, или pick up items. И в зависимости от того, какая задача, ему проставлялись подкрепления за разные. То есть либо за предметы, либо за расстояние и так далее. Это было сделано для того, чтобы потом можно было агенту сказать, что делать, какой цели достигать, он достигал этой цели. Но по факту эту механику я толком не отработал. То есть в датасете все эти разные цели есть, но на практике использовались только задачи kill all. То есть вот эту функциональность я не отработал как следует. 

А. КОЛОНИН [00:37:31]  : Спасибо. Следующий вопрос, если нужно уточнить. И, соответственно, что являлось решением задачи? Решена при любом уровне вознаграждения? 

С. ДОВГАНЬ [00:37:43]  : Если смотреть решена-нерешена, то решением является то, что мы получили вознаграждение больше, чем случайный агент в тех же самых условиях. То есть по факту задача – это схема выдачи вознаграждения. Решение задачи – это последовательность действия агента. Соответственно, если много вознаграждения, значит задача хорошо решена. Если мало вознаграждения, значит плохо решена, либо не решена вообще. 

А. КОЛОНИН [00:38:15]  : Спасибо. Следующий вопрос тоже такой фундаментальный. Как соотносится понятие задача и модель реальности? 

С. ДОВГАНЬ [00:38:28]  : Модель реальности – это нечто, что используется в прогнозе. Это некая штука, которая сообщает нам какие-то факты о будущем. В нее заряжаем факты о настоящем и прошлом, она сообщает нам факты о будущем. Это модель. Здесь модель реальности является неявной частью модели машинного обучения, то есть внутри этого трансформера содержится некая модель, которая позволяет делать предсказания о будущем. Как соотносится задача и модель реальности? Вот в моей концепции, которую я изначально описал, которая концепция того, почему обучение с подкреплением – это и есть решение любых задач, в ней задача – это некое сочетание из среды и схемы вознаграждения. Поэтому модель реальности – это такой инструмент для того, чтобы получить это самое вознаграждение. Инструмент для решения этой самой задачи. 

А. КОЛОНИН [00:39:37]  : Спасибо. Еще вопрос от Бориса Новикова. Можно ли сказать, что у вас каким-то образом решается задача передачи агенту априорных знаний? 

С. ДОВГАНЬ [00:39:53]  : Агенту действительно передаются априорные знания, но я бы сказал, это очень мягкая передача. То есть мы не закладываем, например, когда мы делаем автопилот к ракете, то мы закладываем в него неявную некоторую модель реальности, и если модель реальности изменится, то ракета адаптироваться никак не сможет. Здесь же мы закладываем в датасет примеры картинок и их описаний, что является в какой-то мере априорными знаниями о реальности. То есть мы сообщаем модели, что вот неплохо бы делить мир вот на такие части. Ты, конечно, как хочешь, но неплохо бы делать вот так. Или мы в него закладываем тексты. Эта история не пошла, но предполагалось, что так как модель умеет гнать картинки в тексты и работать просто с текстами, то, соответственно, она сможет из текстов выцеплять какие-то сведения о реальности, какие-то закономерности. Так что в какой-то мере мы в нее действительно закладываем априорные знания о реальности. Но система будет работать, даже если их не закладывать. Я их закладывал для того, чтобы она сходилась быстрее. 

А. КОЛОНИН [00:41:12]  : Спасибо. Еще вопрос из YouTube. Подскажите, пожалуйста, на каком языке программирования все делается? Собираются данные, пройдет процесс обучения? 

С. ДОВГАНЬ [00:41:24]  : Это делается на языке Python. Код можно посмотреть, там код далеко не самый красивый, но вот можно открыть, глянуть. Нейросети сделаны на PyTorch, библиотека HuginFace, с которой я там очень много чего переписал. 

А. КОЛОНИН [00:41:44]  : Спасибо. Коллеги, еще есть вопросы к Сергею? 

В. СМОЛИН [00:41:51]  : Ну, раз мы так быстро договорились, наверное, можно позадавать, если не возражаете. Давайте. Насколько я понимаю, что задача более-менее успешно решается, поскольку есть декомпозиция, то есть есть разделение изображения на токены и, соответственно, словесное описание – это тоже некоторая декомпозиция. Я правильно понимаю? 

С. ДОВГАНЬ [00:42:13]  : Я действительно сделал эту декомпозицию с разделением, ну с добавлением вот этого текста в описание, но сейчас я покажу где она использована. Так. Вот пример управления. Вот здесь написано, вначале у нас токен int, типа что начинаются входные данные. Дальше идет последовательность токенов. Дальше у нас идет текстовое описание, сгенеренное самим уже GPT. Вот это действительно декомпозиция. Я проверял, работает и без нее, но с декомпозицией оно сходится все-таки побыстрее. Так что да, некоторая декомпозиция использована. 

В. СМОЛИН [00:43:04]  : Хорошо, а вот про токены можно поподробнее? Как преобразуется изображение с токеном? 

С. ДОВГАНЬ [00:43:11]  : А, существует такая нейросетка, называется VecUGAN. Она берет на вход изображение, бьет его на квадратики, каждый квадратик кодирует в токен. И причем это делает воспроизводимо, то есть можно в нее зарядить последовательность токенов, и она его превратит обратно в картинку. Это сделано с помощью технологии автоэнкодинга, но я туда глубоко не лез. Как именно она сделала автоэнкодинг вот таким дискретным, чтобы он прям в токены превращал. 

В. СМОЛИН [00:43:43]  : И насколько обратно распоряженная картинка как-то будет соотноситься с той, с которой были увеличены токены? 

С. ДОВГАНЬ [00:43:50]  : Зависит от того, на сколько токенов мы разбили картинку. То есть чем больше токенов, тем... Я так понял, что на 81. Я использовал 81 токен. По моим прикидкам, нижняя граница того, что картинка выглядит не вырвеглазной, это где-то 144 токена. Нет, может быть чуть больше 100 токенов. Ниже она выглядит плохо. Вот сейчас при 81 токене картинки выглядят плохо при обратном воспроизведении, но они все-таки читаются. Вот я могу привести пример. Сейчас. Вот здесь прогноз. Это на самом деле спрогнозированы токены, и эти токены WQGAN перегнал обратно в картинку. То есть видно, что картинка в принципе читается, но выглядит отвратно. 

А. КОЛОНИН [00:44:41]  : А подскажите, пожалуйста, я вот просто представляю, чем идет речь, но конкретно не знаком с этой технологией. Там как работает? Вы берете видео, говорите сколько нужно токенов и она сама каким-то образом эти токены формирует или какой-то другой алгоритм? 

С. ДОВГАНЬ [00:45:01]  : Примерно так она режет картинку на квадратики размера 16х16 и сколько она найдет этих квадратиков, столько и будет токенов. И таким образом я картинку вначале привожу к нужному мне формату, чтобы токенов было именно 81. И потом она на них нарезает. то есть там можно любое количество токенов получить. 

А. КОЛОНИН [00:45:25]  : то есть она все-таки тупо режет картинку на квадратики? да. 

В. СМОЛИН [00:45:30]  : нет, а насколько тупо? просто 9 на 9 режет или все-таки эти квадратики могут где угодно располагаться? у меня 9 на 9. то есть просто тупо 9 на 9 режем и соответственно каждый квадратик известно где? 

С. ДОВГАНЬ [00:45:42]  : да. понятно. 

А. КОЛОНИН [00:45:52]  : Коллеги, есть ли ещё вопросы к докладчику? 

В. СМОЛИН [00:45:55]  : Как вы понимаете, у меня ещё много вопросов есть. 

А. КОЛОНИН [00:45:59]  : Если у докладчика есть время, у вас есть время, у нас есть ещё поспособствия. 

В. СМОЛИН [00:46:05]  : То, что вы рассказали нам, что луч рандомного, мы более-менее восприняли. Если случайная модель вообще никого не убила, а у вас убило трёх-четырёх, а может быть даже четырёх, то это, видимо, лучше. Это понятно. Но все-таки соотнести, если человек поиграл неделю в эту игру, то как он соотносится с этой моделью? С человеческим уровнем? Я не хочу, чтобы вы сравнили с чемпионом мира, но не совсем с новичком. 

С. ДОВГАНЬ [00:46:38]  : Она сейчас играет намного хуже новичка. Чтобы получить результат лучше, ее надо тюнить и тюнить. То есть нужно провести еще очень много циклов того, что вот она обучилась в среде, после этого данные скинули на сервер, на сервере обучили модель, после этого модель сняли в среду, поместили. Вот очень много циклов нужно провести, но это в любом обучении с подкреплением MonoTag. 

В. СМОЛИН [00:47:05]  : нет, это не вызывает сомнения. вопрос, насколько вы уверены, что если вы в 100 раз больше циклов проведете, что она будет в 10 раз лучше играть? у вас есть такая уверенность? 

С. ДОВГАНЬ [00:47:14]  : есть теоретические доказательства. это очень интересно, какие? Система использует разведывательную эвристику, то есть она периодически делает случайные действия. Это означает, что из одной и той же ситуации она при попадании многократно в одну и ту же ситуацию, она будет действовать в разных случаях по разной стратегии. Поэтому в датасет будут попадать строки, которые начинаются с одного и того же, в которых разный план и разный уровень подкрепления. Те строки, в которых значение подкрепления больше, они будут давать самый большой градиент при обучении, поэтому они будут наиболее приоритетны. Поэтому агент будет склоняться к тем последовательным системам действия, которые дают ему большие награды. А так как он постоянно привносит рандом, то он будет периодически открывать новые последовательности действий. То есть даже на таком уровне он будет постепенно сходиться к оптимальной стратегии. И на самом деле были эксперименты, я использовал систему Эктор Критик, я ее использовал на трансформере. Были эксперименты примерно того же самого, но в одной среде и без трансформера, то есть на простенькой нейросетке, более узкая система, но алгоритм в целом тот же самый. И оно сходилось не то чтобы к какому-то глобальному оптимуму, сходилось к какому-то хорошему оптимуму, часто на уровне хорошего продвинутого игрока, ну или не продвинутого, но как минимум выше новичка. Я это все качественно описываю, там нужно смотреть конкретные цифры, на каких-то средах оно прям сильно выше новичка получалось. 

В. СМОЛИН [00:49:22]  : Ну, я понимаю, что если играть там в крестики ноли три на три, то, наверное, то, что вы говорите, оно полностью будет работать, потому что там не так много состояний, там всего порядка шести тысяч возможных вариантов партии, и все можно записать. Но когда появляется сколько-либо сложная среда, возникает проблема сложности, когда все состояния мы описать не можем, потому что их очень много. Как вы с этим работаете? 

С. ДОВГАНЬ [00:49:46]  : Самое главное, что здесь использовано для того, чтобы работать с таким большим набором состояний, это использована обобщающая способность трансформера. То есть он может какой-то удачный ход в одном месте подсмотреть в одной ситуации и перенести его на другую ситуацию. То есть обобщение опыта получено за счет того, что у меня машинное обучение. Но в дальнейшем предполагается использовать и другие механизмы для обобщения опыта. Вот тот механизм, который я использую для прогноза, За счет него можно генерировать синтетическую выборку, которая будет, пусть она и менее правильная, менее соответствует реальности, зато ее можно синтезировать в огромных количествах и дообучаться на ней. Это называется обучение в воображении. Я тут пока это не сделал, но есть соображение делать так дальше. И кроме того, таким образом можно проворачивать всякие разные, более сложные планы, вроде того, что можно поместить агента вот в его условном воображении в одно и то же состояние, из этого состояния прогнать там тысячу разных планов и поместить в датасет только тот, где обещается наибольшая награда. То есть борьба со сложностью предполагается такая в дальнейшем? 

В. СМОЛИН [00:51:25]  : Ну, до какой-то степени это эффективно известно, но повторюсь, опять-таки, на не очень сложных моделях. Потому что, понимаете, если у вас, возможно, там миллион состояний, и вместо того, чтобы прогнать 100 тысяч вариант, вы прогоните 10 миллионов, конечно, вы во втором случае будете значительно лучше знать среду, понимаете. Но если у вас там 10,50, и вы вместо 100 тысяч прогоните 10 миллионов, то там разница будет не очень большая. Хотя, конечно, тоже некоторый прогресс, наверное, даст, но, скажем так, степень продвижения будет совсем разная. Вот как вы оцениваете, сколько состояний в той среде, в которой, собственно, вы стоите? То есть, я так понимаю, что у вас, во-первых, действия дискретные, я так понимаю, положение тоже достаточно дискретное. И что там еще дискретное, какие там еще упрощения? 

С. ДОВГАНЬ [00:52:11]  : Количество состояний можно, условно, считать бесконечным. 

В. СМОЛИН [00:52:15]  : Если мы считаем клоун бесконечный, то смысл попытаться пройти их все теряется, потому что мы там можем пройти всегда конечное число состояний. 

С. ДОВГАНЬ [00:52:30]  : Мы не пытаемся пройти, мы пользуемся тем, что у нас машинное обучение. Ему можно подать конечную таблицу какой-то функции, конечную таблицу с иксами, игреками какой-то функции, она ее расширяет до бесконечности. То есть оно предсказывает, какие игреки будут при других иксах. То есть в этом вся суть машинного обучения, оно для этого тут и применяется. 

В. СМОЛИН [00:52:56]  : Мы в курсе, что все нейросети осуществляют аппроксимацию, нас не просветили в этом плане. Вопрос в том, насколько хорошо эта аппроксимация осуществляется. Проблема в том, что если среда низкоразмерная, то эта аппроксимация, попробовали миллионы, еще лучше 10 миллионов вариантов, мы хорошо аппроксимацию настроили, все хорошо работает. Но когда у нас размерность большая, то мы очень маленький процент точек посещаем. И, собственно, качество аппроксимации от этого падает, как вы понимаете. То есть если много мест, где мы не были и пришли в первый раз, то аппроксимация там хорошая, может быть, если вы какие-то специальные действия предпринимаете. Что вы в таком случае делаете? 

С. ДОВГАНЬ [00:53:41]  : В тех местах, где аппроксимация плохая, агент будет делать просто какие-то действия, и за счет этого он будет разведывать среду в этих местах и закидывать это в датасет. После этого эти состояния отправятся на сервер, агент дообучится там, и зона того, где у него аппроксимация хорошая, она расширится. Это не совсем правильное представление, что вот у нас есть бесконечное количество точек, и нам нужно посетить их все. Нет, скорее у нас есть бесконечная плоскость, и мы сейчас накрыли какую-то ее площадь. И вот эта площадь, на ней у нас хорошая аппроксимация. Мы можем выйти за пределы этой площади, и тогда у нас будет хорошая аппроксимация еще и там, куда мы вышли. И таким образом мы просто будем покрывать больше и больше этой площади. Конечно, у нас не будет никогда хорошей аппроксимации вообще на всей этой бесконечной плоскости. Но нам и не нужно, нам достаточно того, что мы можем неограниченно расширять вот эту зону с хорошей аппроксимацией. 

В. СМОЛИН [00:54:44]  : Знаете, какое дело, что если бы мы просто гуляли и смотрели по сторонам, то вы правы, в трехмерном мире там более-менее постареющиеся пейзажи и там встречаются иногда необычные, но в целом все более-менее постареется. Но поскольку у вас там вы не просто гуляете, а встречаетесь с различными монстрами, соответственно как-то от них нужно убегать, стрелять, они стреляют, то вот это число комбинаций там размерно сильно больше трех. Это понятно? 

С. ДОВГАНЬ [00:55:12]  : Ну, это нормально. 

В. СМОЛИН [00:55:14]  : Да. И, соответственно, вот этих вот ситуаций, поскольку размер насильно больше трёх, набегают очень много, даже более или менее в одинаковом релизе, понимаете. Вот. С этим вы как-то боитесь? 

С. ДОВГАНЬ [00:55:29]  : Так. Мы... Я обучил систему на всех картах дума второго. После этого я ее направил на карты дума первого, и она там на третий карт сыграла сильно лучше рандома. Она обобщила опыт с одних карт на другие. По-моему, этого более чем достаточно. Это значит хорошая обобщающая способность, у нас нет каких-то проблем с аппроксимацией. Просто надо лезть дальше в неизвестное, больше набирать знания, больше аппроксимировать. 

В. СМОЛИН [00:56:08]  : Ну, оптимистом быть хорошо, я не спорю, но почему-то не всем так хочется. 

С. ДОВГАНЬ [00:56:12]  : Так нет, если бы я был не прав, то она бы выдала ноль. 

В. СМОЛИН [00:56:16]  : Конечно, вы правы, я не буду вас переубеждать, безусловно. 

А. КОЛОНИН [00:56:20]  : С моей точки зрения, Сергей, есть вопрос, который единственный остается. Насколько она оказалась лучше рандома? На величину в рамках погрешности или на величину выше погрешности? Вот этот ответ, он здесь является важным, с методологической точки зрения. 

В. СМОЛИН [00:56:43]  : Ну, на самом деле, нет. Это как бы не важно. Понимаете такое дело, что если возвращаться к исходам, с чего начались, что сильный искусственный интеллект – это решатель любых задач. Если ставить такое условие, что было лучше рандома, в принципе, конечно, допустим, такомах у нас не получается, но он работает лучше рандома. И много еще задач, которые мы не решаем, но подходы какие-то есть. И соответственно, есть лучше рандома. Поэтому, если верить, что это путь к сильному искусственному интеллекту, салаться на Калмагорова, против него же не какой-нибудь безродный Арнольд, а Калмагоров, соответственно, это авторитет. И тут мы спорить с этим не можем. 

С. ДОВГАНЬ [00:57:26]  : У меня рассуждение немного другое. Во-первых, не сильный искусственный интеллект, а общий. Сейчас, подождите, пожалуйста. Например, есть алгоритм сортировки. Он может быть общий, а может быть узкий. То есть узкий, например, сортирует все до пяти этих... Если у нас некий алгоритм сортировки имеет плохую асимптотику, например, экспоненциальную, то да, у него плохая сходимость, но это не отменяет того, что он общий, это его математическое свойство. Здесь система как бы общая в этом же смысле, и я скорее хотел бы сместить акценты с того, что с задачи сделать общий интеллект на задачу сделать тот общий интеллект, который у нас есть, сделать его более быстрым. 

В. СМОЛИН [00:58:33]  : Нет, я это полностью поддерживаю, да. И даже вы говорили какие-то очень хорошие слова о том, что надо использовать для этого знания. То есть если мы какие-то знания имеем, мы, как ваша модель, подтверждают, что на основе имеющихся знаний обучение происходит быстрее. Собственно, второй вопрос возникает, а вот эти новые знания, которые система получает, они помогают получать в дальнейшем новые знания? То есть, вот сам процесс добавления знаний, насколько он ускоряет процесс дальнейшего понимания ситуации? Ну и, собственно, построение правильного поведения в этой ситуации. 

С. ДОВГАНЬ [00:59:13]  : новые знания система может добывать в формате добычи новых картинок и их разметки. Можно вот это считать знаниями. Это одна вещь, другая система добывает в себе датасет, в датасет новые строки с прогнозированием. Наверное, к знанию все-таки ближе первое – это картинки. Я сравнивался с теми системами, которые были до меня, то есть системы VectorCritic на обычных нейросетях. В них картинок не было. У них где-то по 100 или хотя бы по 50 тысяч кадров, но на одну карту они тратили, чтобы обучиться. Моя система более эффективна, соответственно, видимо, повлияли вот эти знания. Но серьезного эксперимента на эту тему я не проводил. Это один момент. Другой момент. Существует система Dreamer, В ней прогнозируются помимо сигнала подкрепления еще и следующие кадры. То есть решается задача, но она похожа на то, что у меня. И это улучшает качество управления. Поэтому выглядит, что докидывание новых знаний в формате как разбить картинку, оно должно улучшать качество управления. Но это теоретически я распространяю чужой опыт на свою задачу. 

В. СМОЛИН [01:00:51]  : Да, любой опыт он всегда позитивный. Более-менее понятно отношение к вопросу. Я развивать сравнение со своими подходами не хочу. Если кого-то заинтересует, тогда я расскажу. 

А. КОЛОНИН [01:01:09]  : Да, Владимир, нас очень интересуют практические результаты, мы уже видели очень много теоретических ваших лекций, и с нетерпением ждем, когда ваши теоретические результаты будут по-прежнему практическими. На этой высокой ноте давайте поблагодарим сегодняшнего докладчика, поблагодарим всех, кто задавал вопросы письменно, Владимира, который задавал вопросы устно. Сергей, еще раз вам спасибо за доклад. Успехов в оценке и формализации, и квантификации того, что значит лучше рандома. И до новых встреч. Всего всем самого доброго. 

В. СМОЛИН [01:01:52]  : Спасибо. До свидания. 

https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
