## 30 марта 2023 - Нейроморфный искусственный интеллект - теория и аппаратная реализация - Михаил Киселев (Руководитель лаборатории нейроморфных вычислений ЧГУ) — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/MAFhCYSd14w/hqdefault.jpg)](https://youtu.be/MAFhCYSd14w)

Суммаризация семинара:

ема

Семинар посвящен нейроморфным системам и их практической реализации.

Суть

- Нейроморфные системы имитируют структуру и функцию мозга, используя импульсные нейронные сети.
- Во-первых, предполагается обучение традиционной трактовки reinforcement learning без использования градиентов.
- Во-вторых, нейроморфные системы могут использоваться как глобальные, так и локальные, в зависимости от задачи.
- Основной идеей является максимизация интеграла вознаграждения на протяжении времени, а не его действительного числа.
- В теории и аппаратной реализации нейроморфных систем применяются спайки (импульсы), которые передаются на синапсы.
- Принципы обучения не основаны на градиентных методах, а на фиксации корреляций между нейронами.

Детали

- Математические операции в нейроморфных сетях предпочтительно не использовать, или они редки и относятся к семантике.
- Используются объектные и событийные семантические модели, основанные на архитектуре дата-флоу и распараллеливании.
- Нейроморфные системы могут моделировать деятельность или действия на основе актов и совокупностей этих актов.
- В примерах упоминаются анализ бизнес-процессов и моделирование деятельности, как у мухи, так и у человека.
- В результате обучения нейрон формирует законы вознаграждения и наказания локально, для достижения целей.

Результаты

- Основной результат семинара – разработка нейроморфной модели обучения без использования градиентов.
- Применение семантического и импульсного подходов позволяет создавать устойчивые и совершенные нейронные сети.
- Семинар показывает, что обучение нейроморфных сетей возможно путем создания законов для интеграции импульсов и обучения на основе фиксации корреляций.
- Полученные модели могут быть реализованы в различных областях, включая анализ бизнес-процессов и моделирование действия.







S04 [00:00:03]  : Коллеги, всем снова добрый вечер. Сегодня у нас в гостях Михаил Киселев. Он нам расскажет про нейроморфные системы и их практическую реализацию. Наконец-то мы поговорим не о чат GPT-3. Хотя там в группе некоторые товарищи попытались троллить, что уже 20 лет слушают, как новые нейроморфные чипы завалят CPU и GPU. Но, возможно, Михаил сегодня нам расскажет что-то оптимистичное по этому поводу. Михаил, пожалуйста, вам слово. 

S01 [00:00:36]  : Да, спасибо за ведение представления. Ну, я представлюсь. На самом деле тут написана только одна из моих мест работы. Я работаю в Чуваковском государственном университете, где возглавляю лабораторию нервных систем. Еще я работаю в Росатоме, точнее, в структуре Росатома, в частном движении «Цифрум», где я занимаюсь примерно тем и самым. Ну и примерно тем и самым я занимаюсь в структуре Касперского. Тут просто я чуть поясню происхождение этой презентации. Я, в общем-то, с ней практически выступал на недавно составившейся конференции Open Talks AI. Но это была очень интересная конференция, она была так спрессована, что всем давали очень мало времени, поэтому моя презентация была… ей там выделили 12 минут плюс вопросы, и я там, в общем, траторией, скача-галопом по ней скакал, и у меня как бы гифтарь не завершён, и я решил, что я беспокойненько доложу её заинтересованным людям вот здесь. Поэтому, поскольку я там выступал от имени Касперского, то я просто эту презентацию использую для этого. Поэтому и Касперский. Ну ладно. Итак, собственно говоря, о чем речь и почему я не говорю про ЖПТ, как все остальные. Ну, если говорить так совсем билитеристически-лирически, то как я вижу ситуацию текущую? Разумеется, сейчас все Все, что есть интеллектуального, устремлено уже в пяти, три, три с половиной, четыре, пять и так далее. Но дело, конечно, хорошее, и я сам восхищен, впечатлен, но все это очень, во-первых, большое, это какие-то огромные, я уж не знаю, какой объем этих серверов, которые все это считают, но я представляю, что это, наверное, пару зданий. вот и к тому же все-таки это ну в общем можно спорить это уже один и один в общем много чего еще не хватает на мой взгляд и вот как раз собственно говоря как я вижу будущее я вижу так что сейчас все еще года полтора будет тащиться от GPT сколько-то там дальше, 5-6 и так далее, а потом будет понятно, что надо делать как-то так, чтобы это все умещалось не в 3-4 здания, к которым отдельная электростанция, а как-то умещалось хотя бы не в объем черепной коробки, а хотя бы в объем холодильника. И вот тут вспомнят про нас скромных И, следовательно, на ниве нейроморфных всяких штук, чипов и так далее. Ну вот, как раз об этом этот слайд. Я его примерно так же прокомментировал. Чего не хватает? Не хватает энергоэкономичности существующему нейроинтеллекту, вполне понятно. И поэтому отчасти не хватает масштабируемости, но только поэтому. И не хватает также в какой-то степени, хотя сейчас это сильно преодолено, более такие мягкие протоколы взаимодействия между обучающейся сетью и внешним миром. Это какие-то общие слова. Энергоэкономически, наверное, всем понятно, что, в общем, ну да. Мозг, который потребляет, как говорят нейрофизиологи, 20-30 ватт энергии, и который все равно еще круче, гораздо, чем ВПТ, сколько-нибудь. Ну и немыслимые мегаваттные сервера, кластеры, ГПУшек, ТПУшек, потребляющие жуткое количество энергии. То есть разрыв тут много порядков. Не один, не два, не три, а много порядков разрыва. Это понятно. Ну, с этим отчасти связана и масштабируемость, но не только с этим, а с тем, как устроены вообще все традиционные нейронные сети, с их слоистостью, с их глубокой синхронизацией, их работой. Ну, это и синхронизация на уровне работы слоев, и всякие там бочи, которые там, понятно, такими порциями все это дело делается. То есть это все высокосинхронизированные агрегаты этих вычислителей. Поэтому муфтабировать синхронизирующие системы гораздо сложнее, чем муфтабировать асинхронные ансамбли независимых агентов простых. Ну а муфтабировать еще надо, это всем очевидно, несмотря на все эти Сотни миллиардов параметров, если я еще не устарел, это пока меньше, на пару порядков меньше, чем в LAE. Масштабирование нужно, а делать его все труднее и труднее, так уже это проекты типа дронного коллайдера. Дальше так развивать все это сложно. Ну и вот по синхронности нефиксированность, гибкость протоколов взаимодействия с окружающим миром. Ну, в общем-то, тоже более-менее понятно, что в окружающем мире, если мы говорим о боги, у нас там нет никаких обучающих примеров, у нас есть просто жизнь, на которой мы учимся, там что-то переучиваем, доучиваем. В общем, ну, понятно, что это не то, чтобы распознавать картинки, там, отличать кошек от собак и так далее. нет синхронности, то есть это потоки данных, не какие-то статические слепки, какие-то фреймы данных в любом понимании этого слова, а именно потоки данных, которые развиваются в разных временных режимах, в разных модальностях. Все они требуют реакции, как мгновенной реакции, так и обучения. требует отсутствия каких-то жестких регламентов на взаимодействии системы, обучающейся с внешним миром. Ну и вот предлагается ответ на это дело, на эти проблемы, которые основываются, ну, этот ответ называется, в общем, нейроморфными системами, и основан он на двух концепциях, грубо говоря так. если крупным планом, это импульсные нейронные сети, и это как бы его идеологическо-софтварная часть, и не фонейновские специализированные усилители, массивно-параллельные, это как бы его субстрат, железные усилители, на котором это дело все будет крутиться и считаться. Ну, поэтому что нейронные сети, вы, наверное, знают все, поэтому я ничего особенно про них не буду говорить подробно. Просто могу сказать, собственно, почему они являются ответом на вот эти три вызова, о которых я говорил. Ну, тут эти причины, они перечислены. Энергоэкономичность. За счет чего достигается? За счет нескольких вещей. Ну, во-первых, это полностью event-based функционирование, то есть если нейрону нечего делать, то он не потребляет энергии, он не работает. Он не умножает нули на нули, если надо, грубо говоря, но бессмысленные работы он не делает, и поэтому энергия не потребляется. Он просыпается, в сущности, когда на него приходит спайк, когда на него приходит Вторая причина, почему он эргономичен, потому что на него приходит не действительное число или целое число, с которым надо что-то делать, складывать, умножать, то, что обычно делают нервные сети. На него приходит фактически бит, элементарное событие. с которым делать нечего, тем более умножать. Поэтому это очень простые операции, приходит спайк, нейрон его как-то очень быстро обрабатывает, потому что обработать бит там сложно, не придумаешь сложной обработки, и снова засыпает. На самом деле именно на этом в общем-то скорее всего основана и энергоэкономичность нашего мозга, потому что в нашем мозге на самом деле основной объем составляет не тела, клеток, не нейроны, серое вещество, а связи между нейронами, белое вещество. У нас нейроны с проволочками непосредственно, аксонами, дендритами. Поскольку нейроны довольно редко спайкуют в голове, то в основном вся эта масса белого вещества ничего не делает и ничего не потребляет. Импульс пробежал, и воспрождается диссипация энергии, прохождение импульса по нервному блоку. Но это очень короткий процесс. Поэтому идейно, хотя реализация, разумеется, ничего общего не имеет, тут кремний, там все такое, пакеты, а здесь реальные импульсы, реальные проволочки. Но, в общем-то, идея примерно та же самая. И, кроме всего прочего, эти нейроны и в голове, и в моделях имбусно-нейронных сетей, они логически независимы, то есть нет никакого синхросигнала, который там началось пересчет, один слой посчитали, второй слой ждет, пока, ну, понятно, да? Ничего такого нет. Они работают совершенно независимо друг от друга, никак друг от друга не смотрят. Они не явно как-то могут синхронизироваться, но это именно не явно за счет того, что они обмениваются импульсами, там может появиться какая-то коллективная активность глобальная, как у нас в голове такие альфа-ритмы, тета-ритмы, но это не синхронизация, это не причина, а скорее следствие. Такие волновые процессы развиваются, но это не есть какой-то навязанный синхронизирующий механизм. И за счет этого, поскольку и нейроны между собой обмениваются этими самыми спайками, этими атомарными событиями и принимают именно в таком же варианте информацию извне, более того, в таком же виде они ее отправляют нашим мышцам и всем остальному, что у нас реагирует на эти нейроны. Полная синхронность, полное отсутствие каких-либо завязок на какие-то внешние вещи, какие-то фреймы. Понятно, что это как раз очень адекватно тому, о чем я говорил. Очевидные такие пути для решения трех этих проблем. Это была софтварная часть этого всего, теоретически-идейная. А вот теперь, на чем это считать? Ну, на самом деле, тут одно другое полностью определяет, потому что если мы попытаемся эмулировать большую импульсную сеть на обычном процессоре, на обычном компьютере, то мы получим полную ужасающую торможение, потому что понятно, что никакой синхронности там не будет, потому что у нас один процессор будет получить, обсчитывать сети миллионов нейронов, в общем, будет какая-то глупость. Поэтому это, конечно, делается для того, чтобы реализовывалось на специализированных вычитителях, на специальном железе. И вот, собственно говоря, такие вычитители уже есть. Мы их уже знали в нескольких примерах, в том числе и отечественные примеры есть. Чем же они, собственно, отличаются? Вот главное то, что в них есть, Это две вещи. Это параллельность и не фонейминговская архитектура. На самом деле одно с другим полностью связано. Одно без другого не может быть. Потому что фонейминговская архитектура, классическая, предполагает, что у нас имеется несколько сущностей в нашем компьютере. Есть вычислитель, процессор, который делает все операции. А то, с чем он делает, это находится в памяти. Он имеет доступ к такой здоровенной большой гаффи памяти, который залазит и за данными, и за инструкциями. То есть он вытаскивает, что ему дальше делать, смотрит, нужны ли ему данные, вытаскивает данные, что-то с ними делает, снова кладется назад. То есть на самом деле работа фоноидно-скручителя заключается в основном в гонянии вперед информации. между процессором и памятью. Ну, понятно, что это очевидное узкое место. Это хорошо, если процессор один, вернее, плохо. А если их там уже сотня, допустим, то, в общем, никакого ускорения в сто раз не получится просто потому, что они будут дугу ждать за тем, чтобы лазить в эту большую память. Ну и все. Поэтому, в принципе, В общем, очень сложно сделать высокопараллельным фонеймовский вычтитель. Ну, разумеется, там есть кэши, и сейчас, собственно говоря, скорость процессора определяется не столько там частотой, не столько там его нейронизацией, сколько насколько там хорошо устроены кэши, насколько хороша политика, сколько их много, они там уже трехуровневые. Это все делает столько для того, чтобы развязать этот самый ботунек, узкая горлышко. Какая же альтернатива этому делу? Фонеменовская. Это показано у нас на левой стороне. Видно красное – это вычитатели, синее – это память, и между ними шины. Условно говоря, как устроено это у нас в мозге? У нас нет никакого центрального вычитателя. Нейрофизиологи его не нашли. И нет никакого четкого хранилища данных или памяти, именно как абстрагированная такая штука, в которую процессор влазил бы. Вместо этого и то, и другое фактически растворено и диффузно. Можно условно считать, что вычисляем на нейронами, но зачем еще? А то, с чем мы работаем и данные, они на самом деле находятся там же, где находятся нейроны. Они фактически находятся в синапсе. Одно из наиболее общепринятых представлений, что информация записывается как в нейронных сетях обычных в виде весов связи между нейронами. И эти самые веса используются, они ретривятся именно в момент Спайк пришел, он фактически, грубо говоря, считал вес текущего синапса. Реально-то он его не считал, реально там прошел процесс, много этих трансмиттеров вылилось или мало. Но физически, вернее, логически получается, что он его считал. Но он никуда не лезет. Все тут же. То есть, вычисление и память находятся в одном месте. Мы тут не видим никакого горлышка. Поэтому это все может быть реализовано на миллионе независимо работающих агентов вычислительных. Ну и как бы все хорошо. Никакого горлышка нет. И, значит, мы получаем полное, как раз, миллион раз ускорение. По сравнению, естественно, мы делаем то самое. Мы спускаем на что-то последовательное. Очевидно, что такая архитектура является адекватной для реализации на ней импульса нейронных сетей, что это две вещи, которые друг другу соответствуют идеологически. Об этом соответствии этот слайд и написан. Но когда мы работаем с числами, как обычные нейронные сети, традиционные, Там все понятно. С числами можно делить, складывать, умножать, вычислять от них синусы и вообще делать что ни по пути. Человечество уже много тысяч лет делает. А вот что делать со спайками не совсем понятно. Что же организовывать некие процессы вычислительные на их основе, а как? Это некоторый вопрос. Тут надо сказать о том, как информация может представляться с помощью спайков. В случае обычных нейронных сетей она особо никак не представляется, потому что оно и есть число, оно и представляет некую информацию. Другое дело, что мы не очень понимаем, что оно означает глубоко в каких-то внутренних представлениях, в embeddings, но тем не менее эта информация просто в виде чисел передается. Здесь ситуация другая, потому что один спайк, он не информативен, он всегда одинаков. В принципе, так же, как и в голове, в мозге, амплитуда нервных импульсов всегда одинаковая. Она определяется некоторыми напряжениями равноместными при работе нейронных насосов. Она ни о чем не зависит. Но имеется один только информативный аспект у спайка – это время его генерации. Поэтому кодирование информации спайками есть вещь не совсем тривиальная. Она, более того, разнообразно может производиться. Самый простой способ закодировать информацию спайками – это частотно-популяционный. То есть мы говорим, что вот у нас есть какое-то количество нервов, и мы смотрим, насколько они активны, сколько из них избускают спайк в течение какого-то небольшого интервала времени. Многое испустили, значит сильный сигнал. Малое испустили, значит слабый сигнал. Это может касаться и одного нейрона, это может касаться и больших групп нейронов. Принцип один и тот же. Чем больше спайков, тем больше величина. Второй вариант связан с позицией. Допустим, у нас есть некая линейка нейронов. Тогда мы можем говорить, что активность нейронов на одной стороне этой линейки. тут у меня квадратики были зачернены, а я только сейчас понимаю, что они не зачернены. ладно. импорт из одного места в другое не удался. ну, бог с ним. в общем, если мы видим, что у нас слева активны нейроны, значит у нас, допустим, слабый сигнал, а если нейроны активны снова справа, то это значит сильный сигнал. То есть мы по позиции кодируем, так же как у нас в улитке уха кодируется частота звука. Это фактически у нас тоже линейка рецепторов. Одни рецепторы реагируют в основном на низкие звуки, другие на высокие, на дальние, на внутренней стороне улитки. То есть мы по позиции наиболее активных рецепторов кодируем частоту звука, текущую в сущности. Еще есть способ кодировать латентностью. Ну, это фактически сдвиг одной последовательности спайка к какой-то другой. Иногда это делают несколько искусственно, именно такое кодирование часто применяют, когда импульсно-нейронные сети, которые, вообще-то, должны использоваться для анализа потоков данных, всего на свете динамического, когда их используют для какого-нибудь миста, где у нас практически картинки. Там искусственно навязывают это время, потому что импульсно-нейронные сети живут во времени, а мист и фотографии кошек и собачек – это нечто такое. Но там как раз фактически предполагают, что у нас какой-то логический последователь spike – это начало чего-то, а положение спайка по отношению к этому началу, допустим, может кодировать яркость данного пикселя в картинке, то есть именно по задержке. Есть способы кодирования. скорее, было про числа. Есть способы кодирования нечисловых данных, каких-то категориальных данных. Самое простое — это когда мы каждой кветке, вернее, каждому нейрону приписываем некую концепцию, некое понятие, некий класс. Если это активная кошка, если это собака, это бабушка. Помните, такая была концепция Grandmother Cell, что здесь нейроны, которые именно реагируют там, как это наоборот. Сейчас у меня кончается электричество. Сейчас я подсоединю провод. Виноват. 

S00 [00:21:20]  : Очень извиняюсь. Сейчас. 

S01 [00:21:32]  : Еще более сжато и интересно, это может кодироваться с помощью нескольких спайков. Например, если у нас есть 4 нейрона, то если они спускают спайки именно в такой последовательности, как показано на рисунке, в таком порядке, с такими задержками, то это кошка, такое-то собака, а такое-то бабушка. Это способ представления очень хорош тем, что он очень информационно емок. С помощью трех нейронов, если у нас есть какое-то разрешение, более-менее достаточное по времени, можно закодировать, не знаю, сколько, тысячи всяких значений категориальных. И есть основания думать, что так, в принципе, происходит и в мозге, Строгие последовательности определенных нейронов раскодируют какие-то понятия. Что такое нейрон в вычислительном плане? Есть несколько разных моделей. Они различаются уровнем их сложности, ну и отчасти уровнем их биологического реализма. Но работают на самом деле в реальности с довольно простыми моделями, потому что, как мы говорили, это все, в общем-то, имеет смысл только тогда, когда это реализовано на железе специализированной. А специализированное железо, оно, в общем-то, просто, иначе оно будет слишком большое и надупотребляющее. Поэтому практически применяются самые простые модели. Самый простой такой моделью является модель нейрон-пороговый индиратор с утечкой, или Leaky Interval Fire Neuron, или Leaf Neuron, как его часто называют. Он устроен очень просто, показан на рисунке. Фактически его можно сделать с помощью аналоговой схемы. Вот у нас есть конденсатор, каждый спайк добавляет этому или отнимает, если спайк отрицательный. если вес входящего системы, на которую получен спайк отрицательный, меняет потенциал на конденсаторе. если потенциал какой-то есть и ничего не происходит, этот потенциал стекает через сопротивление до нуля, а если спайков приходит много, потенциал растет, растет, растет, и в конце концов, когда он достигает некой пороговой величины, срабатывает пороговый элемент, параллельно находящийся справа, и нейрон сам срабатывает, сам посылает спайк. Вот такая самая простая модель вычислителя. Что это, собственно, означает? Что означает такое оперирование? Это означает, что фактически нейрон производит взвешивание, приходящих на него импульсов. Они все приходят на разные синапсы, разные веса, они по-разному влияют на этот мембранный потенциал. То есть происходит взвешивание и интегрирование сигналов и по пространству, то есть по разным синапсам, и по времени. Надеем тот же синус может поступать много спайков последовательно, и они как бы все интегрируются. И важно, что на самом деле основная функция такого нейрона — это регистрация корреляций. То есть нейрон срабатывает тогда, когда часто спайки приходят вместе в рамках какого-то небольшого временного окна. То есть он ловит события совпадений. То есть, когда как-то согласованно приходят спрайки. Можно сказать, что если базовой операцией обычных компьютеров являются классические логические операции, то базовой операцией, на которой основано все функционирование, Я еще не говорю про обучение, я говорю про инференс, про работу без модификации весов. Это, в сущности говоря, ловля всякого рода корреляций в потоках спайков. Это является базовой операцией, которую, в том числе, и наш мозг, по всей видимости, делает. Этот процесс показан более понятно на рисунке. Вот у нас один спайк, он привел к всплеску меморандума, он стек, и ничего не произошло. У два спайка тоже ничего не произошло, а у три спайка. раз-раз-раз быстро пришли, и вот нас он пересек порог, и сам нейрон сгенерировал спайк. Это все выражается... на самом деле обычно применяется чуть более сложная модель. Она называется модель LIFAT. Это Leaky Integrated Fire Neuron with Adaptive Threshold. Чем она отличается? Тем, что до этого нейрон срабатывает, когда потенциал достигает некоторого порога, а вот в этой модели этот порог является тоже вариабельным, он тоже является одним из компонентов состояния нейрона. который очень просто себя ведет, то есть если нейрон спайкует, то этот порог повышается на какую-то величину, а потом падает до своей базовой, экспоненциально падает до своей базовой величины. Вот это как раз ситуация, показанная этими уравнениями. У этой мембраны потенциал, который просто экспоненциально падает, наглоксирует, а от порога это тоже такая вот, наглоксирующая такая вещь. Это на самом деле дает нейрону сразу очень большое функциональное богатство, потому что, например, он получает функцию реакции на начало чего-то. То есть, если мы начинаем стимулировать нейрон, сначала он генерирует часто спайки, акцентированно реагируя на начало стимуляции, а потом она все меньше и меньше, из-за того, что у нас порог делается больше, а чем порог у нас больше, тем, соответственно, труднее нейрон возбудить, чтобы он генерировал спайки. То есть это дает нейрону, во-первых, германиостатические свойства. То есть если нейрон уже активен, его труднее сделать еще более активным. А во-вторых, дает возможность выраженно реагировать на динамику, на какие-то динамические стимулы, на их начало. На этом можно делать еще всякого рода вещи, связанные с памятью, с помощью рефлективных связей. Как же это соотносится с обычными нейросетями? Имеются два результата, которые говорят, как это соотносится. Во-первых, если мы рассмотрим очень большие интервалы времени, то можно сказать, что если мы все будем усреднять, Если мы используем частотное кодирование, помните, я говорил, что количество спайков может выражать некое число. Чем больше, тем оно больше. Так вот, если рассматривать сеть на больших генералах времени и усреднять эти частоты, передающиеся от нейрона к нейрону, то, в общем-то, мы получим обычную традиционную нейронную сеть. Потому что частота нейрона, генерациям спайков, будет, в общем-то, монотонной функцией от его стимуляции. А его стимуляция – это будет как раз сумма умножений весов синапсов на частоты, входящих в них спайков. То есть, в общем-то, это будет то самое скалярное произведение, которое делается обычными нейронами. Ну, а вот зависимость его активации от этого она тоже будет иметь такой седьмой такой нелинейный характер, то есть это практически будет обычный нейрон, если рассматривать все на очень больших нейроновых рейдах. Поэтому очень формально показано, что обычная сеть, любая, может быть точно приближена соответствующего размера спайковой нейронной сетью, а вот обратное не верну, потому что вот это вот характерные всякие вещи, связанные с Учетом времени, это сложно сделать с помощью традиционной нейросети. Ну и, кроме всего прочего, доказано строго, это недавно было сделано, в 2022 году буквально вышел совершенно точный результат о том, что импульсная нейронная сеть является тюнинг-полной, то есть что угодно на ней можно сделать, любой алгоритм на нее можно закодировать. Поэтому посчитать на ней можно то же самое теоретически, что и на обычных сетях, что и на обычных компьютерах. Но как это фактически можно сделать, как это технически происходит? На самом деле можно показать, что любые операции арифметические во всех этих видах кодирования, о которых я говорил, они могут быть выражены. Они могут быть созданы в соответствующей структуре нейронной, которая их делает. Я в качестве примера взял операции сравнения с константой. Нейрон по своей сути работает так, что он либо все, либо ничего. Либо он генерирует спайк, либо не генерирует. Если стимуляция повышает порог, он генерирует. Соответственно, в сравнении с константами просто он делает как таковой. Это если мы говорим о частотном кодировании. Кодирование латентностью. Это означает, что нейрон должен сработать только в случае, если после какого-то синхроспайка пришел спайк через какое-то небольшое время. Ну, тоже не проблема. Допустим, синхроспайк у нас вызывает срабатывание нейрона, у него мембранный потенциал падает, как мы знаем после этого. Соответственно, если на него приходит следующий спайк, то сам нейрон спайканет, только если второй спайк пройдет довольно недолго, через небольшое время. Потому что ему хватит дотянуть до порога, только если потенциал еще не сильно упадет. Соответственно, таким образом мы делаем сравнение с константой, в случае кодирования латентности. В случае с позиционным кодированием понятно, что мы просто не учитываем нейроны, соответствующие слишком маленьким сигналам. Тем самым мы сравниваем с константой. Это все о том, что здесь, поскольку у нас имеются в принципе разные способы кодирования, то и реализация всех операций у нас в принципе разная, она у нас построена на структурных штуках. То же самое касается, допустим, сравнения двух сигналов. В компульсионном частотном кодировании два сигнала. Один сигнал мы подаем через тормозные синапсы, а второй через стимулирующие, возбуждающие. От того, какой из них больше, мы получим либо спайк, либо нет спайка. То же самое происходит с латентностью. Если у нас тормозной спайк пришел раньше, чем возбуждающий, то есть у нас больше x, то у нас нейрон не спайканет, потому что он будет заторможен и не среагирует на x. Тоже другой подход, но точно так же. Позиционное кодирование. Вот у нас две линейки нейронов, выражающих числа x и y. Мы их соединяем с одним нейроном так, что у них веса на рисунке, чем жирнее линия, тем сильнее вес. Понятное дело, что если у нас эта весалка будет вот так расположена, то торможение от y у нас будет меньше, чем возбуждение от x. Соответственно, если x больше y, то нейрон спрекатится. Ну, в общем, я, наверное, не буду сейчас подробно расстанавливаться на других операциях. Но, короче говоря, можно посмотреть, какие, допустим, операции, какие структуры при разных способах кодирования реализуют операцию, скажем, сложения с константой, сложение двух сигналов и так далее. То есть можно придумать практически, поскольку, как я сказал, такие сети, а не Turing, полны, то можно придумать соответствующие структуры для реализации любых операций. Что, собственно, и приходится делать. Какие-то элементарные вычисления приходится делать и импульсными нейронными сетями. Но видно, что это делается совсем на других принципах, чем это делается в обычных компьютерах. Это достигается причем при таких занятых структурах, более или менее сложных, как в каждом отдельном случае. Я не буду удаляться, как это работает, потому что это будет, наверное, слишком долго. Но на самом деле понятно, что вычитать можешь, что хочешь, но никто, разумеется, не будет вычислять с помощью импульсов нейронной стеки, решать какие-нибудь дифуры или что-то такое. Это зачем? Самое главное, как они обучаются, распознают, как они все это делают. Ну, на самом деле, отчасти похоже, то есть и как в случае традиционных сетей, обучения импульсных сетей, это и есть процесс модификации их весов, весов их связей, весов их синопса. Главная проблема в том, что, как вы знаете, Главная основа обучения обычных сетей – это градиентные методы, которые эффективно реализуются в виде алгоритма обратного состояния ошибки и его многочисленных модификаций. Но все они основаны на том, что у нас имеется градиент от какой-то функции, глобально оптимизируемой, по весам связей. В случае с импульсными нейронными сетями этих градиентов в принципе нет, там не почему дифференцировать. Нейронная сеть дискретна, она возвращает либо спайк, либо не спайк, там чего-то, поэтому почему там брать градиент совсем не ясно. Хотя существуют некие алхимические способы, так называемые суррогатные градиенты, но в общем это скорее такая все-таки алхимия. что-то настоящее. К тому же, в принципе, вот сам подход к небесным нейронным сетям такой, что это скорее оперирование большого количества независимых агентов, где важны именно какие-то коллективные вещи, какая-то статистика. Поэтому модификация единичного веса там, в принципе, ничего дать не может. Более того, этот вес просто можем сюда убрать, у нас нейроны умирают там, плохо себя ведут, не работают, если их там небольшой процент, то это вообще не должно ни к чему приводить. Поэтому, в принципе, задача о том, как мы, меняя данный вес, влияем на что-то глобальное, она не стоит, в принципе. Ее вопрос так не ставится, поэтому так это не работает. Поэтому, ну и, собственно, в биологии мы ничего такого, в общем-то, не видим. Там нет никакого ошибки, как бы распространялось от нейрона к нейрону. Ничего такого там нет. Поэтому принципы совершенно другие. Интересно, почему у меня здесь принцип, а ничего не написано. Как это здесь решается? Принцип локальности, который вполне совместим с общей идеей синхронности всего этого дела, он заключается в том, что вес связи между нейронами, его динамика основывается только на активности двух этих нейронов. То есть мы не видим ничего глобального, а мы видим только вот как эти два нейрона, что с ними происходит, и в зависимости от этого мы меняем веса. Поэтому, опять же, даже и обучение здесь является асинхронным. В отличие от захватывающего всю сеть процесса обратного распространения ошибки в случае традиционных сетей, здесь у нас чисто асинхронные законы, действующие на каждый данный вес. Хотя вот тут тоже еще довольно много всяких подходов связаны с тем, что сеть может динамически перестраиваться. Например, там добавляются нейроны. Если мы видим, что сеть что-то плохо делает, то мы добавляем пустой необычный нейрон, который, значит, не мешает другим, которые уже обучены, что-то новое начинает делать, чему-то новое начинает учиться. Ну и наоборот. Если мы видим, что нейрон плохо себя ведет, часто ошибается, то мы его удаляем. Но вообще говоря, есть общий фундамент, так же как общий фундамент обучения традиционных сетей – это градиентные методы, то общий подход к обучению и после нейронных сетей – это всякого рода изучение, понимание, как мы выявляем какие-то корреляции в потоке спайда. Под это дело под такую схему подходят все три вида обучения, которые нам интересны. Самая обычная, простая вещь, она в первую очередь, когда мы просто структурируем сигнал, уловили какие-то устойчивые паттерны. Фактически мы эти паттерны всегда в случае спайкового посторонней информации можем формулировать в виде каких-то корреляций, может быть слага, может быть антикорреляций, но это так или иначе корреляции подлогов сигналов. на вход подаваемых, а мы знаем, что основная функция нейрона – это фиксация этих корреляций. То же самое касается обучения с учителем. Но тут нам надо искать корреляции уже между входными спайками, и спайками, которые играют специальную роль как индикаторов каких-то классов, которые мы должны там находить, предсказывать по входным спайкам. Точно так же, в сущности, есть задача направления корреляции между потоками спайков. Еще более сложный вариант, но тоже сводящийся к этому же самому, это задача обучения с подкреплением. У нас уже имеются три. виды сигналов, у нас имеются входные сигналы, которые у нас информацию о мире дают, у нас имеются сигналы, генерируемые сетью, передаваемые на какие-то там актуаторы, эффекторы, которые приводят к изменению внешнего мира, и у нас есть сигналы награды и наказания, которые говорят нам, правильно мы делаем или неправильно, вернее, не моя сеть. И то, и другое, и третье имеют вид спайков, и то, и другое, и третье как раз Фактически всегда эта задача корреляции более сложная. Корреляции, в которых вовлечены три типа спайков, это входные спайки, выходные спайки и спайки оценки. Но это все равно трактуется как нахождение корреляции между ними. Поэтому эта задача универсальна, она является неким фундаментом для построения всех алгоритмов обучения универсальных нейронных сетей. Антикорреляции точно так же, поскольку у нас есть тормозные нейроны, Поэтому антиквариация между нейронами и тормозными нейронами может быть к этому сведена. Причем все это может быть настроено и сделано иерархичным, у нас будут какие-то части сети отвечают за супервайс строение, кто там какие-то фичи информативные находит, просто он супервайс в режиме, дальше все это поднимается наверх и работает супервайс в режиме, там ну еще потом еще может, ну то есть это все может как угодно комбинироваться. Мы действительно наблюдаем такой западной пластичности в природе. Это классический результат, который был получен 25 лет назад. Посмотрели на то, как в живой природе меняются веса связей между нейронами, мы увидели, что эта динамика зависит от того, как соотносится с собой приход спайка на силапс и генерация спайка самим нейроном. В классическом варианте SDP это сокращение от spike timing dependent plasticity, пластической основы, основанной на времени спайков, Мы видим, что если часто на нейрон приходит спайк, а потом сам нейрон спайкует, то такой синапс усиливается. Если же делается наоборот, то если нейрон спайканул, а потом на него пришел после этого спайк, то такие синапсы ослабляются. Это был пример чисто локального закона пластичности. Но на самом деле, все более, во-первых, сложно, и в том числе в природе мы наблюдаем и анти-STDP, когда делает все равно наоборот. И, в общем, там это как бы базовый закон, на самом деле даже в природе он сильно очень варьируется. Ну и тем более, когда мы это все применяем для наших прагматических сдач, мы, в общем-то, всякого рода модификации, отчасти упрощение этого закона применяем. Ну вот, например, В случае обучения без учителя, просто приведу два примера, это, собственно, почти последние слайды. Ну, например, почему полная имитация природы не очень хороша. Даже если мы берем ансамбль А, то есть мы должны выбирать корреляции. Но известно, что все живое, оно все не точное. Если у нас какой-то паттерн фактилизуется активностью какой-то группы нейронов, с которыми сроки коррелированы, и мы хотим эту группу найти, то есть мы хотим реагировать только на появление спайков именно с этой группы, то как может получиться, если мы будем использовать этот закон природный, у нас получится, что у нас, допустим, у нас эта группа содержит 10 нейронов. Шесть нейронов спайканули, нам этого хватило для того, чтобы наш нейрон сам, расстановившийся, спайканул, но эти первые шесть будут усилены, естественно, по этому закону. Но потом придут оставшиеся четыре, но они уже опоздали, и они будут подавлены. Но непонятно, с чего бы вдруг, потому что, в принципе-то, это просто случайно получилось, что они сейчас чуть опоздали. Поэтому более отправлено такие случаи использовать симметричный закон пластичности, как нарисован вверху. То есть мы просто говорим, что все синапсы, получившие спайк в небольшое временное окно после того, или до, или после того, как нейрон спайканулся, они все были пустительны. Тогда у нас не будет такой неустойчивости обучения, и у нас все будет гораздо лучше, чем если бы мы использовали этот, который нам мать-природа дает Это закон пластичности. Ну, кроме всего прочего, фактически это одна и двухфакторная модель используется для обучения без учителя. Мы говорим, что если у нас нейрон просто пришел абы как, никак не коррелируя с активностью самого синапса, вернее, спайк пришел на нейрон далеко от времени его срабатывания, то такой синапс будет чуть подавлен. А вот если он пришел близко, значит он коррелирует с активностью самого нейрона, и такой синус будет усиливаться. Двухфакторная модель, потому что это сопоставляется пресно-оптический спайк и осно-оптический спайк. Ну а в случае с обучением с учителем и обучением с подкреплением используются трехфакторные модели, поскольку тут нам знать не только. когда к нам пришел спайк, характеризующий внешний мир, когда мы сами спайканули, как-то на него реагируя, но еще мы должны учесть приход спайка оценки вознаграждения или наказания. То есть имеется три фактора. Ну и базовый закон состоит в том, что если какие-то синапсы получили спайк, потом нейрон сам спайканул, а потом через какое-то время он получил вознаграждение. Это означает, что на специальный синапс, который мы назовем условным допаминовым, мы получили спайк, то в этом случае вот те синапсы, которые получили свои спайки в процессе срабатывания нейрона, они будут усилены. То есть те спайки, которые сделали что нейрон сработал, и он правильно сработал, он молодец, они должны быть усилены. Если же получилось наоборот, что какие-то спайки пришли на какие-то синапсы, нейрон сработал, но оказалось, что он сработал плохо, неправильно сработал, получил наказание, тогда соответствующие эти синапсы будут ослаблены. Совершенно разумный закон пластичности. То есть мы видим, что он трехфакторный, он состоит из трех вещей. Если просто пришли спайки на синус, но нейрон не сработал, то мы не будем реагировать никак на наказание, потому что нейрон ничего не сделал. Его нельзя наказывать или, наоборот, хвалить. Только если пришел спайк на оптический, сам нейрон сработал, потом пришло какое-то там вознаграждение или наказание. Тогда мы модифицируем ВСА-синус. На самом деле, такие простые принципы, никак не связанные ни с каким бэкпропом, ни с чем таким, но, очевидно, они просто реализуются в железе, потому что мы тут ничего не делаем, мы фактически просто счетчики времени. Просто счетчик очень просто сделать, работающий с микросекундной точностью, это аппарат очень простая штука. Мы просто измеряем временные окна, и инкрементируем или декрементируем веса. Все. То есть это очень простая операция. Даже в случае обучения. Никак не связанная ни с каким там градиентом. Ну вот, собственно говоря, моя задача была примерно дать общее понимание, как кодировать информацию. Во-первых, зачем надо? Зачем это надо? Все эти спайковые там неработные штуки, не фонейменовские. Затем, как в принципе на них выражать? обычно в число какие-то категории, как с этим работать, что на них можно вычислять все, что надо вычислять, ну и как это, в принципе, может обучиться на основе того, еще раз повторяю, что у нас базовая операция, которую делает нейрон такой, это фиксация корреляции. Все виды обучения к этому могут свести. Ну вот, на этом я и заканчиваю мой небольшой доклад. Ну, большой, он все-таки час, а не 12 минут, поэтому я счастлив, что я наконец-то могу это рассказать. Ну вот, welcome. Ну, про Agirash все и так знают, а еще вот есть группа по нейронским технологиям, она вот nrme.ai.rus. Вот, собственно говоря, ее я показал. Спасибо за внимание. Вот все мои там регалии. 

S04 [00:48:32]  : Михаил, спасибо. У нас тут драматичные события происходят, пока вы рассказывали. Тут вопросы идут в Зуме, в Телеграме и в Ютьюбе. С одним человеком завязалась бурная дискуссия по поводу Рядозубова. В общем, там немножко драматично, там даже одного человека пришлось чуть-чуть ограничить в правах, потому что он не очень корректно рассказывался, но это как бы интрига. Давайте я по существу пойду. Первый вопрос от Бориса Новикова. О каком нейроне у вас идёт речь в начале доклада? О живом или модельном? Живой реально всегда потребляет энергию. То есть, имеется в виду живая клетка? 

S01 [00:49:25]  : Ну, конечно же, я не биолог. Я, в общем-то, компьютер-сайентист. Поэтому, конечно же, я в основном говорил о... модели нейрона, в том числе аппаратные модели нейрона, зафиксированные. Я специально не говорил про нейрон-процессор, это целое отдельное. Я не нейтронщик, поэтому я в этом деле не очень понимаю. Но в действительности, да, конечно, биологически нейрон всегда что-то потребляет. Но, я думаю, сам он потребляет гораздо меньше, когда он просто молчит. Не надо бы для этого нейрофизиологов думать, это просто здравый смысл. А кремниевый нейрон, там есть микроточки, токи утечки, они настолько безумно маленькие, что их можно в расчет не брать. 

S04 [00:50:15]  : Спасибо. Следующий вопрос. Это я забыл спросить вас на прошлом семинаре. Видите ли вы какие-то параллели или связи с теорией мозга Рядозуова, если вы с ней знакомы? 

S01 [00:50:33]  : Вот мне очень стыдно, но я с ней не знаком. Я на самом деле не особо-то вообще разбираюсь в теории мозга. Все же это для меня скорее источник некоторых Хинтс, да, некоторых наводящих соображений. Я не копирую мозг, и нет, ничего такого я не делаю. 

S04 [00:50:49]  : Ага, ну вот тогда вам возможно будет интересно. Сейчас как раз переключусь в онлайн-драму, которая у нас случилась в Ютьюбе. Суть-то в том, что у нас Алексей Рядозубов еще в начале наших семинаров выступал у нас несколько раз и рассказывал свою теорию. Там много интересных, отчасти спорных, отчасти кому-то может быть даже очевидных. очевидно вам это будет очевидно моментов вот ну и у меня сразу вот возникло ощущение что есть определенные перекликания вот а в youtube значит вопросы были такие сейчас я тут пройду значит ну во первых по порядку вопрос скажите пару слов насчет cerebros cs2 подходят ли их железки для инс видимо нет потому что если подходили я был 

S01 [00:51:41]  : Я бы знал. Есть железки, которые подходят, я их назову. Первый был Tronos, он был совсем спайковый чип, по-моему, не обучался, и там веса были всего-навсего двухбитные, однобитные, то есть есть вес, а цель недовесположительная, то есть было все крайне примитивно. Потом есть лоихи, и сейчас это является главным чипом, по своему функционалу он очень богат, на нем можно делать что угодно. Это чисто спайковая штука. Хотя он может обмениваться и не только битами, он может более сложными вещами обмениваться, но в основном это спайковая идеология. Наш чип Altai, который в общем-то отчасти является аналогом Tronox, но он, с одной стороны, по количественным характеристикам он сильно уступает, но по качественным несколько превосходит. Там можно делать то, что на Altai, на Tronox делать нельзя. Ну вот есть еще чипы такие полу, да, еще Tianzij китайский, хотя они у нас малоизвестны, в Китае там довольно-таки прижимают эту информацию, особо не дают. Есть такие полу-спайковые чипы, например, Akida, австралийский, который комбинирует фактически обычные сети и спайковые, но там очень жестко заданная архитектура, там нельзя сделать любую архитектуру. Ну, еще там есть чипы аналоговые, например, Brainscales, гидельбергский, где нейрон сделан как аналоговое устройство. Ну вот еще там парочка чипов. Наверное, я все на данный момент назвал какими-то неизвестными. 

S04 [00:53:24]  : Спасибо. Здесь просто я тогда продлю сообщение. Другой пользователь комментирует по поводу ЦРЕБРОС СС-2. Думаю, подходит. Все, что нужно ИНС, это совершать тензорные операции за минимальное количество тактов. Ну и механизмы изменения весовых коэффициентов. Не думаю, что в ЦРЕБРОС можно сделать это за один такт. 

S01 [00:53:51]  : Нет, это абсолютно не так. Никаких тензорных операций импульсный нейрон не делают. В этом и большой плюс. Тензорные операции – это сложение и умножение. Умножение общей никаких не применяется никогда, а сложение там весьма ограничено. То есть это не тензорные операции ни в коей мере. Поэтому церебра, скорее всего, к этому не имеют отношения. Я просто не знаю. В мире очень много всяких нейроскорителей, но далеко не все из них импульсные. 

S04 [00:54:23]  : Дальше тут была веточка как раз по поводу Рядозубова, у которого вроде как была идея реализации кодирования через фильтр Bloom. Соответственно у вас фильтр Bloom где-то просматривается. 

S01 [00:54:40]  : Мне приходится краснеть все чаще и чаще. Я не знаю, что такое фильтр. 

S04 [00:54:44]  : Окей. Ну, вот тогда, значит, может быть, тоже интересно. Вопрос. Так, хорошо. Значит, мы тут перепрыгиваем тогда с темы Рядозубова. Вот. Ну, там с Рядозубовым дальше, собственно, у нас произошла некоторая дискуссия. 

S01 [00:54:56]  : В двух продолжениях описать. 

S04 [00:54:59]  : Алексей Рядозубов или что? 

S01 [00:55:01]  : Нет, я не знаю. А фильтр Блума что такое? 

S04 [00:55:03]  : А что такое фильтр Блума я сам не знаю, к своему тоже стыду. То, что теория Рядозубова перекликается где-то с тем, что вы рассказываете, у меня тут тоже вопрос, что называется, с языка сняли. А вот про фильтр Блума я сам слышал, но не видел. Хорошо, давайте двигаемся дальше. Если вы будете копаться в этой теме, там Алексей Рядозубов, у него много интересных мыслей и идей. На семинаре он выступал несколько раз. Последние его видео на Ютьюбе, они, в общем, какую-то совсем, мягко говоря, странную сторону смотрят. Я даже не буду это комментировать. 

S01 [00:55:45]  : Просто, если будете искать, то... Я, в любом случае, смотрю, потому что интересно. 

S04 [00:55:50]  : Ну вот, значит, если будете смотреть, я советую смотреть видео, которые посвящены работе мозга, а не каким-то другим, более таким, эзотерическим вопросам. Кстати, может, вы тоже из-за телек не увлекаетесь? 

S01 [00:56:05]  : Я из-за телек не очень увлекаюсь, поэтому да, хорошо. 

S04 [00:56:07]  : Хорошо, ладно, давайте вернемся к теме. Борис Новиков снова спрашивает, как в этих моделях различаются ошибки и правильные реакции системы? 

S01 [00:56:18]  : Ну, в теменных наказаниях и вознаграждениях. какие-то более формальных, таких простых случаев, близких употреблению обычных нейросетей. Ну, если говорить о супервайсе Клёнин, то там все просто. Там сеть что-то выдала, она выдала либо то, что хочет. Ну, МНИСТ, да, набивший там своему Гросскобину. В принципе, МНИСТ можно классифицировать импульсными нейронными сетями. Непонятно зачем, но можно. Ну, тут она что-то реагирует, щелкнет, свертки делает, потом что-то выдает. Выдала правильно, значит, молодец. неправильно, не молодец. А в более реалистичных случаях, когда это reinforcement learning, тоже поступают некие специальные сигналы. Все равно у нас есть какая-то задача. Мы все равно видим либо конечную цель, которая там достигается, либо не достигается. В зависимости от этого мы посылаем специальные семантики, сигналы, выражающие этот факт. Либо, я не знаю, какие-то жутчатые цели. Сеть сама их как строит, но если они есть, то изменили, то хорошо. Ну и так вот можно оценивать правильно и неправильно на основе этих знаний. 

S04 [00:57:25]  : Спасибо. Следующий вопрос тоже из YouTube. Насчет невозможности дифференцирования. Почему нельзя, допустим, сначала прокинуть градиенты через спайки, а потом посчитать ошибку между V и V градиенты спайков? Суррогатные градиенты, это про это? 

S01 [00:57:45]  : Ну да, просто взять и прокинуть через спайки, а как? Спайк – это дельта-функция. Как там это? Что-то можно прокинуть куда-то. Там ничего никогда не прокинешь. Поэтому, да, делают там всякие вещи, как будто это не спайк, а как будто… Ну дальше там начинается всякая там более или менее работающая всякая алхимия. Но все-таки обычно делают, если уж хочется как-то использовать Backprop, то делают совсем не так. Обычно просто сначала обучают обычным образом обычную традиционную сеть, а потом ее уже обученную для инференса перекладывают в импульсный домен, который, как я сказал, всегда возможен с любой точностью. Вот фактически на практике часто делают именно так. Без обучения. Понятно, обучают обычную среду, где понятно, где градиенты есть, а потом просто транслируют выпускную, вид то, что получилось. 

S04 [00:58:37]  : Спасибо. Еще вопрос от Бориса Новикова. Наказание или вознаграждение глобальны, как с принципом локальности? Кстати, хороший вопрос, да. Спасибо. 

S01 [00:58:48]  : Они могут быть и глобальны, и локальны. В моем подходе, во-первых, все-таки то, что касается традиционной трактовки reinforcement learning, в традиционной трактовке есть некое действительное число, которое может отрицать это и положительно. Мы максимизируем его, практически не столько его, сколько интеграл. Мы должны его максимизировать на протяжении времени. В моем понимании он более близко к природе. У меня вознаграждение и наказание – это не действительные числа, а это есть именно спайки, посылаемые некие специальные сигналы, которые приходят на нужные специальные синапсы. Они могут быть и глобальны, и локальны, но, например, если они редки, то есть в реальных задачах мы наказание и почтение получаем редко. Поэтому мы должны все это сама простраивать в некие промежуточные цели. И достижение или уход от этих целей – это будет внутреннее вознаграждение и наказание абсолютно локально. То есть сеть сама формирует эти законы, которые, она говорит, ближе мы встали к цели или дальше. Ну и в зависимости от этого, значит, там внутреннее вознаграждение, внутреннее наказание происходит. Не знаю, ответил ли я на этот вопрос? 

S04 [01:00:06]  : Ну, как-то так. Борис, уточните вопрос. 

S03 [01:00:14]  : Меня слышно? Здравствуйте. Да, слышно. Связано с предыдущим вопросом. Есть ли миллионы процессоров, нейронов? Как между ними связи? Каждый с каждым или по цепочкам? 

S01 [01:00:31]  : Между нейронами? Не понял. Между чем связь связи? 

S03 [01:00:35]  : Ну, связи между нейронами. У вас же приходят синапсы. Каждый нейрон связан со всеми остальными или только с одним синапсом? 

S01 [01:00:45]  : Нет, конечно. Даже в небольшой сети это будет n квадрат. 

S03 [01:00:50]  : Вот именно. Тогда всегда распространяются по цепочке. Или вообще есть изолированные нейроны, которые вообще между собой не связаны? 

S01 [01:01:04]  : Ну, в сущности говоря, по цепочке. Хотя эта цепочка всё-таки, она не имеет того смысла, какой имеет backpropagation. Там тоже по цепочке, но эта цепочка, она ничего общего с той цепочкой не имеет. То есть, ну, разумеется, у меня нейроны не связаны все со всеми, поэтому всё равно распространяется как-то там, ну, каким-то процессом. 

S03 [01:01:23]  : Тогда у них что, изолированные группы? Или каждый связан с каждым по цепочке? 

S01 [01:01:31]  : Знаете, там используются довольно разнообразные сложные структуры. Отсюда связь с поощрением и наказанием. 

S03 [01:01:46]  : А куда поступает сигнал о поощрении наказания? На один нейрон? Или сразу на все? Или на группу? Или как? 

S01 [01:01:54]  : На специально выведенную группу. Не на все, конечно. На все она не может поступать. То есть это... Имеется некая группа, которая выполняет некие функциональные обязанности в рамках большой структурированной сети. У меня сеть не гомогенна. У неё имеются разные части, которые функционально довольно-таки различны, в том числе и по свойствам нейронов. Так же, в принципе, как и в мозге. 

S03 [01:02:18]  : Там есть несколько... Неизвестно. Давайте либо мы по модели работаем, по вашей, либо мы говорим про реальный мозг. Не надо путать. Либо одно, либо другое. 

S01 [01:02:28]  : Нет, я думаю, по модели. Я просто, опять же, соотношу это с мозгом. Я не моделирую мозг. У меня модели, они разные, то есть разные части сети состоят из разных типов нейронов. 

S03 [01:02:41]  : Хорошо. Вознаграждение пришло на одну группу. Как она дойдет до любого нейрона, который участвовал в обработке информации, по результатам которой появилось это вознаграждение или наказание? 

S01 [01:02:57]  : Там имеется довольно сложная структура иерархических гейтов, которые переключают эти сигналы. Допустим, если у меня работало что-то одно, то вознаграждение должно прийти именно на нее. Если что-то не работало, то туда вознаграждение не надо посылать. Учет того, что работало недавно и что не работало, осуществляется довольно сложными геттинговыми механизмами. Я, честно говоря, именно про модель, не про мозг, конечно. Идея такая, это иерархическая структура, которая регулирует подачу каких-то сигналов, которые в данном случае на грани наказания, туда, куда они должны быть поданы. но эта логика тоже реализована с помощью импульсных нейронов. Это не внешняя подошли вселогика, это просто некие структуры сети, которые так работают. 

S03 [01:03:53]  : Априорны или сеть сама их выстраивает, связи? 

S01 [01:03:58]  : Нет, связи заданы априорно. Имеются пластичные компоненты сети, которые именно реализуют обучение, но сама архитектура, которая высоковыровневает эти структуры, она не собой вырастает, она задана априорно. Зная поставленную задачу, я структурирую сеть именно так. А вот в ней уже имеются обучающиеся куски, которые пластичные, которые обучаются за счет пластичной сети. 

S03 [01:04:31]  : Спасибо. И последний вопрос. В живой клетке есть интервал между спайками, импульсами, и он довольно большой. У вас он есть или физически к системе он может быть вообще сведен практически к нулю? И это естественная разница. У вас так большой есть интервал, нет интервала? 

S01 [01:04:58]  : Да, конечно. Поскольку я использую модель LIFAD, где адаптивный порог, то у меня этот интервал за счет того, что у меня после спайка порог резко подскакивает, и сразу нейрон не может быть стимулирован даже высокой стимуляцией. пока порог не опустится до каких-то значений, поэтому у меня этот механизм, условно, есть, и он важен. За счет адаптивности порога у меня это решается. 

S03 [01:05:22]  : Ну, то есть, если очень сильная стимуляция, то она сокращает время между спайками? 

S01 [01:05:27]  : Да, она, конечно, сокращает, но мягко сокращает. 

S03 [01:05:30]  : То есть, это минимум, меньше которого не может быть? 

S01 [01:05:35]  : Ну да, да, конечно, за счет того, что порог подскакивает до высоких значений сначала, то есть никакая стимуляция не может сделать сразу же после этого еще раз. 

S03 [01:05:45]  : Тогда по длинной цепочке сигнал будет проходить очень долго. Хорошо. 

S04 [01:05:50]  : Борис, давайте мы еще чуть-чуть продвинемся по другим вопросам. 

S01 [01:05:55]  : Я маленькую поправку хочу сделать. На самом деле есть, в том числе и в живом мозге, нейроны, которые сразу пачку спайков могут генерировать. Нейроны Жигевича, у него там зависит от параметров, много разных режимов генерации. Немного, которые редко генерируются, с рефракционным временем большим. А есть, наоборот, которые от стимуляции приводят к генерации спайки. Такое тоже могло быть сделано в моей модели. 

S03 [01:06:22]  : Спасибо. 

S04 [01:06:24]  : Спасибо. Борис, если останется время, мы ещё вернёмся к вашим вопросам. Я только хотел бы, Михаил, уточнить для себя первый вопрос Бориса. Я правильно понял, что у вас есть механизм именно глобального подкрепления, когда в рамках всей системы нейроны, действующие активно и в течение какого-то времени, получают, грубо говоря, подкрепление или формируют какую-то ассоциацию? 

S01 [01:06:52]  : Да, но это не глобальная вещь, то есть подвести сигнал подступления к остальным нейронам это невозможно. Это все равно некая, ну через систему таких иерархических релей нейронов подают ее куда надо. Но это есть, да? 

S04 [01:07:08]  : Да, это есть. То есть, такой механизм есть, но есть и механизм локального связывания, правильно? 

S01 [01:07:16]  : То есть, есть и глобальный... Локальный тоже, но локальный это вот просто хеповский закон, да? Туториальный вариант ACDP, который без подкрепления. У меня две механики пластичности, в принципе, разные. Одна это, условно говоря, супервайс, хебовская пластичность, где не требуется никаких подкреплений. Это просто фактически связь между пре-санаптическими спайками и постсанаптическими спайками. А вторая, опять же, условно называемая, допаминовая, которая требует еще спайки, подкрепления или наказания. 

S04 [01:07:52]  : Угу. Спасибо. Ну и тут нам коллективный интеллект поспешил на помощь по поводу фильтра Блюма. Значит, я зачитываю. Александр Лютуновский написал, что фильтр Блюма Это вероятностная структура данных, придуманная Бёртоном Блумом в 1970 году, позволяющая проверять принадлежность элемента к множеству. При этом существует возможность получить ложноположительное срабатывание элемента в множестве нет, но структура данных сообщает, что он есть, но не ложноотрицательное. Вот. И еще вопрос, который у меня есть. А есть ли в искусственных нейронных сетях, вот в вашем варианте, отрицание, операция нод или отрицательное подкрепление? 

S01 [01:08:39]  : Да, конечно. То есть реализуется так, что вот эти самые силы, которые модулируют пластичность. Фактически, они могут иметь вес и плюс, и минус. Это будет означать знак коррекции веса. Пришел спайк, нейрон сработал, потом пришел оценочный спайк. Если оценочный спайк пришел на синапс с отрицательным весом, этот первый активный будет подавлен. Если положительный, он будет усилен. То есть, есть и в ту, и в другую сторону. 

S04 [01:09:10]  : То есть, у вас есть веса и отрицательные, и положительные. 

S01 [01:09:14]  : Да, у этих весов, у этих синусов, которые модулирующие, у них тоже есть положительные и отрицательные веса. 

S04 [01:09:21]  : А вопрос такой, а у нас эти веса, у тех синусов, у которых веса положительные и отрицательные, как они меняются? Каждый синапс является всегда либо неотрицательным, либо неположительным. То есть, у него вес меняется от нуля до плюс единицы. Или они могут через ноль переходить? 

S01 [01:09:43]  : Нет, через ноль не могут переходить. То есть, те синапсы, которые являются выжидающими, они могут быть только нулевыми. И ограничены сверху. На самом деле, у меня это решается с помощью того, что я меняю не вес, а все чуть более хитро. У меня есть такая вещь, называется синаптический ресурс. Вот она меняется от минус бесконечности до плюс бесконечности. И она меняется инкрементально. А вот вес с ней связан таким сегмоидным законом. Если ресурс отрицательный, то вес ноль. А если он положительный, то он начинает приближаться как-то асинтетически к максимальному значению. То есть у меня вес ограничен нулем и этим асинтетическим значением сверху. Все синапсы тормозные у меня не пластичные. То есть я не использую пластичных тормозных синапсов. И точно так же у меня не пластичные синапсы оценки. У них тоже их вес фиксирован и не меняется. 

S04 [01:10:37]  : У них фиксированный вес, но вы их можете модулировать силой тормозящего воздействия через промежуточные нейроны, правильно? Да. 

S01 [01:10:54]  : отрицательный вес, поскольку часто используются сети типа WinRTXO, то есть это сети с латеральным торможением. Если я хочу, чтобы у меня какой-то фактор ослабил какой-то нейрон, то я вместо этого усиливаю другой нейрон, который с ним связан латерально-блокирующей связью. Фактически предусматривает, что я буду подавлять вот тот нейрон, который хочу подавить. А вообще связи у меня пластичные, только положительные. 

S04 [01:11:20]  : Спасибо. И тут сразу тогда вопрос, раз заговорили про вставочные нейроны. Различаются ли у вас нейроны восприятия, либо исполнительные, либо опять-таки вставочные? Или это условно? 

S01 [01:11:34]  : Это условно. То есть у меня сеть структурирована из разных блоков, и там нейроны существенно разные по своим свойствам в зависимости от того, что они должны делать. Поэтому да, различаются, конечно. 

S04 [01:11:49]  : Хорошо, спасибо. Здесь из искусственного интеллекта, фу, из коллективного интеллекта приходит пожелание Борису Новикову отремонтировать или купить новый микрофон. А вам, Михаил, еще вопрос от Бориса Новикова. Есть ли реализация на физическом носителе? Если есть, то каком? 

S01 [01:12:08]  : Но сейчас я все это реализую пока на GPU. То есть все, о чем я говорил, реально считается, реально моделируется. Есть достаточно удобный симулятор, написанный на SQL, который позволяет описанную на XML структуру сети реализовать оптимальным образом на GPU. Но вот сейчас я участвую в проекте, который ведет Касперский, чтобы это все реализовывать на наших нейросибках Алтай, ну даже вот на их следующих поколениях, которые там, которые будут. А пока это на ГПУ все работает. 

S04 [01:12:45]  : Спасибо. Борис, вы, видимо, хотите еще что-то спросить? 

S03 [01:12:51]  : Я хочу предложить гипотезу. Ну, это сходу, может быть, она никуда не годится, а может и пригодится. Вот если различать механизмы восприятия соответственно создания и механизмы исполнения, то вот сюда, по-моему, можно хорошо включать функции поощрения и наказания. Вот именно в этом переходе. То есть, если хорошо восприняли, то переходит, выдается на исполнение. А если плохо воспринято, то тормозится передача на исполнение. 

S01 [01:13:34]  : Ну... Не знаю, я не знаю, что сказать. Я, во-первых, не очень понимаю, как экспрессировать на моей структуре, но это как бы надо подумать. 

S03 [01:13:44]  : У нас как бы в моделях мозга известно, что есть механизм восприятия и обработки информации, ее осознание, а потом уже идут центростремительные нервы восприятия и центробежные нервы исполнения. И вот на этом переходе, как мне кажется, и хорошо включается функция наказания и поощрения. Какие-то восприятия правильные передаются на восполнение быстро и хорошо, а какие-то неправильные тормозятся и не передаются. 

S01 [01:14:23]  : Ну, возможно. Я, на самом деле, мог бы быть более конкретным. Я вот на нашей группе, которая NRMA.AI, нам делал доклад по производственным результатам, которые у меня были получены в области reinforcement learning. Мне там кое-чего удалось сделать. И там как раз мы были представлены структурой. Они довольно многокомпонентные. Таких сетей, которые, допустим, обучаются играть в пинг-понг, когда они сначала не знают, что им надо делать. Как-то просто двигается ракетка, потом просто отразил шарик, получил награду, не отразил наказание. В конце концов, в нее доходит, что надо делать. Это все реализуется весьма нетривиальной структурированной сетью. Я ее представлял. Наверное, когда я закончу, я некую статью напишу и могу это представить. Это просто некая структура, довольно большая. 

S04 [01:15:15]  : Михаил, спасибо. Александр? 

S02 [01:15:19]  : Добрый день, Михаил. Спасибо большое. У меня вопрос по поводу нейронной бабушки. Какие-либо успехи были именно в семантическом моделировании импульсных сетей? 

S01 [01:15:35]  : Я этим не занимаюсь сейчас пока. На самом деле, если еще не рассказать, чем я занимаюсь, Меня сейчас, по крайней мере, интересует больше другой край. Ни семантика, ни GPT, ни эти вещи. А меня интересует мозг насекомых. Меня интересует, как, допустим, какая-нибудь там дельзофила, у которой всего 10 тысяч нейронов, как она умудряется летать, иметь нетривиальное сексуальное поведение, учиться распознавать новые запахи и еще делать кучу всего мозгом из 10 тысяч нейронов. 

S02 [01:16:04]  : Понятно. А почему это не семантика? Почему это именно вычисление, а не семантика, то есть запах, восприятие какое-то. То есть, по сути же, если мы имеем нейрон бабушки, то мы должны иметь нейрон какого-то запаха и работать именно с не единственным нейроном, с группами нейронов. 

S01 [01:16:27]  : Да, даже в таких небольших мозгах это коллективное кодирование, за счет этого оно является устойчивым и всеми преимуществами обладающими. Но потом все-таки дрозофилы и бабушка это очень понятие далеко стоящее. 

S02 [01:16:46]  : Почему очень близко? Почему вместо бабушки конкретный запах навоза? Или вид чего-то, что привлекает муху, насекомую. Опять же здесь не про бабушку, а про конкретную реакцию на конкретное понятие, на конкретный концепт, на конкретное событие. То есть не просчет чего-то, а, скажем, даже изменение какого-то пятна цветового – это же некое событие, семантическое событие, и оно может тоже как-то кодировать. Здесь не о кодировании речь идет, а именно о каком-то прямом соотношении внешних реакций и спайков нейронов. 

S01 [01:17:27]  : Знаете, мне такие вещи приходится разбирать. Допустим, когда я изучаю, как Моисеевич сыграет в пинг-понг, я должен понимать, где явный какой-то баг, я должен как-то интерпретировать работу. Я должен понимать, с чего он отдал эти или иные команды. А теоретная команда, она дается за счет какой-то информации о положении ракетки, мяча, скорости и того сего. Поэтому это тоже некое понятие, даже в этих простых вещах, как пинг-понг и ракетки, тоже есть такие совсем отдаленные бабушки. С ними приходится разбираться. Я это не предаю какому-то социальному значению, это скорее для меня просто способ понимания, способ подкладки, разбирательства того. То есть я высоким словом семантик не называю, но это более такие технические вещи. 

S02 [01:18:13]  : Нет, почему нужно говорить о семантике, потому что здесь наверное различие именно в том, что идет ли расчет, какой-то расчет сравнения, константам или еще что-то, или чистая семантика, когда у нас нет математики, когда у нас все заключено на уровне каких-то, ну действительно, категорий, понятий, то есть даже не больше и меньше какого-то числа, а соотношение некого пятна больше, другое пятно меньше. 

S01 [01:18:42]  : Ну у меня математики там никакой нет. примитивные операции, поэтому слово математика не назовешь. поэтому семантика, скорее всего. можно сказать, что это семантика, но Семантики – это сразу какие-то семантические сети, антизаусы. Но до этого все очень как до луны. Дело в том, что меня интересует именно почему я… 

S02 [01:19:14]  : С удовольствием вас слушаю, читаю какие-то статьи, потому что то, с чего вы начинали рассказывать про импульсные нейронные сети, это все соответствует очень близко событийной семантике, именно событийной. Именно событийная семантика, не графы обычные, а объектные, а событийная семантика начинается именно с того, что есть некое событие, которое выполняется именно не по Неймовской архитектуре, а именно по архитектуре дата-флоу. Распараллеливание. Каждый элемент графа отвечает на срабатывание какого-то события, отслеживает события предыдущие в направленном и циклическом графе. Это очень все. У меня даже есть какое-то желание попробовать смоделировать то, что у меня сейчас смоделировано на движке, именно на импульсных нейронах. 

S01 [01:20:13]  : Да, вы уже говорили, мы как-то с вами об этом беседовали. Да, возможно, ну... Да, наверное. В принципе, я думаю, что можно это все сделать не в терминах насекомых и ракеток, а в терминах анализа бизнес-процессов. 

S02 [01:20:27]  : Бизнес-процессов, да, можно бизнес-процессов. То есть, по сути, у мухи такой же бизнес-процесс. Здесь неважно, бизнес-процессов нет. Скажем так, моделирование деятельности или моделирование действия. какого-то действия на основе каких-то актов. И тогда каждый нейрон будет отвечать за какой-то акт, совокупность совмещения этих актов будет реализована, на этом совокупности совмещения и потоке этих актов будет какое-то действие конкретное, результативное действие, которое можно писать и на уровне мухи, и на уровне бабушки. 

S01 [01:21:07]  : Можно смотреть на это и так. 

S02 [01:21:10]  : Ну, посмотрим. Я сейчас попробую разобраться, а потом еще, может, если что, где-нибудь пересечемся, пообращусь. Спасибо. 

S01 [01:21:19]  : Вообще, я должен заметить, что интерпретация работы импульсно-мировой сети – это, наверное, еще более сложная штука, чем интерпретация работы обычной сети, что мы знаем, тоже очень мега-проблема. Еще более проблемная проблема. Сложно об этом разбираться и это мониторить. 

S02 [01:21:35]  : У меня в серебре не составляет ощущения, что ориентация на вычисление с помощью импульсной сети это тупиковый путь. что не нужно там ничего вычислять, это не вычислительная система. Я имею в виду, что постоянно говорят, что можно представить константы, можно так-то вычислить. То есть нужно оторвать это вычисление и поднять на семантический уровень, на фиксацию событий. А фиксация событий жестко ложится на импульсные нервы. 

S01 [01:22:17]  : Вы знаете, иногда приходится все-таки чего-то вычислять. Например, скажем, если я хочу сделать сверточную часть. Разумеется, сверточный подход, он как бы здрав, безусловно. Он дает инвариантность, но хотелось бы, чтобы он тоже был в рамках IPCC, а не делать какой-то гибрид, как Юлия Сантонерская в Intel делает. Но это требует вычислений. Грубо говоря, надо некие фильтры формализовать. чтобы их рассчитать, свертки. То есть до какой-то степени в каких-то отдельных частях нечто похожее на вычисление в традиционном плане делать надо, поэтому я вот этот блок вставил для операции. Но это скорее у низких уровнях такие технические вещи, которые иначе бы делались там ручными сетями. 

S04 [01:23:06]  : Хорошо, спасибо. Спасибо. И вопрос еще у меня такой, Михаил. А вот раз уж вы упомянули про пинг-понг, а в пинг-понг ваша импульсная нейронная сеть умеет играть? И второй вопрос. 

S01 [01:23:18]  : Да, я демонстрировал, как она научается это делать. 

S04 [01:23:22]  : То есть, умеет, да? То есть, соответственно, и сейчас вы двигаетесь к насекомым как к более сложному поведению. 

S01 [01:23:29]  : Ну, на самом деле, еще не совсем она пинг-понг умеет это делать, потому что Рефорс леунинг делится на две большие категории. Более простой – это Model-Free и Model-Based. Model-Free у меня фактически реализована, а Model-Based, который нужен по-настоящему, он требует очень сложных структур, и я планирую его реализовать, но пока еще не реализовал. Поэтому у меня пока такой Model-Free вариант. 

S04 [01:23:58]  : Да, и вот здесь вот как раз следующий вопрос из YouTube по поводу того, а есть ли у вас в системе понятие контекста? 

S01 [01:24:10]  : Есть у меня в системе понятие контекста. Ну, поскольку у меня система обладает, то есть нейроны, такие нейроны, что там обладают памятью, контекст это, я понимаю, какая-то история. Не очень понятно, что такое контекст. Если это контекстно во времени, когда процесс быстро развивается на фоне чего-то медленного, то да, потому что вещи, связанные с динамикой и с памятью, у меня есть. Я могу вычислять какие-то фичи, это основано на медленном развитии чего-то, грубо говоря. 

S04 [01:24:46]  : Спасибо. Ну и, кстати, вот тут Александр Балдачёв, вам прилетел комментарий по поводу ваших событий, которые вы выявляете в симпатийной семантике. Александр Лютуновский предлагает эти события оборачивать в смарт-контракты. Про это, по-моему, сегодня разговор уже был. Оборачиваем. Хорошо, коллеги, есть ещё какие-то вопросы или комментарии? Вопросов, комментариев нет. Тогда, Михаил, большое вам спасибо и успехов вам в создании искусственных насекомых. Может быть, вы нам их продемонстрируете в скором будущем. Я только не знаю, жпу-то насекомого трудно будет затолкать? 

S01 [01:25:36]  : Это же надо все-таки... Нет, мы хотим уже что-то более интересное. 

S04 [01:25:41]  : нет конечно ну а я и чип алтай тоже наверное то есть это не почему это будет большое насекомое все-таки такое крупное пока это пока гигантское насекомое хорошо ладно спасибо будем ждать гиганта все хорошо спасибо михаил спасибо всем до свидания 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
