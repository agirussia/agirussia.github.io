## 22 июня 2023 - Термодинамическая модель сильного искусственного интеллекта - Ковтуненко Андрей (независимый разработчик ИИ, ведущий инженер-программист) — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/Ywod8v-Sv_w/hqdefault.jpg)](https://youtu.be/Ywod8v-Sv_w)

Суммаризация семинара:

Семинар посвящен Термодинамической модели сильного искусственного интеллекта, разработанной Андреем Ковтуненко. В центре внимания — принципы и механизмы работы модели, её возможности и ограничения.

Суть


Ковтуненко представил модель, основанную на параметрическом подходе и использовании глубины контекста для прогнозирования последующих элементов текста или других данных. Модель работает с использованием параметрически заданных матриц, которые обеспечивают предсказание на основе накопленной статистики. Суть в том, что модель не предсказывает каждый элемент независимо, а опирается на предыдущие элементы и статистические данные для определения вероятной последовательности.

Детали


Разработчик подчеркивает, что использование параметрически заданных матриц позволяет ограничить ресурсы компьютера. Модель работает с суммированием значений вместо вычитания, что делает её более эффективной для процессора и видеокарт. Также Ковтуненко упоминает использование усреднений и классификации для улучшения прогнозов.

Результаты


Модель показала себя в генерации текстов с правильным расставлением пробелов и не содержащих ошибок, что свидетельствует о её способности к угадыванию правильных словосочетаний. Однако для других данных, кроме текста, эффективность может быть ниже. Ковтуненко также демонстрирует применение модели на простых примерах, таких как пинг-понг, для управления задачами управления, которые сводятся к задаче прогноза.

Особенности


Принципиальные особенности модели заключаются в использовании термодинамических принципов для моделирования мышления искусственного интеллекта. Разработчик стремится к созданию системы, которая может эволюционировать и адаптироваться, в отличие от алгоритмов, которые всегда ведут себя предсказуемо.

Ограничения и вопросы


Ковтуненко признает, что его модель подходит больше для хобби-разработки, чем для профессионального применения. Он также отмечает важность интерпретации инвариантов и статистики, а также ограничения модели в непараметрическом пространстве данных. Разработчик говорит о возможности использования модели в рамках определённых систем и архитектур.

Выводы


Семинар дал представление о едином подходе к созданию искусственного интеллекта, который может находить применение в различных областях, от простых до сложных задач управления. Модель Ковтуненко представляет собой уникальный подход к созданию умного ИИ, который старается отразить динамику мышления, используя принципы термодинамики.

Ключевые моменты


- Использование параметрически заданных матриц для предсказания последующих элементов.
- Применение глубины контекста для улучшения прогнозов.
- Ограничение ресурсов компьютера путём задания максимальной глубины контекста.
- Возможность эволюции и адаптации системы.
- Интерпретация статистики и инвариантов.






S03 [00:00:00]  : Ну, я приступаю тогда. Так, коллеги, добрый всем времени суток у кого. Сколько там времени? Меня зовут Андрей Ковтуненко. Я являюсь по профессии инженером-программистом, работаю более 15 лет, но моя сфера деятельности не связана с искусственным интеллектом, профессиональным, а занимаюсь я им в качестве хобби, можно так сказать. Сегодня хотел бы вам представить свою модель, поделиться своими видениями. того, как может быть устроено мышление искусственного интеллекта. И прежде всего я имею в виду именно искусственного интеллекта общего назначения с самосознанием и мышлением. Потому что видов искусственного интеллекта много, кто-то какие-то задачные подходы делает, там общие какие-то паттерны ищет, ну и так дальше. Но я тут именно хотел бы поговорить о мышлении, как бы оно могло быть сконструировано. Вот. И делать свою лёгкую небольшую копилку знаний нашей группы, в которой я состою уже лет, наверное, 7. По ощущениям, и даже больше. Вот. И хотел бы извиниться за предыдущий семинар, который сорвался по техническим причинам, я вовремя не заметил, просто трансляция отвалилась. Вот. Итак. вообще семинар у меня будет больше как воркшоп, то есть я больше хотел бы показать код, потому что теории у нас много, а вот что-то с практикой у нас не всегда много кода, и хотел бы эту вакууму закрыть немножко со своей стороны. хотел бы начать с кратких теоретических основ, на которые я опираюсь. Ну, как многие, наверное, подозревают, да, согласятся со мной или нет, но вот мышление и вообще эти наши когнитивные процессы — это во многом как бы организованная динамическая статистика. Вот все вот сейчас модные такие большие языковые модели, они тоже в этом мире основаны на статистике, и собственно сама архитектура нейросетей, она тоже своего рода такая конфигурация, в которой раскладываются там статистические информации. Происходит там кластеризация, классификация и так дальше. Вот, ну я в этом смысле не исключение, у меня также статистическая модель, но я как бы попытался ее интерпретировать в рамках термодинамики. И вообще, когда мы имеем информацию, то необходимо начать со основ, как она вообще в физическом процессе может зарождаться. Вообще, что такое информация и как можно ее получать, извлекать, хранить. логически индуцировать. я хотел бы начать с терминов, которыми в моей системе координат я пользуюсь, чтобы быть консистентным данные – это набор значений из читаемого входящего потока сенсора, который кодируется информация – это какие-то повторяющиеся значения на определенном терминале, на определенном терминале разметки в этих данных. То есть данные, они в принципе могут быть хаотичными. Мы принимаем антенны, белые шумы из космоса, это вроде данные, но информации они не насыщенные. в этом смысле, понимание того, что там ничто не повторяется, и вот эта вот мера неупорядоченности как раз говорит нам о том, что мы получаем какой-то хаос, да, и никакой информации мы извлечь из него не можем. И, соответственно, тождественно, могу привести и обратное утверждение, то есть если мы там наблюдаем какое-то биение, какое-то повторение, вот мы можем это воспринимать как информацию. И жизнь в этом контексте, процесс жизни, как мы можем предположить, что процесс жизни – это физический процесс, по сути. Но интуитивно мы понимаем, что он очень радикально отличается от других процессов физических, происходящих вокруг нас. То есть радикально мы это как бы... Вот я это попытаюсь сейчас формализировать, чем же он отличается. С одной стороны. С другой стороны, это всего лишь физический процесс. Так вот, процесс жизни, в отличие от других, является путем консолидации информации и наименьшего сопротивления. Либо тоже самое утверждение, верно другой формулировки, что процесс жизни – это процесс отражения динамики внешнего объема, динамики внутреннего объема, произвольно очерченного, в любом месте, пространстве и времени. Ну и как итог можно сказать, что это информационная гравитация, уменьшение антропии. То есть у нас есть некоторый объем, в котором уменьшается антропия, и нарастает информация, соответственно. Какой-то порядок в ней нарастает. Вот это и есть процесс жизни. И вот как более такой наивысший процесс в рамках этого процесса жизни – самосознание. Самосознание – это непрерывный процесс кластеризации кластеризатором своих метрик или отличение своего класса от всех остальных классов. В моем понимании для этого нужны границы или тело. То есть мы должны очертить какой-то объем в среде и рассмотреть процессы, протекающие в нем. В рамках этих границ отражаются внешние процессы по отношению к этим границам, в рамках внутренних процессов. Ну и вот еще один такой термин, которым я пользуюсь – это эмоции. Эмоции, в нашем понимании, это какие-то химически насыщенные состояния человека, которые сподвигают его к каким-то действиям. Либо к повторению ситуации, которая ему нравится, либо, наоборот, избегание ее, которая ему не нравится. нравится-не нравится, я не буду как бы давать формулировку, это просто такое интуитивное понятие нравится-не нравится, но в рамках вот именно термодинамических процессов мы можем как бы постулировать некоторое состояние хорошо-плохо, если там процесс течет туда, куда надо, не будем углубляться туда, то это хорошо. Если процесс течет не туда, куда надо, это плохо. И вот баланс такой, коэффициент таких соотношений будет состоянием эмоций какой-то. Она может быть на 60% хорошая, на 40% плохая. Сейчас я говорю без насыщения какими-то химическими показателями. Это максимально простое отношение. И, получается, эмоции в контексте информационной гравитации, самоосознания формулируются так, с моей точки зрения. Хорошо, это когда конфигурация тело приводит к максимальному извлечению новой информации из среды. То есть накопление в своих рамках наибольшей упорядоченности. А плохо – это когда конфигурация тела условного приводит к полной остановке извлечения новых знаний. Вот это полярное значение – мод с «хорошо» и «плохо». Все остальное – это динамический баланс этих двух крайних состояний. так и вот хотел бы привести на просто картину некоторой гидродинамики да и как вот может информация вот в таком процессе зарождаться у нас получается вот могут быть какие-то такие вихри вращения и на каком-то этапе вот значение будет там вот определенном радиусе, с определенным импульсом повторяться. Некоторые вихри – это уже какая-то упорядоченная вещь на самом деле, потому что как бы там нету никаких реперных точек, это полностью там жидкость, но тем не менее там есть информация. Скорость обращения, допустим, вихря мы можем посчитать появлением полутоимпульса в какой-то точке, он будет цикличным. И если мы радиус сужаем, так же будет, можем составить целый градиент. И вот такими биениями в природе информация элементарно и зарождается. В рамках же биологической жизни самый корневой несущий процесс информации, на которую намоталась вся последующая информация по закону гравитации, это деление ДНК. То есть у нас какие-то есть простейшие белковые молекулы. 

S01 [00:10:11]  : Вопросы можно задавать по ходу доклада? 

S03 [00:10:14]  : да-да, можно. а что здесь по осям и что тут изображено на этой картинке? я тут привел просто картинку, она ничего не означает. я просто для информативности показал некоторую закрученную динамику типа вихря. по осям у меня тут ничего не изображено. просто можно в метрах это посчитать будет. можно как двухмерную картинку в метрах посчитать, что выглядит, что выглядит, неважно. так, и я остановился на чем? на том, что... на ДНК, да, что у нас там есть какой-то был изначальный процесс биения, который собрался из там, допустим, двух молекул, потом они намотали на себя третью молекулу, стало биение длиннее, потом длиннее и длиннее, вот по гнётам внешней энергии, ну так, либо случайно скорее всего это было случайно, появилась такая бьющая динамика, а потом уже появился механизм деления, тоже наверняка он был случайным. И как бы тут получилась такая цепная реакция вот этих биений, потом какие-то биения затухли, а какие-то там в каких-то условиях намотали на себя еще молекулы, и получилась вот такая вот бьющаяся система в виде нас с вами сейчас по прошествии миллиардов лет. Вот, и как бы, как это вообще, вот эти все биения, вот эта вся динамика, термодинамика. Ну, термодинамика, почему я термодинамикой это называю, потому что термодинамика, она является самым общим процессом по отношению к гидродинамике, газодинамике, там прочим динамикам, она является наиболее общей потому что все законы, стоящие ниже нее, они являются производными. Основная у нас процесс как бы переноски по энергии. То есть любая энергия, она как бы может быть выражена в каком-то количестве джоулей. вот только поэтому. дальше. вот я привел некоторые слайды о том, о чем я говорил выше. вот допустим у нас просто рассмотрим какую-то динамику, происходящую в какой-то среде, вообще неважно в какой, и вот очертив в ней какой-то воображаемый окружность или там 3d шар, мы можем посмотреть динамику внутри нее, и там проходят какие-то импульсы, черным светом я их обозначил, какие-то течения, суммирование и так дальше. тут есть такой простой вывод, что интеграл импульсов по поверхности снаружи вот этой кружности будет равен интегралу динамика по поверхности внутри этого шахта. то есть они друг друга уравновешивают так или иначе. вот ну и казалось бы тут это абсолютно не информативного какой-то там потенциальной среде но когда идет у нас плотность немножко меняется да вот это вот тело нашего я рассматривать как тело мы можем тело любое камень взять там просто у него плотность другая немножко будет Тем не менее, у него есть какая-то внутренняя, средняя плотность у этого камня. внешняя плотность среды. тем не менее среда воздействует на камень, если у камня большая плотность, то какие-то импульсы будут высокочастотные распространяться уже со своей динамикой, натыкаясь друг на друга, но в итоге все равно вот эта интеграла по внутренней поверхности импульсов будет выравниваться с внешней. И вот это как бы простая динамика. И вот с любым телом происходит то же самое. То есть вся наша внутренняя динамика так или иначе является отражением внешней динамики. И по сути все наше мышление, процесс жизни, он сводится к тому, чтобы выровнять вот эти потенциалы в нашем теле, вот этому интегралу. был равен граалам по поверхности. ну и как бы здесь такие чисто визуальные у нас Подтверждение этому, то есть у нас есть определенная ширина стоп и ног, чтобы носить именно наше тело какой-то массой по земле, чтобы мы могли опираться. То есть наше тело полностью отражает ту среду, в которой мы находимся. То есть если нас поместить, допустим, на Луну, на какие-то параметры в нашей внутренней динамике будут излишни, а какие-то недостаточны. И у нас там либо будет… мы схлопнемся под внешним давлением, буквально физическим давлением на Юпитере, или нас разорвет где-нибудь, допустим, на Луне, если мы без скафандра туда отправимся. Ну, очевидно, что там наши вот эти вот вещи не выровняны. Ну и, собственно, процесс жизни и процесс мышления, как производный процесс жизни, он, собственно, и направлен на то, чтобы вот эту внешнюю динамику отразить во внутренней динамике, как раз то, что я говорил. Ну и как бы сделать так, что получается сделать этот процесс на компьютере. Вообще мышление, любые процессы, они у нас являются производными от того процесса жизни самого. А процесс жизни, вот, допустим, от этого процесса, который мы взяли вообще в воде или в газе или еще в чем-то, отличается тем, что он насыщен цикличностями. Вот в воде поток, он цикличностями не насыщен. Там потенциальная среда, она просто течет в одном направлении. И не факт, что она повторится. Кроме вихревых каких-то вот таких вещей, которые я пока называл вот здесь. У нас же вот в рамках процесса жизни вот этими цикличностями все кишит. выстроены один над другим. Самый основной несущий процесс – это у нас получается отделение ДНК. Все остальные процессы, как такие макропроцессы, как кровообращение, сердцебиение, дыхание и прочее – это являются такими же цикличными процессами, только высших порядков. Но если мы продифференцируем, получим первообразные процессы. в данном случае это у нас будет процесс консолидации информации на меньшем сопротивлении. или в физической форме это будет накапливание циклов в определенном объеме. вот собственно то, что и нужно сделать. я для себя так отметил, начал реализовывать эту модель. Ну и вот еще если сказать пару слов о биологии, то получается вот сначала процесс накапливания цикличностей начинался, но это был какой-то там, не знаю, Удар в волны, допустим, оберег, и там какие-нибудь молекулы из этого сложились. Это просто, предполагаю, гипотеза. Это не факт, что так было. А потом уже это перешло в химическое. Сначала была это просто какая-то механическая динамика, биение волны оберег. Потом это переросло в химическую динамику биения молекул, определений. А потом это уже крайний этап развития нашего мозга, где происходят самые быстрые процессы. Это наш мозг, это уже электрическая динамика. И по сути, если абстрагироваться от этой всей биологии, можно уже этот первообразный процесс прямо в поле электрической динамики и воспроизвести на компьютере. при этом воспроизвести эту информационную логику, накапливание этих циклов. вот если мы рассматриваем Жизнь человека, просто не опираясь ни на что, можно сказать, а где же тут цикл? Человек может вести себя в какой-то мере хаотично. Но дело в том, что человеком вообще вся динамика не ограничивается. есть какие-то циклы в популяции людей, есть целых стран и так дальше. Мы являемся просто как бы частью общей картинки, а вот эти циклы между собой, они еще интерферируют и рождают более какие-то там протяженные циклы. И по сути вот вся информация, она циклична. Даже язык, на котором мы говорим, и абстракции, которые мы имеем, мыслим, они тоже нам знакомы, потому что они повторялись много-много-много раз. Поэтому этот закон накопления цикличностей не нарушается. Просто там происходит определенная интерференция в рамках этих процессов, особенно много система, в которой много измерений. вот это все интерферирует, и кажется нам, что динамика достаточно сложная. на самом деле, если ее размотать в обратный порядок, это будет просто накопление цикличности. мне все еще видно, коллеги, Да, Андрей, все хорошо. Я просто проверяю, потому что в прошлый раз все было... Все хорошо. Так, и вот как я решил все это проверить и закодировать на компьютере. Ну, собственно, у меня все просто. На самом деле, то, что я излагал выше, это просто видение того, что происходит в рамках термогидамических каких-то Но при реализации все опять же сводится к нечто нам уже всем знакомому. Потому что тут нового придумать что-то сложно и не нужно. Просто в рамках терминологии других исследователей это просто будут другие терминологические координаты. А вот если копнуть смысл, то будет примерно то же самое. И такие как бы цикличности или повторяемости, воспроизводимости, кому как удобно, находят свое отражение, допустим, в нейросетях. где у нас каким-то образом мы можем делать классификацию, пластеризацию и так дальше, когда нейросеть по активации своих связей может выводить отмечать какой-то класс, допустим, информации. Но это и не что иное, как нечто воспроизводимое. Но я пошел немножко по другому пути. А, ну да, еще есть какие-то формальные системы, когда мы чисто логические выводы можем делать. Но я тут пошел по другому пути к формальным системам, символическим и прочим. У меня есть одна претензия. В меньшей мере она относится к нейросетям, в большей мере к формальным системам, к их внутренней ригидности. нацелен на полную внутреннюю эволюцию, когда сами системы тоже могут меняться, а вот формальные алгоритмы, они на то и алгоритмы, то есть это какое-то метаописание, которое неизменно. Вот одна такая у меня претензия к формальному системам. Но не суть. У меня, чтобы максимально отразить свою логику, я не стал пользоваться ни тем, ни другим, а пытался воспроизвести на динамику физическую. цифрах. и сводится эта архитектура, вернее динамика, к следующей архитектуре отражения этой динамики. то есть я сделаю ее на треугольных суммирующих матрицах. по сути, если рассмотреть некий ряд значений, отмечен желтым цветом. это неважно, какие-то значения на какой-то там шкале, вернее серым цветом. желтым цветом у меня порядковый номер отмечен. на какой-то шкале у нас есть ряд 2, 18, пусть это будет шкала от 1 до 20. вот эта моя матрица, она отражена следующим законом то есть на каждом слое вот этой матрицы хранятся суммы вот этих двух чисел из ряда поступающих входного на какой-то там датчик потом хранится сумма из трех чисел, четырех чисел и так дальше И вот на каждом вот этом слое накапливается сумма, и идущая за ней следом сумма через цикл. Они отмечены красным цветом, зеленым, таким и таким, голубым. И по сути у меня получается вот такая стопка, где хранятся вот эти значения. соответственно, на каждом следующем шаге у нас статистика по какому-то узлу накопилась, мы прогнозируем, что будет дальше. таким образом, накапливая статистику, мы определяем, какое значение будет дальше, узнаем мы его или нет. почему у меня выбраны эти треугольные матрицы? еще на каждом новом повторении, если у нас ничто у меня не встречалось до этого, формируется новый полюс. у меня получается такая динамическая система, где матрица цикличности постоянно растет. И вот по этой метрике я как раз таки могу смотреть, информация у меня накапливается или не накапливается, или наоборот уменьшается. И при том каждый из этих полюсов созданный, если он не используется, параметрически заданное количество циклов, то он просто отмирает. И все это у меня уменьшается. и за счет того что тут вот между вот этими четырьмя числами есть определенная логика суммирующая мы можем сложив одно число и отняв от другого прям сразу же понять какое тут будет число Сделано это так, чтобы сэкономить вычислительный ресурс. Потому что у меня матрица суммирующая, как вы знаете, для процессора операции суммы не самые простые. Затем она вот так сделана, чтобы расчетов было минимум. И как это работает у меня. Я хотел бы перейти к примерам. параллельно показывая теорию, рассказывая, потому что у меня больше как... меня все еще видно? дайте пинг да все хорошо андрей так вот и получается я Вот, получается, запускаю код, он делает следующее, он просто читает какой-то текстовый файл, в данном случае книга, и на основании считанного предсказывает, какой будет следующий символ. Это вроде как работает как T9, или как у нас работают GPT и прочие наши модели. ну вообще моя вот эта модель она под текст не рассчитана, потому что моя модель она рассчитана больше на шкалы на шкалы, где можно какой-то импульс в рамках какой-то шкалы от 0 до 100 нормализировать. с буквами так не очень получается, потому что нет никакой шкалы для буквы. я просто порядковые номеры, адские кодировки, просто считаю, у меня что получается, вот номер символа идет с читанного, а вот я предсказываю, какой будет следующий, где-то они совпадают, где-то не совпадают, и по метрикам у меня получается следующее, то что вот процент угаданных символов, и он у меня, как вы видите, постоянно нарастает. Вот на текстовых данных это не языковая модель у меня ни разу, это просто на тексте я проверяю сходимость. Сходимость проверяем на тексте, потому что это максимально насыщенная информационная среда. моя модель, она предполагается к работе на каких-то шкальных величинах вот я вам показывал уже в предыдущих сессиях это робот вот на нем я собственно его и пытаюсь запускать а на текстовых я просто проверяю сходимость и вот касательно метрик хотел бы упомянуть то, что вот у меня получается количество всего считанных символов и количество совпавших, а это их отношения друг к другу. Допустим, из прочитанных совпало, по моему прогнозу, 31%. Моя модель сходится, потому что если бы она расходилась, то она либо дрейфовала около 3-4%, что является 1 делить на 26 символов в английском языке. Примерно так бы было дрифовало. Либо расходилось, чтобы наоборот процентаж падал. Но процент постоянно растет, модель сходится. А вот в этой метрике отражено как раз таки то количество полюсов, которая создалась на предыдущем этапе, когда я вам рассказывал, что вот эта треугольная суммирующая матрица, на каждом прогнозе записывается, что было до него в сумме, в терминах суммы. И вот получается эта сумма, что она делает? вот допустим у нас есть фактические данные 8, 8, 9, 10 и здесь есть фактическая сумма и мы тут накидываем прогноз какая вот тут в этом узле будет сумма следующая на основании предыдущего опыта и так у нас происходит по каждой сумме вот в этом каскаде и максимальный процент накидывания вот этой суммы дает максимально верный контекст. и чем процент выше, тем вероятнее, что будет подтянута у меня вот к этой сумме вся сумма вот этих вот последующих значений. и получается путем среднего арифметического примерно складывается следующий символ. ну и в итоге тут получается что в метрике еще есть. в метрике получается есть средняя высота контекста, который был наброшен за последние тысячу прочтений. И вот в данном случае он получается 379. Что это значит? Что самый максимальный контекст, с которого нам вернулся прогноз, это было 4 предыдущих символа. И вот по мере того, как модель работает, вот эта глубина контекста все время увеличивается. Допустим, там было 3.7, уже 3.8. Это значит, что в среднем на четырех символах вот эти четыре символа дают вот такой прогноз. если долго гонять эту модель, контекст будет углубляться, но я там максимум до 8 или до 9 доходил сейчас проблема этой модели моей заключается в том, что она работает в однопоточном режиме C++ это не мой основной язык с многопоточностью ну и вот так это как бы каскадно работает то есть процентаж постоянно растет, предсказательная сила растет и глубина контекста растет. на одном параметре я вижу сходимость, то что информация внутри модели нарастает, потому что фактически предсказательная сила у нее постоянно растет. я вот хотел показать на моей рабочей станции, ну вот она уже достаточно давно молотит, наверное сутки, тут уже процент больше 50, то есть больше 50 символов предсказанных совпали на где-то 200 тысяч символов было прочтено, или там 107 тысяч из них совпало. И вот тут даже есть, если мы обратим внимание, вот это то, что было прочитано, первый ряд, think you could eat, а вот это то, что моя модель предсказала, и с какого контекста это вернулось. Вот тут вот согласно тому, что было в предыдущих семи символах со стопроцентной вероятностью будет i. тут уже 8 предыдущих символов со стопроцентной вероятностью будет n. тут с 9 в контексте глубины 0,9 будет k, и с контекста 10 будет пробел. а после этого уже, тут уже не все так очевидно, там процентаж падает, тут, кстати, вот тоже не 100%, а 90, потому что тут могут быть и запятые, и там, не знаю, двоеточие и прочее, а тут уже следующий символ падает вообще до 29% и, естественно, он не угадывается, потому что мы не можем предсказывать будущее. то есть такого, что все буквы будут угаданы, такого не будет. потому что если бы было бы так, то значит, что модель может предсказывать вообще будущее. человек скажет, но это нереально. то есть мы выпрыгиваем за 50 процентов, максимум где-то будет ну, дальше после 50%, там это у нас уже где-то график идет по графику квадратных корней, где-то вот так. я экспериментировал, до 75, до 80 доходит, но не более того. уже, естественно, ограничения антропии языка не позволяют нам заглянуть дальше. 

S00 [00:37:09]  : можно вопрос? 

S03 [00:37:10]  : да. 

S00 [00:37:12]  : Вы это тестируете на том же датасете, на котором вы учаетесь или на отдельном отложенном? 

S03 [00:37:22]  : На том же, ну, как сказать, на том же датасете, ну, смотрите, вот у меня читается книжка, вот она одна, да, И вот по мере чтения этой книжки, будущее книжки мне неизвестно, но по мере чтения книжки у меня процентаж постоянно растет. То есть я не знаю, модель не знает, чем книжка закончится, но у нее постоянно растет, исходя из прочитанного уже в начале этой книжки, вот она предсказывает, что будет дальше. 

S00 [00:37:59]  : получается растут каждый раз на новых данных, с которыми мы больше не сталкивались. А, хорошо. 

S03 [00:38:06]  : Ну вот, собственно, это я показал сходимость на… А, кстати, я вам еще подумал, а почему бы сюда не прикрутить генераторный режим, что-то вроде чата GPT? одно дело, когда он просто читает, предсказывает, а второе дело, когда я ему что-то напишу, а он мне что-то ответит. Ну, я сделал это, конечно, в минимальном режиме, но надо понимать, чтобы он начал что-то вменяемое отмечать, отвечать, вернее. то ему надо прочитать не одну книжку, не 13 тысяч символов, даже не одна книжка, может, страницы 20-30, может, меньше даже. Вот, сейчас он строку дочитает, я покажу, как это работает. Ну, суть в том, что эта модель, она также может работать в генераторном режиме, то есть помимо читания и предсказывания, она еще может генерировать какие-то тексты. Естественно, на такой маленькой обучающей уроке она осмысленно ничего не сгенерирует, но тем не менее, я думаю, что слова с нужными пробелами она поставит. Вот я пишу и предлагаю ей продолжить. возможно мне сейчас что-то ответит. ну вот она мне накидала каких-то... ну вот она, по крайней мере, человека читаемые слова, без всякой подготовки, расставила пробелы, ну и что-то сделала. Как-то это работает. Понятно, что смысла никакого в этом предложении нет, потому что, чтобы зародился смысл, это надо, глубина контекста не вот 3.19, а хотя бы, не знаю, сотни-двести его, чтобы было. как-то так. тогда он уже будет наверное смысленно что-то выдавать. ну вот я как бы хотел просто показать сходимость вот этого всего. вот и хотел еще показать как это работает с роботом. вот мой контроллер для робота средевый bots, и он использует абсолютно тот же самый код, что и что и вот эта модель, которая читает книжки и делает прогноз следующей буквы, ну то есть у меня включены все те же самые заголовки, что и вот в этой ну просто это один в один все то же самое тот же алгоритм робота обрабатывает Так. Вот я его запущу и запускаем робота. Так, вот пошли данные. Ну и вот, собственно, этот робот-спот начинает шевелить конечностями и извлекать данные из среды. то есть он сейчас ищет цикличности, вот так же пытается для себя предсказать, что будет в будущем, нарастить себе эту информационную плотность. Тут я хотел сказать, сразу отметить то, что Ждать, пока он фактически оживет, встанет на ноги, начнет оглядываться и бродить по вот этому полю, мы ждать будем очень долго, возможно, не один десяток лет, можно даже не два. Почему так происходит? Потому что вот вообще вся форма робота, Это нечто замысленное не эволюцией, а нами с вами. Мы предполагаем, что робот должен двигаться каким-то определенным образом. Дали ему конечности, ожидаем от него движения. Этой модели сейчас вообще не очевидно, что он робот, что у него есть ноги, что они должны быть каким-то образом сконфигурированы, что они должны в каком-то виде для нас, людей, быть красивыми эти движения, максимально эффективными. Вот для моей модели сейчас Важно то, чтобы робот, делая свои движения, максимально наращивал информацию внутри себя. Вот это единственная цель, которая им сейчас руководит. И, соответственно, если эволюция пойдет по правильному пути, удачно, перевернется на брюхо, начнет шевелить лапками и начнет воспринимать среду. Вот сейчас он среду воспринимает меня путем анализа положения своих же ног, джойнтов, тут их 12, и координат GPS. И плюс к тому, там еще есть один абстрактный слой. куда со всех кнопок и датчиков, ну кроме датчиков, кроме gps, gps у меня нормализованная величина, а вот ноги у меня сливаются еще в один, в абстрактную, кстати, я еще не упомянул то, что каждая из джойнтов загоняется в отдельную вот такую матрицу, которую я показал для букв И плюс к тому еще есть одна матрица абстрактная, куда со всех джойнтов загоняется информация в своем контексте. И фактически вот этот абстрактный слой дает этому роботу понимание, что какой бы ногой он ни управлял, он управляет ногой. И опыт одной ноги в рамках оптимального движения передается опыту другой ноги. А почему так сделано? Потому что ноги у этого робота абсолютно одинаковые. Если бы у него задние или передние ноги чем-то отличались, я бы сводил только две ноги в один абстрактный какой-то слой. И сделал бы абстрактный слой для всего общего кого-то тела. И в этот абстрактный слой можно напихать там всего чего угодно. разложить вообще даже вот видеопоток вот в такой вот набор графиков. Допустим, у нас первый джойн, второй джойн, третий джойн. У каждого есть своя кривая в течение времени. И все эти кривые загоняются в общий абстрактный слой. И робот понимает, что для ноги, как абстракции, правильнее управлять ногой вот так, чтобы привести к максимальному извлечению информации. И в это абстракционство можно загонять любую информацию. нормализованную, допустим, шкале 1 к 100, ровно как ноги, ровно как какие-то звуковые датчики, микрофоны, что угодно. И тогда этот робот будет видеть в одном другое, составлять абстракцию, видеть в одном процессе другой процесс. А в рамках мышления такое видение процессов в абстрактном слое Оно может переключать контекст мышления. Вот, допустим, мы увидели, как, допустим, человек, дирижер взмахивает рукой в определенном диапазоне, по определенной траектории. Мы, получается, можем сопоставить ее со звуком. Даже если дирижер это делает без звука. Мы можем извлечь то, что раньше я видел. в другом виде информации, допустим, звуки, что с повышением траектории руки повышался тон какой-то музыки. И мы сразу можем в этом абстрактном слое переключить контекст вообще на музыку. Вот так это работает. Ну вот таким образом я вот этого робота запускаю. Я вот на предыдущих семинарах где-то даже показывал, как он начинает ловить со временем он там принимает определенные позы и начинает ходить там уже по полю начинает развиваться, начинает вставать, пытаться передвигать. То есть он не может оставаться в тех положениях, которые не приводят к увеличению цикличности. то есть накопление информации, мы постоянно накапливаем информацию. Если он в чем-то застыл, он будет пытаться выйти из этого состояния. Вот как-то так это работает. Ну, собственно, у меня все. Если есть какие-то вопросы, я готов отвечать. 

S01 [00:49:16]  : Да, спасибо, Андрей. Там вот давайте начнем с вопросов немного пока, поэтому давайте начнем с конца. Значит, есть ли ваш код на гитхабе? 

S03 [00:49:27]  : Да, на гитхабе есть, но у меня там старый код, я подвыложу новый, наверное, сегодня. 

S01 [00:49:32]  : Но это вот по той же ссылке, которая в названии доклада, да? 

S03 [00:49:37]  : Да, да, да. 

S01 [00:49:38]  : Соответственно, коллеги, кто смотрит, там в анонсе доклада есть ссылка на репозиторий, там можно будет посмотреть через какое-то время новый код. И второй вопрос тоже от Ивана. Предсказания выявляются на основе предыдущей статистики? 

S03 [00:49:55]  : Да, совершенно верно. Это статистическая модель. Статистической моделью являются любые недосети, любые правила. Допустим, какие-то формальные правила – это статистика, накопленная человеком и оформленная в рамки какого-то алгоритма или закона. В нейросетях это тоже просто статистика условная. Вот чем отличается мой подход и почему он отличается? Это просто пространственная геометрия этой статистики. Она, я могу сказать, что не лучше, наверное, нейрофитей. Просто почему я использую такой подход? Ну, во-первых, я стараюсь держаться в рамках своих же рассуждений по термодинамике, это во-первых. А во-вторых, ну, как я заметил, мне удобнее, когда я разлагаю, допустим, любую информацию в ряды, вот такие вот, да, Мне её проще выражать одну в другой, смешивать, делать абстрактные слои и прочие манипуляции проводить, редуцировать, статистику собирать, выводить контекст мышления. Допустим, когда робот о чём-то думает, я могу знать заранее, так как у меня информация разложена по компонентам, я могу усилить его звуковые ощущения, визуальные. если брать классические нейросети, я в их архитектурах не разбираюсь. Уверен, что эти проблемы тоже решены. Когда, допустим, мы делаем анализ картинки в обычной трехслойной-четырехслойной нейросетью, мы накидываем случайно по всем координатам какие-то произвольные вещи, из которых трудно потом что-то собрать. У меня не так все работает. Мне самому проще объяснять то, что происходит, когда все разложено в максимально простые абстракции. но при этом я не утверждаю, что именно моя модель в этом смысле работает лучше других моделей. она просто для меня удобнее. удобнее ее проектировать. 

S01 [00:52:25]  : спасибо. вопрос докладчику. вы сами придумали предлагаемую вами терминологию? Почему не использовали общеупотребительное толкование терминов? В связи с этим возникло множество вопросов, от которых воздержусь из-за ограниченности формата конференции. 

S03 [00:52:49]  : Смотрите, тут я, может быть, как бы пренебрегаю Вашим вниманием, вашим статусом как профессиональных разработчиков AGI, но я в начале своего семинара упомянул, Для меня это не профессиональная разработка, а просто хобби. Сделал я это, чтобы привлечь семинар, чтобы внести общую лепту к копилку наших семинаров. Но если бы я этим занимался профессионально, допустим, работал бы на какую-то компанию или рассчитывал на какой-то грант или еще на что-то, я бы, конечно, привел это все в соответствии с общепринятыми терминами и постарался выразить всю логику каких-то общепринятых терминах. А для меня это хобби больше, я постарался вот эти термины которые я использую для себя. вот и все. но я постарался их максимально просто объяснить. если есть вопросы, могу просто их расшифровать. 

S01 [00:54:00]  : Ещё вопрос докладчику. Можете дать ссылку на научно доказанное определение первичности термодинамики относительно гидродинамики и остальных динамик? Или это плод Ваших работ? Имеются ли в открытом доступе данные работы? 

S03 [00:54:21]  : Не утверждаю, что термодинамика первична. Я так полагаю, потому что любая энергия выражается в джоуле. Само вещество у нас масса выражена в энергии. Вот только поэтому. Можно назвать это гидродинамики. Те же самые законы применяются абсолютно. То есть это просто динамика в потенциальном поле описывается любыми уравнениями гидра, газа, динамики и прочих. Динамика в какой-то среде с ненулевой плотностью. Вот и все. Я не вижу смысла именно углубляться в различения термодинамики, гидродинамики. 

S01 [00:55:09]  : Спасибо. Это были вопросы от Виктора Казариного. Еще один вопрос от Виктора Казариного. В чем проявляется связь закономерностей, которые вы назвали информацией, с чем я не согласен, так как информация более широкое понятие, с термодинамикой? В чем проявляется связь закономерностей, которые вы назвали информацией с термодинамикой? 

S03 [00:55:35]  : Я в первом слайде обозначил то, что я не веду под информацией. Вы говорите, что информация – это более широкое понятие. Более широкое понятие укладывается в понятие, наверное, вы имеете в виду данные, когда мы… просто имеем какой-то набор входящих потоков с определенной дискретизацией, да, и можно интерпретировать это как информацию, можно интерпретировать как данные, да как угодно. Просто вот в своих рассуждениях под информацией подразумеваю то, что является повторяющимися значениями данных. Это было заявлено в самом первом слайде. Касательно того, как повторяющиеся значения связаны с термодинамикой, это накопление инерции. Это то же самое, как, допустим, у нас есть какой-то вихрь, у которого есть внешнее давление или гравитация, он заворачивается в вихрь под своей инерцией. с информацией моей треугольной матрицы такой же принцип. мы накапливаем импульс и согласно его инерции мы делаем прогноз. 

S01 [00:56:54]  : Спасибо. Ещё вопрос от пользователя Kilorad. Можем ещё чуть пройтись по архитектуре модели? Что может выразить модель? Какое это множество функций, любую функцию, любой конечный автомат, любую машину тьюринга? 

S03 [00:57:12]  : Смотрите, у меня модель она как бы функцию не выражает. Она выражает у меня только совершенно примитивные вещи. По ним я сейчас подробнее пройдусь. Первое, это я что упомянул и показал на том, как это работает с текстом, это у меня статистическая модель. Она просто определенным образом собирает статистику от предыдущего опыта, при этом пытаясь захватить там наиболее больший контекст. Но сама она там никаких операций над данными, собственно, не делает. Тут ровно так, как это происходит в любом вихре, в любой гидродинамике. какую функцию можно выразить? можно выразить только инерциальную функцию накопления импульса, больше ничего. 

S00 [00:58:18]  : можно я уточню? в случае с текстом у вас было некоторое окно контекста. То есть на входе у нас некоторая последовательность букв, маленькая, 4, допустим, буквы, а на выходе одна буква. То есть вот эта вот штука, которая на вход принимает 4 буквы, а на выходе дает одну, это называется функция с точки зрения математики. Соответственно вопрос, какое множество их? Можем ли мы параболу выразить таким образом? То есть на входе падает 2, а на выходе будет 4. И всякий раз квадратичную функцию выразить. Можем ли синусоиду, экспоненту? Какие границы того, что мы можем? 

S03 [00:59:06]  : хороший вопрос но я не знаю как это вообще вообще по сути у меня как бы вот это стаб модель выдает только максимально то, что выдавалось бы в контексте статистики языка или, допустим, каких-то значений. Если какие-то синусоиды были в обучающей выборке, синусоиды она и выдаст. А так модели… Она линейная, наверное. Наверное, не совсем линейная. 

S00 [00:59:49]  : Ну, допустим, обучающая уборка у нас выглядит как таблица. На входе у нас значение x, на выходе y, который равен синус от x. И у нас таких значений 100 тысяч. Вот мы прогнали эту функцию, а потом ей на вход пришло огромное количество некоторых других новых лицов. сможет ли она воспроизводить правильные y, которые мы в синусу от x, а она для любой функции так может сделать или не для любой? 

S03 [01:00:27]  : во-первых, у меня там не какая-то таблица, а линейные значения. того, о чем вы сказали, если поле определения функции укладывалось в сумму в этой треугольной матрице, то есть сумма значений, которые мы бы подавали с помощью других значений, то да. как это объяснить? тут некоторое усреднение работает. взвешенная линия функции, верно? ну да, наверное так правильно сказать. я сейчас поясню на числах. 

S00 [01:01:18]  : вот смотри, 

S03 [01:01:28]  : Смотрите, допустим, у нас есть пятерка каких-то значений, 2, 18, 2, 5, 2. И вот эта пятерка дает мне какую-то сумму. И после этой суммы может повториться через цикл другая сумма. такая же. так вот если вот в этой выборке она может меняться в значениях, но сумма будет та же и прогнозная сумма будет та же, она ее выдаст и вот по этому механизму контекст подтянется. я не знаю, как это интерпретировать, но если тысячу лиц показать моей модели, И потом дать часть тысячепервого лица, она дорисует его примерно так же, как оно должно было бы быть. То есть тут конкретные нижние и нижние суммы уже значения иметь не будут, но средние и верхние значения иметь будут. 

S00 [01:02:39]  : Я просто к тому вопрос задаю, что обычно, если какая-нибудь система используется для прогнозирования, то ее обычно тестируют на предмет того, что она может, что она не может. Например, для однослойной нейросети известно, что она не может 0,004 делать. Там это ключевая важная деталь. А в индукционной известно, что она может любую функцию аппроксимировать. Теория. Линейная, например, нейросеть, она может только линейные функции аппроксимировать. Квадратично, например, не может. И по описанию, я, может быть, что-то не очень правильно понял, но выглядит так, что у вас что-то типа линейной авторегрессионной модели. Соответственно, она, по идее, может любую линейную функцию, но нелинейную, кажется, награда нет. 

S03 [01:03:23]  : Ну, я тоже думаю, что про нелинейность у меня тут какой-то речи я за себя, по крайней мере, не отмечал. Но вот на что она способна, я показал. Текст на основании предыдущего контекста, она в будущем может предсказывать, который ей не был известен заранее. Вот это она может делать в контексте текстовых данных. Ну и я это смотрел на картинках тоже, правда, пока я не решу проблему там хорошую картинку, например, это может в будущем, кому-нибудь интересно, когда это будет, я покажу. 

S01 [01:04:09]  : Спасибо. Еще вопрос от Виктора Казаринова. Вы построили продемонстрированную систему на основе закономерностей. Что там будет являться новизной и как новизна используется в вашей системе? 

S03 [01:04:25]  : Хороший вопрос. 

S01 [01:04:26]  : Да, действительно хороший вопрос. 

S03 [01:04:31]  : Новизной. Так, я сейчас на коде роботов покажу. Новизной для меня, вот критерием как раз таки метрики новизны и накопления информации являются два компонента. Вот я вам на технике показывал. Первое – это наращивание полюсов. это первое. то есть если полюса наращиваются, значит новизна есть. и второй момент, даже тут их три, трехсложная система, увеличивается глубина контекста, увеличивается точность прогнозирования и увеличивается количество полюсов. Как это работает? Допустим, если этой модели показывать картинку одну, цикл за циклом, цикл за циклом, цикл за циклом, то вот это число полюсов перестанет расти. И соответственно нужно будет внести шум в модель робота, чтобы это изменилось. потому что количество полюсов не растет, оно уперлось в потолок, потому что новизны в картинке нет. Ему надо показать еще что-то. Чтобы показать еще что-то, нужно сделать какое-то движение, какую-то помеху внести во внутреннее состояние, подбросить монетку условно. И вот когда вот эти три метрики нарастают, значит в системе происходит новизна, то есть там происходит увеличение циклов. вот так. а, ну еще я хотел тут помимо всего прочего отметить, я не закончил эту мысль. вот робот может много дергаться, много ловить цикл, ловить движения, фокусировку на каких-то движениях циклических делать, но так как вот этот робот был задуман как плод нашего воображения о том, как он должен двигаться, ему все же в рамках обучения надо дать траектории, те, которые мы предполагаем верными для эффективного его обучения, потому что он сам-то их словит рано или поздно, но чтобы он их словил рано или поздно, это вот все джойнты и пенисводы надо с собой перемножить, это будет колоссального масштаба числа. его надо как-то редуцировать. помимо формы роботу надо дать еще и какой-то кластер движений, которые мы от него ожидаем увидеть. и вот в рамках этого кластера движений он тогда уже будет самообучаться дальше. 

S01 [01:07:40]  : Спасибо. Еще вопрос от Ивана. Можно еще раз пройтись по алгоритму на примере текста? Как так получается, что для предикта используется разная длина контекста? 

S03 [01:07:58]  : Объясняю. Вообще у меня модели полностью параметрические. и вот эта глубина контекста состоит из 30 до 28, она у меня там забита вручную, то есть мы прописываем, чтобы она не была больше какой-то величины, потому что ресурсы компьютера ограничены, мы не хотим, чтобы это долго прожевывалось. Ну и у меня там сейчас стоит, по-моему, 20 символов контекста максимального. Так вот, как это работает? Вот у нас, когда считывается книжка, у нас накапливается статистика. То есть каких-то слов, словосочетаний, и так дальше. И каждому словосочетанию соответствует сумма. Тут даже ее можно не расшифровывать. Это просто мы сложим адские коды вот этих всех букв в какую-то сумму. После этой суммы по статистике будет идти какая-то другая сумма. и на масштабе меньше будет тоже прогнозироваться какая-то и короче вот с этой всей диагонали берется срез прогнозов и максимальный выдается уже как прогноз с учетом подтягивания вот этого контекста и получается каким образом работать когда мы вообще прочитали два слова и мы что можем из этого прочтенного вычленить там окончание иди иди она будет повторяться часто дальше это прошедшее время не образуемая там буквами иди и пробелом иди пробел иди пробел вот в контексте вот этих величины 3 Очевидно, что если идет ED, то с вероятностью 90% будет пробел на маленьких контекстах. На больших контекстах, когда уже много чего прочитано, тебе уже более длинные контексты начинают выдавать более правдивую информацию. они уже начинают целые там словосочетания тебе анализировать и говорить, что вот в таком контексте такой суммы наиболее вероятно будет вот это. И оно, вот этот прогноз, перебивает все остальные более низкие, стоящие там. И вот это свойство, когда более длинные словосочетания начинают предсказываться. Вот этот свойственный матриц у меня. Так это работает. Если я понятно изъяснил все. 

S01 [01:11:01]  : Спасибо. Следующий вопрос от Виктора Казариного снова. Я не понял про суммирование. 1 плюс 3 равняется 2 плюс 2 равняется 3 плюс 1. Если так, то сумма двух последних элементов не уникальна и может дать ошибку. 

S03 [01:11:19]  : правильно правильно совершенно правильно но тут как бы вам объяснить то тут я когда говорил что эта модель она не очень подходит под текст вот если речь идет о каких-то там, не знаю, просто поточных данных, там, может быть, вам совершенная точность не нужна, но будет максимальная точность, будет максимальная, допустим, 99% от правильной величины. Просто тут видите, смотрите как, когда вот сработал контекст на определенной глубине, И он оказался максимальным. То, что ниже него, это не важно. Это значит то, что он усреднил нечто ниже стоящее ниже него. В рамках восприятия это как работает? Если мы увидели, допустим, какой-то маленький образ лица на какой-то фотографии, Нам не важны какие-то промежуточные значения между пикселями. Нам даже вот этой грубой картинки, маленькой, уже достаточно, чтобы спрогнозировать, что будет дальше. Там будет какое-то усреднение. С текстом это работает хорошо, вот как вы видели, в генераторном режиме у меня там все пробелы, все правильно расставилось, ничего там плохого нет, потому что только текст мне и подается изначально, и сумма там всегда уникальна, вот так. Но для других данных, кроме текста, это не так уж и важно. Вот я там хотел показать, я с этим анализировал, как раз отвечая на этот вопрос. Показать еще кое-что, что вы его правильно действительно задали. Вот я эту модель прогонял через то есть через эту модель прогонял там картинки. Да, вот смотрите, вот это оригинал, а вот это то, что было сгенерировано. И сгенерировано было с низкой разрешающей способностью. Но очевидно, очевидно то, что результат очень похож. Если мы нарастим разрешающую способность моей модели она будет более точно отображать и вот я это даже сделал ну она просто будет дольше считаться вот она вообще почти один в один там какие-то там белые точки там нюансы там артефакты есть но это ни на что не влияет по большему счету 

S02 [01:14:26]  : Спасибо. 

S01 [01:14:28]  : Дальше вопрос, который точнее на самом деле комментарий, но я бы его воспринял как вопрос. От Владимира Смолина. Термодинамика отличается от других физических дисциплин использованием понятия энтропии. Если в модели энтропия не представлена, почему она называется термодинамической? 

S03 [01:14:57]  : она и представлена, просто энтропия в обратном порядке представлена со знаком минус. Это отрицательная энтропия или в моих терминах это консолидация. Очевидно, что, допустим, солнечной системе там в рамках нашей земли там есть термодинамический процесс правильно правильно но термодинамические процессы бывают разными все говоря о термодинамике говорят об энтропии но на локальных участках В пространстве и времени энтропия может быть отрицательной, как бы это странно ни звучало, но за счет внешней энергии. Вот, допустим, когда в море возникает вихрь, Это очевидная вещь какого-то порядка. Очевидное циклическое круговое движение происходит. Нарушает ли это законы термодинамики? Нет, не нарушает. Все происходит просто за счет внешней энергии. То же самое контекст жизни нашей происходит за счет внешней энергии. Процесс жизни, фотосинтеза в конечном счете, энергии Солнца. Просто локальный участок, на котором общая энтропия, она естественно растает, локально уменьшается. Если бы это было не так, то сложные организмы, они бы не появились. И не появились бы города, упорядоченное движение, циклические движения, поршни в двигателях и прочее. Потому что все это последствия этой простой физической динамики. 

S01 [01:16:47]  : Спасибо. Короткий комментарий. Раз уж речь шла о терминах, там консолидация вместо какая-то величина обратной энтропии, я в своих работах последних по естественному языку ввожу понятие антиэнтропия. Вот эта величина обратной энтропии. Следующий вопрос от Алексея Удода. Я пока не понял общий алгоритм, но вы используете в качестве закономерностей точное соответствие сочетания входных элементов или не совсем точное? Это вот к вопросу, к вопросу о двух этих самых картинках. Сейчас закончим вопрос секундочку. В первом случае возникает проблема шумов, пропусков и других искажений? 

S03 [01:17:39]  : так, на тексте не возникает, потому что текст, это однозначные данные, не возникает шумов, искажений, имеется в виду, выдаются всегда буквы, просто с какой-то вероятностью, какой-то ошибки быть не может, кроме букв у меня выдаться ничего не может, constraint. касательно других нормализованных личностей, есть шумы, неточности, соответственно, всё в рамках параметров модели. Вот на картинках я вам показывал, что да, он там генерирует шумные картинки, но просто потому что разрешающая способность модели не позволяет прям вот до пикселя всё посчитать, что нет, не нужно. Общая картина-то складывается. То есть да, это не точно, не прям один в один данные. Нет, там они сопределены. То есть эти, даже я вам так скажу. Вот эти вот картинки, я считал по метрикам, они с оригиналом совпадают на 2%. Пиксель в пиксель совпало, а все остальное это максимально близкие по цвету пиксели. 98% пикселей один в один не совпадает, но выбираются максимально близкие к ним. 

S01 [01:19:09]  : Спасибо. Следующий вопрос от Алексея Удот. Если в качестве данных подавать бесконечную линейную последовательность чисел 1, 2, 3, 4, 5 и так далее, то через сколько чисел система начнет безошибочно предсказывать следующий элемент? 

S03 [01:19:28]  : Почему? Ну давайте ответите. Не через сколько. То есть то, о чем вы говорите, Знаете, это, наверное, будет продуктом длительной многокомпонентной эволюции. вот только так, наверное, отвечу. А вот именно в этом контексте оно не будет такого производить. То есть на это модель не способна. Оно, знаете, как объяснить? Тут надо, как сказать, тут немножко надо трюков добавить. что у меня есть в роботе, но они правда еще не реализованы. Конечно, я об этом тоже думал. Там это уже нюансы мышления должны быть составлены таким образом, что это было бы стало возможным. Это я говорю о том, Когда я говорю, что роботы нужны траектории, траектории мышления они тоже на самом деле нужны, но там это уж не такая большая проблема. Я так это вижу. Если хотите, мы можем потом продолжить эту беседу, потому что она, мне кажется, будет длинной. 

S01 [01:20:52]  : Спасибо. И продолжение вопроса. Почему используются сложения, а не более логичные в этом случае вычитания? 

S03 [01:21:01]  : я объяснял, упомянул то, что для процессора это эффективнее, быстрее считать. но я мог бы какие-то усреднения делать, там много чего можно делать. просто вот допустим если это считать на видеокарте, то самая простая операция это сложение. поэтому суммирующее треугольная матрица. 

S02 [01:21:29]  : Спасибо. 

S01 [01:21:30]  : Следующий вопрос от Алексея. Можно ли выделить из накопленной пирамиды высокоуровневые признаки и инварианты для дальнейшего использования? 

S03 [01:21:42]  : Наверное, этот ответ похож на предыдущий. Инварианты-то, наверное, можно вычленить. просто вот как их интерпретировать, вот вопрос. ну то есть да, тут будет у меня какие-то там статистика накоплена, а вот с интерпретацией вопросы могут возникнуть. но по моему закону суммирования его можно будет спихнуть. но это опять же в рамках каких систем это и каким образом будет интерпретироваться. тут видите в чем штука то, то что я еще вот за счет этой треугольной суммирующей архитектуры делаю как бы самоподобие, то есть некоторую фрактальность, то есть чтобы допустим, одни глубины контекстов угадывались с других глубинных контекстов. если эта архитектура, где это будет использоваться, предполагает вот такое, то можно, наверное, как-то использовать, а так я об этом не думаю. 

S02 [01:22:57]  : Спасибо. 

S01 [01:22:58]  : Следующий вопрос от пользователя Килорад. Есть другие примеры того, как ваша система решает задачи управления, помимо рова собаки. Например, что-нибудь простое, вроде пинг-понга. Вопрос в том, как у вас задача управления сводится к задаче прогноза. Нельзя же просто предсказать, что мы сделаем. Это не приведет к хорошему управлению. 

S03 [01:23:23]  : два пути, как я объяснял. первое это там предтренинг сделать, то есть дать траектории и взаимосоотношение ракеток и шарика по траекториям. ну либо она сама научится, но это будет очень долго. Смотрите, вот вы говорите управление. Управление это предполагает что? Это достижение какой-то метрики, правильно? ну допустим, если игра, речь идет о пинг-понге, то мы достигаем метрики, допустим, очков каких-то максимально, значит, все у нас идет по плану. у меня же тут метрика-то одна на самом деле, поэтому тут она, наверное, и не подходит, вот эта архитектура именно для пинг-понга. У меня метрика одна, это увеличение информации в системе, это развитие, это просто как бы жизнь и все. Тут нету задачного подхода у меня, видите, в чем штука. У меня тут задача просто, чтобы… моя модель эволюционировала, охапливала себе информацию. но в принципе, если там метрики поменять и, допустим, завязаться не на рост информации и рост точности прогноза и рост глубины контекста, а на рост очков, ну можно и в это поупражняться, но я, честно говоря, такое не рассматривал. 

S00 [01:25:12]  : По какой причине выходит так, что собака в итоге пойдёт? Кто её заставит пойти? Почему не выйдет так, что метрика ваша сойдётся на том, что собака будет лежать лапами кверху и перебирать этими лапами? И в этом случае будет максимум вашей метрики. Почему не выйдет так? 

S03 [01:25:33]  : Ну, так оно выйдет, но оно так бесконечно длиться не может. В итоге все пространство возможностей заполнится. Вопрос, когда это произойдет? Она просто переберет все варианты рано или поздно. Но это очевидно, что вещь-то неэффективная. Почему? Потому что, когда мы проектировали и внедряли туда модель, мы, как люди, предполагали, что она будет двигаться определенным образом. Тут нам не надо разделять инженерный образец в виде собаки, как внешнего образа, с ее функциями. Нам также надо спроектировать ее функции и просто дать вот эти траектории этой собаке. Все равно, как мы дали ей тело. траектории, на которые она сможет опираться. Это не значит, что она только ими будет оперировать. Эти траектории будут развиваться, уточняться, меняться, так или иначе. Но базис, вот этот, Должен быть, потому что это на десятки порядков сократит просто время обучения, когда собака извлечет всю информацию из среды. А то, что она извлечет, она извлечет, потому что она заточена на то, чтобы устанавливать свое положение так, чтобы был максимальный рост количества полюсов, процента предсказаний и глубины контекста. 

S00 [01:27:14]  : Если собака оптимизирует, например, количество, точность предсказаний, я не очень понимаю последствия того, что она оптимизирует всё остальное, но если она оптимизирует точность предсказания, то её наилучшая стратегия – это лежать и двигаться. 

S03 [01:27:31]  : В этом случае перестанет растить количество полюсов, что неудобно, повторяет условие, будет сноситься шум. 

S00 [01:27:42]  : Как с точки зрения собаки выглядит ситуация с большим количеством полюсов, а как с маленьким количеством полюсов? 

S03 [01:27:49]  : Никак. Полюса должны постоянно расти. Если они не растут, то это неудовлетворительное состояние. 

S00 [01:27:56]  : Много пылесосов — это как бы много разных ситуаций, в которых она попадает. 

S03 [01:28:01]  : Не ситуации, а именно воспроизводимых цикличностей внутри собаки. 

S00 [01:28:09]  : То есть она пытается найти как можно больше всяких разных сложных закономерностей во внешнем мире, столкнуться с ними, верно? 

S03 [01:28:15]  : Да. 

S01 [01:28:22]  : Спасибо. Следующий вопрос от Владимира Смолина. Какие у модели параметры и как они изменяются в процессе обучения? В треугольных матрицах данные. А параметры можно описать? 

S03 [01:28:51]  : параметры Ну вот, параметры модели у меня следующие. Это глубина, шаг между полюсами, то есть разрешение, насколько мы можем себе позволить делать разрешение. плане символов, разрешения. тут у нас шаг между ними должен быть минимальный. просто я сделал, чтобы они совпадали один в один. коэффициент ослабления это тогда, когда у нас поле создался, он начинает постепенно стареть, стареть, стареть, стареть, стареть, стареть, стареть. с 10 тысяч он умирает. шаг закрепления — это когда у нас сумма на определенном контексте повторилась, мы просто инерцию ей накидываем. Инерция к ней пришла, она повторилась, мы ее загоняем. Ну вот, собственно, все параметры. Тут еще есть величина контекстов, то есть когда мы скидываем с нескольких датчиков в одну модель, в одну абстрактную модель несколько потоков значений, то у каждого потока значений должен быть собственный контекст, чтобы не мешаться другим. это тоже можно задать. вот собственно и все. количество полюсов у меня там забито. гвоздями 1000 полюсов на глубину, значение глубины. 

S01 [01:31:10]  : Здесь Владимир Смолин уточняет, что он спрашивал про обучаемые, а не про задаваемые разработчиком параметры. Я не знаю, что вы имеете в виду. Я могу предположить, что имеется в виду то, что, возможно, у вас называется коэффициентами. Сколько параметров у нейросети. У нас не глубокая нейросеть меряется количеством параметров. Сколько она параметров выучивает. 

S03 [01:31:47]  : может быть в этом контексте вопрос задается? тут к моей модели это неприменимо. сколько она памяти может потреблять. потому что это динамические процессы. а вот параметры... допустим, максимальная глубина контекста, у меня там, допустим, стоит 20, значение этого контекста у меня может быть 1000 полюсов. 20 умножаем на 1000, и так вот 19 на 1000, 18 на 1000, и так дальше все перемножим, сложим, получим число параметров, наверное, так. Спасибо. 

S01 [01:32:40]  : Спасибо. Тут Иван из Телеграм. Вернулся с еще двумя вопросами. Во-первых, просят пояснить, что такое полюса. 

S03 [01:32:49]  : Полюс – это сумма. на определенном диапазоне. Вот здесь они отмечены, эти полюса. Вот есть у меня определенная сумма, она сложилась фактически, пришла информация. И потом фактически сложился через вот этот шаг, а шаг равен глубине контекста. вторая сумма, вот после вот этой суммы мы записываем вот такой полюс, после вот этого мы записываем вот этот, после вот этого вот этот, и они потом друг за другом с определенной периодичностью вероятностью повторяются. статистика, статистическая вещь, полюс. но он может и не повторяться. вот здесь вот он повторился, и больше он никогда не встречается. а все это время, каждый из этих циклов, он отмирает, отмирает, отмирает, отмирает. и через 10 тысяч знаков умрет окончательно. но потом частичный знак может опять появиться, он опять создастся, опять встанет в очередь, отсортируется и будет ждать своей очереди. 

S01 [01:34:09]  : Вот это полюс. Спасибо. И дальше вопросы от Ивана. В продолжение темы про текст. То есть, берем часть текста, переводим в цифры и потом все эти цифры иерархически складываем. Верно? Берем правую диагональ и сдвигаем вход, а потом складываем опять все входы, учитывая предыдущую диагональ. То есть, на каждом шаге мы получаем треугольную матрицу. А что потом с этими матрицами делается? 

S03 [01:34:43]  : эти матрицы просто накапливают статистику, и эти матрицы могут быть пассивными, а могут как раз таки быть изменяемыми допустим на джойнте у нас есть треугольная матрица и вот по всей диагонали, если вот эти три условия будут, то есть углубился контекст, процент вырос и количество полюсов увеличилось, вот вся эта диагональ укрепляется условно. ей накидывается больше инерции. то есть робот хочет ее повторить, потому что дает ей больше импульса, потому что при вот такой конфигурации диагонали у нас выросли наши метрики, которые мы хотим увеличить. вот что с ними происходит. а с какими-то чисто реактивными матрицами, звук идет, мы со звуком сделать ничего не можем, просто его накапливаем, отправляем в матрицу абстракции и она просто служит нам для узнавания чего-то. там звук идет, мы можем узнать в нем какое-то состояние своих лап. Да, этот звук приводил к увеличению геометрии. 

S01 [01:36:22]  : Спасибо. И вот здесь Алексей Удот задает вопрос по поводу несчастной собаки, которая десятками лет лежит и монотонно двигает вперед и назад лапой. Вопрос звучит так. Как собака сможет перебрать все варианты, если она не может извлекать и обобщать закономерности и инварианты. Она же просто будет запоминать комбинаторный взрыв в сочетании своих джойнтов, пока у неё не кончится память. 

S03 [01:36:53]  : Ну, смотрите, тут комбинационный взрыв. Проблема присутствует, согласен. Но, во-первых, тут борьба с ним идет постоянно, потому что собака ловит именно ту конфигурацию, которая приводит к увеличению цикличностей и наращения данных. Это во-первых. Во-вторых, как я уже неоднократно упомянул, мы этой собаке, помимо самой собаки, мы должны еще и траектории дать, хотя бы шага какого-то, чтобы вот этот комбинаторный взрыв сократить. Комбинаторного взрыва тут как бы не то чтобы нет, он есть, просто идет очевидная борьба с ним алгоритмически, то есть наращение циклов. Вот и все. Собака не будет просто там лапы раскидывать в произвольном порядке, заполняя пространство возможностей. Там есть абстрактный слой, она будет как минимум делать это циклично, это как минимум. Поэтому вот эти циклы надо будет увеличивать, чтобы делать это лучше и, допустим, У нас там есть шаг погрешности. Так или иначе, сведется к тому, что собака начнет делать именно циклические движения, максимальные, приводящие к увеличению информации. Я не уверен, что у меня есть видео сейчас. Еще оно должно быть. С этой собакой. 

S01 [01:38:36]  : Пока вы ищете видео, тут еще нас слушатели озаботились ее судьбой. Регизар Талипов спрашивает, по какой причине собака должна научиться именно встать и пойти, а не, скажем, лежать и хлопать в ладоши? 

S03 [01:38:54]  : Повторяю, если она будет производить одно циклическое действие, лежать и хлопать в ладоши, рост количества полюсов остановится соответственно это не будет удовлетворять задачи вот но она же лежит и дам машет лапой вперед-назад у вас то есть это все равно что хлопает в ладони назад и машет лапой но у меня Количество полюсов нарастает только тогда, когда цикличности увеличиваются. Не просто цикл воспроизводится, а циклов внутри системы становится больше. Самих циклов, количество циклов. Вот так. То есть, там не цель – воспроизвести цикл, а цель – нарастить количество циклов. 

S01 [01:39:42]  : А тогда резонный вопрос. То есть, тогда, скажем, может оказаться, что от того, что собака встанет и пойдет, и будет монотонно идти, значит, у нее количество циклов и сложность поведения не увеличится по сравнению с тем, что она будет, допустим, лежать и лапами рисовать восьмерки или девятки вот почему она именно должна пойти и наоборот почему не она не должна или может быть она не и не должна эти может быть она должна там значит какие-нибудь фигуры фигуры вот вроде вихрей рисовать лапами ну смотрите вот то что вы говорите это не исключено правильно не исключено только части вашего сказанного то есть 

S03 [01:40:28]  : вихрь она может рисовать лапами, придумать язык глухонемых жестами, язык жестов придумать, но когда она будет просто идти и ничего с ней происходить не будет, но просто будет видеть один горизонт на вот этой модели, он не будет меняться ни в каких метриках, ни в метриках GPS, ни в метриках, вообще ни в чем, она будет вынуждена сменить свое поведение, потому что количество полюсов перестанет нарастать, то есть для нее нету новой информации, она просто идет. И начнется вноситься постепенно шум ее вот в этот цикл. то есть малым изменением вот в этот ее шаг будет вноситься определенная погрешность. погрешность будет вноситься до тех пор, пока у нее что-то перед глазами не изменится. и она это не превратит в цикл и не увеличит свое состояние цикла. А касательно того, что она будет там лапами делать вихри в какой-то цикличности, может, это не исключено, но повторяю очередной раз то, что с проектированием собой собаки надо ей проектировать и траектории. И вот если эту собаку поводить по этим траекториям, они для нее станут базисом, базисом, от которого она сможет отталкиваться, изменять свою траекторию и дальше. увеличивать информацию. если мы как инженера предполагаем, что в такой конфигурации наиболее оптимальные движения для нее будут очевидно циклическое перебирание лапами, вот это и будет как бы стартовым для нее крючком, который отсечет вот эту многолетнюю борьбу с комбинаторным взрывом. 

S01 [01:42:33]  : То есть, есть все-таки вариант, что собака нескорее научится языку жестов, чем пойдет гулять в ровном поле. 

S03 [01:42:44]  : Потому что это будет более сложная поведение. Смотрите, там механизм общий. На самом деле, в эту связку. компонент, который отвечает за то, чтобы собака разворачивала свое состояние, помимо роста процента прогноза, количество полюсов и глубины контекста, можно добавить превращение какой-то величины GPS, чтобы она не просто барахталась, а именно изменяла свое положение в пространстве или прочее. Просто те компоненты, о которых я сказал изначально, они дают максимальную степень свободы для развития. Но для этой степени свободы развития нужен базис. Тогда это будет максимально эффективно. но можно и без базиса обойтись, вводя в общую оценку какие-то координаты, или вот допустим, не обязательно GPS координат, а вот именно датчик с гироскопа, и мы говорим, что нам хорошо, когда мы лежим на брюшке, а не на спине. 

S01 [01:44:03]  : А вот тут интересно прозвучало, то есть мы максимизируем свободу для собаки или мы минимизируем ее? 

S03 [01:44:12]  : Ну смотрите, вот если мы дадим ей траектории, которые мы инженерно задумывали, то лучше, конечно, максимизировать степень свободы для ее максимально возможной оптимизации. Если мы какие-то констрейнты накладываем в эти три, метрики, о которых я сказал, то, соответственно, значит, мы это сделали намеренно, значит, мы по какой-то причине не хотим, чтобы она развивалась за рамками этих constraint, но это уже как бы желание человека, не желание модели. Модели достаточно трех вот этих метрик для развития. 

S02 [01:44:59]  : Спасибо. 

S01 [01:45:01]  : Коллеги, ещё есть вопросы или комментарии? Кроме того, что мы только что придумали ревард и то, что метрика любопытства – это правильный подход, но без обобщения, без ревардов это бесполезно. 

S03 [01:45:17]  : Это комментарии от Алексея Удода. Ну, смотрите. Путь наименьшего сопротивления это ревард или не ревард? Можно сказать, что это ревард. Где мы тратим меньше энергии, туда нам и нужно. ну да, наверное, ревард, но конкретно у меня вот в модели, чтобы она функционировала, и как вы видите, она функционирует, исходится на текстовых данных, вот у меня ревард такой, 3 метрики глубина контекста, рост процента прогноза и количество полюсов. Все остальное – это опционально, оно не нужно. Если мы говорим о собаке, то это наша воображаемая вещь, как собака должна двигаться. Но для моей модели это вообще желание человека ничего не значит. Он может там встать на голову и ходить вверх ногами и также будет получать информацию. и получать ее весьма эффективно. Положение собаки в пространстве и как мы хотим ее видеть, это наши проблемы, не проблемы эволюции. И если мы хотим эту эволюцию подогнать под наши метрики с определенными констрентами, мы будем делать реварт на эти метрики. Это уже опции, которые хочет человек. Но это уже как бы задачно-ориентированный подход, то есть какую-то мы задачу решаем этим constraint. А касательно в плане того, что сделать так, чтобы собака осознала то, что она хочет ходить именно на ногах, Притом никогда на них не ходил до этого. Это вот задача обратного порядка, как вот собрать энтропию в начальное состояние. Это трудно, значит, трудно попасть в состояние мозга человека. Те исследователи, которые занимаются этим, сейчас могут согласиться, что логика человека – это феномен, в который трудно попасть. От него можно оттолкнуться, либо пройти Такой же путь эволюции до состояния человека, если нам повезет, если мы каким-то образом невероятным пройдем вот этот комбинаторный взрыв и примерно попадем в состояние человека, то да, эта модель на это способна, только вероятность этого минимальна. Она может функционировать так же и лучше, чем человек, если она начнет развиваться с момента человека, понимаете. И так вот на этом устроены любые обучающие выборки. Мы даем нечто. нечто, которое уже, по нашему мнению, может содержать что-то хорошее. Допустим, языковые модели мы создаем, очевидно, что там есть что извлекать. То же самое и с этой собакой. Ей надо дать траектории, на основании которых она будет строить свой дальнейший опыт. сходимый, сходимый, вот как формально доказуемо, что он сходимый. Она не будет там расплываться, расходиться и делать так, чтобы информация в ней затухала. 

S01 [01:49:18]  : Спасибо. Ну и последний вопрос от Овин Овин. Есть надежда, что собака научится прыгать, заталкиваясь задними и приземляясь на передние лапы или бегать и на ходьё? Надо ли ей подсказывать? 

S03 [01:49:34]  : Ну, смотрите, тут вопрос тоже хороший. На самом деле, вот, как сказать, Зачем тут моей модели нужны абстрактные слои? Если мы дадим ей начальной траектории ходьба-неходьба, передвижения ногами, она в рамках процесса классификации классифицирует сама себя и будет отличать себя от других классов. И таким образом, смешивая вот эту… почему-то я использую именно такую архитектуру, потому что тут мультимодальность устроена. Вот если мы с видеопотока будем снимать какую-то аналогичную собаку или просто животное, которое в каком-то степени приближения будет повторять те же движения, вот эта собака может научиться от нее делать такое. А всё остальное, новое знание, оно будет рождаться только там, в рамках того, что внутри собаки происходит эволюция, проявляется какой-то шум. То есть вот только таким образом, если этот шум приводит к тому, что какое-то какая-то цикличность будет повторяться, этот шум будет циклирован, запомнен и воспроизводим. Если собака научится глядя на кого-то попрыгать, то научится. А если ей вообще никого не показывать, такое вообще возможно, то это будут пробы и ошибки, которые будут приводить к разным результатам. Может научиться, а может и нет. То есть если это будет так сложиться, что она научится, это будет приводить новую информацию, но этот навык, наверное, закрепится. 

S01 [01:51:40]  : Хорошо. Андрей, большое спасибо за доклад. У нас редкий случай, когда продолжительность сессии вопросов и ответов превысило время доклада. Большое спасибо вам и всем участникам. И я желаю вашей собаке добраться до тех далёких гор, которые виднеются на горизонте, подняться на вершину и открыть для себя море новой информации с этой вершины. Коллеги, всем спасибо. Андрей, спасибо еще раз и до новых встреч. 

S03 [01:52:13]  : Всем спасибо, коллеги. Всем до свидания. 

S01 [01:52:15]  : Спасибо. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
