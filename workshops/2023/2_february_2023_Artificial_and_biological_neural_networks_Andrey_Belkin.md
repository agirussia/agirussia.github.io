## 2 февраля 2023 - Искусственные и биологические нейронные сети - Андрей Белкин — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/PtSE6kwGips/hqdefault.jpg)](https://youtu.be/PtSE6kwGips)

Суммаризация семинара:

Семинар касается разработки искусственного интеллекта, близкого по уровню к человеческому. В ходе семинара были затронуты следующие ключевые моменты:

1. Масштабируемость искусственного интеллекта: Был поднят вопрос о том, как искусственные нейроны могут описать поведение живых объектов, таких как муравьи, которые успешно функционируют с тысячами нейронов. В контексте семинара обсуждалась необходимость искусственных нейронов для описания поведения муравья и других живых существ с ограниченным количеством нейронов.

2. Пример с головастиком: Был приведен пример исследования поведения головастика, которое требует около двух тысяч нейронов. Ученый, выступавший на семинаре, смог моделировать это поведение с помощью нейронной сети, используя всего 150 нейронов. Это демонстрирует, как сложные биологические функции могут быть имитированы простыми искусственными сетями.

3. Обучение нейронной сети с помощью метки и самообучения: Обсуждалась концепция обучения нейронной сети, где метка используется для определенных нейронов, а самообучение – для других. Это позволяет создать искусственный интеллект, который может самообучаться и адаптироваться к новым данным.

4. Программирование нейронной сети с использованием Python: Ученый представил код на Python, который позволяет обучать нейронные сети с помощью простого алгоритма, основанного на принципе победителей. Этот код нацелен на создание компактной и эффективной нейронной сети.

5. Анализ нейронной сети с использованием хай-боксового учения: Был описан метод обучения нейронной сети с использованием хай-боксового учения, где весы синапсов увеличиваются или уменьшаются в зависимости от совпадения и частоты работы нейронов.

6. Взаимодействие контролируемого и неконтролируемого обучения: Ученый подчеркнул важность баланса между контролируемым обучением, где используются метки, и неконтролируемым, где происходит самообучение. Этот баланс необходим для достижения высоких результатов в нейронных сетях.

7. Перспективы и проблемы искусственного интеллекта: В заключительной части семинара были затронуты перспективы и проблемы создания искусственного интеллекта, близкого к человеческому. Ученый выразил мнение о том, что перепрыгнуть через человеческий уровень развития искусственного интеллекта может быть невозможно, и что необходимо повторить некоторые аспекты человеческого интеллекта в искусственных системах.

8. Контактные данные и сотрудничество: В конце семинара были предложены способы для связи и сотрудничества с учеными, включая их контактные данные и группы в социальных сетях.

9. Сравнение с классическими нейронными сетями: Ученый сравнил результаты работы своей нейронной сети с классическими, такими как персептроны и сверточные сети, подчеркнув, что несмотря на меньшие результаты по метрикам качества, его сетей может быть больше схождения с тем, как работает биологическая нервная система.

10. План исследований и методы: Были обозначены планы и методы будущих исследований, в том числе обучение с подкреплением и детектирование нейронных сетей.

Семинар показал, что несмотря на значительный прогресс в области искусственного интеллекта, многое еще предстоит сделать для достижения человеческого уровня интеллекта.








S00 [00:00:03]  : И снова всем добрый вечер. Сегодня у нас в гостях Белкин Андрей, и он расскажет нам про искусственные и биологические нейронные сети. Эта тема регулярно обсуждается. Сегодня мы узнаем точку зрения Андрея, в чем же именно биологические и искусственные нейронные сети похожи друг на друга, в чем они отличаются. и какое значение это имеет для построения сильного или общего искусственного интеллекта? Андрей, пожалуйста. 

S02 [00:00:37]  : Здравствуйте, Антон. Здравствуйте, уважаемые члены, так скажем, группы Гей-Раша. Да, вот такая тема, искусственная и биологическая нейронная сеть. Часто возникают споры о том, является ли искусственной нейронной сети хоть какая-то частью АГИ, так сказать, можно ли делать АГИ без искусственных сетей или все-таки без них его создать нельзя. Ну, я вот склоняюсь к той точке зрения, что всё-таки нам надо ориентироваться именно на биологию в том плане, что у нас есть единственный пример интеллекта как такового — это вот пример интеллекта человека. И подражая, так скажем, интеллекту человека, Вероятно, мы быстрее добьемся результата в том плане, что создадим и полноценный искусственный интеллект. Прежде всего, меня зовут Белкин Андрей. Сейчас я выступаю как независимый исследователь искусственного интеллекта. Моя независимость выражается в том, что Это для меня, так скажем, хобби. Моя основная деятельность не связана ни с интеллектом, ни с биологией, ни даже с программированием. Но я этим занимаюсь довольно-таки давно, в рамках хобби, можно сказать, еще со школы. Какие-то свои наработки я обычно публикую просто в свободной форме на коллективном блоге Habr или Medium на английском, к примеру. Моё кредо в этом направлении – это то, что нужно не просто территоризировать, подкреплять свои идеи какими-то хотя бы простыми программами или моделями, которые демонстрируют хотя бы какие-то аспекты нашей теории, потому что зачастую люди склонны, так скажем, так устроен наш мозг, они склонны наши идеи считать как бы абсолютными и самыми понятными, так сказать, и самыми лучшими. Поэтому лучше все идеи проверять на практике. Да, много-много идей, так скажем, разбивается об объективный мир. Многое, что приходится, так скажем, похоронить в своих идеях, Жизнь учит быть более гибкими и адаптироваться к новым фактам, новым обстоятельствам. Один из примеров моей работы, которую я делал в 2017 году, это такая модель, Такая программа, которая представляет собой редактор конъюнктома головастика лягушки и симулятор головастика лягушки, который имеет тело, имеет среду, которая подражает жидкость. Редактор основан на довольно-таки простых нейронах, которые были подчеркнуты из знаний о биологии нейрона. То есть он достаточно простой, это простой сумматор, у которого есть утечка, так скажем, и Есть побудительные синапсы, есть тормозящие синапсы, есть моделирующие синапсы, достаточно простого нейрона, чтобы создать довольно-таки сложное поведение для головастика. Фактически текущий нейрон, он позволяет создавать любую логику. То есть это и был создан проект для того, чтобы продемонстрировать, что обычный нейрон, который мы все знаем, он позволяет делать очень сложное поведение. То есть, данная модель не предусматривает какое-либо обучение, мы просто создаем как бы безусловный рефлекс. Я нормально слышно? 

S00 [00:06:18]  : Да-да-да, нормально. 

S02 [00:06:21]  : Да. Собственно, почему почему я ищу альтернативные пути, почему я изучаю нейроны, изучаю всё, так скажем, с альтернативной точки зрения, а не прибегая, допустим, к классическим тем искусственным нейронным сетям, которые сейчас, так сказать, в мейнстриме. Для этого есть такая причина. В общем-то, существует понятие нейронная парадигма. целокупность всех научных знаний о нейроне. То есть мы достаточно много знаем о биологической клетке нейрон, мы знаем, как устроены мембраны, какие существуют там медиаторы, даже мы представляем, как устроены Ионные насосы, буквально знаем состав, как скручен этот белок, все он состоит и как выглядит, как работает. Достаточно очень подробно, до молекулярного уровня практически. Знаем, какие происходят антимические изменения, какие химические каскады. Любая биологическая клетка очень сложна, как Вселенная, но всё-таки основу знаний о том, как работает нейрон, мы уже имеем. И любое знание, которое расширяет Вот эта нейронная парадигма, она всегда зачастую отмечается Нобелевскими премиями. Просто зачастую, зачастую используют в своих работах многие, так сказать, спекулятивные версии работы нейрона. И, конечно, я не против, что нейрон может быть, ну, можно использовать спекуляции. Эта спекуляция все-таки помогает развивать науку, но прежде всего нужно уточнять, где здесь спекуляция, где мы все-таки касаемся нейронной парадигмы. Дальше, с другой стороны, у нас есть множество знаний о том, как устроены выше когнитивные процессы. То есть в этом плане нейрофизиологи тоже имеют много исследований. Мы знаем, какие есть эмоции, примерно как они работают, какие области за них отвечают. Мы знаем, какими областями мозга мы можем решать, допустим, разные задачи, какие существуют виды памяти. В общем, накоплено большое количество знаний. Но общей картины у нас нет, потому что у нас нет теории, которая бы объединяла эти знания в одну общую картину, и, казалось бы, такой теорией должны были быть и искусственные нейронные сети. Но на самом деле текущее вложение в нейронных сетях никак не отражает и не дополняет знания физиологии о нервной системе. То есть они как бы сейчас идут неким отдельной, отдельной наукой. Да, они в свое время были подчеркнуты из знаний о мозге, о нейронах, но сейчас они никак не дополняют эту картину, являются отдельной вообще наукой направлений. Давайте начнем с классических нейронных сетей. Я называю этот класс нейронных сетей, которые используют в своем обучении backpropagation. Мы начнем с простого примера. который позволит нам понять, что же все-таки делают искусственные нейронные сети. Этот пример, допустим, он состоит, мы возьмем сеть, которая состоит из двух частей, это энкодер и декодер. Энкодер – это та часть, которая от слоя к слою, будет уменьшаться количество нейронов. Декодер – это та часть, при которой от слоя к слою будет увеличиваться количество нейронов. Здесь показано условно это все, но будет пример связан с рукописным набором цифр в низ. мы можем его предъявлять одной части этой сети, а также предъявлять выходу и производить обучение. Самая узкая часть этой модели, этой сети будет составлять всего два нейрона. То есть после обучения такой сети мы получим то, что мы будем предприявлять на входе какой-то образ цифры, а на выходе мы будем получать примерно такой же образ цифры. То получается вот эти два нейрона, которые являются самой узкой частью, они фактически в двух числах будут кодировать большое количество представлений о цифрах. Идея здесь в том, что вот эта часть сети называется малое репрезентативное представление. Малое, потому что всего лишь два числа отражают достаточно большой вектор, входной вектор из 784 чисел. То есть нейронной сети удалось сжать всю представленную информацию в виде всего лишь двух вещественных чисел. То есть нейронная сеть, так сказать, создает некую модель мира, очень компактно, если учитывая, что миром, внешним миром для сети являются лишь вот эти вот числа, которые я показывал. Если мы разложим один одно значение нейрона по координате x, другого нейрона по координате y, то мы можем увидеть, что из себя представляет это малорепрезентативное представление. Его еще называют латентным пространством. Сейчас картинка эта раскрашена, но на самом деле мы эту раскраску уже получим потом, когда отбросим декодер и обученную уже с малым репрезентативным представлением сеть, обучим ее на предъявление классов. представив к ней обычный персидром, к примеру, то мы можем уже дальше разметить, что в этом малом представлении отвечает за какой класс. То есть здесь разными цветами представлены разные классы цифр. Что мы можем сказать об этом малом репрезентативном представлении? То, что Соседние точки в этом пространстве отвечают за очень похожие образы. Также то, что все эти образы очень тесно связаны. То есть если мы уберём один из нейронов, то сказать, что конкретно видит данный момент нейросеть, будет проблематично. Также известно, что все нейроны в процессе обучения, они вовлекаются в обучение. И чем больше количества нейронов, тем будет сложнее это представление. То есть если сейчас у нас два нейрона, то мы можем разложить на двумерное пространство. Если бы у нас было три нейрона, то нам бы пришлось представить это всё в трёхмерном пространстве. Также если бы у нас были более большие пространства, соответственно, более мирное пространство бы получилось. То есть нейросеть как бы сжала все представления, которые ей были предъявлены, все цифры, вот в такой компактный вариант. Понимая это, мы можем много что получить. К примеру, если нам нужно расплатнование лиц, то мы можем взять большой набор просто изображений лиц и предъявить подобно тому, как мы это делали с цифрами нейросети, и, к примеру, сделать Самая узкая часть не 2 нейрона, а 20 нейронов. Тогда мы получим в этом векторе из 20 вещественных чисел некое сжатое представление о всех лицах. Далее эту обученную сеть мы можем поместить, к примеру, в телефон. И при предъявлении какого-то либо лица мы можем получить в этом 20-мерном пространстве единичный вектор. Далее мы можем предъявить другое лицо и получить еще один единичный вектор. И в любом пространстве мы можем найти угол между этими двумя векторами и ввести какой-нибудь критерий, который обозначил бы, что Какой размер угла бы соответствовал тому, что это тоже самое лицо. Или, к примеру, мы можем взять большую обученную текстовую модель, взять генератор изображений, которые также в своей основе будут иметь малорепрезентивное представление. И дальше уже эти две обученные части нейросетей мы можем просто обучить на сопоставлении текст-картинках, и уже обучать не сами нейронные сети, а обучать именно матрицу сопоставления вот этого патентного пространства. И когда мы в следующий раз будем после обучения будем задавать запрос, промпт, мы будем обращаться прежде всего к некому репрезентативному представлению языковой модели. Далее мы, соответственно, преобразуем это представление представление генератора изображений и в результате получить какое-то изображение. Тем самым работают подобным образом сейчас популярные модели джорни, diffusion и тому подобное. Также мы можем взять большую языковую модель, которое прекрасно будет нам предсказывать продолжение текста, но плохо понимать, к примеру, контекст задачи или наши инструкции. И взять тогда уже, используя обученную модель, используя полученное представление сети, модель, к примеру, он будет иметь размерность 1024 вещественных чисел и создать другую модель, совершенно другую, которая будет обучаться уже с подкреплением, где будут критерии для обучения будет точность выполнения инструкций, генерации текста, и критиками будут выступать люди. И тогда мы сможем обучить эту модель достаточно хорошо, используя то малорепрезентативное представление, которое создали раньше. То есть мы будем правильно находить нужные точки в этом пространстве. Вот так, к примеру, устроена сейчас сеть GPT-3, чат GPT. В общем, так. Говорю к тому, что я часто слышу, что говорят, что нейронные сети – это просто некие аппроксиматоры. На самом деле, это уже давно так не считать, потому что все-таки нейронные сети создают некую малую модель внешнего мира, и стоит относиться к этому именно так. Известно, что человеческий мозг тоже не создаёт никакую модель мира, и как он её создаёт, как выглядит это малое репрессивное пространство в мозге. Давайте обратимся к этому. Прежде всего, изучать мозг достаточно сложно, мы не можем, взять и совокупно посмотреть, что в нем происходит, какие процессы, но есть некоторые способы, к примеру, инвансивные, когда мы можем буквально ставить электрод и узнать, на что реагирует тот или иной нейрон. Когда стали проводить подобные опыты, выяснилось, что Удивительная вещь, что мы можем детектировать, что нейроны могут детектировать конкретный образ. То есть, если бы мозг работал подобно нейронным сетям искусственным, то такое было бы невозможно. Этот феномен даже породил шуточное название «бабушкиный нейрон», потому что считается, что гипотетически в мозге можно найти нейрон, который бы отвечал за любой образ, в том числе и за образ нашей бабушки. Что значит вообще этот термин? Многие его неправильно понимают. Единственное, что важно здесь, что мы можем детектировать такой нейрон. То есть это не значит, что он всего один для бабушки. Это не значит, что он реагирует только на бабушку. Важен здесь именно факт детекции такого нейрона. Ну, во-первых, он никогда не бывает один, потому что, во-первых, к нему должно прийти откуда-то возбуждение, и приходит оно, это возбуждение от множества нейронов. Во-вторых, картина активности мозга, она представляет собой такое явление, которое называется разреженное распределенное представление. То есть нейроны, они активны, или даже не нейроны, а колонки, они активны некоторыми такими редкими распределенными паттернами. Это явление по-разному называют паттерны активности, нейронные ансамбли. И можно сказать, что за каждый такой паттерн активности может отвечать конкретный образ. То есть нейрон у бабушки, он не одинок. И если мы думаем о бабушке, то активизируется еще множество нейронов, которые как бы целокупно представляют некое представление о бабушке. Разрешённое распредельное представление в своё время активно популяризировал Трев Хокинс в своих моделях, но вот это явление, оно является в полной степени наблюдаемым и детектируемым в природе. начиная от насекомых, млекопитающих и практически все высшие когнитивные функции или нервные системы, они проявляются именно в такой форме активности. Что касается Джеффа Хокинса, у него есть замечательная книга об интеллекте. Она прекрасно описывает именно подход Джеффа, именно его стремление искать вот эти альтернативные пути, подобно тому, как это делается в мозге. Но его идея о таком сложном нейроне, артеом нейроне, по-моему, Она немного противоречит нейронной парадигме. То есть это, на мой взгляд, не совсем реалистичный вариант развития. Хочу еще обратить внимание на очень известную работу мозгозрения Кивилл Деда, тоже соновизила, которые занимались кортированием первичной зрительной коры, в основном они брали кошек, они обездвиживали их, они обездвиживали их глаза, кортирование происходило на живом мозге, Перед кошкой размещали экран, на который они проецировали разные отрезки, разные формы, и электродом буквально продвигались по коре. Они иногда заносили электрод перпендикулярно череп, так сказать, корея, иногда двигались вдоль и картировали тем самым, на что реагирует тот или иной нейрон. Эта работа в свое время, она поспособствовала созданию сверточных нейронных сетей, то есть была вдохновлена этими работами. Ну, и Hubel Data, они ввели некую концепцию, концепцию гиперколонки, колонки. Что значит гиперколонка? Гиперколонка – это или моды. Я буду дальше называть слово модой, потому что гиперколонка, она созвучна со словом колонка. И чтобы вам было понятнее, я буду использовать термин модой. Она представляет собой совокупность около 1000 колонок, которые имеют одно общее рецептивное поле. То есть обычно это небольшой участок на экране. Здесь, то есть, что включает себе этот модуль? Он включает в себе все варианты наклона линий, общее рецептивное поле и также в первичное зрительное коре смешанные образы из левого и правого глаза. Они смежены вот таким вот образом, похожи на какой-то раскрас зебры. Все это вместе смешивается. То есть, модуль имеет, главное, что нужно обозначить, модуль имеет общее рецептивное поле. То есть, фактически, Чтобы определить в маленьком области, как расположена контрастная линия, то мозгу требуется довольно большое количество нейронов. Совокупно, если одна колонка, колонка вмещается где-то 200 нейронов примерно в модуле, где-то тысяча колонок, то где-то 200 тысяч нейронов нужно на то, чтобы рассказать, в каком же все-таки наклон имеет маленькая область на экране. Я это говорю к тому, часто хотят оценить, какие вычислительные возможности есть у нейронов. И часто прибегают к тому методу, что берут общее количество нейронов мозги, среднее количество синаптических связей, перемножают и тем самым, грубо говоря, получают такое количество параметров. которые требуются, чтобы смоделировать мозг. На самом деле, вот такой пример. 200 000 нейронов, чтобы распознать, какой ориентации небольшая полосочка. Если аналогичную задачу ставить инженеру, и попросить его смоделировать какой-то либо нейрон или группу нейронов, которые бы определяли этот наклон, то, наверное, эта задача была бы свелась к одному нейрону. То есть если сопоставить математические модели и биологические нейроны, то может быть соотношение 200 000 к одному. Почему так много? Потому что все-таки нейрон – это биологическая клетка. Многие об этом забывают и считают нейрон чуть ли не неким суперкомпьютером. На самом деле природа выбрала, или эволюция выбрала такой путь, что выбирается, так сказать, массой. Давайте я уточню, я говорил о том, что данная работа, она способствует развитию сверточных нейронных сетей. Но давайте представим, как на самом деле как бы выглядела модель первичной зрительной коры. Дело в том, что, к примеру, Детекторы ориентации зрительных полосочек – это самые распространенные детекторы в первичной зрительной коры. Есть и другие. Есть, к примеру, детекторы концов линий разной ориентации, границ. Есть также детекторы сложных, которые реагируют только на перемещение. полоски, то есть в первичной зрительной коре достаточно большое разнообразие детекторов, но большинство все-таки это ориентация линии. Также Пытались картировать вторичную зрительную кару, но столкнулись с большой сложностью с тем, что определить, на какой именно признак будет реагировать нейрон, очень проблематично, если мы работаем со вторичной зрительной карой. Но мы знаем, что, во всяком случае, там тоже присутствует нейродетекторный подход. Давайте смоделируем первичное зрительное кору. Мы возьмем некоторые паттерны для детекции. То есть наш модуль, если гиперколонка состоит из 1000 нейронов, то наш модуль будет состоять всего из 16. таких нейронов, и он будет детектировать маленькую область на экране размером 5 на 5. И так как рецептивные поля пересекаются у зрительной системы, то мы будем двигаться по нашему входу шагом один. Для каждого модуля обозначим некоторую область, которая будет измеряться 4 на 4, 16 мировым, соответственно. Дело в том, что в этой области в модуле активность тоже подчиняется правилам разрешенного подставления. То есть, если одна какая-то колонка активна, то она оказывает ингибирующее воздействие на соседние, тем самым их подавляя, и активность конкретного детектора будет более сильной, чем ее соседей. Соответственно, Ну и границ между… в модулях, они в первичной изучительной корее не имеют четких границ, но наше преимущество в том, что мы все-таки можем математически задать эти границы более точно и определить вот это… ингибирующее воздействие, как заменить его правилом, победитель получает все. По сути, мы получим некоторую карту, которая будет иметь такое разреженное представление о том, видит, грубо говоря, глаз. То есть примерно вот так должна была быть выглядеть активность зрительной кары. Да, то есть она будет сохранять некую топологию, расположение, то есть верхний участок будет соответствовать верхнему участку. Плюс к тому, что это прямое преобразование, и мы можем, соответственно, произвести обратное преобразование и получить все-таки, увидеть, что качество немного падает, но все-таки мы можем различить цифру. Еще один такой момент, что активность, включая и зрительного канала, это нерв, который идет от глаза в мозг, она имеет постоянный уровень активности. То есть независимо от того, что в данный момент видит глаз. То есть либо это полностью закрытый глаз, либо это открытый глаз. и там светят него фонариком. Активность нервного пути, она постоянна. Это происходит за счет офф-ион клеток, которые одни клетки реагируют на свет, другие клетки наоборот активны, когда на них не падает свет. Они взаимно и влияют друг на друга, экипируют. В результате мы получаем входной уровень которые имеют постоянный характер. И это вообще характерно для мозга, целому стремиться к некому постоянному уровню активности. И в первичной зрительной коре мы тоже наблюдаем некую постоянность в этой активности. Так, собственно говоря, представляет собой модель первичной зрительной коры. Как видите, она немного отличается от того, что делают сверточные нейронные сети. Там нет, так скажем, того, что побеждает только один нейрон, и это значение становится единицей, а все остальные, грубо говоря, по нулям. Там эта активность как бы в зависимости от общей суммы получается. Следующее то, что Я долго искал алгоритм, который бы подошел для вот этого нейродетекторного подхода, чтобы, так скажем, дальше обучить сеть реагировать на конкретную активность. создать сеть, которая бы соответствовала принципу нейродетекторности. Долгое время я занимался сетями Кахонина, но в них были некоторые недостатки. Дело в том, что Если не применять к сетям кахони на обучение соседей или свести его совсем к минимуму, то возникает проблема суперпобедителя. Нейрон, получается, один нейрон, который убивается, так сказать, у лидеры и всегда забирает обучение себе. И еще то, что нейрон Кахонина несколько не соответствует нейронной парадигме, то есть в его основе лежит манхэттенская близость. И сейчас я нашел алгоритм, который Лучше для этого подходит. Изначально это был алгоритм, который использовался в спайковой нейронной сети. Он описан в этой статье. По-простому здесь был массив нейронов, которые активировался и возбуждал некий целый другой массив ингибирующих нейронов, которые подавляли все другие нейроны, и тем самым у нас наиболее близкий к входу нейрон, он побеждал и получал обучение, все остальные нейроны не обучались, тем самым При предъявлении этой сети, получалось так, что многие нейроны, они специализировались на определенной цифре. Если это реализация, то есть это была какая-то реализация, она написана на Python, мне не получилось ее запустить, к сожалению. Ну и вообще это была очень громоздкая реализация. Там учитывались спайки, то есть входной сигнал, он разделялся на частоты. Все это работало вместе с частотами, но Я решил сделать более простой вариант, и у меня получилось уместить это все в довольно-таки компактный код на питоне, который, можно сказать, выражается, само обучение выражается в очень простой форме. То есть тут работает принцип победители получают все, и если нейрон побеждает, то он в зависимости от входа минус 0.5 умножить на ставку. Так обучается. Почему 0.5? Потому что это некое среднее значение между 0 и 1. В спайковом варианте чем чаще были спайки, тем вероятность того, что произойдет обучение по HEBU было выше, соответственно, чем реже, тем обучение по HEBU было даже отрицательным. Тем самым вот эта простая сеть может обучаться, ей достаточно предвидят цифры и сформируется некий массив детекторов на определенные цифры. Этот алгоритм, он лишен проблемы суперпобедителя. Его особенность в том, что чем чаще нейрон обучается, тем он становится точнее и тем меньше итоговая сумма его будет получаться. Это приводит к некому недостатку. Здесь вы видите пример после обучения 10 тысяч примеров. Как видите, довольно таки можно различить многие цифры. Но если продолжить обучение и использовать уже 10 эпох по 60 тысяч примеров, то мы видим, что практически все нейроны стали детектором единицы. Это потому, что итоговая сумма для единицы самая маленькая. Это недостаток. Его можно устранить, к примеру, нормализацией входа. Лучший вариант для нормализации входа можно считать разрешенное представление. То есть, по сути, мы можем взять вот эту модель перипетичной зрительной коры и соединить ее с этой моделью, которая представляет более высокий уровень, которая создает кластеризацию. Да, и получить некоторое обучение. К сожалению, да, пришлось немножко модернизировать алгоритм, то есть тут добавилось несколько факторов. Во-первых, это элемент классического хай-боксового учения. В данный момент произошло совпадение, то мы увеличиваем вес. Если не произошло, то уменьшаем вес. Также у нас есть некоторый накопительный эффект. некоторого теплового эффекта, скажем. То есть если синапс сработал, у нас есть массив всех синапсов, который хранит насколько часто он срабатывает. То есть, это значение имеется от нуля до единицы. Если синапс сработал, то мы добавляем, допустим, 0.3. Если синапс вообще не сработал в любом таксе, то мы отнимаем очень маленькое число. Поэтому если синапсис часто срабатывает, то он имеет высокое значение. Если он очень крайне редко срабатывает, то он имеет значение близкое к нулю. Соответственно, некоторая такая статистика, которая тоже идет в подсчет веса. И это тоже своеобразное обучение по ХЭБУ, только одно у нас обучение идет как бы в моменте, в такте обучения, а другое обучение у нас идет, имеет как бы продолжительную историю, продолжительный фактор обучения. И также существует некая деградация, потому что у нас есть постоянное усиление весов, то нужна некая деградация весов, так как существует конкуренция между синопсами одного нейрона. В результате удалось создать нейронную сеть, которая имеет свою основную иерархию, то есть это многослойная сеть, и которая может самому обучаться. Ну, здесь вначале вход. Первый слой – это вот этот слой, Первичная изоляционная модель, которую я представлял. Рецептивное поле у каждого модуля будет иметь 5 на 5. Модуль у нас имеет размерность 4 на 4. То есть модули 5, повторюсь, это некоторое количество нейронов, которые имеют общее рецептивное поле. Следующий слой, он уже будет иметь разменность 80 на 80, то есть мы повторяем ту же самую структуру, то есть весь слой состоит из таких модулей 4 на 4, у которых рецептивное поле уже будет 20 на 20. Почему 20 на 20? Потому что с разреженным представлением, более сложно работать, потому что если у нас будет достаточно маленькая рецептивная пуля, то мы в нем можем засечь пару нейронов, не один нейрон активный. Нам надо больше иметь представление, больше захватить активных нейронов, поэтому это рецидивное поле больше для второго слоя. Грубо говоря, оно соответствует вторичной зрительной карьере. Следующий слой также имеет подобную структуру. Он также имеет рецидивный модуль, который также имеет размер 20х20. Но вот уже третий, который выходной слой, он уже будет иметь самое большое рецептивное поле и, соответственно, один модуль на весь слой. Соответственно, у третьего слоя нарисована, конечно, большая активность, но будет иметь, допустим, активность только одного. Ну и, соответственно, выходной слой. он нужен для того, чтобы сопоставить метку с самообучением. То есть все эти слои, они будут обучаться на самообучение, мы просто будем предъявлять цифры, они в итоге получат некоторое представление, подобно тому, как это делают Искусственные нейроны сети, о которых я говорил в начале, также здесь будет некое представление в итоге. Но здесь оно будет связано с активностью конкретного нейрона. И если провести такое самообучение на 5000 примерах, И выходной слой лишь будет только он обучаться с участием метки для того, чтобы сопоставить, насколько точно мы все-таки, какие образы мы выделяем. И как оказалось, что все-таки мы можем выделить даже на самообучение образцы цифр, классы цифр. настолько, что мы можем распознать дальнейшие цифры на 7-7% в среднем. Это мало, поэтому должен быть способ как-то это улучшить. Дело в том, что какой бы Хороший алгоритм не был для самообучения у нейрона. У него все равно ограниченное поле, рецептивное поле, и ему нужен некоторое общее направление, которое бы подсказывало, в каком направлении лучше обучаться. Общая цель и направление. В классических нейронных сетях сейчас по многому используется backpropagation. Он показал себя очень хорошо. Этот алгоритм фактически сделал нейронные сети на высокий уровень, поднял. Но проблема в том, что для backpropagation нет аналога, так сказать, в биологических нейронах. И вопрос здесь не в обратных связях, обратных связей, а мозги как раз предостаточно, слой к слою. Вопрос в том, что нет механики, не зафиксирована какой-то механики, во-первых, вычисление ошибки на нейроне, и во-вторых, ее передачи каким-либо образом. Дело в том, что известна, так скажем, общепринятая модель работы нейрона, то есть вычислительные Основные вычислительные процессы нейрон выполняет с помощью своей органеллы и мембраны. Правила, по которым он это делает, они определены, они есть в модели Ходжкина-Хаскли. И как бы здесь нет места back propagation, то есть То есть, да, есть множество статей, которые говорят о том, что вот есть вариант более биологического варианта, но это все несколько притянуто. Поэтому, ну, как я и сказал раньше, для нейронов нужен какой-то общий все-таки фидбэк. общий тренд для обучения, потому что просто на самообучении нейрон не сможет эффективно обучиться задаче какой-то очень глобальной. И был разработан алгоритм, который я назвал BackPreference, то есть некоторое некоторые преференции для нижележащих слоев. То есть мы здесь видим не модуль, и в модуле побеждает всегда только один нейрон, и это будет определено тем, какие веса в нижележащих нейронах, какие они будут более сильные, тот нейрон, соответственно, победит. С другой стороны, есть обратные связи, где мы можем тоже повлиять на итоговый результат, какой нейрон в модуле победит. Не всегда, короче, может победить тот нейрон, за который голосуют, грубо говоря, излечащие нейроны. Часто мы даем некоторые преимущества в сумме, грубо говоря, в итоге, для нейрона. Как это происходит? У нас есть метка, мы берем метку, и так как мы знаем, какие нейроны более вероятно приведут к этой метке, мы их, грубо говоря, помечаем. И также помечаем нижележащие нейроны, которые будут более вероятны тому, что способствуют победе тех, которые будут выбраны по метке. То есть здесь нет никакой ошибки, здесь просто те же самые связи того же самого типа, просто они немного добавляют к итоговой сумме на модуле. Это очень похоже. То есть мы таким образом как бы помогаем самообучению. То есть в основе все-таки лежит самообучение, но мы лишь подсказываем самообучению, какие метки мы сейчас видим. В данном случае это очень похоже на то, как обучается, к примеру, человек. Изначально мы много что видим, наблюдаем, а потом изредка у нас появляются некоторые метки на сопоставление модальностей. который говорит, допустим, что мы видим в данный момент, например, видим ли мы там кружку, мы слышим, что это кружка, и это лишь подсказывает тому, то есть у нас уже предварительно до этого был некий образ кружки. И очень важно, что как взаимодействует контролируемое обучение и неконтролируемое обучение. То есть сильнее всего влияние оказывает самообучение на тех слоях, которые ближе к входу. Соответственно, на те слоя, которые ближе к выходу, на них больше влияет самообучение, то есть метка. И в какой-то степени они как бы находят общее общие взаимодействия, общую точку решения, скажем. То есть здесь мы можем наш самый выходной слой обозначить, что 100% метка будет оказывать влияние на победу в модуле. Дальше мы можем обозначить, что лишь на 50% метка будет оказывать влияние на победу в модуле, а остальные 50% будет оказывать самообучение и в самых нижних слоях роль обучение по метке будет самым минимальным, а самообучение будет самым ценностным. В результате совмещения самообучения и контролируемое обучение, наш результат на 5000 примерах составляет уже 93%. Если сравнить это с тем, какой выдают результат классические нейронные сети, то, к примеру, я не нашел результата, который соответствует 5 тысяч, к примеру. Дело в том, что... Почему 5 тысяч, к примеру, почему не все 60 тысяч? Дело в том, что дальше 5 тысяч уже особо нет смысла обучать, там качество будет примерно варьироваться, то есть примерно 2-3% оставаться на прежнем уровне. Дело в том, что здесь нет никакой сходимости, здесь нет никакого контроля ошибки, здесь просто нейроны обучаются на некоторой своей статистике. Если сравнить с теми результатами, которые выдают нейронные сети, Классический персептрон, здесь результаты на всех 60 тысячах примерах с под 30 эпох. Результат, допустим, у простого персептрона – это 92,92%. У более сложных сетей со скрытыми сетями уже 98%. У сверточных сетей уже где-то 99%. Да, получается, что вот этот вариант из сети, он как бы не на самом высоком уровне, но здесь, как я уже сказал, Важны не метрики, получение каких-либо высоких метрик или конкуренция с нейронными классическими сетями, сколько пример того, как это работает в нервной системе, близость к тому, как это происходит в биологической нервной системе. Вообще, я думаю, что по метрикам качества классические вот эти нейронные сети с обратным распространением ошибки невозможно уже сделать что-то лучше, потому что математически эти сети, они, можно сказать, идеальные. Они подбирают те параметры, которые будут максимально соответствовать требуемому результату. И поэтому биться с ними как бы не имеет смысла. Но в чем плюс? В чем плюс той сети, о которой я рассказывал? Во-первых, К сожалению, пока я не дошел до исследований, связанных с обучением с подкреплением, потому что я думаю, именно на обучении с подкреплением эти сети предоставят интересный результат. Дело в том, что Есть известные, ну, есть некоторая модель или представление о том, как происходит подкрепление в человеческом мозге и вообще в мозге, так называемая приорная теория. Ну, о ней я, наверное, расскажу в следующий раз, когда у меня будет модель. Как я уже говорил вначале, я сначала проверяю все свои гипотезы, на некоторых моделях простых, а потом уже о них рассказываю. Еще один плюс, который есть у этих сетей, это легкая интерпретируемость. Дело в том, что интерпретируемость результатов в классических нейронных сетях, искусственных, Это большая проблема. Вначале я рассказывал про нейросети, где у нас всего два нейрона, имеют малорепрезентативное представление. Там мы можем увидеть, что значит каждая из точек в латентном пространстве. Но если этих нейронов много, то мы уже разобраться, что из себя представляет это латентное пространство, просто не можем. Плюс того вот этой сети в том, что здесь за каждое, то есть нейродетекторный подход за каждое представление отвечает отдельный нейрон. То есть мы можем взять буквально как электродом взять и посмотреть за что активность та или иная клетка. искусственный нерв. Здесь на гифках представлен обратный анализ, то есть в начале это выходной слой, мы поочередно активность происходит. В этой клетке в конце мы видим уже на что максимально тот или иной нейрон будет реагировать. Как видим, это некоторое представление чисел. Это уже средний слой. Выбраны некоторые случайным образом. нейроны, и мы видим, что этот нейрон уже реагирует на более какие-то сложные составные части цифр, а вот этот слой еще более ранний, он вообще реагирует на какие-то небольшие фрагменты. То есть интерпретируемое здесь достаточно высокое, мы можем Гипотетически, если мы обучаем более сложному чему-то этому сеть, то мы можем гипотетически выделить какие-то наши бабушкины нейроны, выделить какие-нибудь образы, образ человека, образ того, что этому человеку не нужно причинять вред. скажем, жестко их соединив в более высоком уровне, то мы можем внедрить некоторые принципы Айсика Зимова. Не причиняй человеку вред. Или вот сейчас проблема, допустим, у Google, с выпуском своей варианта чата, так как Google боится то, что сеть будет выдавать компрометирующие результаты, к примеру, какой-нибудь оскорбительной форме. то мы можем, допустим, если у нас все-таки есть нейродетекторность, выявить некоторые нейроны, которые отвечают за эти или иные результаты, мы можем их, грубо говоря, заблокировать и не обучать сеть с нуля. Или искать в обучающей выборке, что те данные, которые нам могут испортить результат. Ну, собственно говоря, причем тут, грубо говоря, агит. Я хочу писать мою маршрутную карту, мою маршрутную карту исследования, мой план того, что я делаю. Дело в том, что, допустим, вот этот пункт, нейросеть с детекторным подходом. Главное, чтобы была сама иерархия и аналогом бэкплопа. Я достаточно много на это потратил времени, чтобы создать такой вариант сети. Более трех лет я конкретно решал эту задачу. Следующая по плану в моем проекте исследование, это достаточно популярная задача игры Pong, где я уже могу продемонстрировать обучение с потреблением, как будет происходить на данной сети, вот это вот обучение с потреблением. Ну, так как в фонге есть некоторая двигательная активность, то также продемонстрирую некоторую модель машичка, о ней я уже писал, но сейчас не рассказывал. Есть еще в модели с подкреплением идея того, что такое новизна. Дело в том, что в такой сети очень легко оценить критерий новизны. Если у нас есть какие-то детекторы, то мы можем оценить, насколько информация наступающая в сеть, она близка к ранее предъявленной. То есть это той памяти, которая обладает сеть. То есть, грубо говоря, если у нас сеть обучалась, к примеру, на цифрах и внезапно ей предъявить, к примеру, букву, то в любом случае она найдет какой-то детектор, потому что у нас есть принцип победитель. всегда есть один победитель. То есть какую-то близкую цифру она найдет, но активность ее будет значительно меньше по сравнению с той, если бы мы предъявляли число. Поэтому можно оценить, насколько новая информация, насколько релевантны детекторы, которые на нее реагируют. Но это связано с тем, что с вопросом о том, какие должны быть промежуточные цели в вопросе обучения, если цель победы достаточно далека. То есть она может быть и анализма. То есть это очень важный момент для обучения, потому что потому что без нее сложно достигнуть цели. Следующий этап это будет так называемая этап карточки домены. Это карточки, которые мы будем предъявлять сети. И сеть будет… карточки эти представляют собой картинки детские с подписями. К примеру, наша сеть должна будет… мы будем предъявлять ей картинку и говорить, что это. И сеть должна как бы… Запомни, так как наша сеть, как я уже говорил, пример обучается всего лишь на небольшом количестве, 5000 примеров, и она уже достигает 93% результата, то небольшое количество примеров должно и привести к запоминанию примеров. На этом этапе уже я затронул не только эту сеть, будет еще дополнительные сети, которые будут отражать кратковременную память, ну и, соответственно, мультимодальность. Ну и следующий этап – это вопрос речи, так как речь – это сложное моторное действие, прежде всего. то и обучаться ему нужно как некому моторному процессу. То есть мы ему и собственно обучаемся как некому моторному процессу, мы жестикулируем языком, ртом, мимическими мышцами, создается звук, мы его воспринимаем, обучаемся тем самым, как производить этот звук. То есть это все постепенно и в принципе такой момент. Ну и Дело в том, что я разделяю эти этапы на некоторые области. Первые этапы – это зрительные анализаторы. 

S00 [01:13:29]  : Андрей, извиняюсь, что вперебил. У вас по плану сколько еще осталось? У нас, наверное, уже минут через 10 нужно будет… Я закругляюсь. Окей. 

S02 [01:13:47]  : Ну, собственно, мой план заканчивается тем, что прежде всего мы должны как-то создать искусственный интеллект близкий к человеческому уровню прежде, а потом мы можем достигнуть цели. Дело в том, что мне кажется, что перепрыгнуть через человеческий уровень, просто говоря, невозможно. В любом случае, мы должны повторить некоторый вариант интеллекта человека. Этот слайд говорит о том, что да, вот сейчас вот очень популярные нейросети, которые используют обучение SB Propagation, вот это плотное представление, оно сейчас очень хорошо себя показывает. Конец 2022 года дал нам несколько прорывов в области генерации изображений, в области текстовых моделей. И, казалось бы, надо еще немного поднажать, и мы достигнем, так скажем, уже человеческого уровня. Но, на мой взгляд, все эти сети, они как бы мы с ними попадаем в некий локальный минимум. А сети, которые имеют, допустим, нейродетекторный подход, пускай они сейчас показывают очень скромный результат, то в перспективе в будущем, я думаю, именно они позволят нам достигнуть человеческого уровня и выше. Также хочу представить то, над чем я сейчас работаю совместно с некоторыми коллегами. Тоже это проект на личной инициативе. Мы делаем некоторый такой веб-редактор коннектома, которым позволит управлять роботом любой сложности. В перспективе это пока просто безусловные рефлексы, просто способ программировать робота на простейших задачах. В перспективе это как развитие такого направления вплоть до создания в будущем некой операционной системы для роботов, в котором будет и обучение, и много о том, о чем я сейчас рассказывал. Сейчас этот проект идет, так скажем, на личной инициативе. Хотелось бы все это ввести в более качественный уровень и реализацию, поэтому если будут предложения по инвестициям, пожалуйста, пишите. Ну, собственно, у меня все. Спасибо за внимание. 

S00 [01:17:32]  : Спасибо. Спасибо, Андрей. Вот тут я как раз замешкался, дописывая вопрос на уточнение. Я правильно понимаю, что то, что Вы показывали вначале, и головастика, и вот эти последние товарищи-шестиноги, которые сейчас на кадре – это не обучение, это просто программирование конъюнктома с заданием, грубо говоря, графа и весов соединений с ручным подбором этих самых соединений для того, чтобы обеспечить правильное движение головастика гиба вот этого шестиногого существа. 

S02 [01:18:15]  : Да, правильно. То есть природа, так скажем, создала некий такой универсальный нейрон, который можно с помощью нее программировать. Грубо говоря, программировать животное или робота. То есть да, сейчас это просто редактор комиктома, который речным подбором можно создать аналогичное поведение. 

S00 [01:18:46]  : Я правильно понимаю, что с точки зрения той дорожной карты, которую вы перед этим показывали, вот этот проект, он находится не сверху и не снизу, а он как бы находится сбоку? 

S02 [01:18:58]  : А он был раньше. В основе было понять, как вообще работают нейронные диалоги. То, что мы видим, вот самые примитивные животные, как они работают, у них же тоже есть нейроны, и это тоже этап эволюции, грубо говоря. 

S00 [01:19:24]  : А вы видите, как вот эти проекты пересекаются или могут пересекаться? 

S02 [01:19:31]  : Да, конечно. Дело в том, что мы, собственно, этому примеру. У нас есть и безусловные рефлексы. В основе всего лежат наши безусловные рефлексы. Наше тело подчиняется большинству безусловным рефлексам. А потом у нас, как настройка, существует такой высокоразвитый мозг. которые могут, если нужно, подавить эти безусловные репрессии или, наоборот, дать им больше воли. И, то есть, есть эта система, как бы, изначально мы будем... хотя бы возможность хождения этого робота, примитивная. Дальше мы можем сюда, потому что здесь в основе импульса, импульсная сеть. А ту сеть, которую я представлял, в основе нейро-детекторов. То есть, по сути, тоже активность одного конкретного нейрона. Это очень легко совмещается. 

S00 [01:20:36]  : Понятно. Спасибо. И вот тут вот сразу еще был вопрос. Недавно вы говорили про 93 процента. Это имеется ввиду на обучающих или тестовых данных? 

S02 [01:20:47]  : На тестовых нет. Это все как у настоящей нейронной сети. На тестовых тестировали 10 тысяч примеров. А тестовые как датасети, иммунисты. Все как положено. 

S00 [01:21:01]  : Спасибо. Еще вопрос из Ютьюба. Сейчас я вот там не очень его понял. Сейчас посмотрю в Ютьюбе не уточнили ли его. Ага, вот значит сейчас. Давайте я сейчас просто текст сброшу, который там в Ютьюбе нам написали. Сейчас. Пока я копирую текст. Вопрос, что называется, как никогда актуальный в последние дни. Как вы относитесь с точки зрения вашего, так сказать, багажа к последним достижениям ЧАД ГПТ? Насколько это близко или далеко от того, что в конце вашей дорожной карты? 

S02 [01:21:50]  : Отношусь я к этому очень положительно. очень хорошая, как с точки зрения интеллекта. Я уже обозначил, что это просто использование этого малого репрезентативного представления, которое сформировало часть GPT-3. Потом, значит, с помощью обучения с подтуплением просто научились правильно управлять этим представлением. То есть здесь нет обучения, похожего на свободное или человеческое. То есть нам сначала потребовалось большое количество данных, а дальше мы Мы не можем, то есть получается неравносеть это, то есть у него какие-то данные там до 2019 года, и какой-то новой информации там не будет. Да, можно с помощью чата, то есть это тоже удобный инструмент, который позволит увидеть контекст. Мы можем использовать весь чат, который всю историю сообщений, как новый промпт. И это позволяет создать некую возможность, иллюзию, наличие некого контекста общения с этой сетью. То есть как такового, если вообще сравнивать с моим планом, то я уже говорил, что это, я считаю, что это тупиковое направление. Если нет внутренних мотивов, нет внутренних целей, то мы не сможем с ним общаться как с другим человеком, другим субъектом. А у этой модели, у этой системы нет внутренних мотивов. Это всего лишь алгоритм. 

S00 [01:24:01]  : Спасибо. Дальше. Там вам пишут, что есть у коллег пересекающиеся темы, в том числе по финансированию, как связаться. Но я так понимаю, что контакты все есть. Соответственно, можно посмотреть. Вы можете в группу написать ваши контакты, если желающие захотят. 

S02 [01:24:22]  : Если по теме движка. то есть почта, которая сейчас на экране, да? Да-да. Также у меня есть почта, есть мой телеграм-канал, есть канал на ютубе, ВК группа. Там я выкладываю некоторый ход разработки своих Я выкладываю достаточно редко, потому что это сопережено все-таки с ходом разработки. То есть это не происходит очень быстро, поэтому информации там не очень много. Но все-таки, если вы хотите следить за ходом разработки, пожалуйста, я всегда готов отвечать на вопрос. 

S00 [01:25:17]  : Спасибо. Еще у нас есть вопрос, тоже в YouTube. Были работы по полной симуляции работы кластеров биологических нейронов. Это требовало колоссальных вычислительных мощностей. А недавно была работа, где было показано, что можно на наборе этих данных симуляции обучить обычную нейронку, которая будет давать такой же ответ на входные сигналы, как и симулированный биокластер или отдельный нейрон. К сожалению, ссылка на работу не дается, но я так понимаю, речь идет о том, что промоделировали нейроны настоящие, а результат получился точно такой же, как на обычной сетке. 

S02 [01:26:07]  : Да, есть много работ. Вообще нет общего идеи, представления о том, как работает мозг. Поэтому есть масса разношущных статей и работ, которые Часто даже плохо между собой сочетается. Что, какой активности они там фиксировали, какой результат. Это надо все подробно смотреть и тогда делать какие-то выводы. 

S00 [01:26:51]  : Спасибо, коллеги. Еще есть вопросы какие-то или может быть комментарии? 

S01 [01:26:57]  : Здравствуйте, можно вопрос задать? Да, конечно, пожалуйста. Меня хорошо слышно? Да, нормально. Спасибо большое за конференцию, за лекцию. Я так понял, Андрей больше практик. Это здорово. У меня вопрос, собственно, про масштабируемость. Если вернуться в самое начало конференции, вы сказали, что есть некий масштаб и как бы биологические нейроны в этом плане проигрывают искусственным нейронам назвали цифру 200 тысяч допустим условные цифры я понимаю вот мне вот просто что вот интересует более конкретно вот мой вопрос звучит так вот если мы берем из практики даже живые объекты биологические то очень много таких биологических объектов у них нейронов немного ну там тысячи может быть нейронов и этот объект как бы живет и удачно живет на протяжении многих миллионных лет и вот если взять допустим обычного муравья у него раз примерно 200 тысяч нейронов да И просто вот на вскидку, как вы думаете, сколько нужно искусственных нейронов, чтобы описать его поведение? Ну, приблизительно, если бы была такая практическая задача. Ну, понятное дело, что не один, конечно, нейрон. Ну, наверное, много. Вот, собственно, вопрос про масштабируемость. Можете поподробнее немножко рассказать? 

S02 [01:28:33]  : Но вот конкретно муравья, пока нету такого у меня соотношения. Вот на примере головастика, головастик лягушки. Его морторная деятельность, то есть вот это гармоничное плавание, гармоничное движение, они требуют около около двух тысяч нейронов. Две тысячи нейронов. Я повторил эту модель гармоничного плавания. Грубо говоря, у меня совокупно всего получилось 150. Сколько? 150. 

S00 [01:29:17]  : А можно тогда сразу, извините, что я перебиваю вопрос, вот функционал этого головастика какой? Он только плавает и то, что он ест, это он случайно находит, натыкаясь? 

S02 [01:29:28]  : Нет. 

S00 [01:29:28]  : Можно тогда функционал головастика в двух словах описать вот в этом месте? 

S02 [01:29:33]  : Да, сейчас. Ну то есть у него есть тело и есть рецепторы касания к телу, то есть прикасания к буртику. Там есть касание с помощью такого инструмента, как палец. Вообще это в открытом доступе, правда, для Windows скомпилировано, есть исходный код. Но есть еще глаз некий, скажем, вот тут лампочка горит, которая которые представлены в коннектоме как один нейрон, который, если лампочка зажжена, то он спайкует постоянно. Если нет, то ничего не происходит. Дело в том, что настоящий головонастик тоже имеет такой подробный класс на спине. Если он находится не в тени, то он стремится куда-то уплыть. И я могу создать такой рефлекс, который бы при включенном свете заставлял его куда-то уплыть. Во-вторых, чтобы отскакивать, то есть отплывать от буртиков, он должен в ту сторону крепсти сильнее. Соответственно, у него есть рецепторы при касании бутика. Также пища. У него есть два рецептора. Если пища где-то правее, то есть квадрат расстояния от условной пищи, что сильнее работает правый рецептор, если левее, то сильнее работает левый. То есть, чем ближе пища, соответственно, тем сильнее это активность этого рецептора. То есть, есть рефлекс, который, опять же, составлен, чем... То есть, во-первых, он плавает, и если активнее правая сторона, то заставляет его грести сильнее в крови, поэтому он маневрирует иногда. Также у него есть некоторый рецептор голода. Чем больше голод, тем сильнее он активен. И рецептор усталости. Чем больше он движений совершает, тем больше накапливается усталости, чем, опять же, активнее нейроны. Соответственно, его поведение меняется. То есть, если он сильно устал, то он, скорее всего, остановит свое движение. Но если голод превалирует, то он опять начинает свое движение в поисках пищи. То есть, у него включается опять же чувствительность к этим рецепторам лево-право, и он опять начинает маневрировать, ища пищу. Грубо говоря, еще есть у него защитный рефлекс. Мы можем по клавише F5 его зажать, условно, и он будет совершать гребки в обратную сторону. пытаясь выкрутиться из этих защит. Достаточно широкий спектр рефлексов, которые я сам знал в головастике. Достаточно хорошо изученный рефлекс. 

S00 [01:33:06]  : Я правильно понимаю, что то, что яркое пятно в середине, это Старр предпочитает его избегать. Это как раз тот самый свет. 

S02 [01:33:16]  : Нет, это условно. Светом условно. Если он включен, то он... У нас рецептор на свет один. 

S00 [01:33:25]  : Соответственно, мы не знаем, где... Сейчас в данном видео это не используется, правильно? 

S02 [01:33:31]  : В данном видео – да. Спросил – нет. Здесь показано его сложное поведение. Вообще есть статья более подробная. Вы просто можете в гугле набрать OpenTable и прочитать. Открытый головастик, грубо говоря. 

S00 [01:33:53]  : По-русски или по-английски? По-английски. 

S02 [01:34:00]  : К вопросу, собственно говоря, почему 2000 требуется, Я использовал всего 100 нейронов на движении. Дело в том, что мой нейрон работает точно, он математическая модель. Он не сбивается, нет риска того, что он исчезнет куда-то, повредится и так далее. Логические нейроны. и вообще нервные клетки, они склонны вообще достаточно быстро гибнуть. Бывает, что и головастика можно повредить, и повредить нервные цепи. И если, допустим, у него не сработает даже один какой-то, то есть если в моей схеме не сработает какой-то один нейрон, то движение прекратится, он не сможет двигаться. То нервная система головастика очень стабильна. его можно, грубо говоря, даже часть нейронов погубить, то он все равно останется на плаву, он все равно найдет способ двигаться. То есть избыточный здесь вопрос не логики движения, а логики надежности и стабильности самой нервной системы, головастик. 

S00 [01:35:24]  : Спасибо. Вот еще есть вопрос Андрея Ершова. Насколько устойчива классификация к мастерстабированию сдвигу изображения другим преобразованием, которых не было в обучении? 

S02 [01:35:41]  : Особенно как и классические нейронные сети. какой-то стабильности такой явной, то есть нельзя, допустим, сильно повернуть цифру и надеяться на то, что тот же самый нейронный детектор на неё отреагирует. Нет, таких сказок не бывает. В принципе, я думаю, что это невозможно, просто есть много иллюзий о том, что вот человеку показали в одной форме цифру, и он ее может видеть в разных аспектах и в разном масштабе. На самом деле, я думаю, это не так. То есть человек довольно-таки много обучается, допустим, много долго обучается и чтению, и распознаванию, прежде чем он видит эти цифры в разных, так скажем, аспектах, в разных наклонах, в разных способах начертания. Нет, какой-то такой супервозможности у этих нейронных сетей нет. 

S00 [01:36:52]  : Спасибо. Андрей, это был ваш вопрос? У вас рука была поднята еще. Видимо, да, это был вопрос. Я правильно понимаю, что вот этот головастик – это спайковая сеть, да? Да. 

S02 [01:37:11]  : Это обычный нейрон, на статье будет подробно описано, у которого есть общая сумма, у которой есть постоянная утечка. Любой побудительный тормозящий синапс меняет эту сумму. Итогу есть порог. После активации, после преодоления этого порога, нейрон вступает в некоторый рефракторный период. То есть, на некоторый период не отвечает на раздражение. Ну, собственно, и все практически. Есть еще модулирующий вид синапса, который может менять порог. То есть, эта дополнительная опция, она как бы позволяет создать много вариантов. Вариативность. Допустим, она работает в маневрах. Обычное гармоническое движение. Очередно активизируется дальняя сторона. Если мы промодулируем одну сторону, то нейроны будут спайковать чаще. Потому что порог будет ниже. За счет этого происходит, ну вообще эта функция очень сильно помогает в реализации логики какой-либо. То есть достаточно вот эти функции, они описаны в биологии, ничего нового здесь как бы в плане нейрона нет, ничего лишнего. Все это достаточно просто и примитивно. 

S00 [01:38:51]  : Скажите, пожалуйста, я вижу, что мелькают картинки по топологиям вот этих нейронных сетей. Мне кажется... То есть, вот точки – это нейроны, которые разного цвета? А, это нейроны. Окей. Хорошо, понял. Спасибо. Да, вот что я хотел спросить. А вот, собственно, саму топологию, получается, Вы подбираете вручную? То есть, там нет такого, что есть некоторая регулярная сетка, и адекватная топология определяется в результате обучения? То есть, Вы именно подбираете эту сетку вручную? Да, подбираю. 

S02 [01:39:38]  : тех гипотез, как устроено, допустим, в реальных животных. В основе вот это есть так называемый генератор периодической активности. Это группа нейронов, которые просто можно запустить, и она будет спайковать постоянно. Можно остановить, и она перестанет спайковать. Вот это в основе генератор. Генератор уже может создавать вот эту... разделить на две фазы. это возбуждение с помощью другой сети. То есть есть некоторые паттерны, паттерны проектирования нервной системы, и они в нейрологии очень часто повторяются, эти паттерны. Я их могу выделить, но это не та тема, какие паттерны есть. 

S00 [01:40:22]  : Спасибо. Коллеги, есть еще вопросы? Может быть, комментарии какие-то? Нет? Ну, хорошо. Если вопросов и комментариев нет, то, Андрей, большое спасибо. Вам доклад. Очень интересно было увидеть редкую в нашем сообществе практическую работу. Ну и можно пожелать вам успехов в движении по вашей дорожной карте с поиском инвесторов и интересантов, заинтересованных в работе над пересекающимися темами. 

S02 [01:41:06]  : Спасибо. 

S00 [01:41:07]  : Спасибо. И всего вам доброго. Коллеги, всем спасибо и до свидания. 

S01 [01:41:14]  : Спасибо. 

S00 [01:41:16]  : Спасибо, до свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
