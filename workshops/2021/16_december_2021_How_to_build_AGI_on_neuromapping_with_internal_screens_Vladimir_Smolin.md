## 16 декабря - Как построить AGI на нейрокартировании с внутренними "экранами" - Владимир Смолин — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/-QqUSMwCUCM/hqdefault.jpg)](https://youtu.be/-QqUSMwCUCM)

Суммаризация семинара:

Семинар касался темы сильного искусственного интеллекта (СИИ). В ходе семинара были затронуты вопросы кортирования и экранов в контексте нейронных сетей и их работы. Особое внимание было уделено процессу обучения без учителя, который подтверждается примерами автоэнкодеров, GAN и BERT. Участники обсуждали пути совершенствования алгоритмов и методов моделирования, в частности, в контексте построения нейросетевых карт.

Был поднят вопрос о том, как в системах СИИ формируются новые понятия и как происходит их накопление. В дискуссии участвовали такие идеи, как многопараметричность и физика открытых систем, которые могут предложить новые подходы к моделированию сложных систем и их взаимодействий.

Важной темой стали экраны как средство оценки и понимания моделей в контексте их валидности и корректности распространения знаний по нейросетевым структурам. Участники обсуждали вопросы обоснованности и оправданности использования экранов как моделирующих инструментов.

Также были затронуты темы управления работой нейросетевых моделей, а также вопросы управления режимами обучения и функционирования, которые могут быть обусловлены различными намерениями и целями.

В ходе семинара были подняты вопросы о том, как формируется и поддерживается обучающий процесс без наличия учителя и какие стратегии обучения обеспечивают возможность любой системы американского штата Айдахо (автоэнкодер с примером Филлиса Кассона как уникального обогащения) успешно функционировать без учителя.

Организаторы семинара упомянули о необходимости развития тематики сильного искусственного интеллекта с точки зрения его использования в различных сферах и разработка новых методов и технологий для повышения эффективности его работы.

В целом, семинар подчеркивал важность кросс-дисциплинарного подхода и интеграции множества методологий и теорий для работы над проектами, направленными на развитие искусственного интеллекта к уровню человеческого разума.

Обсуждение также затронуло вопросы о том, как мы достигаем определенной степени успешности в функционировании систем сильного искусственного интеллекта и какие инструменты и методы стоят за этим успешным опытом обучения и взаимодействия системы с окружением.







S00 [00:00:00]  : Да, открыть. 

S03 [00:00:01]  : Да. И мы в эфире, поэтому, Владимир, пожалуйста, вам слово. 

S00 [00:00:07]  : Хорошо. Так. Чтобы она вся вошла. Будет чуть помельче, но зато она вся войдет. Вот. Спасибо, что мне предоставляется такую возможность. Рад, что хоть несколько человек на сет пришли меня послушать. Значит, планы у меня, конечно, были большие. Про многое все рассказать. Там был ряд проблем, которые большие. Но в целом, значит, весь рассказ, он как бы про кортирование и про экраны. Ну и, значит, больший упор я вначале сделал вообще, зачем нужно кортирование, потому что и так же есть нейросети, которые хорошо работают. И есть вот нейросетевая революция, которая провелась с замечательным успехом, и я ее поддерживаю, и все хорошо, но, значит, мое, собственно, мнение стоит в том, что можно еще быть лучше. Вот, ну и собственно, когда я расскажу про нейрокортирование, значит, дам как бы немножко отдохнуть. В общем, я немножко там подзанимаюсь, в смысле, обсужу немножко о том, что... Подходы очень разные к тому, как вообще строить сильный искусственный интеллект, и чем как бы отличается... Ну, то есть не то, что я заявляю, что мой подход самый правильный, но все-таки как-то вот разделяться, что есть там одни подходы, есть другие, кто-то верит в одно, кто-то верит в другое, и, собственно, постараться легко, что я верю. Ну и, собственно, дальше про то, какую функцию, собственно, должны выполнять экраны, как они связаны с корректированием, Ну и наконец, собственно, тут разделение труда, это авария была со слайдингом, это старая презентация, которую удалось восстановить. Давайте, собственно, про картирование. О чем, собственно, доклад? Это про то, что есть некоторый входной сигнал x, мы его как-то воспринимаем, и воспринимаем мы его в нейронных сетях некоторой активностью скрытых элементов. соответственно есть вот это вот их вектор активности этих скрытых элементов он как-то меняется в некотором пространстве и вот это вот значит вектор разметают некоторую область ну то есть там в разной степени плотности но значит вот он значит не во всех возможных состояниях бывает если мы управали на вход ну даже если бы мы подавали на вход белый шум, то все равно, значит, во-первых, у нас бы не было бесконечной активности, во-вторых, там еще какие-то свойства могли проявиться, все равно вот, собственно, эта карта, в смысле, эта вот область, которую он заметает, она не, как бы сказать, не представляла все возможные активности, которые можно себе представить для элементов внутреннего слоя, или, скажем так, не все их комбинации. Вот. Ну и, собственно, вот это вот, как бы сказать, область, которая реально реализуется в активности нейронов, она может быть как-то представлена, описана, отображена на другие карты, и с этих карт может быть считано некоторое воздействие. Сейчас весь упор в обучении нейросетей делается на то, чтобы научить их выдавать правильный выход. потому что мы подаем то, что подаем на вход, а мы сравниваем, какой получился выход. Если он получился не такой, как мы хотели, мы с помощью функции обратного сравнения ошибки изменяем параметры сети с тем, чтобы выход получился нужным. А то, что там как-то... Вход влияет на карту, но вообще говоря, с х мы не сравним. Есть какой-то х и есть. Мы, собственно, считаем по этой разности х. Но в целом есть представление про каждый нейрон, что он осуществляет некоторые преобразования. Это, конечно, правильно. Есть какая-то линейное преобразование. линейная часть преобразования, и, конечно, это преобразование и есть, но можно посмотреть на нейроны немножко с другой стороны, что вот есть некоторый ключ, который определяется весами входных связей, и, соответственно, он включает этот элемент, и тогда на выход подается некоторая активность, то есть когда вот это «О» жито оказывается больше нуля, ну, или не равна нулю, скажем так, то она умножается на вот эти весосвязи, и вот этот вектор воспроизводится. Причем, естественно, это, значит, я пишу не выход, который воспроизводится, а вклад данного нейрона в выходной сигнал, то есть Δy, который, значит, данный нейрон может собирать в выходном сигнале. И, соответственно, Y в целом формируется из суммы сигналов нескольких нейронов, которые, собственно, активны, и, значит, формируется некоторый суммарный выход, который образуется. Но в целом можно это рассматривать как то, что, значит, некоторым ключом мы активируем, соответственно, различные элементы скрытых слоев, они воспроизводят вектора, которые у них записаны в весах связей, и сумма этих векторов, она, собственно, и формируется. Вот. Ну и, собственно, вот этот вот процесс, о чем вы говорите, это как бы про... То есть не столько и речь идёт про некоторые сложные преобразования, которые у нас существуют в мозгу, а в том, что мы записываем некоторые, значит, наблюдаемые нами свойства внешнего мира на структуру нейронных сетей в виде весов связи, а потом воспроизводим, ну, значит, с некоторыми, значит, утешениями, чтобы как-то аппроксимировать для тех точек, где мы не были, поскольку жизнь у нас сложная, и мы два раза в одном состоянии никогда не попадаем. Как говорили древние, в одну реку нельзя войти дважды. Проблема состоит еще в том, что есть сложность, связанная с размерностью, о чем я тоже отдельно поговорю. ну и отчасти, значит, эта сложность уменьшается декомпозицией и лайнеризацией. ну вот как бы это очень краткий обзор о том, о чем я хотел бы рассказать. но, естественно, надо начать с нейрокартирования. то есть, в принципе, о нейрокартировании можно думать как о некоторой карте. вот, допустим, карта местности. взята там у ориентировщиков и соответственно там у них там не только местность но еще и дистанция но в любом случае значит по каждому пикселю есть какое-то свойство допустим цвет там может быть какой-то знак там стоит и соответственно есть характеристики вот это значит местность собственно рельеф там наличие воды дороги тропинки растительности разные вот Ну и, собственно, это вот как бы отображение того наблюдаемого пространства и тех свойств, которые этому пространству присвоены. Ну и фактически можно представлять себя в нейросетевом картировании о том, что каждому пикселю мы, грубо говоря, поставили какой-то нейрон, и на выход он, собственно, выходными своими слезами сообщает, какие в этой точке у нас свойства вот этой точки пространства. Ну и хотя параметров может быть много, но размерность нашего представления определяет, в первую очередь, свойственность входного сигнала. То есть если мы ходим по местности, местность там обладает двухмерной размерностью, и, соответственно, хотя мы там можем приписывать вроде бы много параметров, размерность местности от этого не меняется. поле нейронное, которое формирует карту, оно будет двумерное, а свойств может быть как меньше двух, так и больше двух, и, соответственно, они не меняются. Вот. Ну, значит, в целом каждый нейрон, конечно, как я говорил, вносит свой вклад, значит, выходной сигнал. То есть, может быть, Каждый нейрон, вот тут традиционная схема формального нейрона, то есть линейная часть умножается на веса, суммируется, потом какая-то выбирается функция нелинейного преобразования и идет на выход. И вот этот выход умноженного на веса связи определяет этот самый выход. Поскольку входной сигнал умножается на веса связи, то можно разбить входной сигнал на две компоненты. Одна компонент идёт параллельно вектору весов связи w, а вторая компонент идёт перпендикулярно вектору весов связи. Ну и в сумме они дают тот х, который в любой точке может быть реализован. То есть в любой точке пространства x, вот тут плоскость координат x, k, x. И, соответственно, x любой можно представить в виде двух компонентов. причем если параметров у нас больше, чем 2, то все равно есть составляющая вдоль вектора w, и, соответственно, вторая составляющая перпендикулярна, просто она может быть в более высокоразмерном пространстве, у нее, собственно, ни одно направление, может быть там 2, 3, 4, 5 направлений, но все равно все будут перпендикулярны, если у нас многомерная пространство. То есть этих двух компонентов, в принципе, достаточно. Важно, что вторая компонента, она всегда равна нулю, потому что если мы берем скалярные произведения перпендикулярных векторов, то она всегда равна нулю. поэтому у нас, скажем так, вот эта функция выхода нейрона изменяется только вдоль направления вектора весов связи w, ну и собственно перпендикулярно она остается постоянной, а вдоль вектора w она воспроизводит ту нелинейную функцию, которую выбрали. здесь вот выбрано relu, нарисовано, ну и, соответственно, в любой точке начинает смещаться перпендикулярный вектор, то значение не меняется. но если бы мы делали в более многомерном пространстве, то то же самое было бы и в более многомерном пространстве. в принципе, описывать там любые поверхности преобразования, что как бы не принято рассматривать, но тем не менее можно сделать о том, что если у нас один элемент в одномерном случае стрелу дал бы какой-то рост и дальше рост до бесконечности, но если в этой точке мы добавим к нему еще один элемент, который на выходе умножаем, умножим на отрицательную связь, то, соответственно, его выход будет стремиться... сумма вот этих двух элементов будет уменьшаться. ну и если мы добавим еще третий элемент, который будет положительный и в сумме с этим даст 0, то дальше их активность будет равна 0. ну и если три такие нейрона с одинаковыми направленными весами w' мы разместим, в сеть, то есть составим сеть из трех элементов, то вот такая у нас горочка, которая перпендикулярна вектору w, которая в данном случае параллельна, она пройдет и вдоль вектора w она будет локализована в более многомерном пространстве. можно локализовать только по одной координате, можно локализовать по двум координатам, то есть если мы добавим еще пару элементов, у которых влияние на выходной сигнал будет отрицательное относительно некоторых значений x, которые вот здесь, допустим. У нас три вот эти перелома, я пунктиром обозначил, а вот эти два перелома мы направим вдоль другого направления и, соответственно, ограничиваем. ну и, соответственно, если они уменьшаются, то будет ограниченный вес и, собственно, будет вот такая шестиугольная фигура, которая будет локализована. но, собственно, зачем вообще локализовать такие описания? то есть если мы делаем вот это сложное преобразование для некоторой скажем, сложной функции, то хотя у нас все хорошо сформируется, вроде бы, но приходится утрясать очень много компонентов. и, собственно, когда мы начинаем улучшать в одной области плоскости, допустим, x, то в другой области мы, естественно, ухудшаем. не то чтобы это нельзя утрясти, но вопрос этой утряски сложный. и, собственно, возможность локализовать она, значит, как бы хороша. Но если, значит, в одномерном случае, значит, на однослойной сети мы можем, значит, полностью сделать локальный, то есть вот такие вот горбики у нас, значит, будут... можно, значит, размещать до ликса, значит, сколько угодно, и они друг на друга не будут влиять, поскольку, значит, вот это вот нулевое, значит, идет воздействие, то, значит, вот с такими, значит, горбиками уже все несколько сложнее. То есть, когда мы вот так вот обрезаем, то вот это вот влияние, значит, по бокам, которое мы вроде бы выключили, сделали меньше нуля, и следующий элемент релу, который будет воспринимать он, значит, отрицательное воздействие будет брать нулевое. Вот. Но если мы еще второй такой же попробуем поставить куда-нибудь вот сюда, то вот это, значит, отрицательное влияние на него будет, и он, соответственно, тоже будет загашен. И, собственно, вот скажем так, что мы можем располагать такие... места, грубо говоря, только вот в этом коридоре, который, скажем так, не имеет вот этих отрицательных влияний. То есть вот в этом коридоре мы, соответственно, можем еще что-то расположить, но там тоже будет проблема, что нужно располагать куже, ну или располагать точно так же, но это фактически сведется к одномерному случаю, и в общем не очень хорошо. Можно пытаться сделать, не обязательно такими параллельными элементами, можно пытаться сделать пересекаешься, то есть вот, допустим, если у нас один перелом и растет один элемент вверх, то есть в эту сторону растет, дается положительный сигнал, а в эти две стороны, собственно, он гасится. Ну, понятно, что если он начнет... там, где ноль, там он никак не меняется, потом он начинает все сильнее и сильнее уменьшаться, ну и, соответственно, где-то здесь он уменьшается до нуля. Ну и здесь, допустим, это тоже уменьшается до нуля. И, соответственно, вот такая вот, друг говорит, треугольная пирамида у нас локализована. И, опять-таки, она как бы хороша, но, значит, если мы пытаемся сделать еще одну такую же пирамиду, то хорошо это делать только вот в этой области, где, собственно, влияние вот этих отрицательных элементов на неё не будет, а в других местах там оно будет. Ну и когда мы там 2-3 пирамиды с трудом таким путём можем разместить, но тем не менее, значит, возможности локализации, они, ну, в обычных сетях, они, скажем так, крайне затруднены на однослойный персептор. Вот. Значит, ситуация такая, что всем хорошо известно без меня, что сети глубокого обучения, которых несколько скрытых слоев, они работают лучше сетей с одним слоем. И, собственно, одна из причин, на которую я в этом смысле выглядываю, в том, что если мы Соответственно, вот таким образом можем локализовать описание каких-то частей нашего преобразования и потом элементы в следующем слое связывать только с частью этой сети. То есть, допустим, сколько-то элементов мы использовали для локального описания какого-то преобразования, а потом сколько-то элементов связываем только с ними, а с другой частью сети не связываем. Тогда, собственно, вот это отрицательное влияние, которое однослойной сети нам бы мешало, оно как бы убирается, и, соответственно, есть возможность развивать вот это локальное описание, и оно в целом оказывается эффективнее. Ну и поскольку можно делать повторять много слоев, и повышается вероятность, что такое локализованное описание будет сформировано, это как бы дает явные преимущества при многослойных персептронов или многослойных глубоких сетях. Ну и, собственно, одна из причин, на мой взгляд, почему они работают лучше, как раз в этом и состоит. Вот. Но, значит, как я говорил о том, что значит, есть, как бы сказать, идея, собственно, обратного состранения ошибки, она очень старая, она там идет с 80-х годов, там находится у нее много авторов и нас, там Александр Иванович Галушкин и Хинтон, там кто-то еще, вот, но, значит, как известно, заработало это только где-то после 2010 года. До этого, не то чтобы совсем эта идея не использовалась, но эффективность была маленькая. Ну и собственно, опять-таки, мое представление об этом состоит в том, что то, что мы настраиваем выходной сигнал, Это как бы не все, что нужно сделать, чтобы обратно расстояние ошибки хорошо работало. То есть мы, соответственно, получаем какой-то входной сигнал, отображаем его в пространство состояния активности, значит, скрытых слоев, и потом, значит, это, значит, пространство скрытых слоев, там, в разных состояниях этого, значит, активности обучаем разным выходным. Ну и, в общем-то, не сложно себе представить, что если у нас, значит, пространство большое, много разных состояний, то мы можем обучить много разным выходам в зависимости от того, какой у нас входной сигнал. Если же это пространство отображается в более маленькое пространство, у нас, значит, как бы ситуация похуже. Но, тем не менее, значит, как-то, наверное, можно. Если совсем все выродится в точку, то есть, вот, какой бы вход ни давали, а внутри, значит, активность одна и та же. Ну, соответственно, как вы понимаете, проблемы возникнут очень серьезные. Сигналы разные, а внутренняя активность одинаковая. Ну, соответственно, как мы будем различать, какой выход. Обязательно нужно какие-то меры предпринимать, чтобы состояние скрытых слоев было различным. Ну и не сказать, что эти меры не предпринимаются. На самом деле, все, что делается в последние 10-12 лет, в той или иной степени, как раз на это и направлено. Вот. Ну, значит, как бы сказать. Хинтоновская и Галушкинская идея состоит в том, что надо распространять некоторые параметры ошибки. Для выходного слоя он считается между активностью последнего слоя элементов и тем, что мы хотели получить на выход. И что важно, посчитав вот этот параметр дельта, мы по этим параметрам дельта можем считать такие же параметры дельта для следующих слоев. По достаточно простым формулам, в чем-то похожем на прямое распространение активности, также примерно обратно распространяется вот этот параметр, характеризующий ошибку дельта. Ну и важно, что и когда мы для какого-то элемента знаем параметры дельта, поскольку мы можем распространять по сети параметры дельта, мы, соответственно, можем считать, как менять, собственно, эти веса сеть. Но повторюсь, что все это более или менее хорошо работает, когда у нас, соответственно, вот эти активности элементов внутри сети, они для разных подных сигналов разные. И чем сильнее они разнятся, тем, соответственно, лучше идет процесс обучения, поскольку если они не разнятся, то процесс обучения, грубо говоря, вообще не идет. Ну и, собственно, все применявшиеся как в нулевых, так и в начале десятых годов методы, они как раз направлены на то, чтобы каким-то образом разбросать состояние активности внутри сети. чтобы было понятно, насколько это необходимо, представим обратную реакцию. мы никак не разбрасываем значения, берем для каждого элемента следующего слоя и задаем какой-то вектор связи, а потом по всему слою его просто дублируем. если будут одинаковые вектора связи у всех элементов этого слоя, то, как бы, на любые сигналы, естественно, все эти активности будут одинаковы просто потому, что они по входам одинаковы, линейно по образованию одинаковы, линейно будут одинаковы. Ну и дальше как бы уже все будет испорчено. Почему? Потому что вся активность одинаковая. Мы пытаемся чему-то обучить, но у нас все элементы одинаковые. И, собственно, разойтись им в разное состояние никакой причины нет. Они так и в этом одинаковом состоянии будут дальше продолжаться обучаться, но, естественно, ничего не обучатся. Поэтому, естественно, никто так не делает, всегда пытались и до сих пор продолжают делать некоторые случайные задания весов связей, чтобы они были разбросаны и чтобы состояния активности элементов были разные. Но если вы помните, были Deep Boltzmann-машин, которые обучали веса связи исходя из входных сигналов. То есть сперва настраивали связи по методике Deep Boltzmann-машин, а потом уже начинали обучать выходному игреку. Ну или другой вариант был Stock Autoencoders, когда тоже давали на вход входной сигнал, Соответственно, строили, значит, из какой-то кусочка сети автоэнкодер, обучали висосвязи так, чтобы на выходе получался именно такой же сигнал, как входной. И вот эти связи уже потом использовали для обучения на выходе. Ну, процесс, в общем-то, хороший, эффективный, он работал, но, собственно, требовал больших вычислительных затрат. Поэтому, значит, где-то начиная с 2012-2014 года перешли к инсервизации XAVI и HE. Ну, тут вот, значит, из книжки Николенко, значит, у меня тут взяты графики о том, что, значит, если вот просто, значит, случайно инстализировать, то, значит, хуже работает, если, значит, инстализироваться к советам, то, значит, работает лучше. Вот. Но на самом деле, значит, не просто перешли к этим инстализациям, а то, что одновременно с этим перешли еще к использованию дропаутов. Этот dropout тоже направлен на то, чтобы как-то разбросать состояние сети при различных состояниях входных сигналов с тем, чтобы мы разный y учили при разных состояниях сети. Напомню, в чем состоит dropout. Если у нас есть какая-то структура сети, и, соответственно, мы подаем какой-то входной сигнал, на каждый следующий входной сигнал мы немножко меняем, как немножко. Николенко рекомендует половину элементов выкинуть из сети в каждом слое, и, соответственно, тогда на каждом при каждом иксе будет веса связи обучаться по-разному, причем потому что элементы выкинуты, и, соответственно, даже если мы изначально задали все связи одинаковыми, то, соответственно, они у нас одинаковыми не останутся, потому что один раз у нас одна структура учится, то есть веса связи учится одному, один веса связи, а потом другие веса связи учатся другому, и, соответственно, вот такая вот появляется возможность того, чтобы одинаковые элементы начинали работать по-разному. вот. ну и, соответственно, если бы у нас вроде бы были случайно распространенные веса, ну, значит, случайно они, тем не менее, могли бы у кого-то и совпадать. вот. ну и хуже того, при обучении, если не использовать дропаут, то, соответственно, какие-то веса связи могли стать одинаковыми. И обратно, значит, если без дропаута, они бы уже не разошлись. А, значит, использование дропаута, оно, конечно, дает вот эти возможности по-разному отображать активность, значит, входной сигнал. Ну и, как бы, я это называю, что в той или иной форме вот это вот кортирование, когда у нас обеспечивается отображение входного сигнала разной активности внутренних слоёв, оно вот такими методами всё-таки обеспечивается. Ну и ещё последние современные модели используют гейтс, которые переключают подачу сигнала в разные блоки сети в зависимости от состояния сигнала, что тоже работает в ту же сторону. Почему я все это клоню? Что в современных сетях в той или иной форме корректирование осуществляется, но оно носит такой вероятностный характер. То есть мы создаем некоторые условия, чтобы вероятность представления разных входных сигналов разными состояниями активности сети увеличилась. И, на мой взгляд, конечно, это положительно, и, собственно, основным недостатком является в том, что этот процесс такой многоугодной вероятности. То есть мы создаем вот эти условия, как правило, они реализуются, но, значит, степень успешности, она, в общем, зависит от того, повезло нам или не повезло. Если, значит, повезло, значит, представление входных сигналов активностью сети, оно может оказаться хорошим, а если не очень повезло, оно может оказаться похуже. Ну, конечно, можно там создать очень много слоев или там ансамбль нейронных сетей создать, которые, собственно, считали бы и было бы выбраться лучше или использовать весь ансамбль, но в целом, в общем, по крайней мере, мое такое мнение, что это, в общем, процесс, который можно улучшать. То есть, если мы будем использовать какие-то методы, которые позволят направленно обеспечивать более широкое пространство в состоянии активности сети при разных родных сигналах, то такие подходы, на мой взгляд, должны эффективнее работать. в современных сетях, там начиная уже с резнет и, соответственно, сейчас всякие трансформеры, там, конечно, гейт используется активно, при этом, значит, от дропаута, я так понимаю, никто не отказывался, вот, и, соответственно, все это вместе позволяет, значит, достаточно сетям эффективно работать. Вот. Ну и, собственно, Значит, возникает вопрос, что, собственно, о концерте. Учимся ли мы только с учителем? Ну, как известно, сейчас уже сети, значит, хорошо учатся и без учителя, то есть самый такой яркий пример, значит, без учителя — это автоэнкодеры. Ну и, собственно, не только, значит, автоэнкодеры, там, собственно, в ГАНах там тоже, на мой взгляд, очень эффективный и в правильном направлении развивается подход о том, что мы исследуем, значит, свойства внешнего мира, ну, на основе того, что мы смотрим, что там происходит. И в целом создаются некоторые то, что принято называть Latent Distributions, то есть активность внутренних слоев. распределена по некоторым пространствам. Ну, о чем я, как я уже говорил, что если бы это все слопалось в скотч, то это, конечно, не работало. То есть, все это работает, если, значит, активность вот этих элементов в скрытых слоях, она, ну, как бы распределена в некотором множестве, ну, в смысле, представлена в пространствах, которые, ну, достаточно разнообразны для разных входов. Ну, и причем, что важно, что направлена на тот именно класс входных сигналов, с которыми, собственно, сеть обучается. Вот. Но, как я сказал, что методы сейчас есть, они работают и позволяют, значит, реализовывать, но, значит, повторюсь, что я не считаю, что эти методы нельзя улучшать. То есть как-то можно улучшать, собственно, процесс вот этого, скажем так, представление входных сигналов разными состояниями, их представление активности внутри модели, внутри нейросетевой модели. Переходя, наконец, к нейросетевому картированию, которое начиналось, грубо говоря, с алгоритма К-средних, оно состоит в очень простой идее. если у нас, значит, у какого-то элемента есть, грубо говоря, веса связи, то они, значит, могут быть соотнесены, собственно, с иксом. Ну, тут может быть некоторый, да, как сказать, сложный для понимания переход, и по нему нужно как-то отдельно делать доклад. Но, в принципе, значит, вот это вот умножение и сравнение, оно еще у Минского описано, что, значит, вот это вот, как бы сказать, евклидово сравнение по квадрату разницы, оно переписывается в линейное преобразование, и оно на стандартно-формальном нейроне тоже реализуется, и вся структура на самом деле работает. но это отдельная достаточно длинная песня, которой я не буду сегодня останавливаться. Но смысл такой, что очень простой алгоритм состоит в том, что если подается какой-то сигнал x, и к нему выбирается просто ближайший элемент, то есть считается евклида в расстоянии, и у этого элемента его параметр весов изменяется в сторону x, значит, с некоторым коэффициентом меньше единицы. то все обучение таким путем в некоторую область сбросаем разные иксы, и, собственно, понятно, что вот эти элементы будут смещаться в ту сторону, где больше разности. Ну и, наконец, все займут такие области, когда каждый элемент будет находиться в центре разбиения Воронова-Дерихле, когда каждый элемент будет иметь некоторую область, где он является ближайшим к тем иксам, которые упадают в эту область, и будет стремиться к центру этой области, потому что если он не в центре, то в некоторую сторону его будет тянуть сильнее по этому простому закону. Ну и, собственно, как раз то, о чем я говорил, что необходимо как-то кортировать входной сигнал и распределять разные активности между элементами, Это вот как раз, собственно, алгоритмы кортирования, основанные на К-средних, который вот этот алгоритм известен, он, в принципе, я так понимаю, с конца 40-х, там в 50-е, в 60-е его там человек 20 по-ненавистному изобретали, но после, где-то с 65-го года уже перестали фиксировать эти изобретения, хотя, я думаю, и сейчас еще кто-то его изобретает. Вот, ну и, собственно, наростевое кортирование, в том числе по Коханину, оно в основном отличается, ну там, конечно, есть дополнительные замечательные свойства, соответствуют топологии у Коханина, но в целом оно отличается тем, что просто наивный алгоритм косредников работает в том случае, если вы все эти вектора без софт-связи расположили изначально в пространстве, где у вас выпадает х, то есть понятно, что если какой-то элемент мы возьмем и унесем далеко от этого пространства, то он ближе никогда не станет, и вообще обработка не включится. Ну, значит, у Коханина там предлагается указать соседство, если, соответственно, какой-то элемент тучится, а его сосед, который находится вроде бы далеко, он тоже немножко подтягивается к области пространства, тогда все-таки все туда затянутся, ну, или можно там, собственно, пороги менять, и таким путем, значит, те, которые никогда не отвечают, у них, значит, порог, значит, снижается, они, значит, начинают отвечать, и тоже, значит, затягивается к пространству, и, соответственно, как бы все начинает работать. То есть алгоритм, с одной стороны, очень простой, но широко не используется. К тому же сложности, конечно, есть. Я, собственно, с нескольких слов про них попробую сказать. Вот. Ну, собственно... вопрос в том, что надо, во-первых, осуществлять по этому представлению какую-то аппроксимацию, это тоже достаточно сложный вопрос, а второе, что есть проклятье размерности, то есть когда мы начинаем строить карты двух, трех, четырех, даже пятимерную, ну, все, вроде бы, ничего, работает. но потом, если мы 20 или 30 раз умножим на 10, то у нас получается слишком много элементов, и, соответственно, ни о каком построении карты там речь не идет. и вот это, собственно, проклятие размерности, оно является... Одной из причин, почему карты напрямую не используются. Мое мнение, что так или иначе в той или иной форме в современных глубочих нейронных сетях карты в какой-то форме реализованы, и там не только возможна локализация в глубочих сетях, но и некоторая декомпозиция, о чем я ниже буду говорить. Вот, а собственно вопрос работы с картами, которые, на мой взгляд, лучше современных нейронных сетей тем, что они направленно осуществляют как раз вот это картирование, то есть распределение активности внутренних элементов сети по пространству в состоянии входного сигнала, что сейчас происходит в значительной степени случайно, а хотелось бы направлено, и карты как раз это позволяют. Вот. Но, соответственно, мой доклад посвящен тому, что есть некоторые идеи, как бы это можно было реализовать, хотя, значит, не то чтобы, значит, у меня нет никаких алгоритмов, как это на самом деле реализовать, но, значит, сегодня я их не успею рассказать, я ограничусь только общими идеями, как можно это реализовать. Вот, ну, как понятно, что есть, значит, вот эта вот топология, значит, и проклятие размерности, то есть, с одной стороны, вот, кубическую решетку мы, значит, легко себе можем представить сколько угодно размерную, то есть, значит, взяли там одномерную, значит, решетку, то есть, грубо говоря, отрезок, разбитый, значит, на кусочки, значит, сделали там из него двумерную решетку, значит, ну, просто отдублировали отрезок и соединили края, потом, значит, сделали трехмерную решетку, И вроде бы так, значит, все можно легко строить. Но если бы наш мир был линейным, то, соответственно, и проблем никаких не было. Мы бы, собственно, вот по координатам, которые имеют, значит, единственную единицу, то есть вот одна координата, там вот эта вторая координата, и вот, собственно, сюда вот этот третий координат с одной единицей. и, соответственно, если бы значения тех функций, которые мы пытаемся отобразить, они бы определялись просто как сумма значений вот этих трех координат, то есть не обязательно там она в краях должна быть, но она может быть в промежутке, и вот эти крайние точки, если они равны, просто берем сумму трех компонентов и выясняется, что в этой эту функцию, которую мы описываем в нашей решетке, она там именно такая и равна сумме этих трех компонент. Ну, все очень просто. Значит, у нас там размерность будет тысяча или даже миллион. Ну, будет у нас миллион элементов, при том, что у нас сейчас последние модели, там вроде бы как уже 10 триллионов элементов дошло. Ну, не элементов, правда, весов сляли, насколько я понимаю. Но, тем не менее, Там миллионы, там миллиарды, это как бы при современных реалистичных точностях немного. Но если оказывается, что наш мир не линеен, то все становится значительно хуже. То есть если мы не можем вот так вот, как сумму, не только вот это уголок выказать, а и остальные углы тоже надо описывать отдельно, то, соответственно, у нас все это начинает расти в 2 в степени n. даже если считать, что внутри все более-менее линейно, а просто к этим углам начинаются какие-то нелинейные свойства, то если даже мы нарисуем вот такой вот кубик тысячеразмерный, то у этого кубика вот таких углов у него там будет уже, соответственно, два в тысячный, то есть два первых у нас один, в смысле, два кубика, два во второе, соответственно, четыре вот таких угла, два в третье, значит, восемь углов, ну и, соответственно, каждый раз мы добавляем по кубику предыдущие размерности и соединяем каждые углы, одноименные углы друг с другом. И, соответственно, вот это вот число, которое, соответственно, примерно 10 в трех сотый, оно очень большое, то есть насколько оно большое. Считается там, что число элементарных частиц во Вселенной там 10 в 81. Ну и некоторые, значит, наивно считают, что вот у нас 300, а тут 81. Ну, что там, в 2,5 раза. Ну не 2,5, извините, 3,5 раза. но это на самом деле не в три с половиной раза, это значит, если каждой элементарной частицы нашей вселенной поставить еще раз, еще одну вселенную, и в той вселенной каждой элементарной частицы поставить еще одну вселенную, а там, значит, где-то там пол-вселенной поставить элементарной частицы, вот тогда вот, значит, вот сумма всех этих элементарных частиц, она, собственно, позволит просто описать вот эту функцию, чтобы в каждом угле, значит, присвоить какие-то значения. То есть, я нахожу какое-то совершенно нереальное число, и повторюсь, что это очень простой пример, значит, мне на самом деле значительно сложнее, то есть, как бы сказать... Если мы хотим отнистывать, сколько ли мы сложные процессы, то декомпозиция обязательна. И моё утверждение, о чём я раньше пытался рассказать, состоит в том, что в современных глубоких нейросетях некоторая декомпозиция происходит. Если бы её там не происходило, то те задачи, которые они решают, они бы успешно не решали. Другой вопрос. Она происходит там некоторым образом вероятностно, а хотелось бы все-таки ее на что-то улучшить. Ну вот у меня еще пара слайдов есть на ту же тему. 

S03 [00:39:28]  : Владимир, извините, я буквально секунду вклинюсь. Я просто вижу, что у нас прошло 17 слайдов из 53, и поскольку сосчитать количество атомов во Вселенной мы явно не успеем за оставшиеся полчаса вашего доклада, то хотелось бы услышать, где у нас, в каком месте начинается AGI, чтобы осталось. 

S00 [00:39:53]  : Буквально там осталось. соответственно, вот этот пример, в принципе, некоторые считают, что вместо кубической решетки лучше брать симплексы, они в некотором смысле лучше, но они тоже не решают проблему, там возникают слоенность проблем. а в целом невозможность строить карты высокоразмерных объектов, она определяет сложность объектов. То есть есть там всякие представления про онтологическую сложность, геносеологическую сложность. То есть то, о чем я рассказывал, конечно, ближе к геносеологической сложности. Но при попытках строения нейросетевых карт сложность определяется достаточно простой вещью. Она соотносится с субъектом, который строит эти карты. Если его свойства позволяют построить карту состояния объекта, то объект простой, а если его размерность высокая и карты поустроить не удается, объект сложный. Ну и, как мы все знаем, общий человеческий подход к сложным задачам состоит в представлении сложных объектов как суммы некоторых простых объектов, то есть вот тут котик на картинке, Понятно, что такой картинки в жизни вы никогда не видели, но вы легко понимаете, что там у него лапки, ракетка, теннисный мячик. И, соответственно, нам в целом эта картинка понятна, потому что мы ее составили из простых объектов. Когда мы не можем… Если бы мы пытались целиком воспринять картинку без разбиения на части, то, естественно, на свой предыдущий опыт мы бы положиться не могли. Никогда мы таких картинок не видели. Но поскольку суммарную картинку, которую мы видим, мы складываем из частей, которые у нас есть… в моем, по крайней мере, представлении, есть у нас в виде карт простых составляющих, то это нам удается. Ну и важно, что, собственно, путь осуществления. Опять-таки, вот так вот напрямую, как я просто рассказываю, так он, конечно, совсем не работает. Общая идея состоит в том, что мы получаем на вход сложный сигнал, и нам его как-то нужно представить. Отвлекаются некоторые карты, которые обладают какими-то знаниями про внешний мир. Какие-то лучше соответствуют каким-то частям этого сигнала, какие-то хуже. И если мы, соответственно, какие-то карты активировали, и они выдали на выход не только некоторые управления Y, но и, собственно, тот сигнал, который они считают своей частью. И, соответственно, если вот это соответственность хорошая, то мы можем из суммарного сигнала вычесть сумму вот таких вот карт у нас много, и каждый, поскольку выдаёт ту часть сигнала, которую он считает своей, мы, соответственно, можем вычесть из суммарного сигнала сумму вот этих частей всех элементов, кроме этого, ну и тогда на этот элемент будет попадать, собственно, его часть сигнала, и она, собственно, будет использоваться как для уточнения значений карт, как для строения новых карт. Если мы строим новую карту, то у нас может быть два варианта. Если она описывает простой объект, мы сможем эту карту построить. Если она описывает сложный объект, то ничего у нас не получится. Но это не значит, что надо прекращать наши попытки. ну, как я говорил, что, собственно, в современном deep learning, который используется, то есть эти возможности декомпозиции, собственно, они тоже есть. то есть поскольку мы, собственно, осуществляем градиентный спуск, то есть идет процесс оптимизации, то понятно, что если у нас мы представляя сложный объект в виде частей и каким-то образом в нейрослоях этот процесс произошел, то процесс вот этой оптимизации, он такое разбиение должен поддержать, поскольку описание вот этих простых частей, они, скажем так, происходят в значительно более низкоразмерном пространстве, оно может быть сделано более тонкой и более точно и, соответственно, улучшить преобразование. Ну и кроме того, как я уже говорил о том, что есть там функции всевозможные внимания, там и, соответственно, гейты, которые переключают внимание, которые тоже, собственно, направлены на поддержание фактически вот этой декомпозиции сложных сигналов на какие-то части и, собственно, выделение этих частей, работать с ними. 

S03 [00:44:44]  : Владимир, извините, я буквально еще пару минут вклинюсь. Значит, у меня есть такое ощущение, несмотря на то, что у меня тут появились некоторые мысли интересные, которые я потом в комментариях скажу, у меня есть ощущение, что многое из того, что говорите, многим участникам достаточно хорошо известно. И наряду с тем, что лично мне хотелось бы услышать, где у нас возникает AGI, Вот я подсоединяюсь к Сергею Терехову, которому бы хотелось обязательно услышать про собственные экраны. про которые вы много говорили раньше, а пока мы видим, про экраны мы пока услышали мало. Поэтому просьба, оставьте время не только на AGI, но и на собственные экраны, чтобы не перекладывать на другой доклад. 

S00 [00:45:29]  : Кроме, собственно, декомпозиции, важный вопрос еще является лионеризацией. То есть мы не можем сразу осуществлять лионеризацию сложного нелинейного мира. делать только после осуществления картирования некоторых простых частей, но когда у нас эти простые части описаны картами, возможно там их лионеризация и, соответственно, это тоже некоторый путь. Но то, что это в реальных современных нейросетях осуществляется, ну вот, собственно, в всевозможных опытах с GAN можно посмотреть, что по двум разным изображениям, то есть, допустим, источник 1 и источник 2, и, соответственно, формируется некоторая, грубо говоря, линейная сумма, хотя есть какие-то нелинейные преобразования, потом параметры суммируются, и обратно нелинейным преобразованием развертывается какое-то изображение, но оно достаточно, скажем так, фотографически качественное. Вот. Ну, собственно, еще там можно на ту же тему картинки показывать. Ну, собственно, вот, собственно, как осуществляется пороговое картирование, я тогда, с вашего позволения, пролистаю, поскольку, как вы говорите, вы все это хорошо знаете. Ну и, собственно, многомерная аппроксимация, она тоже, надеюсь, достаточно понятна, что так же, как симплексы строятся в одномерном, в двух-трехчетверомерном случае также они могут строиться и дальше. Ну и только подчеркну о том, что свойства входного сигнала каждый раз уменьшают размерность подпространства. То есть если у нас у объекта параметров немного и они связаны, то размерность подпространства становится меньше. Ну, собственно, то, что карты, когда мы формируем, мы изменяем метрику пространства. Там, где у нас карты есть, мы распознаем хорошо, то есть наших относительных соотечественников мы легко различаем. А, допустим, там японцев или китайцев нам различать сложнее, потому что мы с ними не работаем. Вот, ну и как любят, значит, сказать некоторые, значит, члены нашей сообщества о том, что вот всё это картирование — это же уровень зверюшек. И это, безусловно, так. То есть, конечно, значит, карты — это, собственно, аппроксимация на непосредственное восприятие, значит, реального мира, которое, собственно, вопрос, скажем так, К высокой интеллектуальной деятельности имеют в некотором смысле поственное отношение. Но подходя уже, собственно, к этой высокой интеллектуальной деятельности, она идет как раз в том плане, что... Я-то, собственно, рассказывал о том, что высокая интеллектуальная деятельность, она появилась у человека по мере сформирования цивилизации. Пока мы жили в дикой природе, никакой цивилизации у нас не было, и высокой интеллектуальной деятельности у нас никакой не было. Но даже когда у нас сформировалась высокоинтеллектуальная деятельность, чтобы реализовать все эти книжные знания, нужен человек. Мы пока что не построили таких систем, которые на основе книжных знаний могли бы действовать в реальной природе. И, собственно, вопрос построения АГИИ, на мой взгляд, он сводится как раз к задаче построения таких систем, которые могли бы приложить книжные знания в реальных ситуациях. Ну и, наоборот, из реальных ситуаций создать какие-то книжные знания. Вот, собственно, какая-то связь начинается с... с сильным искусственным интеллектом. Вот тут недавно предлагали послушать доклад Хомякова на форуме Высшей школы экономики. Он там рассказал, что мы все представляем Покрытизианский театр. Вот эту картинку, может быть, помните. Кто слушал доклад Хомякова, видит два варианта. Я тут подписал. Но пока я не скажу о том, кто не читал, что там подписана какая-то картина. Вариант, кроме как физиономии, не видно. Но, как сказал Хомяков, если сказать, что это эсхема состоящего входа в пещеру, то второй вариант тоже виден. И то, о чем я рассказывал, что композиция карт, которая позволяет описать сложные объекты, может быть сложной по-разному. Если мы из своей коллекции карт, представления сложной картинки, используем разные части, то мы этот пазл из разных векторов, чтобы получить суммарный вектор изображения, можем использовать по-разному, и, собственно, вот это вот как бы получается. Вот. Ну и, собственно, много рассказывается, в том числе вот докладчик от Хомяков, который, значит, все замечательно рассказал. В принципе, мне понравилось, что он рассказывает. Но в целом, собственно, идет речь о том, что если мы считаемся специалистом и не знаем, как что-то сделать, то, собственно, очень распространенный вывод является, что мы не знаем, как что-то воспроизвести. Вот, ну, значит, поскольку Время поджимает. Я перечислил, какие основные причины выдвигаются о том, почему мы не можем построить сильный искусственный интеллект. Но я больше всего согласен с тем, что нет хорошей теории, которую надо развивать. Ну и, собственно, вот вопрос чувства машины, тоже, значит, которое надо развивать, хотя, значит, и тело, конечно, нужно, и если какие-то неизвестные физические явления, значит, помогут нам в этом процессе, это тоже может помочь. Вот, но, собственно, в нематериальности, то, что нам не хватает мощности процессора, я, как бы, не очень верю. Вот, собственно, и второй аспект — это то, что, конечно, значит, нужно делать реальные вещи, которые уже сейчас работают, но, соответственно, серьезные проекты, они развиваются на продвижении теории, и если, собственно, теорию не развивать, то с этим будет достаточно плохо. Так вот, что же такое, в моем понимании, сильный искусственный интеллект? Я, слушая многих докладчиков, разделяю подходы на три. Волшебный, реальный и практический. Практический – это то, что сейчас коммерциализовано, то, что реализует, замечательно работает. Я всячески это поддерживаю и считаю это большой поддержкой к развитию сильного искусственного интеллекта. Но, к сожалению, то, что я пересчитал на два слайда раньше, в логике склонно считать о том, что будет некоторое волшебство возникнуть с созданием сильного искусственного интеллекта, что будет решать любые задачи, возникнет технологическая сингулярность, появятся независимые от человеческой цивилизации цели и прочие чудеса. Я всегда придерживаюсь золотой середины, иногда я там занимаю крайние позиции, но тут я призываю поддерживать некоторый реальный подход, то есть способность функционировать в реальной среде и работать с накопленным человеческим знанием. И обратный процесс, на основе понимания того, что происходит в окружающей среде, участвовать в процессе накопления дальнейших знаний. С одной стороны, есть практический подход, его полезность мне не вызывает сомнений. Говорить о том, что если мы научим машину делать все те задачи, которые умеет человек, то надо говорить о том, что есть еще одна задача, которую умеет человек. Это накапливать новые знания и те знания, которые есть, реализовывать в реальном или виртуальном мире. Это та задача, которую необходимо решать сильным искусственным интеллектом. как бы остальные задачи как бы они тогда войдут в это число вот но собственно про волшебные сказочные вещи давайте я пропущу вот важно что сказать вот то о чем я пытался рассказать что есть необходимость строить описание свойств нелинейного мира, оно больше основано на отражении в реальных словах. Конечно, мы достигли больших успехов в алгебре, в геометрии, в алгоритмах и много доказали интересных теорий. И многим это позволяет верить в то, что мы найдем такую формулу, которая опишет все. То есть найдем некоторую абсолютную истину, достигнем абсолютных целей. Я в это не верю и считаю, что хотя все эти формулы, которые мы нашли и разработали логику, причем оказалось, что логика может быть не единственной, могут быть разные логики для разных ситуаций, все нужные парадоксы в логике возникают. Все это отражает свойства реального мира, и чем дальше мы обследуем мир, тем больше мы выявляем его свойства и более интересные свойства выявляем. Важным является разделение на символы и векторы. То, о чем я рассказываю, что работа в реальном мире — это работа с векторами. Сейчас очень популярна речевая обработка, и она как раз продвинулась за счет того, что слова стали векторизировать. то есть есть там куча методов. конечно, one-hot — это тоже метод векторизации, но соответственно эти вектора в значительной степени неполноценны, поскольку все они между собой одинаково перпендикулярны, если мы осуществили такую векторизацию. то, собственно, никаких свойств они вот от, значит, реального речевого массива не отражают. Вот. А вот всякие, там, GloVe, там, вот TuVec, наиболее сейчас продвинутые способы, они, собственно, позволяют векторами описать свойства. Ну и вот эта популярная, значит, идея о том, что король, там, минус мужчина. плюс женщина – это приблизительно равно королеве. Ну вот тут показано, насколько приблизительно похожи это вектора. Там, конечно, не так все успешно, как многому хотелось бы представить. Николенко в своей книге рассматривает аналогичный пример – математик плюс женщина минус мужчина – с чему равно? Оказывается, там очень разная можно получится. Женщина-математик получается с большим трудом. Ну, конечно, это предрассудок наш, который присутствует в текстах. В принципе, я совершенно положительно отношусь к занятиям женщиной-математикой, но в текстах вот эти предрассудки, как говорят другие проблемы, они, наверное, теме выделяются. И в целом жизнь в реальном мире связана с накоплением знаний. То есть в бытовой жизни идет сравнение данных со знанием информации. В моделях для работы любых устройств информация является то, что позволяет управлять какими-то действиями, которые идут на выход. А, собственно, не все данные для этого несут информацию. Мы получаем значительно больше, скажем так, данных про окружающий среду, чем мы можем использовать для выполнения своих действий. Но польза от этих данных тоже есть. Они формируют у нас нашу систему знаний, которые там, может быть, фактические или абстрактные знания. Более того, эти абстрактные знания мы можем передавать друг другу речевыми и другими методами. И в целом это формирует нашу систему знаний и способов обработки данных и получения из них информации. Вот. Ну и, о чем я уже говорил, о том, что если у нас мы создадим такое устройство, которое будет работать в реальном мире и сможет создавать модели различных явлений в этом мире или объектов в этом мире, то, собственно, так же, как и себя мы считаем понимающими, как работают эти модели, то если, значит, будет наше устройство создавать такие модели, тоже оно будет собственно работать вот ну и собственно последние три вопроса главные собственно экрана наконец я дойду ну вот про необходимые декопозиции я в основном уже рассказал ну и то что есть собственно много значит режимов работы но вот как я говорил, была некоторая авария, слайды сломались, но неизбежность декомпозиции связана как раз с тем, что простые элементы, допустим, если каждую карту мы там строим, допустим, с большими затратами, тратим по миллиону элементов, то при одном состоянии объекта, второе состояние, второй объект может менять все свои миллионы состояний, соответственно, По парам, значит, двух элементов, если мы пытаемся отразить здесь какое-то их суммарное пространство состояние, оно будет, естественно, прямым произведением, это будет миллион на миллион. сейчас, конечно, для нас это не бешеное число, мы можем там вполне себе с триллионами работать, но если мы там еще раз умножим на миллион и еще раз умножим на миллион, то уже там четырех-пяти вот таких операций будет достаточно для того, чтобы наши вычислительные возможности, не только лично мои или какой-то корпорации, Ну и всего человечества, они быстро исчерпаются. В то время как, если мы описываем все эти объекты по отдельности, то сумма хоть десяти, хоть ста, хоть тысячи этих миллионов, хоть миллионы миллионов, она будет вполне себе предстоима. Вот. Ну, собственно, значит, какая роль экранов, и, собственно, они ли используются. То есть, как я говорил, что карт у нас должно быть много, и, собственно, эти карты должны друг с другом как-то взаимодействовать. И, собственно, эти экраны, они направлены на обеспечение взаимодействия между различными картами. Ну, значит, самое основное, как бы... в том, что на этих экранах что-то с чем-то должно сравниваться, и, соответственно, это сравнение используется не для того, чтобы как в партизианском театре показать какому-то гномику, а что там у нас происходит, и как сеть себе видит, чтобы либо мы, либо какой-то гномик могли это на экране посмотреть, а состоит в том, что на экране мы можем сравнивать то, что мы либо реально наблюдаем, как-то все моделируем, ну, либо одно с другим, либо, соответственно, разные варианты моделирования тоже, значит, можно друг с другом сравнивать, и это тоже, значит, некоторое, значит, сравнение. То есть, скажем так, что информация, знания, которые хранятся в картах, они могут быть использованы для сравнения с тем, сигнал, который приходит из реального мира или, собственно, с теми вариантами, которые мы могли себе представить и сравнить их, грубо говоря, между собой. вот то есть сейчас вот есть такой значит вероятно все многие там паслись на сайте значит which face is real вот ну там он дальше собственно выдает два изображения и потом еще когда вы нажимаете он вам показывает какой собственно зелененький рамка выделяет какое лицо правильно ну значит если в этом щелкали там нас тоже на самом деле достаточно Быстро выясняете, какое лицо неправильное. Даже если тут не был выделен зеленый рамок, видно, что сережки у девушки разные, и ушки немножко тоже разные. Но, соответственно, было бы достаточно причины для того, чтобы сказать, что вот это изображение нереальное. Хотя, значит, при первом взгляде похоже. 

S03 [01:01:40]  : Владимир, извините, можно еще походу все-таки пояснить, в чем разница между экраном и картой, потому что вы очень легко переходите от экранов и к картам, что одно помогает для другого, то есть, что такое карта, понятно, но что такое экран, я все-таки, честно говоря, не очень понял. 

S00 [01:02:08]  : грубо говоря, карта – это некоторая структура, которая ображает активностью нейронной сети. с картами понятно. вот про экраны, пожалуйста, расскажите. но эту активность с входным сигналом, ее же никак не сравнишь. активность – это одно, а входной сигнал, допустим, это другое. их сравнивать нельзя. так ведь? А можно на основе этой активности на экран отобразить такой сигнал, который можно сравнивать с входным сигналом. И на этом экране осуществить сравнение входного сигнала с тем, что у нас удалось отобразить по активности вот этой карты. 

S03 [01:02:49]  : То есть это вот вы как бы сейчас возвращаетесь к тому началу вашего доклада, где говорите, что с X никто ничего изначально не сравнивает, да? 

S00 [01:02:57]  : Не совсем, не совсем. я как бы возвращаюсь совсем не туда, скажем так, совсем не туда возвращаюсь. То есть, понимаете, вот этот вот картезианский театр, который, собственно, вот тут, значит, предлагал недавно в своем докладе Хомяков, вот я вам покажу, то есть, что мы, значит, куда-то смотрим, у нас там есть проектор, сидит гномик и, значит, смотрит, а что мы, собственно, видели. Вот она яичница, вот она перед нами на сковородке. Вот. Значит, как мы все понимаем, что гномика там никакого нет. А зачем нужны экраны? Экраны нужны, значит, для того, чтобы сравнить то, что мы видим, и насколько мы хорошо знаем о том, что мы видели. То есть вот как там пел Высоцкий о том, что нас жизнь научит. Чему нас учит жизнь? Тому, что вот мы что-то реально видим и как-то это представляем. И, собственно, можно сравнить то, что нам, карты, удалось получить какие-то знания и отмоделировать внешний мир, который мы наблюдаем. И вот, собственно, насколько это наше представление внутреннее соответствует реальному миру, мы, собственно, сравним. Ну, собственно, как делается в автоэнкодере. То есть, фактически, то, что на выходе в автоэнкодере, это можно себе представить, как экран. То есть, просто говоря, на вход Входной сигнал подается на выход, и подается на выход автоэнкодера то, что, собственно, преобразованность. Значит, если автоэнкодер хорошо обучен, то они хорошо совпали. Если автоэнкодер плохо обучен, есть разница, это повод для обучения. И, собственно, это обучение позволяет настроить наш автоэнкодер и построить хорошую модель тех сигналов, которые мы наблюдаем. ну и, собственно, о чем я хочу рассказать, что, в смысле, пытаюсь о чем рассказать, что, значит, структура не такая простая, структура должна быть иерархическая, и, собственно, функция экрана не только, значит, в прямом таком сравнении с входным сигналом и с тем, что мы воспроизвели, есть и другие функции, вот, но, значит, простейшая функция экрана, она вот, собственно, такая, что вот мы не Для гномика, собственно, проецируем то, что мы воспринимаем сенсорами, а мы для самих вот этих карт проецируем разные сигналы для сравнения и, собственно, использования для процесса обучения, который, на самом деле, с моей точки зрения сводится к тому, что мы записываем те входные сигналы, которые мы воспринимаем, и, значит, те действия, которые мы действуем, мы тоже записываем, к чему они приводят, и, собственно, потом, значит, выбираем, какие, значит, действия лучше воспроизвести, чтобы, значит, был какой-то хороший результат. 

S03 [01:05:33]  : Так понятнее? Хорошо, да. Ну, давайте уже близиться только к завершению, потому что там Борис уже хочет задавать вопросы и комментировать. 

S00 [01:05:41]  : Я, как бы, с удовольствием послушаю его вопросы, значит, нет проблем. Вот. Ну и, соответственно, как вот у меня тоже тут была некоторая авария, соответственно. Экраны, собственно, от автоэнкодера отличаются тем, что есть несколько уровней карт, и, соответственно, есть экраны, как средство взаимодействия между различными уровнями. И, собственно, если у нас этих уровней несколько, то и экранов тоже несколько. То есть, хотя, значит, принято считать, что вот это вот картезианский театр для гномика, Это единственный экран, букномик там один, и экранов много не надо. Если мы считаем, что у нас там много уровней эротического представления сигнала, то и, соответственно, вот этих сравнений тоже нужно делать несколько, но и для каждого сравнения, естественно, в своем формате. То есть представление сигналов, которое на верхний уровень идет, это уже не то, что мы сенсорами получаем, там уже другой формат представления сигналов. Соответственно, там должен быть свой экран в своих видах представления сигнала, и вот эти сравнения происходят. Ну, и в целом не только должны быть сравнения, но и на эти же экраны можно... ну, не эти экраны, скажем так, экран должен быть устроен несколько сложнее. То есть мы не только сравниваем текущую наблюдаемость с производимым, мы можем сохранять те цели, которые ставят верхние уровни. То есть, если нижний уровень управляет, грубо говоря, непосредственными секторами, то верхние уровни для нижних уровней ставят цели. То есть, скажем так, Управление осуществляется не только по текущему состоянию, но и по этой цели, которая была сформирована. И, значит, если эта цель сформирована, потом пройдет некоторое время, прежде чем эта цель будет достигнута, там верхние уровни могут пока поработать в другом режиме, если, значит, идёт процесс успешного достижения цели. Вот, как бы, собственно, вторая, значит, функция экрана. Ну и, соответственно, Надо, значит, отметить, что на экран, естественно, отображается не единственная карта, то есть при достижении цели, то есть поскольку мы воспринимаем там не единственный объект, а наблюдаем сложный сигнал, то есть среди карты выбираются, собственно, те, которые соответствуют тем объектам или явлениям, которые мы наблюдаем. Ну и более того, карт на самом деле еще больше, поскольку, значит, если мы строим карту какого-то объекта, полезно сделать несколько ее копий, допустим, если мы там увидели зайца, то можно потом еще встретить не одного, а двух зайцев. И, соответственно, эти два зайца мог быть уже насторожены несколькими картами и, соответственно, двумя картами. Но, соответственно, если сразу выбежит тысяча Зайцев, конечно, на каждую Зайцу своей карты не хватит. Мы это воспримем уже как некоторый поток живой Зайцев, и разделять будем только несколько из них. Но в целом, собственно, карт много, и на экраны отображаются, скажем так, характеристики Состояние каждого объекта для передачи на верхний уровень и сравнения с тем, что там те цели, которые ставятся в тех же, скажем так, векторах представлений. Ну и, собственно, еще между картами, там здесь, собственно, некоторые характеристики объектов могут меняться, то есть какие-то карты. скажем так, для больших зайцев какие-то, для маленьких каких-то, для серых какие-то, для белых, ну и соответственно разные карты у нас могут реализоваться для разных объектов, которые похожи более или менее по свойствам, но карта дублируется, и когда мы встречаем незнакомый объект, используем карту, которая близка по свойствам. вот, ну и, как я говорил, это управление, оно строится не алгоритмическими преобразованиями, то есть не ищется какая-то абсолютная функция, которая позволяет все во все преобразовать, ну или не строится какое-то сложное преобразование, которое реализует, а, собственно, есть записи свойств объектов, мы отслеживаем, как изменяются свойства объекта, и когда активность переходит с нейрона на нейрон, то у нас идёт воспроизведение выходных сигналов, которые записаны векторами из выходных функций. Естественно, от того, что мы наблюдаем кошку, непосредственно какие-то действия возникают. совместно с какой-то целью, например, подойти ее погладить. Какие-то циклические движения у нас могут возникнуть при наблюдении наших рук и ног, при наличии цели двигаться в сторону кошки. Если кошка куда-то пойдет, то у нас цель тоже должна поменяться. Но и в целом... Вот, процесс формирования наших действий сводится к тому, что смещаются активности нейронных, скажем так, ансамблей, и вот эти, значит, активности, которые смещаются, они, собственно, воспроизводят по тем траекториям их смещения те действия, которые, собственно, осуществлялись там, ну, или похожие, значит, с ними. Ну, повторюсь, это надо более подробно, конечно, рассказывать, но как бы пытаясь просто общие идеи изложить. ну и, собственно, поскольку есть у нас необходимость декомпозиции, есть такая же необходимость в иерархии. то есть если мы что-то разбиваем на части, нужно это как-то объединять. объединять это можно с помощью иерархии. Есть также еще польза от иерархии дополнительная, что какие-то переменные мы, собственно, можем не в нижних уровнях описывать, а в более высоких, связанных взаимодействий, то есть в нижних уровнях описывать какие-то части сложной сцены, а в верхних уровнях иерархии все более общие взаимодействия между этими частями, ну и, соответственно, вот такое описание идет. Ну и, как я уже говорил, что если мы нижним уровнем управления поставили цель и сенсором наблюдаем в какой ситуации, то эти нижние уровни могут соотнести, как эту цель можно достичь в этих условиях и, собственно, осуществить ее выполнение. Ну и в этот момент, собственно, верхние уровни могут осуществить моделирование совершенно других процессов, которые не связаны с текущими действиями для того, чтобы выработать новые цели, которые, значит, могут появиться, которые мы там ожидаем. То есть проанализировать, ну там не обязательно 2-3, может быть и 20-30, а может быть у хорошего гроссмейстера там и 300-400 ситуаций. просчитать успеет, вот, а там альфа-зеро там считает миллионами эти ситуации, вот, но тем не менее, значит, вот эта возможность просчитать различные ситуации, пока выполняются какие-то текущие действия, вот, то есть вот я сейчас сижу на стуле и так особо на это не задумываюсь, но не падаю, вот, но тем не менее, значит, думаю о другом и, соответственно, об этом пытаюсь рассказать. Важным свойством, которое описывается с картами, состоит в том, что когда у нас формируется как отдельная карта, так и система этих карт, которая описывает какую-то сложную ситуацию, это то, что пока мы карту не построили, мы не понимаем, как нам действовать. Соответственно, если мы можем моделировать, и у нас появляется карта, то с использованной картой мы можем построить и оценить разные варианты наших действий, и, собственно, нам становится ясным, каким, как наилучшим образом действовать при данном нашем понимании ситуации, и, собственно, этого как можно рассматривать как озарение там или инсайт, который, значит, многие до сих пор считают чудом. Вот. Ну и, собственно, что такое сознание в этой модели, что, значит, Во-первых, понимание ситуации основано на том, что если мы осуществляем декомпозицию и она у нас сходится, то есть вектор сложного входного сигнала мы можем разбить на части, совпадает с тем, который мы наблюдаем, значит, собственно, мы понимаем, из чего состоит сложная сцена, которую мы наблюдаем. Может быть, понимаем, как в ней действует, что необязательно, но, тем не менее, что происходит, мы как бы понимаем. Или, если у нас не совпало, понимаем, что мы чего-то не понимаем, что в этой сцене нам не все понятно. Это тоже как бы... Мы в сознательном состоянии, такую ситуацию можно ощущать. И, соответственно, она как бы... То есть, в принципе, процесс... сопоставление сложных сигналов значит тому что мы как бы как мы их моделируем он собственно характеризует процесс сознания ну и собственно как идет Процесс достижения тех целей, которые мы у себя вырабатываем, это тоже процесс, связанный с сознанием. То есть если у нас всё идёт хорошо, то мы можем о чём-то подумать. То есть прошли промоделирование, не связанное с текущей ситуацией. А если всё идёт плохо, то нужно в верхнем уровне вырабатывать другие цели, которые необходимо менять. Ранее поставленные цели достичь не удаётся. и, соответственно, к ним стремиться. Ну и, собственно, вот возможность переключаться между различными режимами, она, собственно, позволяет в некотором смысле осознанно выбирать те режимы, которые имеют лучшие оценки полезности в сложившей ситуации. То есть, когда-то там надо подумать, а ситуация такая, что не всегда это возможно, зачастую приходится непосредственно быстрее начинать действовать. Вот, ну и собственно, чем я порадуюсь, что у меня осталось два слайда, и, наконец, я дам возможность задать вопросы. Вот, что важно, что у нас есть много режимов, что мы не только формируем некоторые При образовании входного сигнала в выходной мы, собственно, вот это кортирование, которое я считаю очень важной частью, оно, собственно, позволяет нам накапливать знания, которые мы в дальнейшем можем использовать при формировании наших действий. Вот. И, собственно, когда-то мы можем себе позволить заняться вот этим, собственно, формированием карт, а когда-то нужно непосредственно действовать. Вот. Ну и, на самом деле, можно выделить еще много других режимов, о которых сейчас я не буду разговаривать. Но, собственно... Управление работой, активностью нейросети не сводится к тому, что это хорошо, а это плохо. Есть много разных режимов, и современные даже нейросети имеют много параметров, которые тоже можно независимо управлять и менять режимы обучения сети, и что в значительной степени используется. Но важно, что есть не просто оценка работы сети, что лучше-хуже, а оценка того, что вот сейчас лучше в один режим, сейчас лучше в другой режим. Это как бы у живых систем, на мой взгляд, соответствует разному гормональному фону. В моделях сильного искусственного интеллекта мы можем контролировать векторным управлением этими режимами, что в некотором плане можно соотнести как различные чувства, которые не сводятся к тому, что хорошо-плохо, а могут иметь сложный смысловой окрас. Ну и наконец, последний содержательный слайд, основные идеи, про которые я рассказывал, что нужно улучшать алгоритмы картирования, создание экранов, которые сейчас в автоэнкодерах на самом деле тоже есть, на мой взгляд, и есть они также в гаммах. Алгоритмы нужно улучшать также моделирование и формирование векторных оценок, и можно это осуществлять как по отдельности, так и в системе. Ну и кроме того, есть кроме идеи градиентного спуска, которая важна и полезна, и хорошо работает, я ее всячески приветствую, есть еще идеи локализации, декомпозиции и линеризации. Это тоже очень мощные математические идеи, их бы тоже надо развивать. При мыслях о построении сильного искусственного интеллекта вот эти, значит, идеи тоже хорошо было, собственно, реализоваться. Ну и вот, наверное, всё, что я хотел с вами рассказать. Немного долговато, конечно, я рассказывал, но прошу прощения. 

S03 [01:17:59]  : Владимир, спасибо. Нормально вы уложились полтора часа. 

S00 [01:18:02]  : Хотелось на час уложиться, да. 

S03 [01:18:06]  : А на самом деле у меня тут довольно много содержательных комментариев, которые, мне кажется, обязательно нужно услышать. Но вначале хотелось бы отобрать несколько вопросов на понимание. Один вопрос у Николая Рабчевского. Как в описанной схеме происходит формирование новых понятий? Вот у меня, кстати, есть ответ на этот вопрос, но вот мне интересно, какой будет ваш ответ? Совпадет ли он с моим или нет? 

S00 [01:18:33]  : Ну, грубо говоря, что вот эти вот карты, о которых я говорил, это, в принципе, некоторые структурные единицы. То есть это не то, что мы скажем так, сделали машину Больсона, когда все связаны со всеми, и вдруг оттуда выделились карты, то есть в структуру нейросети должны быть заложены некоторые единицы. Изначально это как бы некоторая модель понятий разного уровня, то есть содержание каждого понятия, которое будет реализовано на нейронной сети, связано с тем, чему она обучится. Но, в принципе, когда структура заранее рассчитана на то, что в ней будет много понятий, это определяется той структурой, которая изначально создана. То есть если там может быть всего три карты, то там будет три понятия. Если там три миллиона карт, то там может быть три миллиона понятий. Но, соответственно, чем будет наполнена каждая карта, это сильно зависит от того, что мы наблюдаем в процессе нашей жизнедеятельности. 

S03 [01:19:30]  : Вот у нас есть карта, допустим, для русского языка. Вот мы наполнили эту карту. 

S00 [01:19:36]  : Знаете, какое дело, что карта русского языка, ее не может быть. Почему? Потому что это сложное понятие. Понимаете, карты бывают только для простых понятий. Соответственно, простое понятие тоже может быть разного уровня. То есть, конечно, русский язык может отличаться от английского языка, в этом смысле оно может быть понятием простым. Но, соответственно, на нижних уровнях это понятие высокого уровня основано на большом массиве слов или каких-то грамматических конструкций. Поэтому, собственно, Скажем так, что… Чтобы появилось простое понятие «русский язык», должно быть, скажем так, как минимум несколько тысяч карт простых понятий нижних уровней. Тогда, значит, на высоком иерархии может сформироваться относительно простое понятие. 

S03 [01:20:33]  : – Владимир, смотрите, вот у нас есть, я правильно понимаю, что каждая точка, вот там вы показывали эти, так сказать, кривулины на поверхности, вот каждая точка на некоторой карте – это понятие. Это так или не так? 

S00 [01:20:44]  : Нет, нет. Одна карта – это одно понятие. Допустим, кошка – это одна карта, грубо говоря. Но кошка может быть в разных состояниях. Вот каждая точка на этой карте – это разное состояние кошки. А в целом вся карта в разных состояниях кошки. Это одно понятие – кошка. 

S03 [01:21:00]  : Хорошо. То есть, у нас в мозге есть бесконечное количество карт, правильно? Нет, не бесконечное. 

S00 [01:21:05]  : У нас конечное количество карт. 

S03 [01:21:07]  : То есть, как только мы заполним это количество карт, то мы перестанем воспринимать новое понятие? 

S00 [01:21:14]  : Ну, ситуация, понимаете, не такая, что, скажем так, что речь, сама идея кортирования состоит в том, что мы, скажем так, можем свои ресурсы использовать на те задачи, которые мы решаем. То есть, если мы начинаем решать какие-то новые задачи, а у нас все раньше заполнено, то часть того, что было заполнено, мы используем на решение новых задач. Причем, поскольку у нас там было все с запасом, то, как правило, это не повредит. У нас, значит, каждое понятие там дублировалось. Если потом, допустим, у нас было 50 моделей кошки дублированных… 

S03 [01:21:51]  : Хорошо, то есть короткий ответ на вопрос Николая. Когда нам нужно новое понятие, мы берем и какую-нибудь самую ненужную карту стираем и на место ее записываем новое понятие. 

S00 [01:22:01]  : Нет, мы ее не стираем. 

S03 [01:22:02]  : Понимаете, вот пришел… Оно само затирается новым понятием. 

S00 [01:22:07]  : Нет, оно не затирается. Значит, видите вы какое-то новое животное, значит, вы для себя говорите. Вот на что оно похоже? На верблюда, кошку или на жирафа? Соответственно, из 10 моделей жирафа, 50 моделей кошки и 5 моделей верблюда, вы выбираете какое-то ближайшее к тому, что наблюдаете, и начинаете его модифицировать под то, что вы наблюдаете. реально и потом там к нему подтягиваются значит какие-то другие карты которые мало используются и тоже соответственно настраиваются под это новое что вы увидели вот но в целом как бы как сказать операции стереть ее нету есть как бы операции изменить 

S03 [01:22:50]  : Хорошо, спасибо. Следующий вопрос. Вы упомянули, когда заречь наконец дошла до AGI, вы сказали, что главное – это иметь возможность переключаться между режимами. Что такое экран, я надеюсь, понял, а что такое модель? Что такое режим? Чем в вашей модели является режим и каков механизм переключения между режимами? 

S00 [01:23:17]  : Ну, значит, видимо, наиболее распространенное объяснение по Канеману есть две системы. Система 1 и система 2. Вы, наверное, с ними знакомы. Я думаю, все здесь знакомы. Не надо объяснять. то есть эта модель как раз направлена на то, чтобы служить для формирования как минимум двух этих режимов. хотя режимов на самом деле может быть больше, потому что они не сводятся к тому, что система понимания не исчерпывает всех моделей поведения. поведение на самом деле сложнее может быть. но действительно это очень важные два сильно отличающих режима, когда мы все свои ресурсы тратим на выполнение задач и, собственно, подумать нам некогда, а есть, собственно, ситуация, когда мы можем о чем-то подумать, и, собственно, вот этот вопрос мышления, он может быть быстрый, может быть медленный, мы можем два варианта сравнить, можем тысячу вариантов сравнить, если нам неделю дадут подумать, и, собственно, Здесь, поскольку есть иерархическая структура, а управление, в принципе, может реализовываться на нижних уровнях этой структуры, то верхние уровни, когда цель поставлена и управление успешно реализуется нижним уровнем, верхние уровни, которые, собственно, ставили эту цель, они как раз могут заняться тем, чтобы под те ситуации, которые ожидаются или сейчас наблюдаются, придумать, какая будет следующая цель, когда будет достигнута данная цель. То есть мы строим карты простых объектов, но мы, естественно, не можем построить карту всевозможной комбинации простых объектов, потому что, как я говорил, это быстро уходит за наши вычислительные возможности. 

S03 [01:24:56]  : Сейчас вы рассказываете про борьбу со сложностью, а где про режимы? 

S00 [01:25:02]  : Я говорю о том, что эти два режима необходимы. Почему? Ну их всего два, подождите. Нет, ну два основных режима. Есть, конечно, куча других режимов, психологи вам расскажут, я тоже могу много рассказывать, но это как бы уведет нас в сторону. Давайте эти два режима основные. Почему необходимы эти два режима по канниману? 

S03 [01:25:23]  : Вы хотите сказать, что основная функция AGI – это переключение между этими двумя или несколькими режимами? 

S00 [01:25:30]  : Под режимом вы не понимаете… Я там, конечно, много говорил, может быть, я что-то там не так, у меня язык повернулся, но основная функция AGI – это связать поведение в реальном мире с наработанными цивилизацией знаниями. Ну или, скажем так, научиться использовать свой опыт для развития цивилизации. Вот как бы мое представление о BGI. То есть не просто лопачивать те знания, которые человек наработал, и как-то их улучшать, а жить в реальном мире и, собственно, улучшать цивилизационные навыки, которые, собственно, у нас есть. Вот это основная функция EGI, а не два режима переключать. Два режима переключать – это средство для достижения этой цели. 

S03 [01:26:13]  : То есть режимы – это, в первую очередь, вы имеете в виду именно те, про которые говорил Канеман, да? 

S00 [01:26:19]  : Ну да, то, что Канеман говорит «система 1, система 2», в первую очередь я имею в виду это. Хорошо, спасибо. 

S03 [01:26:26]  : Еще буквально пару вопросов на понимание по поводу экранов. Я могу сказать, что с точки зрения теории функциональных систем, Экран – это образ результата на уровне конкретного слоя. Каждый конкретный слой генерирует образ того, что он увидел на экран. И это, соответственно, формирует таким образом образ результата действия в рамках зоны своей ответственности. И если этот образ совпадает с тем, что он видит, то получается то, что он отработал правильно. И таким образом он самоподкрепляется. 

S00 [01:27:10]  : Я бы сказал наоборот. Если он отработал неправильно, тогда он обучается, чтобы было правильно. Если он отработал правильно, то не надо обучаться. Это не все функции экранов. Есть еще и функции, связанные с тем, что нужно помнить цель и смотреть, насколько хорошо идет процесс достижения цели. 

S03 [01:27:35]  : Хорошо. И еще вот с этим связанный вопрос. Когда мы говорим о То есть, кто имеет экран? У нас экран относится к чему? У нас экран – это свойство каждого слоя нейросетевого или это свойство какой-то группы слоев? Если группы слоев, то как эта группа слоев ограничивается? Функциональная какая-то группа или это колонка? Что это такое? К чему присуще наличие экрана? 

S00 [01:28:07]  : ну значит можно там по-разному ответить на этот вопрос но значит как вот мы уже с вами выяснили что есть собственно активность какой-то группы нейронов ее нужно сравнивать нас с тем что на эту группу приходит вот и соответственно для этого нужен экран то есть каждая, скажем так, группа, получающая некоторую проекцию другой группы нейронов, то есть это могут быть сенсоры, а могут быть какие-то нейроструктуры. Грубо говоря, всякие структуры, которые мы потом можем использовать для каких-то своих Действия по... скажем так, сознательных действий, которые мы можем моделировать, они, собственно, нуждаются в экране, потому что мы можем тогда на этом экране что-то сравнивать и, соответственно, делать. То есть, когда у нас идет на следующий слой какая-то проекция, значит, аксонов на следующий слой, то, соответственно, вот это проекция уже является входом. Но с моей точки зрения, она должна быть не прямая от каких-то нейронов, а все-таки через экран. То есть вот этот экран, который использовался, от него этот экран должен идти в функции еще некоторое преобразование этой активности, то есть не просто каждый нейрон. 

S03 [01:29:36]  : Можно я уточню, то есть все-таки вы хотите, я правильно понял, что экран может иметься у каждого конкретного слоя, то есть если мы говорим о слоях. Ну, грубо говоря, да. 

S00 [01:29:45]  : То есть если у нас есть иерархическая структура, то между каждым слоем иерархии вообще говоря желательно иметь свой экран. 

S03 [01:29:51]  : Хорошо. Логически переходя к вопросам Бориса, последний вопрос от меня. К примеру, если у нас есть цифра 3, которая проецируется в определенную карту. то каким образом в некоторых ситуациях цифра 3 проектируется в Z? Или наоборот, картинка Z или 3, а это одна и та же картинка, в каких-то случаях проектируется в 3, а в каких-то случаях проектируется в Z? С точки зрения модели экранов, почему так происходит? 

S00 [01:30:28]  : И как это обеспечивается? Поскольку у нас есть иерархическая обработка. Если мы хотим думать, что это цифра, мы думаем, что это 3. Если хотим думать, что это буквы, мы думаем, что это… Нет, а что значит «хотим думать»? 

S03 [01:30:38]  : То есть вот где у вас «хотим думать» в вашей модели? Контекст. 

S00 [01:30:42]  : Ну вот Хомяков в своем докладе в ноябре, он как раз привел вот эту картинку с эскимосом, стоящим перед пещерой. Пока мы, значит, думаем, что это лицо, мы, конечно, никакого эскимоса себе не представляем. Но стоит нам сказать, что вот это эскимос, стоящий перед пещерой, мы его легко представляем. Вот. Вот речь про это, что, значит, есть какая-то верхняя уровня обработка, которая говорит о том, что, скажем так, то наше моделирование вот этого явления, хотя оно одно и то же с точностью, он может быть разным. То есть вот этот пазл может быть сложен по-разному. 

S03 [01:31:20]  : Хорошо, спасибо. Борис, скажите, как удобнее, мне пойти по вашим вопросам или вы сами словами начнете их озвучивать? 

S01 [01:31:29]  : Я бы хотел сказать два слова в общем плане. Значит, во-первых, я хочу поблагодарить докладчика за столь систематическое изложение своей позиции на языке, который отличается от моего, что сильно стимулирует мое мышление в выстраивании своего языка. А понравилась мне очень ссылка на Высоцкого, что жизнь научит. А это, на мой взгляд, противоречит тому, что говорилось про обучение без учителя. Если мы моделируем мозг, то у мозга есть учитель, это организм. Мозг не существует сам по себе, он существует только в организме. И в целом он обеспечивает благополучие организма и благополучие рода, то есть потомства. И это и есть учитель для мозга. И если мы моделируем мозг, то нельзя говорить, что он может без учителя. И это вот главное замечание. А так, мне многое понравилось, что нужна система иерархических моделей. Я бы добавил только еще стратифицированных, то есть по разной тематике. И, как я говорил, если попытки перевода, то, на мой взгляд, экраны – это и есть модель определенной предметной области. И переключение между режимами – это переключение между состояниями созерцателей деятельности, может быть, например, или некоторыми другими. Ну... И еще общее замечание, что модели делаются не от объекта, а под задачи. Главное при построении модели – это постановка круга задач, для которых мы их делаем. Сам объект моделирования выделяется из универсальной реальности под этот круг задач. выделяют один круг задач, один объект, а социологи совсем другой, а поэты совсем третий. Можно ответить? Конечно. 

S00 [01:34:03]  : Во-первых, нет никакого различия в том, что организм является учителем. Даже не организм, а у нас есть то, что принято называть центром удовольствия. Но он на самом деле не только удовольствие, но и болевые ощущения и прочие отрицательные ощущения тоже как-то интегрирует. И как бы он нам дан эволюционно. Естественно, если у нас бы его не было, то никакого хорошего поведения мы бы в своей жизни не формировали. Имеется в виду, что без внешнего учителя, за счет свойств системы формирования и оценки различных состояний, она вот, как я говорил, сложная и векторная, и, соответственно, такой учитель у нас, конечно, есть, я, значит, с этим совершенно не спорю, вот, а то, что, значит, действия идут от задачи, то, конечно, значит, сперва, значит, не так все просто, но тем не менее выбирается цель, и дальше ищутся средства для ее достижения. Чтобы была возможность достигнуть этой цели, нужно, собственно, накапливать знания про окружающий мир, и чем у нас этих знаний больше, мы, собственно, можем выбрать из тех данных, которые у нас есть, и тех знаний, которые у нас есть, какую-то информацию, и построить какие-то действия для решения этой задачи. А если у нас нет этих знаний по окружающий мир, и, собственно, ситуация такая, что выбирать-то не из чего, то, естественно, мы своей цели не достигнем. Поэтому тут можно, конечно, рассказывать, что цель там преимущественна. Но, как бы с моей точки зрения, здесь существует некоторый баланс. То есть цели мы себе можем ставить очень хорошие, но если мы средствами не обладаем, то мы их не достигнем. Мы, наоборот, при наличии очень большого средства, там куча ставят совершенно бестолковых целей, и, собственно, средства эти используются ужасно. 

S01 [01:36:00]  : Избыточные знания могут быть вредны. 

S00 [01:36:07]  : Понимаете, избыточные знания вряд ли могут быть вредны. Хотя, конечно, вопрос важен не только наличие знаний, а наличие способов их применения. То есть, если это чисто книжные знания и без способов их применения в реальном мире, то от них, конечно, пользы немало. 

S03 [01:36:27]  : Борис, у вас будут еще вопросы или комментарии? 

S01 [01:36:31]  : У меня были вопросы. 

S03 [01:36:34]  : Ну, озвучить их, да? Хорошо, озвучу. Да, пожалуйста. 

S01 [01:36:39]  : Об определении сложности. Сложность, опять же, относится не к объекту, а к модели. Например, солнечная система может быть очень простым объектом, если это модель Кеплера-Ньютона, или очень сложным объектом, если мы рассматриваем образование жизни, ну, всякие тонкие процессы физико-химические, образование и распространение жизни и формулы жизни в Солнечной системе, то это будет очень сложная модель. То есть характеристика сложности относится не к объекту, а к его модели. и, соответственно, к тому кругу задач, которые мы решаем. Если нам нужно просто прогнозировать движение планет с не очень большой точностью, то модель будет очень простая. 

S00 [01:37:36]  : Ну я, собственно, понял вопрос, я полностью согласен с тем, с генезиологическим подходом о том, что понятие сложности, во-первых, оно субъективное, то есть в зависимости от того, какие у вас есть аппаратные средства для представления тех явлений, которые вы пытаетесь описать, и не только аппаратные, но и знания, какие у вас есть, то есть если вы можете из вот этого сложного явления выделить простые объекты, то эти простые объекты и явления вы достаточно легко сможете откорпировать. А если у вас знаний нет, то, собственно, разбить его на простые объекты вы не сможете, и, соответственно, оно останется для вас сложным. Ну а с технической точки зрения, как я говорил в рамках той модели или того подхода, о котором я пытался рассказать, То есть, сложность, конечно, определяется субъективно в зависимости от того. Но повторюсь, что зависит не только от того уровня рассмотрения, но и, собственно, от аппаратных свойств и от... Наличие знаний об этом мире технически определяется очень просто. Если удалось построить удовлетворительную карту объекта или явления, то вот эти свойства, которые мы построили, мы считаем, что они для нас теперь простые. А если карту построить не удалось, значит объект, который мы называем, он сложный, мы его не можем описывать и он нам непонятен. 

S01 [01:38:56]  : Ну, на моем заке карты это то же самое, что модель. И возникает вопрос удовлетворительность для кого и по каким критериям? И для каких ситуаций? 

S00 [01:39:07]  : Ну, это, конечно, да, критерий важен. То есть для каких-то задач описание может быть удовлетворительным, а для каких-то неудовлетворительным. Но и опять-таки этим определяются сложности. Если карта позволяет удовлетворить эти требования, значит ее удалось построить. А если карта не удовлетворяет эти требования, значит ее не удалось построить. Повторюсь, это субъективно зависит от каждого индивидуума и устройства. 

S01 [01:39:31]  : Сложность чего? Понятно, что одна формула сложнее, проще, чем многостраничные но есть доказательства теоремы Пифагора, очевидно, что оно проще, чем доказательства теоремы Ферма. 

S00 [01:39:55]  : Ну, как сказать, понимаете, конечно, да, там карта может состоять из трех элементов или из трех миллионов элементов, конечно, это разные по сложности карты. но, значит, если вы можете доказать теорему Фермата для вас, она не представляет такой сложности, как та теорема, которую вы не можете доказать. Поэтому можно, конечно, делать дополнительные градации сложности, потому насколько у нас сложной получилась вот эта карта или описание, вспоминать про сложность по Колмогорову, там еще чего-нибудь. Но в том описании, которое я сказал, в первую очередь качественное определение сложности в том, что либо удалось построить карты, либо не удалось. Хорошо. 

S01 [01:40:37]  : Владимир Борисович. Вопрос. Что означает соответствие карты А топологией Х как входного сигнала или чего? Что такое топология Х и что означает соответствие? 

S00 [01:40:55]  : Ну, как сказать, если брать, скажем так, Вопрос аппроксимации. То есть, если у вас аппроксимация везде дает какую-то ошибку не больше эпсилона, то у вас топология соответствует. В какой метрике? Если есть какое-то несоответствие в топологии, эта ошибка обязательно будет превосходить некоторый средний уровень. И, собственно, вот эти места, где она там превосходит, она будет определять отсутствие… И в глидовое расстояние. 

S01 [01:41:23]  : Борис, метрика это уже вопрос техники. Это принципиальный вопрос. У меня вопрос. Метрики отклонений очень разные. Для разных ситуаций, разных организмов. И как выбрать правильную для этой модели. Когда мы прогнозируем движение планет, у нас очень простая метрика. Хорошо. 

S03 [01:41:50]  : Борис, вы задали вопрос про метрику. Владимир, давайте короткий ответ. Важна метрика или нет? И если важна, то какая? И откуда берется дальше? 

S00 [01:41:59]  : Коротко. Метрика может быть, вообще говоря, разной. В основном подходит и в Клидову большинство задач. Хотя вот сейчас популярные идеи про всякие... гиперболические пространства, еще какие-нибудь. Не вижу причин, почему нельзя использовать другие метрики. 

S01 [01:42:16]  : Хорошо, Борис. Я могу взять единицы к тысяче или тысячи к единице. Будет совсем разное расстояние Эвклидова между точками. 

S00 [01:42:29]  : Ну, безусловно, я говорил о том, что за счет вот, вспоминая вот этот пример из фильма «Мимоно», о том, что для, в зависимости от того, что для нас знакомо, а что не знакомо, тех областях, которые мы часто используем, там одна метрика от областях, где мы, которые нам неинтересны, там другая, другие расстояния, вот. И, соответственно, то, что нам, хотя, значит, в чисто как сказать в чисто физическом представлении расстояние вроде бы одинаковое субъективное представление конечно другие это как раз связано с тем что когда формируются карты там они располагаются в пространстве там по-разному и этим определяется нужность определяет борис борис борис борис борис борис борис вы метрику определяете от субъективных знаний 

S01 [01:43:17]  : А я считаю, что её нужно определять от практики. 

S00 [01:43:24]  : Такие задачи, они, естественно, определяются практикой. 

S01 [01:43:26]  : Нет, наоборот, практика определяется. Хорошо. 

S03 [01:43:30]  : Борис, спасибо. Владимир, спасибо. Давайте все-таки двинемся дальше, потому что у нас много содержательностей. Да, у нас еще содержательностей и, возможно, вопросы есть у Александра Гургенидзе. Но, прежде всего, у меня два вопроса еще на понимание. Один свой. Сергей Терехов спрашивал вопрос по поводу того, что экран – это способ понять, правильно ли мы моделируем или аппроксимируем окружающую действительность. Скажите, а вот когда Берт тренируется предсказывать последовательности на этих самых последовательностях, он же на самом деле это же и делает? То есть, я правильно понимаю, По сути, вы просто в своей модели описали некоторым оригинальным образом то, как работает supervised learning. 

S00 [01:44:22]  : Ну, значит, в отличие от, скажем так, ряда авторов, которые говорят, что у них там что-то совершенно новое, я, наоборот, говорю о том, что то, о чем я рассказываю, оно в той или иной степени в ряде моделей развито. То есть я говорил, что автоэнкодеры там делают похоже, GAN-ы делают похоже, BERT-ы делают похоже. Да, конечно, как бы сказать, что бы вы ни предложили новое, обязательно выяснится, что это уже до вас предлагали. И вопрос в том, что чем-то отличается от того, что предлагали, или все-таки полностью совпадает. То есть то, о чем я предлагаю, в основном отличие стоит в том, что много уровней иерархии строить. Вот в этом сторону, в основном, мое отличие. Спасибо. Спасибо. 

S03 [01:45:01]  : Ничего принципиального нового придумать нельзя. Спасибо. Вопрос Сергею Терехову. Вы удовлетворены разъяснением про экраны? Да, да, конечно, все понятно. 

S00 [01:45:15]  : Ну, то есть, естественно, у меня есть своя пропитация, но это будет уже мое выступление. 

S03 [01:45:24]  : Хорошо, ну, может быть, останется еще время на комментарии, но вот сейчас хотелось бы Александра Гургенидзе услышать. Александр, у меня сразу организационный вопрос к вам. Я так понимаю, что вы в следующий четверг у нас выступаете, да? Все, хорошо. Тогда, пожалуйста, вам слово. Вы тогда все свои комментарии, вопросы сами озвучите. 

S04 [01:45:41]  : Ну, я несколько ремарок хотел просто сообщить. Первое, к вопросу о сложности. Нельзя говорить, что правильная модель, она простая. а неправильное – сложное. Это не степень понимания, это степень сложности самой системы. В данном случае, я думаю, что здесь имеет смысл говорить о многопараметричности. То есть, когда мы выходим за порог, скажем, нескольких десятков параметров в векторе состояния, то система по количеству взаимосвязей возможных, которые могут возникать, внутри системы становится сложной. Если мы адекватно ее отображаем, то она остается сложной, хотя мы и правильно ее моделируем. Теперь к вопросу о экранах. 

S00 [01:46:37]  : я совершенно не спорю с тем, что мы не можем… я весь доклад говорил о том, что если число параметров уходит далеко за 10, то мы, естественно, такую систему моделировать не можем. 

S04 [01:46:50]  : можем. 

S00 [01:46:52]  : – Можем. Но иначе. – Нет, моделировать мы, конечно, можем. И более того, я говорил о том, что у нас на экранах за счет простых моделей происходит моделирование. Мы не можем ее полностью описать во всех состояниях. – Можем. 

S04 [01:47:06]  : Для этого существует физика открытых систем. Она подходит к этому вопросу иначе. Но такой метод есть. 

S00 [01:47:14]  : – Понимаете, какое дело? Мы можем верить, что мы можем описать. То есть, допустим, Ньютон выяснил закон всемирного тяготения, и он совершенно справедлив. Мы же с ним не спорили. Но выясняется, что есть такие ситуации, когда его надо немножко подправить. А есть такие законы, которые вроде бы в одних ситуациях справедливы, а в других нет. То же самое, о чем вы говорите. Конечно, мы можем говорить о том, что вот это мы посмотрели вроде бы независимо, и, соответственно, можно строить прогнозы для любых ситуаций. Но выясняется, что в некоторых ситуациях прогнозы не работают. Так вот и с логикой происходило, что вроде логика все описывает, а некоторые парадоксы там начинают находить. Там Рассел что-то рассказывал, просто различные парадоксы. То есть мы можем говорить о том, что мы создали простые модели и части, а теперь мы можем комбинировать как угодно и все описать. Это наша гипотеза, что мы можем всё описать. Когда мы на самом деле столкнёмся с той ситуацией, как мы её описываем, может оказаться, что в той ситуации всё не так, как мы думали. Поэтому эта ситуация не совсем простая. 

S04 [01:48:18]  : Здесь я согласен. Я просто хотел обратить внимание, что больше того, что мы наблюдаем в окружающем мире, мы всё равно смоделировать не можем. Если у нас неполная картина, непредставительная выборка, мы можем моделировать только то, что мы увидели, и предполагать те взаимосвязи, которые мы зарегистрировали. 

S00 [01:48:38]  : Речь как раз об этом, что мы можем смотреть простые объекты или явления, а их комбинации мы во всех комбинациях посмотреть не можем. И когда возникнет эта комбинация, может оказаться так, что на самом деле не так, как мы думали. 

S04 [01:48:51]  : Согласен. В данном случае, когда я упоминаю физику открытых систем, это методология, которая позволяет систематизировать множество состояний динамической системы и вскрыть фактически внутренние взаимосвязи в ней на основе анализа параметров и их связности. Естественно, требуется регулярная актуализация этой модели с учетом вновь поступающих данных. Но это вопрос того, как можно смоделировать то, что мы наблюдаем. Теперь я хотел бы уточнить по поводу экранов. Правильно ли я понимаю, что вот эти экраны в слоях нейросети – это фактически формирование образа, который мы можем использовать для обратной связи в нейросети, то есть для того, чтобы мы могли декодировать сложно преобразованный сигнал и убедиться, что мы действительно интерпретируем его правильно. Ну, в некотором смысле, да, конечно. То есть, вот это фактически какие-то контрольные точки промежуточные в моделировании системы для того, чтобы убедиться, что мы движемся правильным путем. На каждом слое итерации преобразований. Потому что у нас, вот если мы говорим о мозге, то в мозге по мере продвижения от нейрона к нейрону, к центральной нервной системе, у нас идет множественное кодирование с перекрытием, многоканальное. Но ни на одном уровне иерархии вот этих перекодирований у нас не происходит восстановление исходного сигнала. Мы нигде в мозге не сравниваем с оригиналом, с исходным воздействием то, что мы чувствуем. Обработка идёт хирургически снизу вверх, но на каждом уровне идёт своё кодирование, и свой вот этот экран не имеет такой обратной связи, о которой говорите вы. Мозгу почему-то этого не требуется, он анализирует и интерпретирует картину несколько иначе. 

S00 [01:51:00]  : Я и говорил о том, что там никакого домика не сидит, и, собственно, вот эту вот сковородку с яйцами ему показывать не надо. Это для нас контролька. Это мы для себя можем нарисовать. Когда вы закрываете глаза и видите там, не знаю, кошку или арбуз, то это не значит, что где-то у вас там это изображение восстанавливается в виде изображения. Есть как бы активность, картина активности различных нейронных слоев на разных уровнях, которые там несколько раз перекодированы в других переменных, и вот эта активность, она устанавливается. Она, конечно, имеет отношение к тому, что если бы вы видели арбуз, она была бы похожей активностью тех слоев, но она, конечно, напрямую с этим изображением никак не связана. 

S04 [01:51:49]  : Она сказана, но не напрямую. Но на самом деле мы сравниваем конечный результат в нашем мозге с предъявленным образом, который также докатился до центральной нервной системы. Мы не докодируем то, что докатилось до мозга. с тем, чтобы восстановить исходное изображение. 

S00 [01:52:05]  : Нет, исходное изображение ни в коем случае не восстанавливается. Идет отношение между двумя слоями и в тех параметрах, которые есть в каждом слое. Поэтому надо много экранов, потому что для каждого следующего слоя представление другое. Все понятно. 

S04 [01:52:20]  : Спасибо. Спасибо. 

S03 [01:52:24]  : Николай, у вас будут комментарии? 

S02 [01:52:32]  : Ну, комментариев, пожалуй, нет. У меня возник такой вопрос по поводу этих экранов, что это очень похоже просто на описание текущей ситуации. И если это так, то тогда непонятно, зачем придумывать новое название. 

S00 [01:53:06]  : Вроде бы как оно и не новое, насколько я себе представляю. Вот, вопрос в том, что это не единственный экран, а то, что их там множественно, это да, может быть, это какая-то новизна, а то, что экран, это как бы в психологии широко используется представление. 

S02 [01:53:21]  : Да, в психологии есть, но, понимаете, тут возникает вот Такая проблема. Есть у нас психологическое описание экрана. И есть у нас идея о том, что мы можем реализовать это технически в компьютерной системе. При этом получается такая ситуация. Реализовать мы ещё не реализовали, Но когда мы говорим о свойствах и том, как оно будет работать, когда мы реализуем, мы используем терминологию наработки психологии, но при этом, поскольку мы еще не реализовали, у нас нет никакой уверенности в том, что наша реализация будет соответствовать вот той психологической схеме, которую мы пытаемся реализовать. 

S00 [01:54:24]  : Ну, как вы понимаете, даже с человеком у нас нет никакой уверенности, что, допустим, вы думаете так же, как я, я думаю так же, как вы. Вот, в этом смысле, что каждый человек по-своему уникален, вот, хотя, конечно, много общего есть, вот, но вот это, значит, вопрос, значит, третьего лица, значит, в оценке он как бы всегда был и будет, хотя, значит, если мы реализуем основные принципы, нам, конечно, понять, что там происходит, как у живых систем, так и у, собственно, агентов сильного искусственного интеллекта, будет проще, вот, но конечно значит по крайней мере на первых этапах наш опыт и будет не очень удачными то есть полностью воспроизвести значит скажем так Скажем так, структуру вот этих оценок, которые вот правильно было сказано о том, что внутренний-то учитель у нас есть, то есть жизнь нас научит, что хорошо, что плохо, если мы умеем оценивать эту жизнь. И воспроизвести систему оценок, которая у нас есть от рождения, ее, конечно, будет сложно. Это одна из важных задач при создании агентов сильного искусственного интеллекта. 

S02 [01:55:34]  : Окей. 

S03 [01:55:43]  : Извиняюсь, да, я был на мюте. Коллеги, спасибо за вопросы и ответы. У нас, на удивление, мирно прошла дискуссия. У кого-нибудь еще есть комментарии? Ну хорошо, тогда спасибо, Владимир. Вам спасибо. Хороший вопрос. И до встречи в следующий четверг. Александр Гургенидзе у нас будет выступать. И всем приятного вечера и всем до свидания. До свидания. 

S02 [01:56:11]  : Спасибо. 

S03 [01:56:12]  : Привет Москве из Питера, кстати. И всем остальным. Счастливо. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
