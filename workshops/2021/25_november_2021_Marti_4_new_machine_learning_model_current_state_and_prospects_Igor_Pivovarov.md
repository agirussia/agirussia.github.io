## 25 ноября - Marti 4 - новая модель машинного обучения. Текущее состояние и перспективы - Игорь Пивоваров — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/9MXek9o11UI/hqdefault.jpg)](https://youtu.be/9MXek9o11UI)

Суммаризация семинара:

ТЕМА
- Семинар посвящен обсуждению новой модели машинного обучения, известной как Marti_4, и ее перспективам в области искусственного интеллекта.

СУТЬ
- Основной фокус семинара - это расширение понимания и обучение причинно-следственным связям. Упоминается о сложности обучения при наличии "sparse reward", когда награда приходится значительно позже действий. Это является одной из главных проблем в области "reinforcement learning".
- Обсуждается возможность использования ревордов (наград и наказаний) как механизма обучения, где отрицательные реворты могут быть использованы для предотвращения нежелательных действий.
- Приводится аналогия с экспертами, где для принятия решения собираются мнения различных специалистов с разными бэкграундами. В контексте модели Marti_4 это относится к работе различных "колонок" (структур в модели), которые специализируются на обработке определенных "стейтов" (состояний системы).
- Ключевым моментом является понимание того, как модель формирует свои решения, опираясь на успешность в мире и способность прогнозировать будущее.
- Важным аспектом является упрощение мира и огрубление информации для создания понятных и управляемых моделей искусственного интеллекта.

ДЕТАЛИ
- Приводится сравнение с человеческим мозгом, где сложная информация упрощается и обрабатывается в виде отдельных элементов, а не целостной картины.
- Обсуждается использование кластеризации и колонок для обработки информации и принятия решений, а также механизмы голосования и триатума для выбора оптимальных решений.
- Упоминается о том, что в модели не используются бэкпропы, а обучение происходит на основе матриц корреляции.

РЕЗУЛЬТАТЫ
- Семинар подчеркивает сложность и неоднозначность процессов обучения и принятия решений в искусственном интеллекте.
- Выделяется важность понимания причинно-следственных связей и успешности в мире как критериев понимания.
- Подчеркивается необходимость упрощения и огрубления информации для создания функционировавших моделей.







S02 [00:00:05]  : Коллеги, всем добрый вечер. Наконец наступил тот самый долгожданный день, когда Игорь Пиловаров нам расскажет, что такое марти-4, а также некоторых интересует, что такое марти-1, марти-2 и марти-3, и чем марти-4 круче, чем все предыдущие. Так что, Игорь, пожалуйста, тебе слово. 

S03 [00:00:27]  : Да, друзья, всем добрый вечер. да сегодня я порассказываю что такое марте и я честно прям большой план на сегодня сформулировал помимо рассказа про марте рассказать про некоторые основные идеи которые лежат в его базе и рассказать про тестирование агентов но думаю что все это получится сделать хотя честно говоря в общем я оставил свидетель конце ноября для того чтобы себя подстегнуть подтолкнуть его в общем довести до какого-то уровня но в общем так и до того уровня, которого хотел бы, конечно, не довел его, но тем не менее, наверное, что показать, в общем, более-менее есть. Так, значит, я сейчас зашарю экран. Зашарю экран, теперь мы... пустим слайды что у вас должны появиться слайды да да все нормально окей ну вот значит расскажу сегодня про марте это название моей модели машинного обучения ну для тех кто так как для тех кто со мной не знаком пару слов у себя скажу я физик теоретик по образованию Закончил он по квантовой теории поля, а аспирантуру заканчивал по кафедре биофизики. И там занимался математическим моделированием клеток. Вообще меня всегда привлекал именно вопрос мышления, интеллекта, сознания. Но в свое время меня отговорили этим заниматься, сказали, что если пойдешь этим заниматься, будешь фриком, никто к тебе серьезно относиться не будет. В общем, там мой такой неформальный научный шеф. Я пошел, в общем, занимался там более серьезными вещами. Ну и из релевантного к этому семинару я, в общем, всю жизнь программирую, начиная с детства. Чего только не программировал, свои все сети первые программировал, нейросети где-то в 95-96 году. тогда это еще не было уже тогда но тогда это не было хайпом и было в общем чем-то другим совсем а сейчас снова вернулся в эту историю на другом уровне что я сегодня хочу рассказать во-первых, начать с того, что такое интеллект и как его моделировать с моей точки зрения, потому что каждый из нас, когда говорит про модель машинного обучения, тем более про модель сильного искусственного интеллекта, я говорю про модель сильного искусственного интеллекта, под этим что-то подразумевается и как мы выясняем все по-разному это понимаем вот я хочу свою значит линию про это немножко рассказать потом сказать что мы сегодня знаем про мозг особенности в части интеллекта кое-какие вещи и в этом смысле отнестись к тому что моделирует нейронные сети а почему нет а почему я считаю что нейронные сети это не есть правильный путь для моделирования сильного искусственного интеллекта расскажу про марте про архитектуру но сразу скажу что не смогу рассказать прямо вот совсем все я специально упущу пару важных деталей но потому что я собираюсь подавать это сейчас на там на хорошую конференцию чтобы не было там вопросов с публикабельностью просто заранее там пару важных деталей опущу потом когда они будут увлекованы могу их отдельно рассказать и еще поговорим сегодня про тестирование агентов это оказывается связанные вещи тестирование и метрики как я его тестирую и я вот тут написал сравнение с айджент 57, но в общем нет, с айджент 57 я просто не дочитал ту статью, но я просто расскажу сравнение с первыми дипмайдерскими моделями, какое оно. оно конечно пока явно в их пользу, но просто чтобы как-то можно было отнестись к этому. ну вот значит начнем с того, что такое интеллект и как его моделировать. Для меня интеллект, как я его для себя определяю, это способность организма, агента, субъекта адаптироваться, ставить цели и достигать их. Иногда это определение расширяет, ты говоришь, способность адаптироваться в любой среде, в условиях неопределенности, ставить цели и достигать их с ограниченными ресурсами. это дополнение но вот для меня важная вещь именно вот это адаптироваться ставить цели достигать тех целей соответственно в этом плане модель и джай с моей точки зрения для меня это такая модель которая вырабатывает целенаправленное поведение для достижения любых целей, демонстрирующие понимание входной информации. Тут много таких слов, которые нуждаются в отдельном дополнении, поэтому я сюда еще вставил пару слайдов со своего доклада с конференции. Один из них про понимание вот на этом слайде такая картинка прекрасная мальчик который видимо разбил играя в футбол вазу мамину любимую и она ему там говорит чтобы ты больше так не делал ты меня понимаешь и что будет пониманием он может сказать понимаю но продолжать играть в футбол и может ничего не говорить но как бы перестать играть дома вот для меня понимание это когда мы видим что некто на случай человек или модель скорректировала свое поведение в соответствии с получаемой информацией мы тогда можем говорить что внутри что-то поменялось она как таково приняла к сведению то что происходит и значит поняла в этом смысле модель должна демонстрировать понимание входной информации какой-то Ну и цель. Цель замечательная, интересная вещь. Мне очень нравится это видео. Я его сюда тоже вставил из доклада Константина Владимировича Анохина про голубя, который явно демонстрирует понимание. У него явно есть цель и он пытается ее решить. в общем ее решает достаточно успешно и в этом плане когда есть цели ну вот модель должна должна их достигать конечно это сложный момент и по него еще скажу у марки в этом смысле сейчас он не совсем еще на том уровне на котором хотелось бы значит какой должна быть модель сильного искусственного директора с моей точки зрения какими она должна обладать как бы качественным принципиальным первый момент это упрощение главная особенность интеллекта он и упрощает мир, выявляя из него важные сущности, создавая там какие-то категории внутри себя и дальше оперировать упрощенными сущностями. кто-то называет это сжатием информации это тоже нормально то есть как бы в некотором смысле информация о мире сжимается и мы оперируем вот этими сжатыми концептами это можно назвать символьный оперирование символами как вам нравится у нас много разных людей в группе и поэтому в этом смысле значит модель должна токлинчи информацию о мире как-то значит упрощать сжимать и строить внутри себя некоторую модель у нас есть один большой любитель слова модель еще строится внутри себя некую модель реальности дальше оперировать этой модель или эти модели во-вторых должна быть эта модель должна модель сильного искусства должна быть иерархична должна уметь как бы настраивать иерархии потому как вот это упрощение и настраивание иерархии похоже что ну как бы определяет все основные свойства ну и например вот там как пример, чтобы сделать доклад на конференции, сперва он какими-то крупными мазками может быть набрасывается такое планирование, а дальше он детализируется в каких-то мелких, более мелких деталях и доходя до этого слайда я знал что там 25 ноября я буду рассказывать но вот там какие-то крупные мазки могут быть и вот это иерархия она как бы в обе стороны работает что вест у них что при планировании как бы сверху вниз или там снизу вверх и при мышлении снизу вверх и такая иерархия какая-то должна быть сегментация по разному назвать ну и наконец третье свойство интеллекта который мне кажется важным и который я думаю фундаментально это предсказание вообще мозг это машина для предсказания но это как бы то ли то точка зрения который я разделяем и сергеем шумскими разделяет ну и как бы многие другие люди разделяет и поэтому собственно все это упрощение мира выстраивания рамки нужно именно для того чтобы предсказывать что будет дальше вот я думаю что правильная модель должна делать именно это ну а горизонт предсказания может быть разный давайте я я про него говорил уже на расслышали бывает у гениев там горизонт предсказания по 100 лет у моего толстого котика который сегодня прогуливает пока семинар значит горизонт предсказания примерно две минуты как я оцениваю часто вот но жпт 3 современный очень очень маленький на уровне 0 1 секунды моим сегодня оценкам, потому что она буквально одно слово может предсказать следующее, и то не всегда точно. это как бы о том, что какими свойствами должен быть интеллект и модель. и теперь я подумал, что как бы сразу я немножечко еще поговорю чуть-чуть про тестирование, потому что потом органичнее получится от мозга прийти к марте. значит представим себе что мы сделали такую модель и что у нас есть там какая-то идея как ее как ее то есть мы воплотили какой-то в коде там какую-то часть как понять что и что это и джай и вот у нас тоже в нашем сообществе джай нам регулярно разгораются значит обсуждение по поводу тестирования как может быть устроен тестирование но я оттолкнусь от своего определения и скажу что если как бы интеллект это способность адаптироваться в любой среде и ставить цели достигать их то то, соответственно, тестирование должно, в свою очередь, моделировать любые среды и проверять возможность агента, ставить цели и достигать их. Это выглядит логичным, если мы придерживаемся этого определения. При этом у нас есть два варианта действий. значит один вариант это собственный тест ну как бы взял и сам написал какую-то там штуку там кто-то ящериц там рисует там кто-то страусов я начинал с марте я тоже начинал там у меня была такая модель я взвал козявка в общем такая ползала букашка по лабиринту мне казалось все очень понятно Но потом, когда ты показываешь ее кому-то другому, все смотрят и долго пытаются понять вообще, что это такое. Почему вот она такая? Почему такой лобби? А как он устроен? Миллион вопросов. И в этом плане плюсы. Понятно, что мне это понятно, как сделано. И вроде как, мне кажется, все логично. Но минус очевидный всем остальным, зато непонятный. И поэтому нельзя сравнить с другими, к сожалению. А с другой стороны, есть всякие открытые open-source тесты, которые всем понятны, они лежат, их можно сравнить с другими. Минус в том, что нужно разбираться. И многие люди не хотят через это проходить. Но вот я для себя пришел, что если ты хочешь показать людям что-то и чтобы они к этому могли отнестись. ну надо как бы придется в этом разбираться. поэтому на самом деле нужно искать правильные опенсурсные тесты для того чтобы на них тестировать. я для себя выбрал бенчмарк такой игры от ари. которая является таким способом тестирования рейдерных агентов. тут вообще все должно было двигаться. я хотел с сайта OpenIGM видео записать, но оно записалось, но оно кодеково не проигрывает. презентации жаль. ну ладно. игра Atari это большое количество игр, в которые играет нормальный человек. есть вот пара фреймворков, которые позволяет него играть. И они основаны на reinforcement learning. Реинфорсмент ленинг для тех, кто вдруг не знает, я вставил слайд такой, это обучение с подкреплением. Это очень похоже на то, как мы учим своих питомцев. Например, вы учите там собаку. Если она сделала что-то хорошее, что-то правильное, то мы ее подкрепляем. Здесь на рисунке мальчик, он бросил мячик, собака его принесла и он ее погладил. И собака понимает, что она сделала что-то правильное. И это positive reward, позитивное вознаграждение. А если она сделала что-то неправильное, на тапки погрызла, то она получит негативный реворд. И сегодня очень многие люди и считают, что, ну, недавно была такая, значит, статья под названием reinforcement learning is all you need. Сейчас вообще как бы это мода на слова is all you need, то attention is all you need, то значит еще что-то. В общем, были, была статья reinforcement learning is all you need. Я не до конца придерживаюсь этой концепции, потому что все-таки объективно человеческий организм, тот интеллект, который мы знаем, человек он как бы учится много сложнее чем просто на обучение с креплением но тем не менее как базовая модель ну пока вот из написанных фреймворков и ничего лучше не нашел мне кажется это наиболее адекватные пока вот вещи скажем так как физик который делает оценку на или некую модель вот модель тестирования на основанная на реинфорстном глянинге, на играх Atari, мне кажется, пока достаточно, ну наиболее адекватные из тех, которые я вижу. значит минусом или там не минусом а особенностью является то что отсюда как бы пропадает постановка ставить цели то есть все эти игры отари они как бы то есть там как бы цель некая уже есть твоя задача задача модели к ней к ней пойти поэтому в некотором смысле вот это выпадает эта часть поэтому вот она здесь зачеркнуто но и строго говоря в марте сейчас модель в модели марте сейчас отдельной постановки каких-то больших целей тоже нет я про нее тоже ничего сказать не могу это как бы некоторая вещь которая будет на будущее также что еще я как бы что еще не входит вот и в это тестирование и соответственно в то что я буду сегодня и то что и себя представлять марте значит сюда не входит какое-то оперирование абстрактными понятиями язык и то что многие любят там проверка общих знаний здравый смысл как бы глобально сейчас есть такие две линии большие и в сообществе сильного искусственного интеллекта. Есть те, кто движется со стороны языковых моделей, и поэтому они считают, что сильный искусственный интеллект это та модель, которая сможет общаться, отвечать на все вопросы, демонстрировать здравый смысл. А есть другая большая линия людей, которые не языковых, которые строят модели не языковые. И в этом смысле я строю, во-первых, я строю не языковую модель, но во-вторых, я хочу напомнить, что для меня IGI это модель, вырабатывающая целенаправленное поведение, демонстрирующая понимание входной информации. поэтому я здесь хочу сделать такой промежуточный тезис, который мне кажется важным и существенным. вот он такой. мы на нем немножко остановимся. промежуточный тезис. тестирование модели на базе reinforcement learning показывает в целом общую способность модели учиться и как бы понимать входящую информацию если она учится на этих разных играх и как бы учится в них выигрывать то мы можем быть о том что она как-то понимает вот эту как бы модель как бы игру и как мне играть и тезис состоит в том что я думаю что при правильном построении архитектуры эта способность учиться и понимать входящую информацию плюс возможности внутренней модели наращивания внутренней и иерархической в том числе я думаю, что это даст все остальные фичи интеллекта, в том числе и работу с языком со временем и там с абстрактными понятиями, потому что иерархия и как бы рост это как в глубоких нейросетях рост количества слоев и как бы фичи в них, то есть все эти фичи они постепенно там они в этой иерархии будут появляться чтобы не рассеять которую мы обычно смотрим это такая иерархия только как бы вид сбоку так сказать а для меня иерархия это вот нечто это как бы мы добрались до момента собственно тестирование дальше мы перейдем на то в мы знаем про мозг. скорее даже я бы сказал так что с моей точки зрения что релевантного я сейчас могу сказать про мозг понятно что мозг очень большая сложная история но тем не менее значит, некоторые отдельные факты, которые, мне кажется, релевантны. Ну, во-первых, сегодня мозг за много сотен лет там много раз препарировали, он там весь разделен на всякие части, к которым можно относиться по-разному. Есть люди, которые изучают форму, цвет и прочее. Вы знаете, все это было Много лет назад, когда читаешь про какую-нибудь лимбическую систему, она состоит из всяких хвостатых тел, шаров и прочего. Но глобально мы сейчас... можно выделить в мозге три существенно большие части. У нас есть неокортекс, новая кора. головного мозга, которое, как сегодня принято считать, отвечает за рациональное мышление, как-то. Никто не знает там точно как, но есть много разных соображений, но как-то отвечает за рациональное мышление. В центре мозга там много, там целая такая лимбическая, там много разных структур, мы про несколько из них дальше поговорим. Древний мозг, что называется, он отвечает за эмоции, сформирование поведения. Я бы не хотел говорить иррациональное, но именно эмоции и поведение. Есть самая большая часть мозга с точки зрения количества нейронов и достаточно маленькая с точки зрения объема, мы же жучок. у человека, по крайней мере. Он отвечает, как сейчас принято считать, за автоматические движения. Мужичок перехватывает в результате то, чему мы научились, и это выполняет. Также мы, конечно, знаем, что... Известно, что мозг состоит из нейронов. Но мы это все хорошо знаем. Ой, а у меня здесь какая-то картинка. пропала сейчас. Мозг состоит из нейронов и поэтому ну все вы знаете не хуже меня что там была все время модель Ходжкина-Хаксли, потом там модель Мак-Алака-Питса, появился персептрон и И сегодня появились все эти нейронные сети глубокие, которые демонстрируют прекрасное решение некоторых задач. Потому что, очевидно, раз мозг состоит из нейронов, то моделируя нейроны, нам кажется, что мы моделируем весь функционал мозга. При этом, например, ну, мы знаем, что вселенная состоит из атомов, но нам как-то не приходит в голову, там, моделировать, например, вселенную из атомов. Ну, заведомо, потому что мы понимаем, что это бессмысленно, и мы ничего не получим. И хорошо, что мы хотя бы вот на таком уровне понимаем. Но, например, мозг, ну, кажется, нормально моделировать из отдельных клеток, хотя, ну, в общем, не очевидно, что это достижимая задача. Но, тем не менее, это вот сейчас это мейнстрим, вы знаете про него не хуже меня. Мейнстрим во многом был продиктован, как мне кажется, значит, очень сильным движением, которое все время сделал Ян Ликун. на базе зрительной коры. Одна из частей мозга, зрительная кора, собственно, отвечает за зрение. Она находится в задней части мозга и она устроена более специфически, чем, скажем, остальная кора. И Ян Ли Кун, изучая эти вещи, подсмотрел это дело и выдвинул концепцию, которая, собственно, перевернула компьютерное зрение, а дальше она стала переворачивать многие другие области, так называемые convolutional neural networks, по-русски, я не знаю, конволюционные сети. Но смысл их в том, что это как бы такое пулирование. когда у нас есть большая картинка и мы эту картинку значит разбиваем на сегменты каждый этот сегмент там становится частью как бы следующие через следующего слоя там в свою очередь пулируется и как бы все это идет до какой-то и таким образом вы что называется выявляются некоторые фичи некоторые существенные признаки по сути здесь мы имеем дело с иерархией как бы с построением некоторой если хотите модели мира с абстрагированием от исходной картинки с абстрагированием в некоторые так сказать в некоторые признаки и дальше из них можно делать классификацию. вы все прекрасно знаете глубокие сети они сейчас во многом взяты по образу и подобию структурозависимой коры. они прекрасно распознают и классифицируют. так что-то случилось. 

S02 [00:24:49]  : включи пожалуйста это скриншаринг а то я тут на безобразничал я пытался гайки закрутить а у меня написано хост дизейбл все-все я снова разрешил включи пожалуйста так сейчас просто стал активно заходить народ и я на всякий случай решил подстраховаться но что-то перестраховался только ты знаешь у меня почему-то секунду у меня не то это и сейчас я проверю мне опция это почему-то того экрана я просто шарю на другой экран 

S03 [00:25:17]  : секунду я сейчас ее выйти из показа так вот теперь появилась там так а 

S02 [00:25:43]  : Да, все отлично, извиняюсь. 

S03 [00:25:44]  : Да, вот сейчас. Соответственно, глубокие сети нейронные прекрасно распознают и классифицируют, потому что они по образу и подобию взяты зрительной коры, которая, собственно, для этого природы и предназначена. Но они абсолютно оказывается непригодны для моделирования поведения я с большим интересом наблюдаю там за сотнями тысячами людей которые значит берут вот там эти сети которые сделаны как бы для одного и пытаются их применить для другого но я уж молчу что в мозге нет бэк пропагейшн даже не молчу нет впрочем важный аргумент его там нету и как бы некоторые отдельные корифей не пытались его там найти с большим трудом но как бы нет его там я конечно я не буду сейчас говорить здесь про есть и другие сети там все трансформе как-то ночью очень популярные и прочее про всех про них можно поговорить но это не моя цель сегодня я просто как бы отнестись к тому что нейросети не делают и сказать почему дальше мне собственно интересно перейти к марте что марк делать и почему он это делает снова возвращаемся значит в этом плане к мозгу к мозгу и мы понимаем что неокортекс формулируют рациональное мышление а вот есть такой лимбический мозг который занимается эмоциями поведения и видимо он важен а про него ну как-то вот там никто им не занимается давайте мы посмотрим что что с ним и что он делает но сначала мы посмотрим значит на на неокортекс еще раз И уже давно известно, что неокортекс состоит в каком-то смысле не из отдельных нейронов, а состоит из групп нейронов. Эти группы устроены так. Есть колонки, мини-колонки. вот как бы мини колонка состоит примерно из 80 там стай нейронов но в общем они разные там но но примерно и дальше и мозг содержит но там порядка 200 миллионов этих значит мини колонок а дальше эти мини колонки еще случайно организованы как бы в группы там от 50 до 100 штук которые называется там гипер колонка просто колонка там и по-разному называется, но мы их будем дальше называть колонкой или гиперколонкой. Некая модульная структура. В свое время еще Джефф Хокинс высказал идею, что не зря кортек состоит из таких модулей, вероятно, они что-то делают. Ну и не только он, там многие это понимают. И собственно, модель, которую Сергей Шумский придумал, она основана на том, что эти колонки выполняют некоторую задачу. сейчас поговорим такое начнут колонки их там достаточно много они выполняет некоторую функцию и колонки если вы помните значит не кортекс это такой тонкий слой там примерно 2 до 4 миллиметров по тонкий слой коры и И он вроде как такой шестислойный. У человека пока имеют эти шесть слоев. И эти шесть слоев они устроены все достаточно специфически. Интересно, что колонки принимают информацию на один слой, а отдают ее в среднем с другого. У них там такая внутренняя есть некоторая логика работы этой колонки. откуда они получают информацию. Это интересный момент. Получать информацию они из таламуса. Таламус это часть вот этого самого лимбической системы которая находится сильно глубоко ниже. таламуса такой центральный распределительный узел значит мозга он с одной стороны получает сенсорную информацию а с другой стороны ее отдает в колонки и получает обратно значит какой-то выход оттуда и причем вот эти связи значит таламуса с колоночками так и называется рекуррентные они могут автоколебания иногда такие возникают и тем самым похоже что таламус сильно может влиять на как бы на на усиление поддержания внимания то есть те колоночки с которыми начинается такой интенсивный обмен туда обратно они как бы возбуждаются они там вот как бы эти колонки в моменте работают не все и и условно вот я так таламус значит можно обозначить так это такая как бы шина данных если хотите в нее приходят всякие сенсорные данные соответственно он поставляет информацию в колоночки им они отдают обратно ему и свой сигнал и дальше еще фишка в том что этими колонками еще то есть но по таланту сам по себе он как бы как он как бы как шина данных работает но он не не занимается активным управлением для этого там есть еще одна интересная структура такой командный центр называется базальные ганглии и совсем такая структура в глубине мозга. Раньше было такое, кстати, была такая, как говорят, уже неправильная модель рептильного мозга, так называемая, что там вот мозг есть, там самая древняя часть мозга рептилий. В общем, сейчас говорят уже, что это не так, но красивая была идея. В общем, базальные ганглии в самом глубине, в самой древней части мозга. И это, судя по всему, главная часть, главный отдел, который принимает решения, если хотите. При этом интересно то, что это самая сильная тормозная система мозга. и ее главная задача торможение и таламус по сути базальные гланглии делают как раз занимается тем что они тормозят множество колонок причем они тормозят их на самом деле на уровне таламуса То есть они тормозят эти таламо-кратикальные связи, не давая развиваться вот этим вот контуром, так сказать, автоколебательным и не давая большинству колонок. В норме, значит, базальные ганглии тормозят почти там все. И, ну, вы знаете, что обычно там несколько процентов колоночек всего лишь работают. Принято говорить, что мозг у человека используется там на один-два процента. Но это говорят те, кто не очень понимает, о чем речь вообще. Но смысл такой. и как бы когда базальные ганглии растормаживают что-то это собственно они как раз растормаживают когда в стриатуме часть часть базальных ганглий которые занимается я первый раз все это рассказываю поэтому у меня так все немножко сумбурно написано но это поправку. Часть базальной ганглии, которая занимается торможением стриатума. Таламус это стриатум, такая субстанция. И когда стриатум возбуждается, какие-то его клетки, это соответствует растормаживанию соответственных частей таламуса вот можно это примерно как-то так отобразить вот это предыдущая картиночка таламуса дает в колонке там свою информацию сигналы сенсорные колоночки нечто делают и отдают это как бы в стриатум и мы сейчас поговорим, что они делают. Астриат, в свою очередь, либо тормозит эти связи каталомо-кратикальные, либо наоборот их растормаживает. И тогда вот в данном случае там пятая колонка так схематически отобразил, что она как бы расторможена и тут будет идти активное взаимодействие. В некотором смысле как бы Базальные ганглии в лице стриатума посмотрели значит так на колонки и так сказали ну это пятая вроде ничего пусть работает. Интересный момент состоит кстати в том что базальные ганглии это единственная система мозга которая выключается во время сна. и потом включается, когда мы просыпаемся. Поэтому часть людей считает, что это связано с сознанием, что, наверное, базальные ганглии – это и есть сознание. В общем, я про это говорить сегодня не буду, но то, что базальные ганглии точно являются центром принятия решений, как они это делают мы сейчас увидим вот значит это вот такой тезис но собственно мы переходим к базовой идее которая у нас лежит в основе нашей модели, она как бы следующая. Мозг это машина для предсказания такая глобальная, которая позволяет нам смотреть на мир и понимать, что происходит и предсказывает, что будет дальше. Неокортекс это та часть мозга, которая строит модель мира. воспринимает информацию, упрощает ее, категоризирует, строит иерархию. И делает свои прогнозы. Но как он это делает? А делает он вот как. У него кортек состоит из этих множества гиперколонок, каждый из которых является модулем прогнозирования. Модулем, который получает свою информацию, свою какую-то маленькую картину мира, неполную какую-то. И как бы на своем уровне предсказывает. А дальше есть базальные ганглии, которые управляют всем этим ансамблем колонок, выбирая наиболее релевантный в данной ситуации вариант действий. каждая колонка занимается какой-то своей специфической истории и это вот такой знаете как какой оркестр который вот играет базальный гангли если хотите режиссер дирижер у которого ну как бы готов который весь там план вот выполнение который в моменте решает то ли там трубам надо значит подать там то ли значит надо летавры то ли там все стихли сделали паузу в общем вот вот такая такая движение и И таким образом отбираются некие варианты, они начинают исполняться. И потом, когда через время есть некоторые удачные варианты исполнения, то удачные варианты потом подхватывает мужичок. Это отдельная история. Не буду сегодня вообще про это ничего говорить. И в марте пока мужичок никак не смоделировал. Но основная идея такая, что действия, которые мы хорошо делаем, они передаются на уровень автоматизма. и мужичок их перехватывает, и про них уже дальше не нужно думать. Мы ходим обычно, мы вообще не задумываемся, как мы ходим, потому что наша голова занята другим. Перехватчик на уровне мужичка. Поэтому нас это интересовать не будет, нас будет интересовать как раз вот ансамбль колонок, управляемых, дирижируемых базальными ганглиями. С точки зрения моделирования, я помните, тоже на конференции, когда делал доклад, я рассказывал, что вот есть там нейроны, кто-то идет вглубь от нейронов, идет к рецепторам, кто-то вообще идет чуть ли не к молекулам, я уж не знаю, но кто-то мы моделируется когнитивной архитектуры мы моделируем собственно марте весь это про такой средний уровень это про уровень колоночек которые являются неким минимально важным элементом но собственно давайте мы поговорим тебе про марта как он устроен а марти это гибридный в том смысле что там с одной стороны цифровое с другой стороны символи и иерархический модульный и эти модули не просто модульные а синхронные они все работают они могут работать в разных вычислительных потоках при желании все на разных машинах ну как бы искусственный интеллект причем именно ну как бы у нас есть ощущение, что это как бы интеллект зачаточный, предназначенный для моделирования целенаправленного поведения, хотя в принципе он там тексты читает и прекрасно с ними работает, но там это пройденный этап у Сергея Шумского там он есть, так что мы знаем, что он это тоже может, но это не является объектом нашего интереса сегодняшнего. как он устроен архитектурно. вот эта колоночка сделана такой модуль, который собственно классифицирует и предсказывает. колоночка отдельная, но в терминах марки будет парсер. дальше мы увидим как работает. есть таламус, который собирает информацию, раздает ее, является центром обмена и и вот эти самые базальные гангли, которые управляют ансамблем колонок, выбирающие прогноз, который будет реализован. мы сейчас можем пока пропустить вот я сейчас буду показывать слайды которые там совсем все раз показываю направление отсюда часть выбросил потому что невозможно будет понимать но тем не менее попробую по частям пройти значит вот у нас есть вход это какой-то цифровой вектор любого размера для вот этих вот моделей от ари который про который я еще чуть позже скажу на них посмотрим это просто но там там там есть разные варианты поставки информации для модели но один из них это вот там просто цифровой вектор длинный и вектор устроен так, что большая часть его это сенсорные данные о текущей ситуации, потом есть какие-то моторные данные, есть некое текущее подкрепление. марки это модель, которая сейчас работает на обучение с подкреплением. он просто принимает на вход такой вектор, а на выход выдает действие. устроен очень просто. значит первый базовый слой это stalamus. stalamus получает входной вектор, разделяет его на части и раздает информацию всем модулям. там дальше можно всю эту конфигурацию настраивать. но сегодня сейчас конкретно вот сделано пока так. что таламус бьет этот изначальный входной вектор на несколько маленьких подвекторов терминами нейросетей если хотите то грубо говоря это своего рода выбор просто случайным образом как бы частей картинки или там картинки или входной информации и как бы каждая из них смотрит на некоторую свою там кусочек сети, если бы мы говорили про сеть. А в данном случае это просто некий модуль, который принимает на вход цифровой вектор. Но сейчас для скорости я перестал передавать ему вообще все. Там сейчас стоит некий анализатор частотного сигнала и вырезается только высокочастотная часть сигнала, который приходит. И маркер реально работает, но там с 10 процентами высокочастотным, как бы, как это сказать, не со всей, если так можно выразиться, данными, а вот с теми, которые наиболее часто меняются. этого достаточно, чтобы делать там... чтобы учиться. Создаются такие модули. Задача этого модуля такая. Пара кодер и парсер, они как раз будут себе такую колонку представлять. Задача кодера, он должен этот вектор перекодировать в символы, потому что марки это цифровой цифро-символьный модель сфорсированная. как это делается? кодер набирает просто какое-то количество векторов. задаваем большое количество векторов и дальше эти вектора кластеризуют. операция кластеризации всем хорошо известна. у вас есть там множество точек, и вы эти точки разбиваете на кластеры. Есть много разных алгоритмов кластеризации, я не буду сейчас про это говорить. Неким алгоритмам не суть важно, их там много. Кластеризует при этом, как вы понимаете, одни и те же данные можно кластеризовать разными способами. поэтому здесь принудительно специально делается не одна кластеризация, а много. на каждый этот кодер делается много разных кластеризаций для того, чтобы каждый входной вектор в каждой из этих кластеризаций попал как бы в разные области. вот эти кластеры они являются ну грубо говоря номер кластера грубо говоря является там буквы и поэтому когда вот этот кодер впоследствии кластеризовавшись получает очередной вектор он как бы переводит это в термины букв сейчас марки для простоты сейчас марки как бы прототип он такая то это модель и с которой я там работаю и она для простоты работают прям реально в текстах ну то есть это прямо символы хотя в принципе это не экономно там когда дело дойдет до этого это все будет переписано там в правильные целочисленные быстрые операции, скорость, я думаю, что в тысячу десять раз увеличится. Ну, в тысячу раз точно увеличится. Но сейчас просто мне проще в буквах это все делать. Читается проще. Поэтому сейчас кодеры переводят буквально в буквы. И дальше эти буквы поставляются в следующий уровень. это перевод который называется парсеры ситуации парсеры это прямо вот парсеры вот как есть слово такое ну это текст это термин который пришел из там я даже не знаю откуда он пришел в общем это из текстового процессинга Парсер это, так сказать, некая программа, которая разбирает текст. В данном случае это тоже такие парсеры, которые разбирают текст. Только текстом является вот у него такая последовательность букв на входе. И каждый из этих парсеров, вся его картина мира, вернее так, вся его входная информация, это вот просто последовательность букв, которые у него есть. Он с ней работает. как он с ней работает, он просто, значит, строит таблицу корреляции для себя внутри, пытается изучать последовательности этих, какая буква за какой и пытается и ведет таблицу подкрепления, значит, что, значит, какие слова, если хотите, оказались успешными, какие слова оказались там неуспешными там за что он получил по шее за что там он не получил за что он получил пряник и строить там некую свою функцию ценности состояния и вот эти парсер они как бы занимаются тем что они уничтожат после каждого после каждой значит буквы они там пытаются дальше понять и предсказать что будет дальше какая буквы будет следующий вся его задача состоит в этом он получает буквы пытается предсказать какая буквы будет следующие значит дальше да и это парсеры первый слой в марте он такой это парсер ситуации там нету действий ну как бы это специальным образом сделано сенсорно-моторная информация разбита на отдельно сенсорную и отдельно моторную. и вот на первом слое это только сенсорная информация, а дальше есть еще отдельный коннектор, который перекодирует моторную информацию тоже в некоторые буквы. Дальше следующий слой оперирует уже сенсорно-моторной информацией. Причем тоже в форме букв. Только у него буквы такие сложные из нескольких букв. Терминами нейросети опять-таки это как бы некоторые слои иерархии следующие. Просто он получен не в числах, а в буквах. ну и пожалуй в этом как бы вся разница то есть это вернее так если хотите вот эти буквы это как бы ну это просто связки так устанавливается и там в общем вот этот следующий парсер следующего уровня они называются над этими коннекторами сторис попарсирующий уровень становится парсера действий а они уже работают с буквами, содержащими в себе движение и ситуацию. Эти парсеровые действия тоже ведут свои таблицы корреляций и тоже получают буквы на входе и пытаются понять, что будет дальше, предсказать буквы, которые будут следующие. они тоже ведут свою таблицу подкрепления, она у них чуть более сложная, чем у парсеров, но это не принципиальная разница для понимания. Соответственно, все эти парсеры занимается значит по сути у парсер это модель колонки и соответственно на наше скажу ну вот так каждый парсер рассчитывает там функцию ценности оценивает а дальше парсеры когда получают свои буквы и как бы каждый приходит в некоторое свое очередное состояние то исходя из того если хотите объема знаний которые как бы у них есть они все делают предсказания следующие вы что будет дальше и к этим предсказаниям относится стриатум стриатум смотрит и все эти предсказания идут в стриатум и стриатум на основании голосования парсеров принимает решение о том какое будет следующее движение и как бы объявляет там самый уверенный парсер который дал прогноз победителем но если хотели терминами мозга как бы растормаживает там данную колонку. Соответственно, Таламус берет прогноз победившего парсера и его декодирует его двигательную часть. Он декодирует его обратно и спускает его вниз как бы в форме моторной. Отдает ее обратно. А если подкрепление поступает выигрыш или проигрыш, то соответственно она тоже приходит в Таламус, но она приходит как часть вектора. и соответственно с триатом это подкрепление раздает парсером пока еще там есть идеи как это сделать прямо очень круто но пока еще до всего руки дошли но пока так и того марти это такая многоагентная модель где каждый агент видит некоторую маленькую часть, если хотите, ну не то что реальность, это неправильно, маленькую часть общей картинки и по ней строит свою некоторую внутреннюю картину мира. Он исследует причинно-следственную связь, которая ему доступна и предсказывает наилучший исход, который ему кажется правильным. дальше на как бы на каждом уровне вот этот ансамбль из на этом ансамбле ну как бы определенным образом проводится голосование принимается решение о том каким будет следующее движение отдельно интересный момент состоит в том, что как бы следующий слой, чем выше каждый следующий слой, первые слои получают информацию сенсорную от таламуса, а следующие слои получают информацию от предыдущих. следующие слои уже не получают сенсорную информацию, они получают состояние предыдущих слоев. и если первые слои и тем самым значит второй и последующий слой они предсказывают уже не то есть они получают также на вход свои буквы на выходе у них свой буквы только их буквы уже имеет другое значение это не уже не сенсор на моторной информации а это как бы то как они думают нижние дадут свой прогноз и каким он должен быть и в этом смысле как бы верхние слои смотрит на состояние нижних и прогнозирует их и чем выше слой тем он как бы так дальше поднимается и получается что высшие слои чем выше слой тем он работает с более абстрактными траекториями или объектами лезть хотите фичами имбидингами там все слова применим что я оставлю за кадром и не могу точно как бы про это сейчас сказать это про механизм голосования он оказался неожиданным для меня признаться но здесь кстати можно сказать почему марте 4 потому что как бы там этих версий было я три года с этой моделью работаю и в общем там 4 версия отличается от 3 тем что я вообще все снес и с нуля переписал заново что я понял что дальше невозможно тянуть все эти костыли и просто вот ну вопрос ну почти практически с нуля все переписал и 4 версия в результате имеет недостаток в том, что в ней сейчас слои, начиная с третьего уровня и выше, не работают. Двухслойная пока структура. Я вот надеялся к 25 ноября запустить, но не собрался. И вторая вещь, которую я здесь за кадром оставляю, в этой архитектуре есть особенное удивительное свойство это механизм выявления таких точек поведения как оказалось но я это я про это скажу просто когда демонстрацию буду делать но не скажу пока как собственно вот архитектура которая вот я вот сейчас мы посмотрим как она работает в демонстрации да она вот графически если хотите выглядит там примерно так если таламус есть та конкретная, которую мы сейчас будем смотреть, она выглядит так. 7 коннекторов внизу, которые исходный вектор из него выделяется примерно 7 кусочков с которыми дальше марки оперируют и над каждым из этих над каждым из этих векторов строится 5 парсеров а из комбинации этих пяти парсеров строится следующий слой и всего там примерно 168 колонок, вот в текущей, в той модели, которая сейчас там, ну обычно, причем она создается сама, как бы я не проектирую количество колонок, оно в каком-то смысле, оно органически возникает, исходя из сложности входящей информации, чем более сложная входящая информация, чем она чаще, чем ее больше, чем она чаще меняется, тем как бы сложнее модель, то есть модель разворачивается, исходя из сложности и объем информации вот так но и с три атом фуф я добежал до этого места и давайте сейчас я покажу как это в жизни работает так вам сейчас видно мой экран вот со статьей правильно да так ну вот я сейчас покажу пару записей экрана как это все начинается и как это выглядит надеюсь что будет работать надеюсь что будет работать значит вот у нас здесь нижнее окно марти вот сейчас он здесь запустился верхнее окно это там где запускается питоновский скрипт и где идет скрипт потому что реально как бы есть три части вот появилось окно значит с игрой а я сейчас немножко увеличу это окно которое генерирует open и айджин И сейчас идет просто некоторая просто рандомная игра, потому что Марти не готов, у него нету, как бы он не кластеризовался, нет никакой информации. Видите, здесь идут рендом экшен, скрипт питоновский, который является связкой между Марти и OpenAI Gym, он как бы от Марти ничего не получает и он генерирует рандомные экшены, а Марти как бы изучает эти рандомные экшены. Дальше в какой-то момент времени, когда станет достаточно много данных для того, чтобы уже Марти мог начать как-то обучаться, он начинает как бы эти асинхронные кластеры, они начинают кластеризовываться. Вот мы сейчас увидим, как они там сейчас пока идет случайный хаотичный поэтому ну там игры бывают разные но правая ракеточка зеленый это марки а левая это компьютер и обычный счет но пока как бы рандомная игра там 9 0 15 0 иногда бывает 8 1 ну когда как вот сейчас марки кластеризовался и картиночка здесь поменялась видите что сверху пропали пропали как бы случайные экшены и вот собственно он начинает играть, он начинает как-то учиться постепенно. Единственное, что мне кажется, что скорость какая-то быстрая. что у вас вот у вас вообще как бы там не будет там мельтешения или ну нормально то есть мельтешения нет да я думаю что я записывал еще не на скорости 24 кадра в секунду она скорости 15 кадров и крат записывал и похоже что это но не очень хорошо было то есть надо было записывать почаще а потом помедленнее показывать в общем начинается все с того что вот если вы видите вот значит здесь на правой части экрана как бы марти стандартно там естественно проигрывает там 15 0 там 10 13 0 и так далее это как бы вот статус да и существенный момент такой это игра как бы сейчас каждая игра это 500 шагов и 500 шагов в пинг-понг. Начинается с нуля и дальше делается 500 шагов. И через 500 шагов игра обнуляется и начинается заново. Я потом про это скажу. Это был принципиальный момент, который я сделал. В общем, отказался потом немножко сложно. В начале, когда Марти учится, он стандартно проигрывает, и это нормальная история. И до тех пор, пока все коннекторы не кластеризуются, пока модель не развернется вся в полную, так сказать, в полный масштаб для данной операции, он как-то будет там двигаться. дальше со временем он начинает куда-нибудь подальше скакнем играть там понятно все лучше и лучше он перестает уже то есть он уже начинает как-то отбивать чаще и в частности здесь вот какая есть вещь про которую я скажу пока но не скажу как работает значит сложности реинфорсмент ленинга как выяснилось потом читая литературу я взял одну из самых сложных игр пинг-понг не надо было брать в общем это только русский мог это сделать значит пинг-понг сложная игра во-первых потому что против тебя играет как бы целенаправленно противник то есть по сравнению с breakout игра где надо просто мячиком выбивать там вверху шарики тут кого против тебя играет целенаправленно противник и во-вторых даже есть например марте отбил зеленая ракеточка отбила и то хорошо, если там гол будет на той стороне, если противник пропустит, тогда есть надежда, что это действие как-то заревордится, пусть и задним числом, но постепенно функции до него дотянутся, функции ценностей и прочего. Но если противник тоже, например, отбил и забил, и Марки пропустил, то получается как бы очень такая болезненная история называется sparse rewards в этой как бы в терминологии этих реинфорсмент ленинга, редкие награды. когда ты сделал что-то правильное, а потом награда за это попалась потом. ну вы, например, яркий пример, когда ты играешь в шахматы, когда ты можешь сделать много правильных ходов, а потом в конце выиграешь. если неправильный ход, то он быстро приводит, как правило, к штрафу, а вот недостаточно сделать правильный ход, нужно сделать там иногда несколько правильных ходов. так вот в архитектуре марки оказалось что можно так сказать обнаружить вот эти самые точки где он сделал нечто правильное поэтому он учится сильно быстрее чем там вот этот стандартный модель но тем сейчас покажу сразу кусочек уже такой более привычный это 700 игр он сыграл ну и вот скажем он здесь сейчас играет на вот он ведь он вполне уже там в районе 700 игр он же вполне зеленая ракеточка уже вполне себе как бы мы сказали, осмысленно двигается, старается отбить, старается как-то успеть. Но сразу скажу, что сейчас Марти не хватает уровней, потому как OpenAI Gym делает следующее. Не OpenAI Gym, а вот этот компьютер на той стороне. Он очень любит гонять по углам. когда тебе забили когда ты от одного угла отбил а тебе тут же переводят в другой как бы между ними очень большие траектории когда вот он начинает гонять по углам марки да даже когда он уже более-менее научен но как бы вот по углам он бегать с двумя уровня иерархии не может тут нужно как бы наращивать потому что для тех кто понимает там много шагов это более сложная история но тем не менее вот он он доходит до уровня примерно там он играет там 0 0 1 1 2 1 там 1 2 1 3 3 1 и вот примерно на этом уровне более-менее останавливается останавливается в том смысле что он сильно перешагнуть за этот уровень пока не ушла объективно у меня пока такая ситуация рад был бы показать вам выигрывают сухую но пока не могу но тем не менее это вот уже выглядит неплохо и Могу еще раз включить или нормально? Хватит пока. Давайте пока хватит, потому что еще про другую вещь хотел сказать. Если посмотреть с точки зрения того, как это выглядит в неких... Я строю для себя графики некие. здесь графики вот так выглядят значит зеленый цвет по горизонтали отложено количество номер игры номер игры по вертикали плюс-минус это как бы количество забитых мечей значит синий это мечи забиты компьютером то есть джимом зеленые мечи забитые маркет оранжевые это разница мечей запитых и пропущенных то есть грубо говоря когда марти начинает играть у него разница мечей там минус там 10 минус 12 стандартная а когда он начинает учиться то он начинает меньше пропускать больше забивать и ну теоретически как бы эта кривая должна пойти вот куда-то туда. от минус 10 допустим до 10. это как бы в идеале. тогда он будет выигрывать в сухую. но вот на двух уровнях иерархии он объективно как бы он достаточно быстро вырастает там к 500 что-то игре и дальше вот примерно на этом уровне остается ну как и вот это вот эта линия она становится со временем совсем как бы там более менее такая сплошная но тем не менее объективно он он не да это средняя значит вот это вот эта вот линия оранжевая это как бы скользящая средняя примерно 30 игр окно 30 игр так это вот конечно сейчас прекрасно так значит марти сегодняшнем состоянии себя представляет следующий это прототип написано яви то что увидели это это стандартно работает на одном ну это просто обычный компьютер с одним цпу модель примерно 8 гигабайт размером вот это прекрасно я не тронул что-то пришел ко мне значит примерно 8 гигабайт она учится в реальном времени и работает пока уверенно устойчиво с опен и джимом ну и учимся мы с другой средой вот и теперь последняя часть я еще немножко порасскажу про протестирование про сравнение как раз чтобы можно было с другими сравнить правда я честно не успел раздел доделать поэтому будет несколько скринов из статей ну и словами расскажу Возвращаясь к этим играм Atari, есть два инвайрмента для игр Atari. Первый инвайрмент был представлен в тринадцатом году. Это вот ссылка, ну вот статья для тех, кому интересно, я потом статьи могу там при желании все сбросить. Значит, Arcade Learning Environment, сокращенно ALE. Вот был представлен из университета Альберта в Канаде. И, собственно, DeepMind в 2013 году, когда они представили свои результаты, что они выиграли там сколько-то игр в Atari, после чего их, собственно, Google купил, это было сделано вот на этом январь. и DeepMind до сих пор на нем остался, на вот этом аллее. Невзирая на то, что уже есть другой, но вот они остались на нем. Это вполне себе нормальный environment. Что такое environment? У вас есть библиотека который вы эти сейчас несколько шагов вперед вот сюда спущусь это как бы я перескочу немножко но для понимания это из опана и джима здесь просто строки на на питоне для тех кто пишет значит инвалидная библиотека на питоне, которая сама генерирует, делает за вас большую часть работы. все, что вы делаете в данной ситуации, это вот, например, n-это как бы некая переменная, которую вы создали там окружение, например, пинг-понг. вы делаете n- равно там создать пинг-понг. дальше вот он делаем резет и значит делаем некую некое действие вот это об 0 это значит это может быть какое-то число допустим которое мы говорим что что что что сделать некое действие потом делаем некий шаг, степ, получаем от environment обратную информацию. В первую очередь так называемый observation. Observation это состояние environment. там же есть реворд и есть значит флаг окончания игры. потом снова делаем некое действие и снова получаем некий observation и снова делаем действие и снова observation и так в общем до без конца. и вот эти библиотеки, библиотеки как раз рисуют это окошко в котором там ты вот сейчас не кстати мне совсем сори да но кот очень любит видеозвонки а уж когда есть возможность тоже посолировать он не может это просто так упустить значит инвалидно делать за вас вот всю эту работу и в этом смысле подключение к нему как бы не очень сложно с одной стороны с другой стороны все-таки нужно значит я так возвращаюсь все-таки, но это требует некоторых затрат силы времени приноровиться к тому, что он выдает. В частности, например, этот environment, который называется ALE, он делает несколько разных типов данных, предоставляет в качестве observation. самый бейсик не влез ну в общем как бы часть часть способов связано с тем что он возвращает он ни в одном из них не возвращает просто чисто картинку всю картинку как она есть а он возвращает как бы предобработанную картинку разными способами и например там бас это как бы один метод где там предобработана картинка разделена там на какие-то фичи условно говоря некая нейросетка считайте что это предобработка некоторой нейросеткой диска это как бы метод который объекты пытается получить на экране от ари значит а вот например рам это просто 128 байт памяти Atari, который в моменте, ну как бы изнутри есть, так можно выразиться. Это вообще не про картинку, это про память Atari. И по сравнению с этим, скажем, OpenAI Gym, он тоже предлагает такой RAM, то есть как бы 128 байт памяти Atari или просто чисто картинку, целиковую картинку. так как я исторически начал с окуна и джима и там были две два варианта я вот и стал использовать этот рамп я работаю марти сейчас работает вот с этим 128 байтами памяти потом уже когда читал литературу оказалось что вот эти ребята которые исследовали этот алле выяснили что рам это самый сложный там самые плохие результаты у них модели были получены самый лучший это когда у тебя есть картинка и в ней есть фичи, которые там можно вырезать. И вот на этих фичах модели обучаются лучше всего, на ранних хуже всего. Ну вот я пока в таком состоянии. Чтобы понять, как у них работает, вот обратите внимание, это таблицы, которые в первой статье опубликовались. Самая нижняя строчка это Pong. они тестировали другим способом и в чем он состоит значит в понк есть две возможности играть то в понк и в любую игру ты можешь задать количественное количество шагов или можешь играть до победы у них длина эпизода 18 тысяч шагов либо победа победа это в понге кто-то может выиграть со счетом 21 чтобы выиграть нужно брать 21 очко соответственно я в свое время когда марти начинал тренировать науку на джиме я так подумал про себя подумал и думаю что до победы играть он всегда проиграет с неизбежностью. поэтому если я сделаю какое-то конкретное фиксированное количество шагов, то на этом фиксированном количестве шагов будет виден прогресс. сфера будет проигрывать в сухую, потом меньше-меньше и когда-то будет выигрывать как-то. и так оно и получается. на 500 шагах марти перестал проигрывать в сухую, иногда выигрывает и крутится около нуля. эти ребята задали другую планку они играют до 21 или там 5 минут и смотрят с каким счетом как бы в среднем модель выигрывает и вот по их первым показателям модель без планирования без без модели мира как бы reinforcement без модели мира она в среднем проигрывала минус 20 всегда вообще ни разу не выиграла как бы их результаты, которые там у них были. ну вот просто в этом смысле они говорят, что понк это сложная игра, потому что там просто так на шаг на два предсказывать нельзя. нужно строить какой-то план. и поэтому они же там предложили некую модель мира, планирование и вот значит глядя на результаты с этим планированием тоже пункт нижняя строчка они значит вот одно из лучших их решение с планированием выигрывает 21 0 это как бы если хотите сота ну то есть вот для данной игры это state of the art то есть кого их модель вот со включенным таким там планированием это все и в статье есть тем кому интересно можно там подробнее почитать выигрывает 21 0 и в этом смысле как бы ну вот высокая планка. я ничего не могу сказать. после этого OpenAI Jim выпустил свой этот фреймворк. и я, честно говоря, найдя этот ALE, я до сих пор не понимаю, зачем OpenAI сделал свой, пока я еще не понял. то ли потому, что в ALE нету просто нативной картинки, они хотели просто чистую картинку получить, я не знаю. но, в общем, вот есть фреймворк от OpenAI Jim, который вот так вот выглядит с точки зрения с точки зрения наш кода и последнее что я в этом разделе хочу сказать это про deep mind который вот в 13 году не была первая статья которая вот на этом али они выигрывали и вот тут если посмотреть на эту таблицу вот понг в середине значит их сеть в среднем выигрывала там как бы 20 0 но там там там тоже это тоже игра до победного у них понятно что это была большая модель там много десятков тысяч джипов часов было потрачено и обучение и как бы лучшие варианты выигрывает 21 0 но тем не менее высота сегодняшняя которая есть она вот она такая и в этом плане мой сегодняшний уровень марки он прямо скажем скромный на этом уровне ну как бы когда эти модели выигрывает 21 0 как бы там бессмысленно говорить что но нужно есть чем сравнить объективно вот можно сравнить что да пока пока это не так круто но но как бы прогресс явно виден потому что те модели учится там за несколько тысяч игр марки учится среднем но там за несколько сотен и причем на обычном цпу без без каких-либо там без без больших вычислительных затрат вот ну и наконец что дальше дальше ну вот я снова буду восстанавливать иерархический уровень они работают сейчас они в моменте не работают к сожалению то есть маркер сейчас два уровня и перепутал букву значит на цирке подключить этот али не поиграть в разные игры и на окон я и джин и на али мы сейчас этим занимаемся мы не чуть в разном формате отдают данные кому интересно скрипт ну как бы на питоне который я там для написал я могу в принципе отдать стоя поправка что для марки написан но как бы неважно а для али мы сейчас еще там добиваем он еще пока не до конца работы и еще есть особенность что эти разные игры в реинфорст тренинге в этом джиме они по-разному как бы и по-разному устроены подкрепления там в разном формате иногда выдаются по разному устроено окончание игры и в этом смысле такой челлендж но то есть в общем в разные игры играть это нужно еще немножко изучать игрушки потому что движок марка бы движок 1 но вот скрипт который работает с игрушкой он похоже должен быть там он там в деталях он немножко раз поэтому в общем это заняло времени занимает больше, чем хотелось. Ну вот, собственно, я думаю, что я все на сегодня рассказал. Все, что хотел. Глядя по уставшему лицу Антона, я понимаю, что уже нет сил дальше слушать. Так что готов ответить на вопросы. 

S02 [01:18:05]  : Игорь, спасибо. Я, видимо, устал писать вопросы вначале. Правда, часть на них ты уже ответил. Вопрос первый. По твоему опыту, там же есть негатив ревард и позитив ревард, про которые ты говорил. У тебя негатив ревард, он помогает или мешает? 

S03 [01:18:29]  : очень помогает помогает потому что смотри это все все данные которым получаем это информация о мире негатив ревард это тоже информация о мире причем даже ну даже может быть ну то есть ну не менее важная чем позитив другой просто их сильно больше да но но они такая же и помогают 

S02 [01:18:52]  : Я еще точню. Я же тоже похожие эксперименты ставил, но я понял такую вещь, что там многое зависит от того, как мы этот негатив-реварт используем. И в некоторых случаях я понял, что если негатив-реварт использовать именно как реварт, а не как информацию о формировании модели мира, то просто отбивается у агента желание учиться и экспериментировать. 

S03 [01:19:18]  : вот у тебя таких ситуаций не было это тонкий момент это правда да я тоже много с этим возился вот все вот эти архитектуры все эти версии промежуточных там было много сотен там было много разных вариантов это правда с ревардом надо я тоже пробовал много разных много разных вариантов, но если ты обратишь внимание и вспомнишь там слайды, которые я показывал, то в конечном итоге у меня у парсера две таблицы. Одна как бы это картины мира, таблица корреляции, а другая это как бы мы про эту картину что про эту картину мира может сказать с точки зрения как бы ревортов и это как бы 2 2 части информации разным окей 

S02 [01:20:19]  : про постановку целей. Ты сказал, что выпадает пока постановка целей. Вопрос, а там нет такого, что все-таки для того, чтобы сделать какое-то действие Мартии в неявном виде все равно нужно Поставить, если не цель, то по цели. То есть, понятно, что цель выиграть игру он поставить себе не может. Эта цель ему дана, так сказать, свыше на уровне генов. Вот как у теленка сосательный рефлекс. Но по цели... 

S03 [01:20:56]  : которое ему да вот по цели он ставит себе с одной стороны но вот в предыдущей архитектуре который я убил которая была не так эффективно я по-другому в иерархии с точки иерархии когда тесть иерархия то, грубо говоря, каждый следующий слой иерархии в некотором смысле ставит цели для предыдущего. Грубо говоря, если вернуться к одному слайду, например, сюда. сейчас был такой слайд с под целями не знаю вот он сейчас сейчас не видно сейчас видно сейчас видно Вот слайд с... Ой, сейчас, извини. Вот так. Когда на верхнем уровне иерархии, грубо говоря, следующим действием является доклад, то на нижнем уровне иерархии это автоматически ну как бы ставит свои под цели как бы такие декомпозиции но и в этом плане когда когда когда модель как бы доходит до до некоторого уровня иерархии то для нее элементарными действиями уже являются крупные если так можно выразиться цели то есть то что для нижнего уровня является на самом деле цели и это такая ну неявная постановка цели если хочешь но она есть но мой то ты здесь еще вот был про что про то что с точки зрения тестирования в Atari вот этого Reinforcement Learning Environment там нету постановки целей, то есть там нету как бы механизма постановки целей и соответственно механизм проверки как модель ставит эти цели. Ну то есть выиграть игру это вот она как бы неявная цель, а дальше там, но для этого надо там что-то делать, но мы не можем, глядя со стороны на модель, уверенно сказать, что она там строит какие-то там цели или не строится, как бы нет. Для этого нужен какой-то environment другой, который будет задачу решать. 

S02 [01:23:43]  : Ну а там нет такого, что цель это зарабатывать максимум положительного реварда? 

S03 [01:23:48]  : Да, но цель поставленная не моделью, а извне. 

S02 [01:23:52]  : Да, но она единая для всех моделей. 

S03 [01:23:55]  : Верно, и я про что. Но если мы говорим про определение интеллекта, что это умение адаптироваться, ставить цели и их достигать, то то как бы вот этот этап постановки цели, он как бы выпадает из этого тестирования в принципе, то есть оно там отсутствует, ну оно за кадром остается, понимаешь? 

S02 [01:24:16]  : Хорошо, спасибо. Александр Тетеркин комментирует, что мозжечок не только движение, а любая автоматизация, хотя мысли это тоже движение с другой стороны. Соглашусь. Вопрос по поводу торможения. Там у тебя на схемке было нарисовано, что стратутор тормозит связи таламуса. Причем именно связи. Вот здесь можно уточнить, тормозятся или именно связи? 

S03 [01:24:41]  : если связи то что подразумевается под связями это как бы вообще вопрос архитектурный в мозге насколько мы понимаем там сегодня вроде как тормозятся конкретные это сказать области таламуса которые связаны с критикальными колонками но как бы главное что в любом случае тормозят то есть грубо говоря, когда мы затормозили, когда мы обрубили, так сказать, канал связи к колонке, она и не работает, у нее информации нет на входе, она там ничего не получает. То есть в этом плане без разницы, что затормозить колонку, что связь с ней, что кусочек, который ей передает информацию, но как бы вот в мозгу сделано так. Архитектурное тоже можно сделать, можно колонку тормозить, можно связку, неважно. Важно, что что-то здесь должно какой-то элемент затормозит, чтобы данная колонка, грубо говоря, не участвовала в голосовании. Этого депутата мы не выбрали. Он, так сказать, пролетел мимо выборов. 

S02 [01:25:41]  : Спасибо. Вопрос терминологический. Ты там говоришь про парсеры, но из того, что я услышал, мне кажется, что с точки зрения НЛП это скорее не парсеры, а токенайзеры, потому что парсеры все-таки устанавливают связи между словами в предложении, то есть они все-таки про структуру, а выявление элементов это все-таки токенайзер скорее. 

S03 [01:26:05]  : Может быть, может быть и так, но это еще Шумский такой термин в свое время задал, но я его исторически следую. 

S02 [01:26:12]  : У меня тоже называются парсеры, тоже исторические, тоже я понимаю, что это неправильно. Я просто уточняю. То есть, парсер, он выделяет просто объекты, правильно? Которые выстраиваются в некоторую последовательность. То есть, парсер, он выделяет вот эти самые символы, из которых потом строится последующая обработка. Он не строит какие-то структуры из этих символов. 

S03 [01:26:31]  : Строит. Внутри себя, конечно, строит. структуру все-таки строит до внутри себя устроить некоторые структуры и для него одни структуры важнее ценнее чем другие и он конечно же пытается грубо говоря если парсер получает на входе там мама мыла мама делает торт потом он получает реворт положительный потом снова мама делает торт и он получает реворт положительный а потом он получает предложение мама дала мне по шее и получает отрицательный реворт 

S02 [01:27:13]  : то потом в дальнейшем при поступлении слова мама он предпочтет продолжение мама делает торт чем мама да вам не по шее окей окей простая окей спасибо вопрос технический вот смотри ты говорил что там вот у тебя вот и вы разделяешь данные о Данные из Environment, данные о текущем действии и данные о подкреплении. Это с точки зрения сенсоров, с точки зрения того, что подается на вход. Это одни и те же данные, которые имеют разный смысл. Или все-таки вот он понимает, да, то есть я вот смотрю на свои руки и понимаю, что вот мои руки – это то, что мои действия, а то, что там на экране, на экране – это сигналы среды. Но с другой точки зрения… Да-да-да, понятно. 

S03 [01:28:06]  : нет смотри это разбиение имеет только один на самом деле смысл мозг это некий как бы объект ну как как принято сейчас некоторыми людьми считать именно в том числе это некий объект сидящий в черной комнате в который ничего не видит, не слышит, и к нему просто стекаются сигналы. А он эти сигналы процессит и обрабатывает обратно. То есть в некотором предельном смысле мозг это просто черный ящик, который обрабатывает сигналы. в этом смысле просто есть сигналы есть двух типов ведь ему нужно что-то делать если он не хочет получить по шее то он должен что-то делать а что-то делать это означает выдавать какой-то сигнал то есть в этом смысле архитектурно есть грубо говоря сигналы к нему входят все сигналы то есть как бы и и и сенсорный моторный и в этом смысле это единый сенсор моторный поток но но он знает что часть этих сигналов ну как бы он смысле архитектуры это знает что вот кусочек этого сигнала это как бы выходные ему нужно подбирать выходные данные это тоже цифровой вектор ну тоже какого-то любого размера в данной ситуации спасибо с точки зрения отари вот что конкретно подразумевается под высокочастотной составляющей ну вот это легко объяснить когда у нас есть там марки получает ну вот сейчас он конкретно учился и учится на на образе памяти. есть образ памяти 128 байт. он как бы на входе имеет 128 байт. из этих 128 байт первый вообще никогда не меняется. 192 он равен. я не знаю почему, но он такой. у него частотность 0. а условно координаты ракетки там координаты мяча меняется быстро часто и в этом смысле это сигнал вот это компоненты вектор она она более высокочастотная и он просто берет этот вектор и выделяет из него высокочастотные компоненты они потому что весь вектор можно можно и всем оперировать но просто сильно дольше Это как взять нейронную сетку и всю ее гонять или сразу выделить какие-то наиболее важные части. 

S02 [01:30:34]  : Спасибо. Пошли вопросы от Игоря Романенко. А что если этот низкочастотный сигнал не влияет на принятие решения? 

S03 [01:30:41]  : Да, такое тоже возможно. но как бы в первом приближении кажется что нет и ответом будет только если модель учится без него значит хорошо если не учится мы будем возвращать может быть с водой выплеснули дом когда 

S02 [01:30:56]  : Следующий вопрос от Игоря. Правильно ли, что каждый кодер получает лишь свою уникальную часть вектора? Могут ли эти части повторяться в разных кодерах, в разных комбинациях? 

S03 [01:31:08]  : В моей конфигурации сегодня да, они могут повторяться. Я пробовал много разных. Я сейчас показываю версию, которая работает неплохо. не обозначает что другие комбинации не будут работать может быть я просто до этого чуть другой не так крутил вполне то есть сейчас у меня например но вот там просто исходный вектор он там разбивается на части как-то и да могут повторяться ну вот там мне кажется это не с точки архитектуры это не принципиально просто лишний вычислительный ресурс но если бы он был неограниченный то можно было бы делать как бы вообще все по чесноку то есть другими словами я так и делал сперва но просто дольше вообще бить его там на n абстрактные случайных кусков и процессить все у тебя просто получается большая модель и там и все дольше происходит шума больше так вопрос от меня про подкрепление вот в deep learning и deep learning подкрепление оно 

S02 [01:32:17]  : Пропагируется, распространяется, точно так же как в нейросети backpropagation распространяется, ошибка распространяется, точно так же в DQN подкрепление распространяется, но по шагам, а не по слоям. Я экспериментировал с глобальным подкреплением, когда подкрепление действует на всю последовательность действия, но возникает вопрос, а где началась последовательность действий. А у тебя фигурирует N шагов для подкрепления. Соответственно, откуда берется число N? 

S03 [01:32:49]  : Хороший вопрос. Прекрасный. Я стараюсь в своем моделировании... совсем физиологичный модель потому что ну как бы вот модель там то есть парсер как модель колонки тоже так ну в общем с вопросом но тем не менее в каких-то сущностных моментах но мы пытаемся быть плюс-минус более физиологичным в этом плане например там есть оценки что допустим колонка может там если там конкретная колонка классифицирует допустим около там 30 плюс-минус там 10 объектов как классификатор если она работает я имею ввиду колонка в неокортексе поэтому у нас парсер скажем тоже работает там ну с 30 плюс-минус там 10 буквами например поэтому с точки зрения шагов на мой взгляд ну как бы не честно выдавать этот реворт ну как бы до бесконечности обратно потому что ну как бы я не представляю себе механизму в мозгу который бы это делал но мы знаем что например у крысы когда значит она получает там некий как бы реворт у нее включается некий механизм реплея таким обратным ходом и она пытается понять что она сделала до этого а что было до этого а что было до этого а что было до этого и по порядку величины это там но десяток шагов она как бы повторяет в этом плане у меня сейчас вот закрепление в моменте стоит 10 шагов назад и есть механизм, можно вообще без этих шагов обходиться, можно теоретически вообще делать, ну если у тебя есть функция ценности и просто ты следующие, ты каждый раз уточняешься на значение, уточняешь предыдущее значением следующего, с каким-то коэффициентом. Но это сильно медленнее передается, чем ты передаешь там реворды на 9 шагов. Я передаю сейчас на 10 шагов назад, не знаю, насколько это прям совсем физиологично, но это не выглядит прям ну как-то Совсем не честно. 

S02 [01:35:03]  : Ну, не знаю. Смотри, вот это вот по чуть-чуть передавание, это как раз вот как Дип Кюленинг делает. А насчет 10 шагов вопрос. А ты есть, нет гипотезы, что если сделать, к примеру, 20, то вот будет чуть-чуть лучше? 

S03 [01:35:21]  : Будет лучше, конечно, да. окей но опять-таки это простой способ выиграть то есть как бы я стараюсь избежать простых способ выиграть которые сразу как бы дадут как кажется быстро выигрыш но но не приблизит к какому-то фундаментального понимания мне кажется что мне внутренне кажется что вот как бы много шагов обратно ревард это неправильно 

S02 [01:35:47]  : Поделюсь гипотезой, что как раз вот эта фишка – это в том, как правильно определять число шагов. Но это ладно, это уже дискуссия. Интересно. Следующий вопрос от Владимира Потапова. Правда, по-моему, ты про это говорил. Что является входными данными и как входящий сигнал преобразуется во входной вектор? 

S03 [01:36:09]  : Ну да, я про это подробно говорил в начале, видимо Владимир может опоздал, но в общем входной вектор на самом деле зависит от environment. так от среды с которой работают марки марки это универсальный как бы движок если хотите там ну просто некая модель интеллекта которые которые на входе там ну как какой-то как какой-то цифровой вектор например миллион чисел там или 10 чисел или 500 сколько-то откуда эти числа берутся это вот другой вопрос но В данном случае мы работаем вот с инверментом, который как бы на каждом шаге генерирует какой-то этот вектор, он берется оттуда. А дальше вот этот входной вектор цифровой преобразуется внутри, ну вот в некоторые, так сказать, символы, с которыми дальше идет работа. Я, собственно, рассказывал в свете кластеризации, как это происходит. 

S02 [01:37:06]  : Спасибо. Тут у нас участники размышляют о том, чему аналогичен Марти. Да, я вижу. Агент в Марте. То ли это аналог колонок коры. 

S03 [01:37:16]  : Да, да. Это аналог колонок. В прямую это и есть колонка коры. 

S02 [01:37:21]  : А Олег Колотунов предположил, что это нейроны ядер Таламуса. 

S03 [01:37:25]  : Нет, это не нейроны. колонка мы думаем что колонка то есть в каком-то смысле схожей идеи развивает джефф хокинс он как бы по-другому немножко его архитектура там другая но но базовая идея такая что колонка это некоторый фундаментальный элемент и как бы я так вам скажу по-другому скажу значит если кто-то из вас смотрел вот недавно было с Анохиным прекрасное интервью с Познером я прям с большим удовольствием посмотрел и там анохин говорит он уже он до этого и лекции у него была вот там про темную материю где тоже про эти вещи было про некоторые базовые принципы что вот должны быть некоторые базовые принципы как в теории эволюции дарвина которые вот там что как бы которые все определяют и в частности он как раз приводит эти три принципа эволюции дарвина один из которых там конкуренции между видами так вот я думаю что здесь похоже что этот один из принципов он есть и он состоит в том что эти колонки так в чем была сила теории дарвина в том что когда мы как бы природа придумала некоторую так сказать фреймворка если так можно выразиться да то потом отдельные экземпляры этого фреймворка вот все эти отдельные особи конкурируют друг друг с другом имея возможность там наследовать свои свои свои изменения они как бы вы вытягивают на себе всю эволюцию здесь явно просматривается такая же история вот эти колонки они конкурируют друг с другом ЗАТАМ если хотите внимание стриатума за там за за за работу с таламусом там в общем по праздну можно сказать они находятся в конкуренции друг с другом и как бы и и тоже там работает некоторая так сказать эволюция и вот там побеждает ну не сильнейший а тот кто дает наиболее релевантный прогноз судя по всему и в этом плане это тянет на принцип прямо на на на на фундаментальный принцип то есть колонка природе не нужно было делать нейрон не является элементарным элемент он является элементарным объектом но он не является элементарно функциональным объектом а функциональным объектом является колонка которая как раз и природа когда когда появилась колонка концепция и вот это конкуренция между ними за как бы правильное предсказание вот я думаю что вот это вот и есть оно это как бы ну это мой прогноз одного из этих фундаментальных принципов я думаю что он такой 

S02 [01:40:35]  : Слушай, ну насчёт фундаментальных принципов, насколько я помню у Хокинса, я, правда, значит, буду это говорить своими словами, но суть в том, что колонки не только конкурируют, но они ещё и как бы совместно договариваются, вот, и, собственно, в награду получает та колонка, которая лучше всех договориться с другими колонками о том, в чем же все-таки заключается ситуация и что нужно сделать. То есть там пресловутый социальный консенсус и кооперация наряду с соревнованием. 

S03 [01:41:06]  : Это его модель, она такая. 

S02 [01:41:09]  : Голосование в твоей модели отличается от голосования по Хокинсу? 

S03 [01:41:14]  : я пока вот не готов рассказать такое голосование я хочу его там как минимум опубликовать просто для публикации некоторые вещи нельзя рассказывать и так тут рассказать а где ты хочешь ну я сейчас на семей хочу статью дать там на hk по моему сейчас принимаются Ну, у меня есть две конференции, на которые я ориентируюсь. Не у Рипса, а в ICMA. Остальное меньше. 

S02 [01:41:44]  : Спасибо. Так, значит, Олег Култунов пишет, что у него создалось впечатление из твоего рассказа, что парсеры находятся перед Таламусом, а не за ним. 

S03 [01:41:55]  : вопрос терминологии господи что там перед чем как хотите потому что это разные сущности которые работают там по разному я даже не понимаю перед после но там на слайде на слайде то есть они на слайде они нарисованы за таламусом таламус получает как бы сенсорную информацию извне и дальше и передает как бы парсером так окей а почему их 7 ну вот в данности как бы модель которую я показывал модель, которую я показывал, вот как она играет, вот да, которая там ну плюс-минус как-то видно, что она училась играть там за 700 игр. Вот эта конкретная модель для этой конкретной игры на Atari для вот этих 128 байт развернулась вот в такую конструкцию. Как бы я уже говорил, я предполагаю, что в отличие от нейросети в чем здесь фундаментальное отличие с нейросетью нейросеть это как бы мы строим миллиард этих нейронов там сто миллиардов параметров и поди где-то в этом гигантском случайном пространстве найдется какое-нибудь место которое ну как-нибудь опишет ситуацию в отличие от этого марки делает не так как бы мы смотрим на входной сигнал вот он какой-то идет и И дальше, исходя из характеристик этого сигнала, ну как бы строится какой-то кусочек модели. Если он работает хорошо, если он там со временем видно, что плохо работает, оно наращивается. Но это пока еще тоже не доделано, но глобальная мысль, то есть оно архитектурно так. Поэтому в данной ситуации 7. Ну вот так получилось. О чем я знаю. Спасибо. А, кстати, в брейкауте 3, так к слову. В смысле, тот же марте, но 3? 

S02 [01:43:54]  : Ну да. То есть, все-таки это число, оно подбирается под игры, да? 

S03 [01:44:01]  : но в данной ситуации это высокочастоты это анализ частотности и как бы оно поэтому там как-то соответствует характеру игры я не знаю ну чему-то но сейчас это с моей стороны как бы быть немножко самоуверенно потому что я пытался вот к этому семинару хотя бы еще хотя бы одну игру там что чтобы ну вот как бы и этом брейк-ауте и там такие вообще там главная проблема в этих играх она состоит в том что игры написаны все эти симуляторы написан с косяками и потом когда ты до них добираешься в стране этих косяков вот на них уходит там количество времени и вот например брейк-ауте я там не сумели пока устроить косяк поэтому 

S02 [01:44:48]  : Косяк в симуляторе? В самом симуляторе? На стороне OpenAI Gym? 

S03 [01:44:54]  : а что удивительного, программисты пишут программы, что такого-то, что OpenAI Jim святые, что ли люди, господи, умоляю. кто там пишет OpenAI Jim, я вообще не знаю. я могу рассказать, в чем косяк состоит, если интересно. Environment генерирует данные, данные идут в модель. при этом ты видишь, что как бы у тебя есть экран, на экране что-то происходит, да, модель получает данные, ты ожидаешь, ну подсознательно, нормально, да, что вот у тебя, делаешь шаг, у тебя на экране там что-то, сдвинулся мячик, сдвинулась ракетка, модель получила новые данные, делаешь следующий шаг, у тебя сдвинулся мячик, сдвинулась ракетка, получил новые данные, ну как бы, а как еще, это же логично. а в брейкауте бывает что-то такая вот иногда иногда когда летает внизу эта ракетка отбивает мячик для тех кто не знает наверху кирпичи цветные там она разбивается и вот иногда когда ракетка мяч пропустила и он ушел вниз Эпизод не обрывается, а данные продолжают идти. Такое ощущение, что этот мяч ушел в какую-то вселенскую космическое плавание и дальше Environment продолжает еще там тысяч 30-40 шагов слать информацию об этом спутнике, который уже к Марсу куда-то улетел, но из экрана уже исчез. я это обнаружил скажем ну там совсем недавно думаю что такое вообще за фигня смотришь а модель там как бы а данные какие-то идут модель то чему-то учится чем там учится непонятно а я не знаю что это происходит если это не косяк то пусть мне объяснят что происходит я не знаю короче вот ну я просто за что купил зато продаю другие 

S02 [01:46:45]  : Вы эти косяки решаете на своей стороне или вы как бы сабмитите исправление в OpenAI Gym? 

S03 [01:46:52]  : Хорошая мысль, надо просто сабмитить действительно. Я темнота, я сам все решаю. 

S02 [01:46:58]  : Понятно. Спасибо. Так, вопрос от Владимира Потапова. Наверное, надо коротко, потому что, по-моему, очевидно было. Можно ли подробнее про кластеризацию? Что кластеризуется и какой в этом смысл? Это распознавание образов? Если нет, то есть ли в этой модели узнавание знакомых образов и запоминание новых? 

S03 [01:47:17]  : видимо просто Владимир не знаком с тем что кластеризация есть такой как бы математический термин кластеризация когда у вас там много данных много точек в некотором цифровом пространстве вы их можете разбить на группы там условно вот вот допустим вот вот если вот мы сейчас сидим в семинаре у меня написано 20 23 один только что вышел партизан был 24 23 участника и Если вот сейчас взять, например, и каждого из нас там ярким прожектором посветить в небо, а дальше мы посмотрим со спутника, выяснится, что мы все в разных городах сидим. Но сидим там, ну как бы двое, например, сидят там пятеро в Москве, там двое как минимум в Новосибирске, Николай Робчевский вообще там сидит в Нью-Джерси. И окажется, что вот как бы мы все кластеризуемся на разные группы. в одной группе будет там три точки в другой 5 в 3 1 вот эти группы называется кластерами это как бы такой способ ну разбить пространство на группы если хотите вот просто марки разбивает большое векторное пространство то есть это способ если хотите провести цифроаналоговое преобразование но такое специфическое вот 

S02 [01:48:39]  : Спасибо. Я тут, извиняюсь, пропущу несколько вопросов, потому что, по-моему, есть вопросы, на которые было подробно рассказано. Поэтому, если останется время, к ним потом вернемся. Следующий вопрос. У Марти сохранение обучения есть или обучение сбрасывается, когда стартует новую игру? Что, если в игру добавить пол, стены, сможет ли он переобучиться? Можно поподробнее, как обучение проходит в реальном времени? 

S03 [01:49:07]  : ну как бы второе проще сказать ну в реальном времени это значит он просто вот как бы то что вы видели видео да я его даже замедлял чтобы она нормально воспринималась как бы она работает со скоростью инвайрмента при том что марки написан на я вед прототип написан так ну в общем как удобно мне не так как надо с точки зрения оптимизации те кто знает пугами у тебя знаешь это разные вещи И в этом смысле в реальном времени. А что касается сохранения обучения, то грубо говоря, вот я его каждый раз... Вообще он написан, есть сохранение, но я им не пользуюсь. Он каждый раз начинается с нуля, если хотите. Разворачивается новая модель. и каждый раз с нуля но но но как бы пока вот он идет этот один один как бы сеанс обучения что ли он естественно он как бы запоминает свое состояние то есть он учится играть и каждая следующая игра теоретически должна быть ну как бы лучше чем предыдущие но они так и просто они разные гоняет по разным углам поэтому есть игры допустим все были бы абсолютно одинаковые друг за другом шли я думаю что прогресс был бы виден быстрее интересно а что касается пол стены и переобучиться марки учится как бы у него пока проблемы с как бы вот проблема переобучения или точнее обучение новому и давайте так скажу у марти не будет проблемы катастрофического забывания которые свойственны существующим моделям и но как бы я пока не буду рассказать про то как это сделано будет потому что это еще не сделано сейчас знает как сменить там шаг следующий запланирован ну просто руки не дошли 

S02 [01:51:02]  : поэтому поговорим когда вот будет видно разные игры когда я на этом покажу как-нибудь вот тогда понятно как я извиняюсь а про управление времени там же в инвалидности временем ты управляешь то есть получил когда ты получил сигнал от инвалидности ты сгенерировал экшен и передал его 

S03 [01:51:22]  : ты получил то есть вот что значит в реальном времени но то есть это значит что ты не управляешь время на самом деле это вот ты сейчас не только сколько тебе надо на обработку столько ты взял просто будешь долго тратить просто игра будет но есть ведь еще быстродействие environment мой тезис в том что марки работает так что я скорее сейчас определяюсь быстродействие environment они быстродействие марки окей то есть то есть то есть марти инвайрмент не тормозит я бы сейчас не понял но но как бы то твои мысли кто там кого тормозит но в общем есть инвайрмент который работает со своей скоростью есть марти который работает со своей скоростью понятно что общая связка будет работать со скоростью более медленного из них мой тезис сейчас в том что инвайрмент медленнее работает чем марти 

S02 [01:52:14]  : Окей, принял. Спасибо. Планирует ли Марти, где должна быть ракетка через некоторое время в зависимости от предыдущего движения мяча? Если да, то как это происходит в текущей архитектуре? 

S03 [01:52:30]  : Да, это хороший вопрос. В текущей архитектуре это происходит очень на коротком промежутке времени. А вообще я про это говорил уже. Я понимаю, что это идея сложная, которую сразу сходу не воспринять. Знаете, еще раз, когда мы поднимаемся по иерархии модели, то каждый следующий уровень вбирает в себя некоторое количество предыдущих. Если так можно сказать, В общем, чем выше уровень иерархии, тем дальше марки планируют, где будет ракетка в зависимости от текущей ситуации. Я так выражусь. Но сейчас в моменте для данной конкретной реализации, которую я сегодня показываю, это пока буквально там пара шагов. 

S02 [01:53:30]  : Спасибо. От Владимира Смолина. Можно повторить про понимание и его расширение на уровне 6? 

S03 [01:53:37]  : Я не понял, про уровень 6? Я тоже. 

S02 [01:53:43]  : Про уровень 6 я тоже не понял. Владимир, может быть вы поясните, а пока давайте вопрос от, пока Владимир подключается, Дмитрия Салихова. Я уже подключился. Ага. 

S00 [01:53:54]  : Да, да. Может быть это неправильно я назвал уровнем, там была цифра 6-2, какие-то названия были, и там как раз у меня там связь была плохая, я так не прослушал. 6-2. 

S03 [01:54:10]  : я честно не помню меня эти цифры просто из других слайдов которые я для себя делал и поэтому там часть честно говоря пропущена большая сейчас я посмотрю где-то 62 сейчас извините я лучше здесь посмотрел сейчас забью шесть шесть один я вижу они еще не по порядку это правильно чтобы слушатели сбить так давайте я вам сейчас сначала покажу потому что я шесть два числа не был один один один два два 4, 5, 3, 1, 3, 2, 4, 5, 6, 1, 7, 1. 

S00 [01:55:01]  : Ну хорошо, может быть 6, 1, там была фраза про расширение понимания, в каком смысле? 

S03 [01:55:09]  : Про расширение понимания? Что-то я не помню, что я такое говорил. 

S02 [01:55:17]  : Ладно, давайте тогда дальше двинемся. Вопрос от Дмитрия Салихова. Непонятно, как происходит обучение причинно-следственным связям. В интервал минус 100 кадров могут попасть действия, могут попасть действия, наоборот, вредящие достижению цели. Как происходит кредит-ассаймент? 

S03 [01:55:33]  : А что такое интервал минус 100 кадров? Откуда он взялся? 

S04 [01:55:40]  : я, наверное, неправильно понял тебя, Игорь. ты говорил, что где-то у тебя там промелькало цифра минус 100. когда пришел реворд, тебе нужно посмотреть историю на какое-то количество шагов назад. я не понял, с каким окном ты это делаешь. да-да, верно. 

S03 [01:56:02]  : окно 10 шагов, ну вот в данной модели 10 шагов, минус 100 это как бы реворд. просто я в марке, внутри него реворд плюс 1 минус 1 конвертируется в плюс 100 минус 100. с технологическими целями это не имеет смысла никакого специального. в этом плане на 10 шагов назад реворт раздается как бы вопрос который правильный абсолютно непонятно как происходит так в интервал минус 10 шагов могут попасть действия, вредящие достижению. это правильный вопрос. он всегда есть. он у всех есть. он не относится к марте, он относится вообще к reinforcement learning. когда у тебя sparse reward, когда у тебя reward как бы сильно позже, чем ты сделал что-то правильное, то очень сложно понять, ну как бы что конкретно наградить. это главная челлендж, который есть в рефорсном тренинге. я считаю, что я его... что я придумал офигительную штуку, которая это решает, но я про нее как раз сегодня вот ну там по неким причинам предпочитаю не рассказывать. но тем не менее твой вопрос остается правильным по поводу действий а что касается обучения причинно-следственным связям то на уровне парсера я уже говорил про это там как раз причинно-следственные связи очень просто получается вот смотри еще раз представь себе что у парсера на входе значит слова буквы буквы которые складываются слова а он как бы пытается причинно-следственные связи установить он значит слышит слова мамы делает торт Хоба получил там подкрепление положительное. Плюс 100. Потом опять мама делает торт. Хоба опять плюс 100. Мама делает торт. Опять плюс 100. Потом мама дала мне по шее. Хоба минус 100. ну он он он для себя как бы ну пока с небольшим коэффициентом но чем дальше тем больше он как бы для себя делает постепенно вывод что мама дала по шее это как бы ясная причина следственная связь которые ну как бы этого и этой этой как бы если хочешь фразы надо избегать чтобы не получить негативный реворт как-то пытается и он как-то со своей стороны пытается подействовать на мир. значит, чтобы избежать этого негативного реворта. В своих терминах. Причем прикол заключается в том, как бы особенность вообще всей этой конструкции, она необычная немножко. Знаете, ее можно сравнить с экспертами. Вот есть, вам нужно решить какую-то задачу. Менеджеру некому там нужно решить задачу. Он зовется экспертов. сидит у него за столом там 10 человек все с разными бэкграундами один философ другой математик третий там я не знаю дворник разных набрал и каждый высказывает свое мнение на базе да ну некоторые из вас знают а некоторые еще кстати не знают я тут анонсирую я тут написал книжку которая выйдет надеюсь на в начале декабря она просто про это как в людях все вот этой истории работает и каким прикольным она результатов приводит но на мой взгляд очень интересно в начале декабря до тех тех кто слушает и потом будут смотреть в общем надо будет погуглить слово тербидень такое тербидень тербидень, но оно пока еще его нету нигде, этого слова, но я потом в комментарии под видео напишу, когда книжка появится и просто ссылку на книжку дам, но я сейчас, я сейчас к другому. 

S02 [02:00:00]  : То есть в Google Alerts можно вбить слово тербидень и как только книжка выйдет, то придет алерт, да? 

S03 [02:00:05]  : Хрен знает, не знаю, я не знаю, как это работает в Google Alerts, я честно, я отсталый человек. может быть да но значит возвращаясь к экспертам вот у вас там все 10 экспертов и каждый со своей картиной мира каждый в меру своего понимания ситуации имею свое понимание слов как-то высказывается какие-то свои соображения дальше менеджер должен принять решение вот как он будет его принимать чем мне не более важно кто там какими словами оперируют крайне интересный вопрос вот марте он прям про это то есть как бы вот это в каком-то смысле в верхнем уровне вы если хотите она аналогия того что происходит там как я думаю у нас мозгу во многом 

S02 [02:00:51]  : Спасибо. Задает вопрос Алексей Удот. Используется ли бэкпроп, ну по-моему про это говорили, или все обучение на матрицах корреляции. Вот матрица корреляции есть или нет? 

S03 [02:01:03]  : Да, конечно, матрица корреляции есть. Никакого бэкпропа нет. Нет, есть только не считать бэкпроп, в некотором смысле бэкпроп, вот когда мы получаем реворд, то этот реворд, как там Дмитрий правильно сказал, я в терминах всегда слаб, вот в правильных, в каком-то окне там назад оно там раздается. Это не совсем бэкпроп, потому что мы знаем, ну вот я упоминал, что физиология у крыс это прям, ну и у человека это есть, но просто на крысах экспериментировать. У крыс это есть, мы знаем, что есть реплей, она как бы себе такое делает себе заметочки на полях, но я ничего не называю это бэкпропом. 

S02 [02:01:45]  : Спасибо. Здесь несколько участников просят снова рассказать про механизм работы колонки. Потом пересмотрите семинар. Да, да. Ну или подождать следующего доклада, где будет более подробно. Так, наконец дошла очередь до руки Александра Балдачева. Александр, пожалуйста. 

S01 [02:02:04]  : Добрый день всем. Игорь, очень большое спасибо за доклад. Было очень интересно. У меня, как всегда, пара теоретических вопросов. И, кстати, очень хорошо было получить по шее. Прямо так вот в жилу зашло к моему вопросу. Значит, ну прежде всего с пониманием. Вот если ребенок получил по шее или получил конфеты и после этого перестал играть в футбол, это называется, что он понял? 

S03 [02:02:32]  : Мы достоверно не знаем, но мы можем сделать второе предположение. Я делаю такое предположение, что он понял. 

S01 [02:02:40]  : То есть я же хотел бы сказать так, что действие, которое совершил человек после того, что нужное нам могло произойти, либо путем именно подкрепления, он получил по шее и понял, что да, это больно, не буду делать, и куснул пальцы в розетку. И больше не стал. Он не понял, почему. Что это электричество? Либо получил конфетку и тоже понял, я буду это делать, поскольку я получаю конфетку. Он ничего не понял. Есть еще разница между знанием и пониманием. Есть огромное количество знаний, которые мы получаем. Скажем, та же блондинка знает, как управлять машиной, но она ни черта не понимает. Поэтому понимание и результативное действие, соответствующее, оно связано очень-очень далеко. И если говорить именно о подкреплении, вот той ситуации, которую мы описываем, как работает Марти, там понимания и близко нет. 

S03 [02:03:38]  : Александр, я так скажу, я не поддамся на вашу провокацию, она замечательная. Провокация в том смысле, что вы меня провоцируете выйти из области математики в область, так сказать, философии, не хочу здесь. А это не математика и не философия. Не про это. А я говорил про математическую модель. Хорошо. Работает, учится, и что бы мы о том себе не говорили. Понимание знаний. 

S01 [02:04:01]  : Тогда вторая провокация. 

S03 [02:04:03]  : Она работает и учится, ну блин, ну о чем тут говорить. Вы понимаете, без обид, я с удовольствием поспорю по поводу понимания и знания, но вот не конкретно к этому семинар, потому что это здесь сейчас. 

S01 [02:04:16]  : Хорошо, значит вторая провокация, если значит так, это по поводу предсказания. Что интеллект – это тот, который предсказывает. Но, понимаете, история с естественным отбором, рассказанной вами, с эволюцией, она тоже учит зверюшек так ли, сяк ли предсказывать. То есть через несколько поколений, множество поколений сначала как-то предсказывали клетки, потом как-то предсказывают малькопитающие. Это по-разному. И мы можем сказать, что вот есть некий интеллект, который нам позволяет предсказывать. Но есть другой способ предсказания. Это закрыть глаза, подумать и предсказать. И вот эти два разных предсказания, каким мы должны отнести к интеллекту? Первое или второе? То есть предсказание зверюшек на большом опыте проб и ошибок, либо предсказание, сделанное человеком, нисходя с места мышлением. 

S03 [02:05:18]  : опять-таки я не поддамся на вашу провокацию потому что потому что не вижу смысла выбирать нет провокация является сам вопрос или или а почему надо выбирать просто на разных уровнях но смотрите предсказание большое слово даже как сознание большое слово начинается когда люди начать а неужели там вот у мыши есть сознание и в скобочках они наверно понимают что ну мыши английскую литературу средневековую не читают конечно не читают но как бы слово сознание такое большое слово предсказание такое большое давайте мы будем считать что есть градация предсказания от нуля там да я не знаю до миллиона 

S01 [02:05:56]  : И на каком-то уровне зверушки. Я с вами полностью согласен. 

S03 [02:05:59]  : Почему я должен выбирать между вашими данными? 

S01 [02:06:04]  : Выбор не этот. Выбор должен четко сказать, что когда мы говорим об интеллекте, мы говорим о предсказании методом тыка или о предсказании методом мышления. 

S03 [02:06:14]  : А что такое метод тыка? 

S01 [02:06:18]  : Эволюционный метод или метод подкрепления – это метод тыка. Что такое метод тыка еще раз? Ну-ка расскажи. Случайного подбора, метод подбора. 

S03 [02:06:29]  : Вот допустим, если мышка случайно, грубо говоря, нашла где-то сыр, это методом тыка. 

S01 [02:06:39]  : Это методом тыка. А если я подумал, что я не пойду в обувной магазин, а пойду в продовольственный магазин, потому что там есть сыр, и мне хочется сыра. Это метода мышления. 

S03 [02:06:52]  : Вы знаете, вы сейчас не обижайтесь, я, может быть, там вас... ну просто человека с мышкой сравним, но и у того и у другого мозг, в моем понимании, как бы разный по размеру, это как бы некоторый процессинговый центр, который на самом деле, просто к нему приходит какая-то информация, он выдает какую-то информацию наружу. И дальше все зависит от уровня этого процессинга. на мой взгляд грубо говоря терминами иерархии я бы сказал так видимо большая разница между нами людьми и животными состоит в том что мы научились ну это природа эволюция в нас научилась наращивать вот эти иерархии до очень высокого уровня а у животных не научилась грубо говоря терминами марти Нижние колонки оперируют сенсорно-моторной информацией, а верхние, высшие уровни оперируют с предыдущими уровнями. То есть, другими словами, верхние колонки смотрят на нижние колонки, а не на сенсорную информацию. Возможно, у животных этого нет. Может быть. Но это не спекуляции. Я к тому, что и то, и другое предсказание разного уровня. 

S02 [02:08:13]  : Коллеги, Игорь, спасибо. Я предлагаю идти дальше. У нас вот есть Владимир Потапов, который хотел бы все-таки уточнить некоторые интересующие его вопросы по кластеризации. Владимир, пожалуйста. 

S06 [02:08:31]  : спасибо за реально очень интересный рассказ, но на самом деле не то чтобы по кластеризации, я скорее не очень знаком с OpenAI Gym, в каком формате они предоставляют этот вектор данных, и хотелось бы понять, что кластеризуется. если кластеризуется несколько векторов входящих, группа векторов, или кластеризуется информация в рамках одного вектора. Вектором является экран, картинка, RGB, но в каком-то цветовом пространстве. То кластерами там будут расположены объекты. Одна ракетка одного цвета, другая ракетка другого цвета и мяч. Это одна ситуация кластеризации. Другая кластеризация – это когда мы кластеризуем множество входящих векторов, множество входящих кадров, и тогда мы распознаем отдельные ситуации. ракетка слева, ракетка справа, мяч в одной стороне, мяч в другой стороне. другая кластеризация. вот это я имел ввиду. 

S03 [02:09:35]  : так это и другое возможно. как бы, ну, сейчас там, ну, какая-то сделана, какой-то сделан механизм кластеризации. На самом деле, возможно, ну, и так, и так, и эдак. Просто кластеризация – это способ перевести, там, исходно какую-то графическую информацию или, скажем так, богатую информацию в некоторый более сжатый компактный формат. вы можете параллельно сделать и ту и другую кластеризацию и там и параллельно на ней там запускать партнера одни с такой другой на самом деле вот я я же не знаю как правильно я просто но или так я не знаю как будет лучше работать то как вот я конкретно сейчас делаю вот прямо вот вот сейчас и как она работает мы там кластеризуются Вашими терминами группа векторов. У меня нет картинки. Я с картинками оперирую. Но я не исключаю, что с картинкой будут работать лучше. Я просто хочу насладиться этим работой с картинкой, просто сил не хватает и времени. надо деньги зарабатывать, а это пока другая часть жизни. 

S06 [02:10:55]  : в таком случае связанный вопрос. если мы получили кластер в группе 2, то это, я так понимаю, некий стейт системы. в каком-то представлении. То есть мы получили некий стейт, и дальше мы получили некие кластеры. То есть состояние такое, состояние такое, и дальше колонка специализируется, получается, на обработке определенного стейта. Им что, определенные стейты выдаются в кластеры? 

S03 [02:11:26]  : Я бы выразился так. Колонка специализируется на обработке определенного кусочка информации о мире. это если это как если бы вот у вас идет там конференция в 10 потоков параллельные и вы не можете быть в каждом зале но вас там 10 человек коллег вы пошли каждый свой зал и каждый слушайте там что-то в своем зале а потом встречаетесь и пытаетесь там рассказать каждый чувствую что у вас у всех разные информация хотя вроде как она наверно к чему-то одному относится может быть и нет и вот это про это же потому что в нашем мозгу из того что мы знаем про зрительную коробку ну вот тоже те самые конкуренционные сети которые я показывал все это пулирование мы же не оперируем у нас нет ни одного ни как бы ни одного нейроны ни одной структуры которые оперируют сразу все информацию слишком много и гигантское количество всех этих нейронов и мы в каком-то смысле все это пулируется упрощается и каждое искусство надо в том чтобы научиться работать вот с кусочками информации а дальше как-то вот из этих кусочков устраивать общую картинку но один из способов например это там все пулировать сделать свести это все там ну к очень грубому изображению например там как-то квадратиками там большими там и работать с ними у вас это пространство маленькой размерности ну например но все равно это как бы некоторые огрубление ситуации кластеризация в данной ситуации просто некое сильное огрубление ситуации а вот тогда к к что называется заглавие презентации про понимание да и 

S06 [02:13:11]  : когда у нас появляются некие кусочки информации, каждый кусочек информации, то есть сейчас непонятно, за что отвечает каждая колонка, каждый кусочек информации, и, собственно, откуда сложится понимание. То есть я так себе представляю, что если каждый кусочек, каждый кластер, каждая колонка отвечает за распознавание некоего своего образа, это, наверное, с точки зрения того, как мы представляем о своем мышлении, это может быть неправильно. Но если каждая колонка отвечала бы за свой образ, и у нее было взаимодействие разных колонок и взаимодействие разных образов, например, положение одной ракетки, положение другой ракетки, направление движения мяча, и они бы договаривались Как при этом, в этой ситуации, в этом стейте модели, какое дальнейшее движение делать, то это выглядело бы как некое понимание, по крайней мере, с точки зрения человека. 

S03 [02:14:22]  : ну смотрите я мне ваш вопрос очень нравится там можно про него с разных сторон по-разному говорить с одной стороны скажу так что например когда в нейросети формулируется все эти еще мы тоже не понимаем как там оно и как вырабатывается общее понимание из миллиона этих нейронов в следующем, во втором, в третьем слое, как эти фичи выстраиваются, ну как бы неочевидно, хотя вроде как можно сказать, что там веса, кто больше, откуда получил, ну как-то. но вообще, я бы сказал так, вот вопрос, который, знаете, он такой, он нетривиальный и у меня нет на него готового ответа, это некоторая чарующая, знаете, история такая, вот как формируется это, я до конца еще, наверное, сам не понимаю, но прикол в том, что, как бы, давайте так по-другому сформулировать терминами терминами экспертов которые про которых я говорил вот у вас там сидит у некоторого менеджера за столом там есть экспертов и он там должен решение принять это потом принимать какие-то решение компании делать какие-то действия мы как можем мы про компанию когда говорим там вот там nvidia молодцы они там двигаются круто взрывского слова компании а вот молодцы они там делают все круто а вот компания b там значит вот не понимает вообще куда идет рынок вообще делает одно провальное действие за другим как мы это как бы мы как-то оцениваем что компания вся в целом что и а на самом деле ведь были какие-то экспертные обсуждения какие-то люди высказывались как-то было принято выработано решение и тот кто как бы должен был его исполнить он его там исполняет одно другое третье они там одни заваливаются другие идут наверх и все что мы можем сказать как бы про целую систему про ее понимание как бы окружающего как она понимает окружающий мир это грубо говоря смотреть на ее успешность или не успешность в этом мире если хотите а как внутри отдельные части формирует это вот как бы понимание внутри этой системы это тонкая механика я не уверен что я ну как бы готов сейчас там прям про все это порассказать не потому что не хочу просто даже я до конца все не понимаю но но но у меня есть ощущение что именно так оно формируется из этих вот когда я говорю про вот это вот конкуренцию между колонками и там голосование и там выбора с триатумом там есть еще более тонкие вещи но просто я по них сегодня там не говорю которые дальше идут глубже в общем еще но короче и 

S06 [02:17:31]  : насчет насчет понимания да то есть вот замечательная вещь да я согласен с тем что конечный критерий до понимания мира это успешность в нем и Планирование и прогнозирование – это конечные критерии. Но есть еще один критерий, что система может рассказать про свое понимание, каким-то образом его представить, потому что это тоже понимание. Человек не всегда успешно действует в мире, он ошибается, но рассказать о своем понимании, представить его каким-то образом он может. Это тоже критерий понимания. 

S03 [02:18:08]  : ну да но вы знаете тоже не всегда иногда люди рассказывают очень странные вещи про то что они делают и выясняется что он совсем по-другому на это смотрит не так как вы совсем по другим причинам как бы делает это они то что вы подумали и мы обнаружим самые неожиданные вещи в людях в этом смысле умение рассказать хорошие тезис мне нравится 

S06 [02:18:38]  : Но при этом человек исходит из некой собственной модели мира. То есть я, когда говорю, что человек рассказывает, он передает вам свою модель мира. То есть понимание – это его внутренняя модель мира. 

S03 [02:18:49]  : Не обязательно. Она может не соответствовать действительности. А может быть он хочет сформировать у вас определенный кусочек картины мира. Люди – существа сложные, совершенно не обязательно они излагают то, что думают. и не обязательно у них есть понимание того, что они делают. В общем, я бы сказал, я про эту книжку написал, когда книжка выйдет, можно будет про нее подискутировать. Это вопросы к ней, а не к модели. А с точки зрения модели, я бы сказал, что есть один критерий фактически, к сожалению. Я долго экспериментировал с этой моделью, прежде чем хоть что-то показать, потому что у тебя может быть куча правильных идей, но пока целая модель не начнет показывать результат, хоть какой-то, я не могу сказать, что маркет показывает прям там, то есть он пока DeepMind не бьет, объективно. Тингтонг не выигрывает, но у меня есть полное ощущение что он работает но пока там еще надо подкрутить повинтить но в целом и вот когда в целом модель начинает показывать что она работает ну сложно сказать, что она не демонстрирует понимание, как она устроена внутри, ну сложный момент, да, но тем не менее вот то есть я в этом смысле сейчас оперирую скорее не философскими терминами, не словами, а математикой, что проще, то есть вот как бы есть кусок кода, он работает, причем он работает всегда, то есть сейчас его запустил, завтра, послезавтра, он всегда дает результат, несмотря то, что модель по-разному собирается, там разный сигнал и так далее. Вот в этот смысл. 

S02 [02:20:24]  : Хорошо, Игорь Владимирович, спасибо. У нас есть еще короткий комментарий от Игоря Романко. Игорь, пожалуйста. 

S05 [02:20:32]  : Добрый вечер. Я хотел бы по поводу Игорь рассказывал, что он применяет какой-то высокочастотный или низкочастотный… отсеивать сигналы в зависимости от этого. И просто хотел сказать, что обычно нам не надо выполнять никакой математики над нашими сырыми данными, пока мы не получили никакого фидбэка. То есть, если, например, наша ракетка, она находится в разных частях, грубо говоря, нашей там среды, и это вроде как высокочастотный сигнал, но в то же время она может в каждой клетке находиться, но это всегда вести к плохому результату, потому что, например, она была далеко от мяча. Но в то же время, если мы будем смотреть на нашу ракетку и на мячик, то когда они, грубо говоря, вместе, в схожих точках, то мячик всегда отбивается и, грубо говоря, это для нас всегда положительный результат. Один, конечно, момент, как мы будем это трактовать, как получать правильное награждение, всякое такое, но, грубо говоря, это низкочастотный сигнал, который нам нужен и к которому мы стремимся. И просто если мы будем выполнять какую-то математику над нашими своими данными, то мы потенциально можем отбросить те результаты, которые нам надо. что я советую, так это делать всю математику, когда мы уже получили реввард, то есть когда мы уже получили или хороший или плохой реввард, мы можем смотреть на наши данные, которые у нас есть сейчас, которые мы получили для этого, и сравнивать, грубо говоря, те хорошие, которые были при хорошем ревварде, плохие при плохих, и уже смотреть на низкочастотные, высокочастотные и так далее и тому подобное. 

S03 [02:22:12]  : Слушайте, ну вы же можете сами сделать, делайте по-другому. 

S05 [02:22:17]  : Я работаю, да, спасибо, я просто работаю с Димой, то есть модель, просто как бы хочу поделиться своими соображениями. 

S03 [02:22:23]  : Слушайте, я с этим работал, у меня на это два простых ответа. Во-первых, модель работает, ну как бы видно, что она работает, ну все, теоретические споры закрыты. А во-вторых, ну вот я так вас спрошу, вот вы смотрите на картинку. сколько вы видите пинг-понг сколько вы видите на ней объектов просто как бы в чем 3 объекта Давайте представим себе, сколько может быть с ними связано чисел. Ну 6, 10, ну сколько? Ну сколько-то? А там 128 байт передается. С очевидностью понятно, что не все байты несут информацию. Я вам больше скажу. Первый байт всегда равен 192. Всегда. Я не знаю почему. Нет, вы поймите правильно. Теоретически, я же физик-теоретик. Господи, ну что мне рассказывать про теорию? Я прекрасно теорию знаю. Но есть теория, а есть конкретная практика. И практически я вижу, что без потери всякого смысла мы можем выбросить, знаете, как почти из любого доклада, вы можете 90 процентов, я надеюсь, не из моего. 90% текста выбросить без потери смысла. Из любого текста почти всегда. Так же и здесь. Но просто я не вижу смысла работать с полной бедой. Вы правы, что может быть в каком-то случае это приведет к каким-то потерям. Доживем, увидим, посмотрим. 

S05 [02:23:43]  : Можно комментарий. Я просто хочу сказать о том, как бы по мне основная из Мы отбрасываем какие-то данные, мы смотрим только на их часть, но когда у нас получился негативный фидбэк, мы на этих данных видим, что, грубо говоря, ничего не выходит, мы ничего не знаем, что сделать, мы возвращаемся к нашим исходным данным и смотрим, какой именно момент мы пропустили, какой мельчайший низкочастотный сигнал или что-то еще может как раз повлиять на наш исход. и если вдруг, какой бы он малый не был, какое малое расстояние между данными не было, но если вдруг он… на наш результат, мы получаем очень хороший фидбэк в 100% случаев, мы как раз будем смотреть и на него. То есть, грубо говоря, у нас тематика нашей модели должна подстраиваться. 

S03 [02:24:35]  : Вы меня невнимательно слушали. Извините, что я вас перебиваю, я уже не могу. Я уже устал просто от всего этого. семинара, вы меня невнимательно слушали, мы с вами живем в разных парадигмах, вы, наверное, дата-сайентист, работаете с большими потоками данных, где, наверное, это может быть правильно. но я живу в другой парадигме и как бы я физик который делает оценки и модель это всегда упрощение это всегда огрубление и только тогда она работает если ты пытаешься все в деталях смоделировать и закопаешься в деталях и мой первый тезис про фундаментальные свойства интеллекта был про упрощение и интеллект упрощает окружающий мир выбрасывая из него множество деталей я смотрю на окружающий мир там вижу картинку размером я не знаю если в пиксель фотографии делаешь хорошо 12 миллионов пикселей она не и там дом и Для меня это один объект, все, точка. 

S02 [02:25:38]  : Игорь, извиняюсь, коллеги, коллеги. Спасибо. Значит, нам уже надо закругляться. Я, к сожалению, вынужден был пропустить несколько вопросов, которые очевидны, но вот последний вопрос я не могу не задать. И, Игорь, я знаю, что ты на него сможешь очень быстро ответить. Внимание, вопрос. Марти, это больше генетический алгоритм или нейронная сеть? 

S03 [02:25:58]  : Я боялся, что ты задашь вопрос. который свидетельствует о полном непонимании того, что я излагал. Господи, ну, конечно, это ни то, ни другое. Одна модель, генетические алгоритмы, другая модель. То, что я сегодня рассказываю, это вообще другое. такой модели еще нету как бы ее нету в классификациях там нигде этого новая история вот это ни то ни другое и не надо спешить у себя как бы на своих полочках своего ума разложить по каким-то классификациям это другая история для тех кстати кому интересно все это тщательнее вот подумать есть такая замечательная книжка шумского называется машинный интеллект наверняка и многие там видели она уже даже где-то в пиратских копиях бесплатно появилась в интернете вот очень многие вещи нам намного более подробно чем я сегодня там бежал ну просто ну правда не уже не с конкретным приложением но некоторые базовые идеи там изложено на порядок в точнее и лучше. Так что можно ее почитать. 

S02 [02:27:03]  : Игорь, огромное спасибо. Всем спасибо за участие и вопросы. И до новых встреч. Спасибо. До свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
