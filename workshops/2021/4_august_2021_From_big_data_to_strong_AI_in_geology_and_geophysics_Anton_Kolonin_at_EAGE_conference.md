## 4 августа 2021 - От больших данных к сильному ИИ в геологии и геофизике - Антон Колонин на конференции EAGE — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/tVqBKlWl_5A/hqdefault.jpg)](https://youtu.be/tVqBKlWl_5A)

Суммаризация семинара:

Семинар посвящён теме применения искусственного интеллекта (ИИ) в геологии и геофизике, а также обсуждению перспектив развития сильного ИИ. Основная идея семинара заключается в том, что геологические и геофизические исследования часто сопровождаются сбором больших объёмов данных, которые могут быть использованы для обучения систем ИИ без необходимости тщательного подбора тренировочных данных.

В ходе семинара были рассмотрены такие темы, как:

1. Системы обучения без учителя: Обсуждалась возможность обучения систем ИИ на основе больших объёмов мультимодальных данных, что позволяет системам учиться без плотного подбора тренировочных данных. Примеры включают обучение системы разговаривать на человеческом языке и читать книжки, используя комбинацию изображения, звука и текста.

2. Проблемы быстрого обучения и обучения на одном примере: Были отмечены сложности, связанные с быстрым обучением или обучением на одном примере, в контексте геологических и геофизических задач. Примером служит необходимость показать системе машинного обучения 5000 аномалий кимбердитовых трубок, чтобы она могла начать распознавать эти трубки, в отличие от человека, который может распознать типичную магнитную аномалию после одного примера.

3. Сильная генерализация и выявление структуры: Было подчёркнуто значение сильной генерализации, когда модели ИИ выбирают не просто совокупности пикселов, а выявляют характеристики, такие как наличие максимумов и минимумов. Также обсуждалась способность систем ИИ понимать элементы структуры, что позволяет им распознавать кота или собаку по наличию конкретных характеристик и особенностей.

4. Кластеризация и задачи машинного обучения: В семинаре были рассмотрены методы кластеризации, такие как метод Кайминса, и обсуждалась проблема определения числа кластеров (числа K), которая может варьироваться в зависимости от анализируемых свойств. Также были упомянуты методы обработки изображений и фича-инжиниринг.

5. Проблемы интерпретируемости и объяснимости ИИ: Были затронуты вопросы интерпретируемости и объяснимости систем ИИ, включая задачу создания систем объяснимого искусственного интеллекта и обеспечения возможности передачи знаний между системами обучения и моделями.

6. Перспективы развития сильного ИИ: Семинар завершился обсуждением перспектив создания общего искусственного интеллекта, который сможет сравняться с уровнем человека, и даже превзойти его, в частности в контексте управления бурением и поиска полезных ископаемых на астероидах.

В заключение, семинар подчеркнул важность интеграции знаний из геологии и геофизики в развитие сильного ИИ, а также необходимость решения ряда сложных задач, связанных с интерпретируемостью и объяснимостью систем ИИ.




S00 [00:00:00]  : Вова, спасибо, что вы с нами. 

S01 [00:00:04]  : Да, коллеги, спасибо за представление. 

S00 [00:00:06]  : Мой экран видно? 

S01 [00:00:11]  : Коллеги, спасибо за представление. Мой экран видно? Меня слышно? 

S00 [00:00:14]  : Антон, мы видим вашу презентацию, только включите, пожалуйста, микрофон. 

S01 [00:00:19]  : Да, коллеги, добрый день. Меня слышно, да? 

S00 [00:00:23]  : Да, теперь слышно. Слышно отлично. 

S01 [00:00:25]  : Спасибо большое за представление и спасибо за возможность высказаться. На самом деле вещи, которые я буду говорить в первой половине презентации, они давно у меня крутятся в голове и я очень рад, что у меня будет возможность их сегодня озвучить. Ну и в общем я постараюсь Постараюсь пробросить мостик от того, что называются большие данные, которые в геологии и геофизике были практически всегда, к тому сильному или общему искусственному интеллекту, который сейчас мировая индустрия пытается создать и на самом деле пока не знает, как она это сделает. Мостик я только попытаюсь пробросить, потому что с одной стороны Первые 20 лет своей карьеры я занимался вопросами геофизики, обработки больших данных геофизики, применительно к решению обратных задач, в первую очередь в инженерной и рудной сейсморазведке. А последние 20 лет я занимаюсь как раз вопросами пресловутого общего или сильного искусственного интеллекта. И последние 10 лет я геофизикой практически не занимаюсь, поэтому не очень хорошо знаю, что сейчас происходит. Поэтому я попытаюсь задать некоторые направления, некоторый вектор, но о детали, наверное, можно будет уже обсуждать отдельно. Итак. Я начну с тезиса, который, наверное, многим известен, что Россия – это родина слонов. И перейду к другому тезису, что на самом деле геофизика и геология являются родиной тех самых больших данных, о которых сейчас говорят практически на каждом углу, причем самые значительные люди в нашей стране. Дело в том, что еще когда я был студентом, где-то я услышал такую фразу, то ли на лекции, то ли в каких-то кругах, что вообще сейсмика это область, которая является лидером по количеству данных и по объемам данных, которая обрабатываются в ходе решения тех задач, которые перед ней стоят. Причем это было где-то начало середины 80-х годов. И тогда сейсмика по объему обрабатываемых данных перерасходила даже космическую отрасль. Сейчас, возможно, это не так. Скорее всего, сейчас, если совокупить все данные, которые собираются онлайн, включая аудио и медиа потоки, генерируемые пользователями многочисленных соцсетей, наверное, все-таки люди генерируют данных больше. Но суть не в том, кто сейчас генерирует больше данных, а суть в том, что на самом деле практически все методы и методические приемы, принципы, решения, которые сегодня используются машинным обучением дейто-сайентистами, они в том или ином виде использовались 40 лет назад в геофизике. Вот, и если вот мы посмотрим на этот слайд, в середине здесь свежая, значит, картинка того, что подразумевается под интеллектуальным анализом данных в Громнефте, мы, если внимательно посмотрим, мы увидим, что практически все, что на этой картинке представлено, делалось 40 лет назад, но немножко на других вычислительных мощностях, может быть немножко под другими именами. Вот, ну и соответственно, Если переходить к практике, то я сам в решение задач в области искусственного интеллекта удивительным образом пришел из решения задач в области геофизики, при том, что никаких слов вроде искусственного интеллекта, большие данные или машинное обучение тогда вообще не было. Вот первый пример. Я начинал с того, что занимался решением задач сейсмической томографии, которая является частным случаем той самой томографии, с помощью которой сейчас диагностируется степень поражения легких коронавирусом. Но вот только самым удачным примером решения этой задачи применительно к сейсмике была локализация кимберлитовых трубок на основе данных многократных перекрытий и сейсморазведки. И мы оказались способны локализовывать геометрию и локацию трубки по профилям сейсмики гораздо точнее, чем любые другие методы обработки сейсмических данных. Мы использовали то самое преобразование Роддона, которое товарищ Роддон изобрел в 2017 году. И, кстати, в 1993 году, когда ночью у меня нашло озарение, что можно использовать это же преобразование радона, которое переводит данные из пространства декартовой системы координат в полярную систему координат, чем делает объекты инвариантными к повороту, У меня возникла идея, что можно использовать это в задачах образования образов и за неделю на основе геофизического пакета был разработан пакет распознавания символов, который по качеству в общем был сопоставим с качеством популярного в то время пакетом Fine Reader. Но поскольку бизнес-задачи решать, задачи распознавания символов тогда не было, эта разработка осталась. Но вот я, когда готовился к докладу, обнаружил, что в 2015 году появилась работа использования преобразования радона как раз для решения задачи компьютерного зрения. Другой пример. Когда мы решаем обратную задачу томографии, в данном случае сейсмической томографии, есть разные методы решения задачи. Например, в медицинской томографии, поскольку у нас имеется регулярная система наблюдений и полное покрытие среды данными, удобнее использовать алгоритмы типа свертки обратной проекции. В геофизике это не работает на практике, потому что при нерегулярности систем наблюдений и существенной нелинейности среды по сравнению с тем, что мы имеем в случае человеческого тела, В геофизике методы решения на основе свёртки обратной проекции чудовищно неустойчивы. Они действительно дают хорошую разрешающую способность, но они неустойчивы, поэтому обычно в геофизике для восстановления поля скоростей на проходящих или рефрагированных или отраженных волнах используется и градиентные методы обращения матриц, то есть мы на самом деле представляем в среду, дальше у меня будет слайд, как представляется прямая обратная задача в сейсмике и как это можно протранслировать в машинное обучение, но так или иначе мы описываем по среду и регистрируемые данные времен прихода сейсмических волн или амплитуде сейсмических волн ослабленных при прохождении через поглощающую и расщеливающую среду. Мы это все описываем системой линейных уравнений и решаем эту систему линейных уравнений. И практика показывает, что наиболее устойчивым методом является итерационный метод. И наиболее эффективным из итерационных методов решения системы линейных уравнений является метод сопряженных градиентов. Сейчас мы обнаруживаем, что градиентные методы, в частности метод градиентного спуска или градиент бустинг, он используется для решения практически той же задачи. Мы не восстанавливаем коэффициенты поглощения или скорости в среде, а мы подбираем коэффициенты в связи между нейронами в слоях, постепенно распространяя ошибку так называемый метод backpropagation на основе отклонения получаемых данных от тренировочных данных, точно так же как мы распространяем ошибку в градиентных методах в сейсмотомографии, подбирая модель таким образом, чтобы она максимально точно соответствовала наблюдаемым полям времен. Другой пример, который просто каждый раз, когда я вижу такие вот картинки, когда мы любую изображение с помощью DeepMind, DeepDream можем превратить в лица собак или кошек, вот это мне сразу же напомнило, когда я это увидел первый раз, историю про деконволюционные фильтры, которые решают задачу выявления аномалии заданного типа. Я, к сожалению, не нашел слайда, как работает фильтр обнаружения аномалий заданного типа на двумерной плоскости. В сейсмике это обычно делается функция деконволюции для восстановления сигнала, в частности, свип сигналов вибросейсмики. Но вот в двумерном поле здесь пример того, как мы обращая, строя оператор обратной свёрки относительно функции Эйлера, которая продолжает играть потенциальные поля в верхнее полупространство, мы строя обратный оператор свёрки обращаем потенциальные поля в нижнее полупространство, тем самым выделяя более точно те аномалии, которые находятся на определенных уровнях ниже земной поверхности. Соответственно, наряду с этим мы получаем много ложных аномалий, так называемых артефактов. Часть этих артефактов достоверна, часть нет. Ну и собственно то же самое мы получаем в современных методах обработки изображений, когда мы, если мы очень хотим увидеть собак, мы начинаем видеть собак практически во всех точках изображения. И дальше возникает задача отбраковки ложных аномалий, как это происходит в геофизике, и отбраковка ложных объектов, как это происходит при практическом использовании данных распознавания в современном машинном обучении. Если говорить о методических аспектах, то опять-таки то, что сейчас называется фича-инжинирингом и реконструированием особенностей в машинном обучении, это то, чем всю жизнь занимались люди, которые занимаются комплексированием методов разведочной геофизики. С левой стороны здесь показан пример многопериметрической визуализации данных комплексирования электромагнитных, геохимических и магнетометрических методов при поисках оруденения в Забайкале. где визуализация делается с помощью того пакета программ, которые мы разрабатывали в 80-х годах. Все проблемы с факторным анализом, связанные с фактором анализом, с оверфитингом моделей, с недостаточным набором данных, с сэмплированием как положительных образцов, так и отрицательных образцов, с которыми сейчас борются Data Scientist. Эти все проблемы были достаточно хорошо известны. И сейчас эти проблемы решаются в машинлёнинге, но только на примере других финансовых или социальных данных, а не на основе измерений магнитного, гравитационного или геохимического поля. Если перепрыгнуть от методологии к математике, то можно посмотреть на то, как математически структурируется с одной стороны прямая задача, а с другой стороны задача обучения и распознавания в геофизике и в машинном обучении. Представим себе, что мы пытаемся решить обратную задачу восстановления скорости среды по Временам прихода сейсмических волн в предположении о малых величинах отклонений скорости, то есть предполагаем прямолинейное прохождение сейсмических лучей. Мы можем описать эту среду, разбив ее на какие-то элементы, как систему уравнений, где в каждой клетке среды у нас имеется значение скорости. Есть коэффициенты, которые описывают геометрию схемы наблюдений таким образом, что для каждой точки возбуждения, каждой точки приема, каждый объем среды имеет некоторый вклад. И на выходе мы получаем времена, в правой части мы получаем времена прихода сейсмических волн. Соответственно, у нас на входе, если мы решаем прямую задачу, у нас на входе есть условия измерений или геометрия наблюдений. Также у нас на входе задаются параметры среды. Это то, что здесь считается переменными, если мы рассматриваем это как систему уравнений. И на выходе мы получаем результаты измерений, то есть времена прихода сейсмических волн. Это прямая задача, которая на самом деле обычно в геофизике значения не имеет. То есть в конечном итоге мы в геофизике обычно решаем обратную задачу но прямая за по понимание как решается прямая задача она во многих случаях полезно для того чтобы на основе построенной прямой задачи решать обратную задачу значит если же мы говорим про машинное обучение то аналогом по сути здесь является решение задачи классификации или предсказания то есть что происходит если предположить что коэффициенты это у нас коэффициентами являются те входные данные, с пространством которых работает нейросеть, а коэффициенты сети являются переменными, которые в случае обратной задачи нам предстоит восстановить, то, совершая операцию над этой матрицей, мы получаем результаты классификации. Я говорю, что то, что мы здесь видим на экране, это адекватно в случае того, что мы решаем задачу линейной регрессии для простой нейронной сети без промежуточного слоя. Если мы работаем с многослойными нейронными сетями, то там уже приходится иметь дело с тензором, а не с матрицей. И если функции не линейные, то там тоже все становится сложнее. то есть линейное уравнение использовать нельзя. Но в случае линейной регрессии или двуслойной нейронной сети нахождение коэффициентов, точнее нахождение переменных коэффициентов нейросети решается задачей линейной регрессии, которая является самым дешевым и во многих случаях самым эффективным с бизнес-точки зрения Я просто приведу пример, что сейчас, допустим, на задачах финансового прогноза, которыми я занимаюсь, вот у нас наиболее устойчивые результаты, ввиду большой зашумленности данных, получаются именно на линейной регрессии, а более тяжелые нейросети, они только сжирают машинное время. Теперь давайте посмотрим, что мы имеем дело в случае обратной задачи. Сначала в случае геофизики. То есть в случае решения обратной задачи у нас на входе имеются условия измерений, поскольку мы знаем, где у нас источники и приемники стоят. У нас имеются результаты измерений, которые мы сами генерим. То есть по сути уравнения с результатами измерений мы генерим по ходу выполнения измерений. И в результате обращения матрицы и решения обратной задачи мы должны собственно получить переменная или параметр среды. И вот как раз именно эта обратная задача в геофизике, она оказывается на логичной, самой больной, как я скажу, ниже задачей машинного обучения. Потому что с машинным обучением у нас опять-таки на входе есть входные данные. На входе у нас есть предсказание или разметка этих входных данных. И в результате обращения матрицы или системы матриц нам нужно получить коэффициенты нейросети, на основе которых мы потом сможем решать прямую задачу. Теперь давайте посмотрим на соотношение истории про большие данные, интеллектуальный анализ геофизики, геологии и, соответственно, задачу классификации и задачу кластеризации в машинном обучении. Вот работа, например, которую мы делали вместе с Пермским ПИЦ геофизика. лет 15 назад. Мы разрабатывали систему прогнозирования геологического разреза на основе данных акустических скважинных исследований. Справа видны достаточно интересные результаты, а слева показана методология, которую мы пытались развивать, пока этот проект двигался. вот что на основе построения некоторой модели то есть если у нас есть какие-то большие входные большие входные данные на основе которых мы можем построить модель вот и положить эту модель даже на данную базу данных где модель предполагает соответствие различных наборов параметров среды различным петра официальным условиям, уровнем обводненности, уровнем газонасыщенности, уровнем водонасыщенности, давлению и всему подобному, мы можем осуществлять распознавание. То есть, в каких-то случаях мы можем, зная измеренные физические параметры, предсказывать петрофициальный состав, обводненность, газонасыщенность, нефтенасыщенность. В каких-то случаях, имея петрофициальный состав и какие-то параметры, мы можем предсказывать другие параметры, допустим, обводненность, плотность среды или сопротивляемость среды и степень ковернозности среды с точки зрения последующих операций, бурения. Обслуживание скважин. Если посмотреть с другой стороны, то вторая задача, противоположная задачей классификации в машинном обучении, это задача кластеризации. Классификация – это когда у нас есть классы с известными свойствами и на основе вот этих частей измеренных свойств мы можем восстановить другие свойства, где тип породы или ободненность она тоже является свойством. Из-за них свойства предсказываем другие. Из одних классов, из признаков, мы предсказываем целевые категории. Это задача классификации. А вот задача классифицирования, это когда у нас есть данные, и мы, не имея информации о том, у кого какие признаки, мы должны построить, разбить эту систему на какие-то интервалы. И вот как раз эту задачу мы пытались решать в проекте геотома, который я развивал первые 20 лет своей деятельности в геофизике. Вот, например, здесь показан пример того, как, получив сечение среды межскважного пространства методом томографии, Пытаемся выделить интервалы пород, которые соответствуют восстановленному полю скорости или коэффициентов поглощения. Но и та проблема, которую мы пытались тогда решить, она проблема определения границ этих интервалов и проблема определения количества интервалов. она на сегодняшний день в машинном обучении практически не решается. На самом деле максимум, что может давать современная технология кластеризации в машинном обучении, это выделение заданного числа классов, на основе либо критериев отличия этих классов одного от другого, либо на основе общего числа класса. Например, на этом разрезе мы можем задать, что мы утверждаем, что на данном разрезе должно быть всего три типа пород. Породы с высоким коэффициентом поглощения, оранжевые, породы с низким коэффициентом поглощения, синее, и породы со средним значением коэффициента поглощения, зеленое. Соответственно, вот если мы зададим вот это вот число, магическое число 3, то система кластеризации по методу Кайминс или любому другому, она может построить нам геологический разрез, разбив соответствующие области на интервалы. Но, что если мы неправильно угадали это магическое число? Вот, если вдруг на самом деле здесь присутствует 4 типа пород, да? А теперь зададимся вопросом, а что если с точки зрения, допустим, петрофициального состава тут имеется 3 типа пород, а с точки зрения условия обладненности тут имеется 4 типа пород? И тут получается, что в зависимости от того, какие свойства мы анализируем и какие нам нужно задавать разное число K. И у нас будут разные разрезы. И в общем случае существует ряд методик в машинном обучении сегодняшнем, как мы... Самый расхожий это пресловутый силуэт индекс, который позволяет оценивать качество классификации. Но, например, практика использования этого индекса силуэт, она как раз показывает, что у нас Устойчивость системы кластеров относительно этого параметра мультимодальна. То есть, на разных числах кластеров мы можем получать различные результаты. И какие результаты правильные, дальше должен определять интерпретатор. То есть, в полностью автоматическом режиме задача кластеризации не решалась ни у нас 30 лет назад, ни она не решается сейчас. Теперь я начинаю переходить, собственно, к тем проблемам, которые с моей точки зрения и с точки зрения тех общений, тех разговоров, которые мы имеем в сообществе разработчиков сильного искусственного интеллекта, задач, которые стоят. Вот две задачи, которые стоят по большому счету в искусственном интеллекте машинного обучения. Нижняя задача – это задача классификации, распознавания, предсказания, прогноза. Это, что соответствует прямой задаче в геофизике, решается достаточно хорошо. Как и прямая задача в геофизике, кстати. То есть у нас есть входные данные, у нас есть модель. Построенная в результате машинного обучения. Или у нас есть среда в геофизике. И в результате мы получаем некоторые выходные данные. Все хорошо. Мы распознали. Мы получили времена прихода. Мы счастливы. А вот задача построить эту модель среды, модель предсказания, или построить модель среды по данным геологи-физических наблюдений, это решается проблемой, что в геофизике, что в машинном обучении. В машинном обучении эта задача решается практически всегда исключительно за счет суперзнания так называемого супервайзера, человека, который осуществляет Supervisor Training. То есть человек должен разметить большое число данных, что вот здесь вот у нас такие сопротивления, вот здесь вот у нас известняки, вот здесь вот у нас такие сопротивления, это у нас там обводненные песчаники. После этого система, имея большое число данных, размеченных с ожидаемыми данными, с разметки, когда система пропускает это через себя, она в итоге в результате того или иного метода формирует модель, которую мы потом можем использовать при предсказании. И вот эта задача обучения на сегодняшний момент решается достаточно плохо. Теперь Перейдем к тем задачам, которые на сегодняшний день стоят в области машинного обучения и искусственного интеллекта. На сегодняшний день системы так называемого искусственного интеллекта являются программируемыми. Потому что их пишут программисты для того, чтобы построить так называемый пайплайн машинным обучением. Кстати, само слово пайплайн, которое сейчас используется в Data Science System в кругом году, оно называлось граф обработки в древние времена СС-3 и СОС-ПС 40 лет назад. Тогда это называлось графами обработки, но принципы были те же самые такие. У нас есть большой данных, которых которые нам надо проследовать, прокачать через различный набор обрабатывающих модулей. Деконволюция обратной фильтрации, или сначала эмиграция до суммирования, а потом деконволюция. И вот магия по подбору правильного графа обработки, которой занимались товарищи в обрабатывающих центрах на СССР и СОСПС, это примерно такая же магия была И есть сейчас, которую машинленеры используют в своих пайплайнах, когда нам делать осреднение и какие параметры нам подбирать для градиентного бустинга, какой лернинг рейд нам выставлять для обратного распространения ошибок. Соответственно, их нужно программировать. Дальше. Искусственные интеллекты являются управляемыми. То есть, мы не можем сами сказать, система, вот давай, значит, иди и построй нам разрез, собрав все данные. Мы должны подобрать и данные, мы должны эти данные скормить, мы должны выставить параметры. И в результате вся обработка, проведение данных по пайплайну с получением каких-то результатов, она проходит под непосредственным ручным контролем. Системы являются узкими. То есть, если мы систему научили распознавать гимберлитовые трубки, вряд ли эта система начнет распознавать источники воды. Если мы натренировали систему распознавать котиков, она вряд ли нам будет распознавать номера автомобилей. Соответственно, под каждую из задач системы программируются и настраиваются и управляются. Ну и вот все перечисленное говорит о том, что для всего системы искусственного интеллекта, которые мы на сегодняшний день, они являются слабыми, а то, к чему мы хотим в какой-то момент прийти, являются системами не сильного искусственного интеллекта, которые вот находятся в правой стороне. данного слайда. Ну и суммируя это, то что мы имеем сейчас это так называемый узкий искусственный интеллект или AI в антитезу так называемому сильному или общему искусственному интеллекту или AGI. И вот собственно я сейчас последние несколько лет веду русскоязычное сообщество, там уже более тысячи участников, это некоммерческое открытое сообщество, где заинтересованные люди собираются и обсуждают проблемы, связанные с тем, как нам строить этот пресловутый AGI на русском языке. И до сегодняшнего дня тема геологии и геофизики там не поднималась. Хотя, если вы обратили внимание на первом слайде, одним из направлений является создание персональных ассистентов. Вот, например, я стал, так сказать, вовлечен в какой-то степени в создание персональных ассистентов для анализа новостей, для анализа, для решения задач персонализированной медицины. Вот. А в конце прошлого года, например, мы обсуждали такую историю, как создание интеллектуального персонального ассистента бурового мастера с решением специфических задач, связанных с управлением бурением. Соответственно, если вдруг кого-то какая-то тема интересует, то можно начать эту тему прорабатывать с участием тех сил, которые у нас в сообществе имеются. Ну и, наконец, то, что у нас на сегодняшний день имеется в наборе – это технологии так называемых больших данных, машинного обучения. По сути, это та самая обработка сигналов, которой геофизики занимались как минимум 40-50 лет. Некоторые называют это еще экспериментальной статистикой, потому что мы просто изучаем большие объемы накопленных данных и пытаемся строить статистические модели, вручную подбирая их параметры. А когда-то в светлом будущем мы создадим общий искусственный инцидент, который сначала сравняется с уровнем человека, И мы сами сможем отправить робота с буровой установкой на какой-нибудь астероид, и он сам там начнет ползать по этому астероиду и бурить скважины, находить там полезные ископаемые и загружать их в ракету для отправки на Землю. И для этого ему нужна будет способность адаптивного, автономного поведения, решения широких круг задач, начиная от перемещения, включая бурение и кончая погрузкой добытых ископаемых на транспортный корабль. В какой-то момент есть предположение, что интеллект станет настолько сильным, что он превзойдет человека. Потому что если мы создадим систему, способную осуществлять все эти функции, то просто увеличением вычислительных возможностей, добавлением памяти или добавлением числа процессоров система автоматически мгновенно сможет начать решать человеческие задачи гораздо быстрее и с гораздо большим объемом, чем человек. Перейдем к проблемам, которые стоят на сегодняшний день перед машинным обучением. Первая проблема – это создание системы объяснимого искусственного интеллекта. Второе – это обеспечение возможности передачи знаний как между системами обучения, так и между моделями. То есть возможность повторного использования моделей, предобученных для решения одних задач, на других задач с другой тематикой, а также передачу знаний между системами. Отчасти это может решаться так называемой нейросимвольной интеграцией, когда системы, основанные на нейросетях, могут интегрироваться с системами, основанными на правилах, построенными людьми. Система обучения без учителя – это построение систем, позволяют учиться без плотного и тщательного подбора тренировочных данных просто на основе больших объемов информации, которая будет мультимодальной. То есть, если мы, к примеру, обучаем систему разговаривать на человеческом языке и читать книжки, то если мы подаем на вход не только книжки или на только звук, а комбинируем изображение, звук и текст, как это делается при обучении детей, системы будут обучаться быстрее. В геофизике и геологии это, что называется, дается забесплатно, потому что мы обычно делаем исследования одновременно и геологические, и геофизические, и собираем все данные вместе. Комбинируя эти данные, изначально постановка географических задач предполагает мультимодальность. Большой проблемой является так называемое быстрое обучение или обучение на одном примере. То есть, если вы машинному обучению для того, чтобы она начала распознавать комбиридовые скважины, Нужно показать 5000 аномалий кимбердитовых трубок, после чего она в состоянии будет начать распознавать эти кимбердитовые трубки. Человеку достаточно один раз показать типичную магнитную аномалию и он сможет выделять ее на данных аэромагнитно-разведки. Соответственно, понятие сильной генерализации, когда мы, строя модель какого-то объекта, выбираем не просто совокупности пикселов, а выявляем характеристики, допустим, наличие максимума и минимума, а не просто наличие комбинации точек с высокими и низкими полями. Когда мы учимся понимать элементы структуры, когда мы распознаем кота или собаку не по цвету или не по текстуре шерсти, а по наличию конкретных характеристик и особенностей, которые есть у данного объекта, который мы распознаем. Соответственно, выявление структуры. Способность с одной стороны избежать того, что называется катастрофе фагеттинг, когда, к примеру, мы научились распознавать кемберлитовые трубки в Якутии на известняках, а потом с этой нейросетью стали баловаться с данными в Южной Африке на гранитах. И выяснилось, что там все выглядит по-другому. Мы перетренировали нейросеть на африканских или канадских данных, после чего она забыла, как выглядят кибернитовые трубки в Якутии. То есть, произошла так называемая катастрофа кфагетин. С другой стороны, на практике в некоторых случаях этого не удается достичь. То есть, есть проблема катастрофик ремемберинг. То есть, если нам надо дообучить систему на каких-то новых наборах данных, она не в состоянии выкинуть из памяти то, чему ее обучили в раннем детстве. Ну и, соответственно, из этого вытекает необходимость обеспечения возможности постепенного дообучения и обучения в течение всего жизненного цикла. Проблема интерпретируемости... Антон, может быть у нас уже время... Да, коллеги, я тогда суммирую. В общем, если эта тема интересна, то я предлагаю, приглашаю просто на те семинары, которые у нас ведутся. Можно сделать отдельный семинар, посвященный именно состоянию тех задач, которые есть в геофизике и решениям, которые в области сильного искусственного интеллекта есть, и как они могут быть применимы к геофизике. Я с удовольствием поучаствую в продолжении этой дискуссии. Спасибо! 

S00 [00:33:43]  : Да, спасибо большое. У нас есть буквально пару вопросов в чате от Василия Демьянова. Первый вопрос. Как отбраковываются ложные аномалии при распознавании сейсмообъектов? Первый вопрос. Второй. Насколько эффективно обоснованы использования линейных моделей в геофизических задачах? 

S01 [00:34:08]  : Значит, смотрите, на первый вопрос я не отвечу, потому что я никак сейчас обрабатываются ложные аномалии при распознавании абсейсм-объектов сейчас я не знаю. На состояние 15 лет назад они отбраковывались плохо, насколько я знаю. То есть, всё равно нужен был либо человек, либо нужно было комплексирование. То есть, если мы увидели разлом, Да, то нам и не уверен на данных на сейсмическом разрезе, то для того, чтобы убедиться, что это разлом, нам нужна вот какая-то дополнительная, нам нужно, допустим, и не сверить это, например, с данными магниторазведки или геохимии. То есть комплексирование как раз предназначено в геофизике для того, чтобы верифицировать аномалии, которые мы видим на одном методе, на данных одного метода, Те навалия, которые мы видим на данных другого метода. Кстати, на примере разломов, вот, допустим, тот метод сейсмической томографии на отраженных волнах, который мы использовали для кимберлитовых трубок, эту задачу решает, собственно, для сейсмических данных. То есть, с одной стороны, мы можем увидеть разлом на данных разрезом ОВГТ, а с другой стороны мы можем увидеть тот же самый разрез по другому на асисметротомографическом разрезе. У нас получаются данные одни и те же, но модели принципиально разные и мы можем одной моделью компенсировать другую. На второй вопрос ответить проще. Значит, все зависит. То есть, если у нас слабая неоднородность в среде, мы можем работать с линейными моделями. То есть, если у нас есть межскважинное пространство, если у нас есть две скважины в гранитном массиве и нам нужно реализовывать там какие-то трещины или зоны оруденения, В этом гранитном массиве линейные представления использовать можно. Если у нас высокоградиентная среда с эрозией или с переходами в осадочном чехле скоростными, то тогда линейная аппроксимация не работает. 

S00 [00:36:10]  : спасибо я предлагаю если вдруг еще вопрос у кого есть пишите их в чат я думаю что какое-то время еще антон будет с нами он сможет 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
