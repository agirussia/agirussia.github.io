## 25 февраля 2021 - Обратная разработка некоторых функций сетчатки / Иерархическая память последовательностей как когнитивная архитектура для Сильного ИИ и её аппаратная реализация — Олег Серебренников — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/Mq2LLMY53JU/hqdefault.jpg)](https://youtu.be/Mq2LLMY53JU)

Суммаризация семинара:

Семинар посвящен обсуждению и анализу новых подходов в области искусственного интеллекта, в частности, в контексте обратной разработки функций сетчатки и памяти последовательности. Участники семинара представили свои исследования и идеи, касающиеся создания искусственных систем, способных к обучению и прогнозированию на основе анализа последовательностей и контекстов.

Основные тематические блоки


1. Обратная разработка функций сетчатки


- Тезисы:
  - Обсуждение работы Олега Серебренникова по созданию бесполотного летательного аппарата и его вклад в развитие "Рунета".
  - Представление новых идей в области искусственного интеллекта, связанных с изобретательской деятельностью Олега.
  - Анализ вопросов аппаратной реализации и их связи с функциями сетчатки.
- Итог: Олег Серебренников представил свои работы по обратной разработке функций сетчатки, подчеркивая важность внимательного отношения к патентным заявкам и патентам, уже выданным в США и России.

2. Память последовательности и её преимущества


- Тезисы:
  - Обсуждение понятий понятности, интерпретируемости и энергоэффективности в контексте памяти последовательности.
  - Сравнение памяти последовательности с известными способами, подчеркивая её суперэнергоэффективность.
  - Приведение аргументов в пользу использования памяти последовательности в качестве основы для создания искусственных систем.
- Итог: Память последовательности выделяется своими преимуществами, включая понятность, интерпретируемость и энергоэффективность, что делает её привлекательной для использования в искусственных системах.

3. Модель мира и её связь с языком


- Тезисы:
  - Обсуждение модели мира как системы, формирующейся из опыта человека.
  - Анализ связи между моделью мира и языком, подчеркивая, что демонстрация последовательных слов работает в рамках этой же модели языка.
  - Приведение примеров, показывающих, как человек может понять смысл фразы, не зная слов, через опыт и контекст.
- Итог: Модель мира человека формируется на основе опыта и может быть связана с языком и последовательностями слов, позволяя понимать смысл через контекст и опыт.

4. Иерархия и прогнозирование


- Тезисы:
  - Обсуждение иерархии в контексте глубоких сетей и их способности к прогнозированию.
  - Сравнение иерархии в глубоких сетях с иерархией памяти последовательности.
  - Анализ вопроса о том, где берется иерархия в памяти последовательности и как она связана с прогнозированием.
- Итог: Иерархия в памяти последовательности может быть связана с иерархией в глубоких сетях, но конкретные механизмы и их взаимосвязь требуют дальнейшего изучения.

5. Воспроизведение и самосборка системы


- Тезисы:
  - Обсуждение воспроизведения как процесса самосборки системы, включающего активное воссоздание воспоминаний.
  - Анализ процесса записи памяти и его связи с воспроизведением.
  - Приведение примера из исследования группы спецпроектов в МФТИ, показывающего, что воспроизведение может достигать 99-100% предсказания памяти последовательности.
- Итог: Воспроизведение и самосборка системы важны для понимания процессов, связанных с памятью последовательности, и могут быть использованы для улучшения прогностических способностей искусственных систем.

Вывод


Семинар представил ряд инновационных идей и подходов в области искусственного интеллекта, включая разработку функций сетчатки и создание памяти последовательности. Участники обсудили вопросы иерархии, прогнозирования и воспроизведения, подчеркивая важность понятности, интерпретируемости и энергоэффективности в создании искусственных систем. Семинар показал, что новые подходы могут быть эффективными, но также подчеркнул необходимость дальнейшего изучения и разработки этих идей.




S02 [00:00:02]  : Коллеги, всем добрый день. На всякий случай напоминаю, что у нас все семинары по умолчанию записываются и все записи выкладываются в публичный доступ. за отдельными исключениями, которые будут отдельно оговорены. Сегодня я представляю нашего нового спикера, докладчика. Олег Серебренников принимал участие в создании первого в истории человечества бесполотного летательного аппарата под названием «Буран». Потом он, как инвестор, принимал участие в создании «Рунета». Наверное, много еще чем занимался, про что он сам скажет, если считает нужным. Но сейчас, насколько я знаю, он в основном занимается изобретательской деятельностью в области искусственного интеллекта, о чем подробно сейчас и расскажет. На самом деле, была его презентация на OpenTalks.ai, но там было не очень много времени. Он рассказывал про память-последовательность и кто не успел Его не смог доклад его посмотреть на конференции. Я думаю, сегодня наверстает. Я думаю, он более подробно, кроме того, расскажет о вопросах аппаратной реализации. А начнет он с вопросов того, как это все происходит в сетчатке. Олег, вам слово, пожалуйста. Спасибо, Антон. 

S03 [00:01:21]  : Спасибо за приглашение и возможность выступить. И всем добрый вечер, коллега. Но давайте, чтобы времени не терять, я быстренько начну. В таком случае сверну. Так, обратная разработка некоторых функций сетчатки. Все материалы, которые я докладываю, просто предупреждаю не для того, чтобы их не использовали, а для того, чтобы относились к этому внимательно, это материалы патентных заявок и патентов, которые уже выданы, в частности, в США и в России. Рецептивное поле. Всем, наверное, известно, что такие поля были обнаружены довольно давно на сетчатке глаза. Они высланы, как правило, огромным количеством светочувствительных элементов. Если стимул попадает на такое поле, то такое поле реагирует. В своей работе по реверс-инжинирингу этого процесса, я пришел к мнению, что, скорее всего, нужно возвращать площадь такой зоны в качестве ответа на раздражение. Поэтому в каждом случае такая зона отвечает своей площадью. К чему это приводит? Что это нам дает? Довольно простую систему можно измерительно разместить на сетчатке в таком случае. Если разместить шкалу со значениями отметок, в каждой такой отметке разместить центр такой рецептивной зоны, а площадь рецептивных зон увеличивать соответственно отметкам, То есть пропорционально. А то мы получим измерительную ось такую. Вот если эту ось теперь параллельно самой себе копировать и размещать на сетчатке, то тогда вся сетчатка окажется мерным полем вот этой одной оси. Ну, соответственно, для того, чтобы иметь некую систему координат двумерную, мы сделаем две оси и соответствующим образом размножим эти оси параллельно самим себе в накладку одно над другим. Тогда у нас оказывается, что одни и те же светочувствительные элементы, они будут отвечать, давать свои сигналы в разные рецептивные зоны, которые пересекаются, элементом пересечения которых они являются, такие светочувствительные элементы. Понятно, что если мы с вами сделаем Если мы с вами сделаем угол сигма и омега 90 градусов, то мы получим прямоугольную систему декартовых координат. Понятно, что из-за того, что поля пересекаются, то несколько полей на один тот же раздражитель могут отвечать несколько полей. Это позволяет, во-первых, усреднять значение, а во-вторых, находить те поля, которые граничат с теми, которые отреагировали, но вот эти вот граничащие поля не среагировали, то есть на них стимул не был обращен. Это позволяет очертить внешние границы таких стимулов достаточно четко, потому что окружности достаточно много этих зон, если их достаточно много, то граница будет очень четко экстраполирована как вот этими вот дугами, обрамляющих стимул зон, в которые стимул, тем не менее, не попал. Известно, что изображения насетчатки, которые представлены в данном случае окружности, они в коре головного мозга, в зрительной коре, они обращаются в прямые линии, вертикальные. Спираль обращается в наклонные такие линии. Понятно, что функция должна быть соответствующая наклоном, чтобы это именно прямые были. Ну и звездочка, она транслируется тоже в прямые. То есть, вообще говоря, в данном случае везде мы имеем дело с прямыми, но имеем дело с разными углами. Соответственно, углы могли бы отвечать за распознавание инвариантов вот в таком контексте. Как это можно использовать вообще? строить догадки не будем я здесь какие-то вещи доложу а какие-то ну наверное и я их не знаю или вы их сами додумайте важно вот что заметить что поскольку система координат глаза – это все-таки полярная система координат, то чем она интересна? Да, и еще важный момент – глаз всегда фокусируется на объекте своего интереса, то есть объект интереса всегда находится на самом деле на оптической оси глаза, то есть в центре. Так вот, если происходит некоторое движение, то есть возникает некоторый дополнительный интерес, к которому глаз должен перейти, то есть точка другая, куда ему следует перейти, то эта точка всегда будет лежать на радиусе полярной системы. И вот это вот свойство полярной системы, при том, что глаз всегда фокусируется на предмете своего интереса, указывает на кратчайший путь между двумя такими точками интерес то есть глаз путешествия между точками интереса всегда совершает простое движение вдоль радиуса и это может тоже иметь значение поэтому для нам следует создать еще и полярную систему координат в глазе. Для этого мы берем ту же самую ось, которая у нас вот в начале была, закрепляем в точке начала, поворачиваем на определенные углы, и у нас получается радиальное поле, то есть с тем же шагом, что было создано, то есть на той же шкале, а с теми же самыми отметками, мы можем сделать радиальную шкалу, и тогда у нас шаг, шкал будет и в полярной системе координат радиального шага, и в декартовой будет одинаковый, то есть не придется преобразовывать координаты другими словами. Мы можем также создать поле мерных окружностей, которое позволит измерять углы. Делаем то же самое, только здесь шкала будет у нас круговой, и в каждой отметке шкалы размещаем пропорционально отметке шкалы рецептивные зоны с площадью и пропорциональной отметке шкалы. Соответственно, они увеличиваются. Здесь в данном случае показано Круговая шкала на 360 градусов, но вообще говоря, ее можно делать хоть на 40 градусов, хоть на 10, хоть на сколько, при необходимости и требовании системы. Теперь мы берем эту мерную окружность и начинаем увеличивать ее диаметр. Желательно, чтобы диаметр увеличивался с тем же самым радиальным шагом, который мы в самом начале задавали для картовой системы. Тогда, соответственно, у нас шаг будет радиальный, тот же самый, что у радиуса, и мы в одних и тех же точках будем измерять, что, опять же, позволяет очень удобно преобразовывать координаты из одной системы в другую. Полярная система координат, в результате у нас вот это мерное поле, радиальное мерное поле, как я говорил, вот это полимерных окружностей, мы их накладываем друг на друга, совмещая центр систем, и получаем классическую систему полярных координат, которая позволяет измерять место, где расположен стимул, и возвращать эти координаты системе. Достаточно важное место в жизни имеет выявление симметрии. Связано это с тем, что мы живем в гравитационном поле и неживая природа и живая, она по этой причине имеет в значительной степени, ну, не живая в меньшей степени, а, скажем, в некоторой степени, а живая-то уж точно имеет вертикальную симметрию прежде всего. Это и на Флоре и Фауне очень хорошо заметно. Если бы симметрии не было, то одна сторона перевешивала бы, соответственно баланс было бы труднее держать. По этой причине все стремится к симметрии. И распознавать симметрию, к примеру, для автопилотов автомобиля является критической задачей с моей точки зрения, хотя существующие системы этим, может быть, не интересуются в явном виде создатели современных систем автопилотов. Но я думаю, что если создавать систему интерпретируемую и понятную, то это обстоятельство должно иметь место. Ну и, кстати говоря, на это указывает разделение в хиазме зрительного нерва, который проецирует две левые части зрительных полей обоих глаз в левую часть и две правые в правую часть ЛКТ, то есть в разные височные доли. Есть что-то связанное с вертикальной симметрией, почему это происходит. Можно предполагать, что они сверяются друг с другом, допустим, левая и правая, и при нахождении оси симметрии изображения должны совпадать. Ну, не будем делать сейчас никаких выводов, но, тем не менее, симметрия имеет место, она важна, поэтому это можно отобразить вот таким вот образом, создав симметричные, теперь уже полуокружности, мерные симметричные относительно вертикальной линии. Таким образом, любые два стимула, симметричные относительно вертикали, будут возвращать нам одинаковые значения окружности. То же самое можно сделать и для горизонтальной. Здесь уже мы имеем дело с четвертями, как вы видите, то есть четыре четверти. И можно измерять стимулы, которые оказываются симметричными или относительно горизонтальной, или относительно вертикальной оси. Что это в частности позволяет нам делать, поиск симметричной симметрии? Он в частности нам позволяет… Как найти эту точку симметрии или ось симметрии? Ось симметрии есть исследование, есть даже книга одного из американских авторов о симметрии. Он там, собственно, приводит интересную картинку. Ось симметрии любой фигуры – это множество точек, равноудаленных от точек поверхности. Пользуясь этим свойством, довольно просто найти точку, равноудаленную от этих двух точек А и Б. Предположим, что у нас есть два стимула, и центр находится здесь. Если ответ рецептивного поля в стимуле А является более большим, то есть он дальше от центра находится, а здесь поменьше ответ, соответственно, они несимметричны относительно центра полярной системы. Мы по радиусу теперь сдвигаем в сторону большего потенциала. Кстати, площадь я называю потенциалом рецептивного поля. Это короче в объяснении. Так вот, если сдвигать центр поля в сторону большего потенциала, то найдется такая точка. Вот здесь она уже показана справа. при размещении в которой центра полярных координат оба стимула А и Б будут возвращать одинаковые потенциалы. Таким образом, можно найти ось симметрии в некоторой фигуре, допустим. И вот это очень важное свойство. Довести здесь этого, конечно, недостаточно для того, чтобы ось фигуры провести. Я только наметил, что нужно делать. Но тем не менее, если мы научимся размещать глаз на линии симметрии, допустим, тела, то мы будем обнаруживать, что существует симметрия. И в этой ситуации симметрия уже в ЛКТ, т.е. складывая теперь две половинки ЛКТ. И это нам может указывать на искусственное происхождение объекта, с которым мы имеем дело. Т.е. это живое существо или это дерево, я не знаю, или еще что-то, но это не скала, к примеру, или еще что-то. Теперь он и оффрицептивные зоны – это известная вещь. Они обнаруживают либо кольцевой стимул, либо пятно, либо кольцо. Как это работает, достаточно просто понять. Если мы засвечиваем внешнюю часть кольца, а внутренняя часть кольца будет темной, мы можем предполагать, Мы имеем дело с кольцевым стимулом. Если будет наоборот, то есть мы воздействуем на центральную зону, а кольцевая, оказывается, не возвращает нам никакого потенциала, мы будем считать, что имеем дело с пятном такого размера. Причем мы можем таким образом измерять пятна разного размера и кольца тоже разного размера. Для этого достаточно в каждой точке разместить множество таких детекторов, увеличивая их размеры. И вот как это будет выглядеть условно сверху, потому что это виртуальная, разумеется, разметка рецептивного поля. То есть, если мы видим, что возвращает, допустим, кольцо самое последнее, вот эта зона, самая крупная, то мы можем посчитать, соответственно, и размер такого кольца. Еще одна штука интересная, заключается она в следующем, если мы умеем обнаружить, что у нас стимул является пятном, но при этом мы можем измерить соотношение стимула, то есть потенциала, как сказать-то. То есть, если мы сумеем вычислить, какое число светочувствительных элементов в этой зоне засвечено и будем знать полную площадь, то исходя из этого, к примеру, можно обнаруживать и детектировать правильные многоугольники. Каким образом? Если мы умеем при этом распознать количество углов такой ломаной, Дело в том, что в каждую окружность можно вписать множество многоугольников с одним и тем же числом углов. максимальным соотношением площади многоугольника к площади описанной окружности будет обладать именно правильный многоугольник. Таким образом, можно, к примеру, обнаружить, если бы мы умели посчитать отношение этих площадей, то тогда мы могли бы твердо утверждать, что имеем дело с правильным многоугольником. Вот здесь для треугольника, для квадрата показано и так далее. Пятиугольник, шестиугольник. Вот соотношение площадей. Вот здесь оно указано. Дирекционные поля тоже штука известная. Как они работают, исходя из вот этой модели потенциалов или площадей, достаточно просто тоже можно объяснить. Следующим образом. Вот у нас с вами, допустим, возьмем вот это поле, которое регистрирует смещение линейной границы. А вот это поле угловой границы. У такого поля есть главное направление. Здесь оно показано в виде направления F. Если стимул движется с этого направления, То есть вот у нас направление, и если бы стимул, граница B, двигалась по направлению F, то в конечном итоге граница B бы совпала с границей G. заняв всю вот эту вот область. И после этого изменения бы прекратились. Теперь, если мы предположим, что командой к отключению, к прекращению измерений будет засветка половина внутренней зоны, то есть вот сейчас показана вот эта вот часть, если будет засветка, или неважно какая, важно чтобы граница проходила через центр этой зоны. Вот если мы сделаем внутреннюю зону триггером, и будем этот триггер включать в момент, когда произошла засветка стимулом половины площади этой зоны, то тогда мы сможем измерять угол. Достаточно просто это делается. Вот здесь это показано. Вот зона B, она сместилась теперь в центр. Мы отключили измерение, а перед этим измерили Измеряли всё время, в каждый момент времени, передвижение этой границы, измеряли, какую площадь вот этого сектора она занимает. Понятно, что отношение вот этого сектора, площади этого сектора к площади всего кольца, ну или этого сектора к площади всей окружности, и умножить это на 2π, мы получим угол. Вот внизу это написано. Таким образом можно определить, под каким углом стимул двигался в отношении главного направления этой зоны. Что это нам даёт? Да, детектор смещения угловой границы действует аналогичным образом. Вот я вам показал угол, вот эта внешняя часть засвеченная, будем считать, что стимул здесь. Соответственно, когда будет закрыта половина вот этой вот зоны центральной, то внешняя часть будет закрыта более чем наполовину, и это будет указывать на то, что граница точно не прямая, а изогнутая какая-то. Будем считать это углом. Инвариантом углов здесь можно считать… Разница углов наклона прямолинейных стимулов, то есть те, которые мы рассматривали вот здесь. То есть если у нас есть две границы линейных, одна расположена через множество зон так, а вторая, допустим, расположена вот в этом направлении, то разница между этими двумя углами, альфа и плюс один, даст угол между этими направлениями, которые кажутся нам линейной границей, углом между двумя линейными границами. Соответственно, инвариант ломаной линии может быть представлен в виде множество таких углов бета. Если их расположить в порядке, в котором они есть, то тогда можно подобные фигуры искать в частности, понятно, что без афинных преобразований. Ну, углы можно измерять по-разному. Я говорил, что можно две точки искать, между которыми искать равноудаленную. Ну, в частности, это частный случай такого. Я здесь не буду останавливаться, потому что это не так может быть интересно. Важно, что если мы умеем найти точки касания, а они должны иметь одинаковые потенциалы, то мы построим среднюю точку и можем построить на самом деле линию симметрии угла, что во многих приложениях очень важно. Ну и также мы можем измерять угол в полярной системе координат, зная, что вот эти точки имеют равные потенциалы. Таким образом можно узнать размер угла достаточно точно. Вот это экспериментальные данные о том, как работает зона, дирекционная зона для обнаружения смещения линейной границы. И вот как хорошо согласуется мое приближение с тем, как работает реальная зона. Вы видите, что здесь, вот на самом деле это прямая, Это зависимость площади сектора, деленная на площадь всего круга. Понятно, что это прямая линия, но здесь она в полярной системе координат приобретает вот такую форму, и сердечко получается такое же, как было получено в экспериментах. То есть теория в данном случае хорошо согласуется с результатами экспериментов, которые широко известны. Круговой детектор линейной границы, нам бы хотелось с одинаковой точностью измерять смещение линейной границы, независимо от направления. Соответственно, хотелось бы в одном и том же месте разместить много детекторов линейной границы, которые будут отличаться только главным направлением детектора. Ну вот если друг на друга их наложить, то мы видим, что множество детекторов с равным распределением по окружности вот этих направлений, они позволяют с одинаковой точностью усреднять значение угла смещения границы. То же самое можно сделать и для смещения угловой границы. Интересно следующее. Здесь возникает вопрос сразу же, а сколько таких направлений, вот этих, сколько таких главных направлений должно быть? Какое минимальное их число достаточно? И здесь мы, наверное, придем к общему мнению, что таких направлений должно быть не менее трех. Так вот интересно, что у зайца и еще там у каких-то животных были обнаружены дирекционные поля, у которых было три главных направления, и это объяснить не могли. Вот я думаю, что объяснением является минимальная, так сказать, достаточность этих направлений для усреднения и с целью получения достаточно точно измеренных углов движения. Возможно, с этим как-то связано то, что упаковка светочувствительных элементов во всех живых системах, она вот тетрагональная или гексагональная, а не ортогональная, которую делают на матрицах зачастую. Причем эта упаковка, она самая плотная является. Теперь к приводам матрицы, почему это важно, поясню. Дело в том, что когда мы научились измерять, понятно, что мы можем распознавать какие-то инварианты каких-то объектов и их возвращать в виде множества. Но есть еще некоторые задачи, которые ГЛАСС выполняет. В частности, он должен точно переключаться с одного объекта на другой, перемещаться с одного стимула на другой. И еще одна задача, которая не явно совершенная, я думаю, что это тоже хорошая находка, к которой я пришел. Смысл в том, что глаз управляется тремя парами мышц, две из которых, две пары, одна вертикальная, другая горизонтальная, на самом деле это Декартова система, где есть вертикальная ось и горизонтальная, кстати говоря, еще позволяет утверждать, что вот симметрия, о которой шла речь раньше, она имеет место и поддерживается природно в глазу. Так вот, интересно, что если разместить поля, измеренные поля в том же направлении, в котором действуют мышцы, прямые, горизонтальные и вертикальные, то мы приходим к возможности прямого управления мышцами через сигнал, который глаз получает. То есть для этого даже мозг можно не включать, как говорится, а работать исключительно на коротком звене, когда сигналы из глаза являются управляющими для этих мышц, для того чтобы поддерживать, допустим, стабилизировать изображение. Ну, давайте рассмотрим, как это делается. Предположим, вот у нас есть с вами привод некий, а это матрица, то есть это считательная матрица или сетчатка глаза. Вот привод раздвинулся и повернул каким-то образом сетчатку. Понятно, что у сетчатки есть еще и вертикальные заделки, поэтому здесь показано смещение вот на ΔД, но на самом деле это поворот вокруг оси со смещением. Вот таким вот, да, будет. в плоскости перпендикулярной заделкам. При этом, если бы на сетчатку был в это время спроецирован некий стимул, который является неподвижным, а движется только сетчатка, то тогда стимул, находясь в одном месте, допустим, в начале смещения он находился в центре, он сместится куда-то на периферию сетчатки, на расстояние ΔL. Так вот, если поделить ΔD на ΔL, мы получим передаточное отношение привода. Это, собственно, и позволяет нам, если мы умеем обнаружить, на какую дистанцию в мерной системе глаза сместился стимул, то мы можем очень просто вычислить расстояние, на которое нужно перевести привод с тем, чтобы стимул оказался снова в центре глаза. То есть стабилизация изображения становится очень простой функцией самого, собственно, самой матрицы по сути, которая управляет своим же приводом при необходимости. Я не утверждаю, что так и это имеет место в живых системах, но это возможно. Для того, чтобы измерять, передавать смещение стимула на привод, нужно соединить две системы. координатные, первое – это полярная система сетчатки, и второе – это система привода, это мерное поле привода, то есть привод находится, допустим, в начальном положении где-то здесь, он может сместиться сюда и сюда. Если это мерное поле, не дирекционное, а обычное, т.е. когда мы возвращаем всю площадь просто, то тогда, значит, он движется, если он будет вправо двигаться, то площади должны увеличиваться, если влево, то уменьшаться. Соответственно, мы можем задействовать левую или правую мышцу, потому что глаз движется в одном направлении только одной мышцы. Это, кстати, очень важный момент. И он имеет место во всех мышцах, то есть мышцы всегда парные, они имеют антагонистов, и в одном направлении движется только, работает только одна мышца, поэтому мозгу не надо знать, где левая, где правая, ему достаточно знать, на какой сигнал отвечает одна мышца и на какой другая. Для него это имеет значение, все остальное нет. Так вот, если нам нужно дирекционные поля сделать вместо обычных, то есть главное направление вправо, а здесь главное влево, для чего это может быть надо? Если нам нужно включать одну мышцу, допустим, которая движет влево. то мы измеряем вот этим полем. Видим, что смещается стимул вправо, мы его измеряем, и тогда включаем левую мышцу, которая уравновешивает это движение и сдвигает, соответственно, сетчатку таким образом, чтобы стимул оказался неподвижным на сетчатке. Соответственно, это для горизонтальных мышц, ну или приводов сетчатки нужно вот таких два поля иметь. Вот они друг на друга накладываются, и соответственно, в одном и том же поле, на том же геометрическом размере находятся два поля. Ну и то же самое нам нужно сделать для вертикальных мышц. Соответственно, будет вот так. Выглядит стабилизация изображения, которую я говорил, таким образом, что когда сместился стимул куда-то, или он просто был обнаружен на периферии, обратил на себя свою внимание, допустим, мерцание. Для того, чтобы сетчатку, а именно оптическую ось сетчатки передвинуть в местоположение стимула, нам достаточно знать вот это расстояние на сетчатке, умножить его на передаточное отношение привода, и мы получим. Причем по обоим осям, естественно, соответственно, определится и та мышца, которая это делает, потому что мышца только одна может это сделать, движение вверх и вправо. Вправо одна мышца, вверх другая. И, соответственно, передав это смещение на приводы, мы переведем глаз в нужную нам позицию. Теперь интересная штука, как можно измерить кривизну поверхности. Дело в том, что все вы, наверное, знаете, что плотность колбочек падает от центра сетчатки к периферии. Почему это происходит, не знают, догадок нет. Моя догадка следующая. Вот я нарисовал здесь некую сферу, на которую падает вот таким вот образом свет, и вот в этой точке образуется самая большая засветка, потому что здесь угол падения 90 градусов. Во всех остальных случаях он не 90, он меньше, поэтому, соответственно, и отражения будет меньше. Так вот, если мы теперь посмотрим, измерим освещенность в точках P и в точке A, то можно найти, как они соотносятся. Они соотносятся вот так. Здесь я использую вместо EP, то есть вместо освещенности точки P, я использую оригинальное. Почему? Если мы имеем сетчатку, в которой плотность светочислительных элементов от центра к периферии меняется, то когда мы переводим центр глаза из точки А в точку П, то точка П оказывается в том месте, где плотность меньше. Измеренная освещенность окажется меньше, поэтому нам нужно использовать ту освещенность, которую мы измерили, когда смотрели непосредственно в точку P, то есть когда центр глаза совпадал с точкой P. Вот зависимость. К чему это ведет? Это ведет к очень забавной штуке, что если мы предположим, Мы измерили функцию изменения освещенности. Измеряя глазом, мы построили функцию изменения освещенности поверхности такой сферы. Если мы теперь начнем изменять плотность существительных элементов на щетчатке по этому закону, то у нас получится вот такая интересная штука. У нас получится вот это. штрих штриховая линия это изменение плотности элементов на сетчатке а вот это это поверхность шара вот смотрите что будет происходить когда мы будем менять посмотрите что будет когда мы будем с вами менять точку зрения, точнее фокус. То есть мы сначала сосредоточимся на точке П, то есть они будут совмещены, точка О и П. Мы измерим освещенность точки P. Поскольку мы будем измерять ее в той точке, где самая высокая плотность существительных элементов, то понятно, что мы оригинальную, условно говоря, получим, освещенность точки P. Затем мы начнем смещать центр глаза в сторону, то есть к точке A в какую-то. Посмотрите, что произойдет. Поскольку плотность снизилась настолько же, насколько снизилась освещенность поверхности в точке О, то измеренные освещенности не оригинально, а вот мгновенно измеренная вот эта освещенность и вот эта освещенность. Вот эта освещенность. При таком смещении глаза они окажутся одинаковыми. Я не знаю, поняли вы или нет. Боюсь просто повторять. Времени будет мало. То есть, если мы будем смещать точку О вдоль сферы, то каждый раз та точка, на которую мы будем смотреть, ее освещенность будет на сетчатке, измеренная освещенность этой точки будет такая же, как измеренная в это же самое время освещенность точки П, только точка П будет измеряться, и освещенность ее на периферии уже, а не в центре глаза. Если бы природа захотела, чтобы мы реагировали на сферу определенного размера, то есть проекция, которая имеет определенный размер на сетчатке, по какой-то причине, то вот это была бы хорошей причиной для того, чтобы снизить… изменить вот таким вот образом плотность светочувствительных элементов. Понятно, что мы можем такую же картинку сделать, только с другой поверхностью, другой формой, и будет… эффект будет тот же самый. Здесь видно, что и здесь, и здесь расстояния одинаковые, соответственно, измеренные освещенности тоже окажутся одинаковыми. Можно не делать, на самом деле можно еще проще сделать, этому не сложно показать, что можно создать карту поверхности просто исходя из измеренных освещенностей. Просто берем любое место точку P, измеряем ее освещенность, считаем ее неким эталоном, затем меряем в любой другой точке освещенность, опять в центре глаза теперь уже только, только центром глаза это делаем. И отношение этих освещенностей можно считать изменением кривизны поверхности, можно в неком приближении на самом деле считать, это можно показать высотой этой точки относительно точки P, то есть ниже или выше она будет. Инвариант 3D угла. Каким образом его можно найти? Вот у нас с вами есть угол и есть направление AB. Это направление светового потока. Здесь угол α1 больше, чем α2. Вот здесь видно, что α2 внизу. Поскольку по освещенности непонятно, вот эта грань находится с этой стороны светового потока или по другую сторону, вот здесь, то проще перенести ее по одну сторону, то есть взять минимальный угол. Сделать минимальную разницу углов А1 и А2. Для этого берем вот эту сторону. симметрично переносим, зеркально переносим относительно светового потока, получаем вот такую. И теперь измеряем освещенность этой и этой поверхности. Так вот, разница альфа-1 и альфа-2, вот этот угол бета, он будет у нас равен аркосинусу освещенности этих поверхностей. Таким образом, в качестве угла можно использовать либо отношение освещенностей, либо аркосинус освещенности. И он не будет меняться в зависимости от поворота таких поверхностей. Потому что угол между поверхностями не меняется, и отношения освещенности, при том, что сами они будут меняться, отношения меняться не будут. Понятно, что если у вас есть многогранник, то вот это отношение будет характеризовать многогранник, или вот такая последовательность. Таким образом, мы приходим к тому, что мы имеем дело с инвариантом с некоторым, который можно распознавать. Спасибо, здесь я закончу. Итак, что я хотел сказать для следующей презентации, это важно, что на самом деле, если у нас распознаны некие инварианты, допустим, Если мы обнаружили, на какой глубине эти инварианты находятся, то такой ансамбль объектов на самом деле можно представить и последовательностью, почему там через саккады можно и через что угодно, но на самом деле они сами последовательностью могут быть представлены даже потому, что у них разная глубина, к примеру. Ну, если есть вопросы, видимо, Антон их в конце надо задавать, а не сейчас. Верно я понимаю? 

S02 [00:39:08]  : Да, да. У нас так принято. Ну, если вы чувствуете, что здесь подходящий момент для вопросов по первому блоку, то, может быть, какие-то принципиальные вопросы имеет смысл задать сейчас. На ваше усмотрение. 

S03 [00:39:22]  : Я не думаю, что здесь принципиально что-то интересное такое есть, что бы хотели прямо сейчас выяснять. Это вспомогательный материал. Если прямо захочется что-то, я потом... Хорошо, давай тогда дальше. Да, буду готов ответить. Давайте к памяти последовательности теперь перейдем, поскольку у нас теперь будем считать, что работа по зрению нам позволяет предположить, что канал зрения способен формировать некие объекты, инвариантные, да? который можно представить некой последовательностью или ансамблем, то нам нужен некий механизм, который позволит с этим ансамблем дальше работать и извлекать из этого дополнительные знания. А именно причина следственной связи скорее вот так вот это будет выглядеть. Опять предупрежу. Все это материалы патентных заявок или патентов, поэтому прошу внимательно к этому относиться. Я открыт к сотрудничеству, ну просто это нужно знать. Память последовательности, их главная идея, собственно, это взаимная встречаемость объектов. Для того, чтобы построить модель мира, о законах которого мы ничего не знаем, мы можем только наблюдать за встречаемостью объектов. То есть, если они встречаются чаще, мы можем сделать вывод, что они как-то друг к другу связаны. Если они встречаются реже, значит, они меньше друг к другу связаны. Если не встречаются, то, наверное, связи нет никакой. Кстати, это Антон к отложенному подкреплению. Если они не будут встречаться в явном виде, нельзя будет уловить связь, то, конечно, и вывод о их связи очень сложно сделать. на научные изыскания уходит очень много времени. То есть это не вещь, которую можно вот так вот просто выцепить из реальности, из внешней. Но я вот обращаю ваше внимание на то, что это именно модель мира, которую мы можем осознать, увидеть и построить, не имея какого-то математического аппарата и так далее. Кстати, позвольте меня вернуть прямо на мгновение к предыдущей презентации. Дело в том, что я говорил о мышцах. Мышцы, я был очень удивлен, я думал, это нужно делать нейронами, а выяснилось, что, оказывается, есть два типа мышц. Одни мышцы, значит, иннервация мышц приводит к ускоренному движению, а иннервация других волокон красных, белые и красные волокна, а красные волокна, если их иннервировать, то это к пропорциональному движению приводит, понимаете? Как любопытно. То есть мозг придумал вот такую специализацию, и ему не нужно думать, ускоренно движется или нет. Он просто включает ускоренное движение, не зная о том, что это ускоренно, и видит результат, который его устраивает. Это здесь тоже будет важно в некотором контексте. Ну, давайте продолжим. Начнем с последовательности слов текста, потому что это самая известная штука. с которой мы повседневно имеем дело – это Google, Yandex, поиск и так далее. И терминология, соответственно, всем нам известна. Если мы пытаемся найти что-то в сети, то используем для этого ключевые слова. Вот по аналогии я назвал объект, который будет в фокусе нашего рассмотрения, это будет ключевой объект. А те объекты, которые с ним встречаются, они будут частотными объектами, то есть насколько часто они с ним встречаются. Хочу здесь замечание сделать, которое достаточно важным является. Дело в том, что если мы изучаем последовательность слов, то мы изучаем не законы мироздания, а законы построения текстов, т.е. законы языка, грамматика, орфография и т.д. Т.е. когда GPT-3 изучает тексты, она, вообще говоря, не строит модель мира, она строит модель языка. И с этим нужно аккуратно относиться, потому что когда мы ставим задачу, которую хотим решить, нужно понимать, что система будет решать именно эту задачу, а не какую-нибудь другую. Это очень хорошо видно на примере, который я здесь привёл. Лёд треснул, когда я на него наступил. Это вот текст. А порядок событий на самом деле обратный будет. Сначала я наступил, а потом он треснул. То есть логика мира, причинно-следственная связь, я наступил, а потом он треснул, она из текста неочевидна, если не знать там смысла языка, а не сопряжение слов. Модель встречаемости объектов, она построена следующим образом, достаточно простая. Мы берем каждое слово, это как в поисковых машинах, SNIPPET, берем таким образом, чтобы от вот этого ключевого слова C0 была цепочка вправо в будущее и влево в прошлое, то есть до и после него определенное число объектов. Ну, в данном случае такая область, она имеет Вы видите, да? Затем берем кучу сниппетов. Допустим, мы взяли поиск, провели в Гугле и нашли множество таких включений этого объекта в разные строки и выписали эти строки. Вот они здесь показаны. Давайте сначала о терминологии договоримся. Я это называю кластером. Лучше так и продолжим, потому что тогда мы будем друг друга понимать. Кластер прошлого, он включает все объекты, с которыми ключевое слово C0 встречалось в прошлом. Соответственно, кластер будущего – это все объекты, с которыми C0 встречается в будущем. Кластер будущего и прошлого можно разбить на ранговые кластеры. Ранг определяется числом объектов, которые разделяют эти два ключевой объект и другие объекты. Ну, в данном случае вот эта колонка, она непосредственно примыкается к 0, соответственно, это первый ранг. Ну, будет второй, третий и так далее, и последний ранг будет минус R. Почему минус? Это прошлое. Соответственно, то же самое мы можем сказать и о кластере будущего, можем его разбить на ранговые кластеры от 1 до R. Терминологию я уже упоминал, C0 – ключевой объект, частотные объекты те, с которыми он встречается. Вот этот кластер, о котором мы говорили, минус 3 объекта C0, это значит, что это кластер третьего ранга в прошлом. Полный кластер, он просто минусы 0 обозначается, полный кластер прошлого объекта для ц 0. Соответственно, весь кластер прошлого, его можно представить как сумму кластеров, ранговых кластеров от 1 ранга до ранга R, в прошлое в данном случае. Встречаемость объекта C0 со всеми другими объектами кластера можно вычислять достаточно просто. Число появлений конкретного объекта частотного к числу появлений всех объектов частотных, с которыми этот C0 встречается вообще. Вот этот поправочный коэффициент, он может быть необходим. В частности, если мы берем, строим общий кластер, то встречаемость со словом в третьем ранге может отличаться с этим же словом. Объект С0 может встречаться в разных рангах. Соответственно, мы захотим понизить влияние встречаемости в конкретном с ростом ранга, к примеру. Надеюсь, что понятно. То есть, это поправочная функция, функция ослабления. Представление объекта в множестве его связей можно представить либо вот такой, как комбинацию, весовую взвешенную комбинацию, ну, вектор по сути, да, и либо множество, то есть параметрически задать, ну, это те же самые весовые коэффициенты, просто представление другое. Давайте спрогнозируем появление объекта в будущем или в прошлом. Процедура будет одна и та же, только кластер будет другой. Предположим, нам с вами известны три последовательных объекта из прошлого в будущее. То есть вот этот самый последний введенный объект, нам нужно узнать объект С плюс один, каким он будет. Достаточно просто выяснить. Мы берем для последнего объекта, строим кластер первого ранга, потому что вот этот объект непосредственно с ним связан будет. Соответственно, он будет находиться в кластере первого ранга. Для второго объекта кластер второго ранга. Для третьего объекта кластер третьего ранга. Находим их пересечение, и в этом пересечении обязательно должен появиться объект, который мы ищем. Чем выше суммарный вес этого объекта вот среди, в этих кластерах, то есть мы складываем его веса в каждом из кластеров и находим суммарный вес. Чем выше его суммарный вес по кластерам, тем выше вероятность его появления в будущем. Ну, соответственно, этот объект не может появиться в симметрической разнице этих же кластеров, потому что симметрическая разница этих кластеров, на самом деле, она указывает нам на объекты, которые не могут одновременно быть продолжением вот для всех известных объектов последовательности. Вот такой физический смысл у симметрической разницы. родители произвольного кластера. Понятно, что если у нас к каждому объекту соответствует кластер, то, наверное, и каждому кластеру может соответствовать некий объект. Это предположение можно проверить достаточно просто. Мы берем три объекта. Ну, в данном случае три, понятно, что объектов может любое количество быть. B, C и D. И предполагаем, что у них есть общий родитель, причем в первом ранге. Это может быть и во втором ранге мы можем захотеть искать такие объекты, и в третьем, и в десятом, но в данном случае в первом. Мы для каждого из этих объектов строим кластер первого ранга в прошлое, и в пересечении этих кластеров находим множество объектов некоторых А, условно называемых А, то есть это претенденты на то, чтобы быть родителям этого множества объектов, вот этого кластера, который у нас был исходным. Почему их может быть несколько, вот этих родителей? Ну, вариабельность существует, во-первых. Во-вторых, вообще говоря, допустим, синонимы могут оказаться все вот в этом месте. То есть, если синонимом, то есть словам с одинаковым смыслом будет соответствовать одинаковый кластер, то, соответственно, они все здесь могут появиться. Таким образом, мы можем работать, в частности, по поиску синонимов. Ну и другие вещи можно делать. Об этом можно говорить и думать дальше. Давайте посмотрим, как контекст последовательности меняется. Сначала поясним, что это такое. Возьмем с вами два слова – молоко и сыр. Понятно, что это слова из молочного производства. И понятно, что у них семантическая игра, наверное, будет общая. Там коровы, доярки, сеновалы и прочее. И если найти пересечение этих кластеров, то оно будет определенным. Вот если теперь слово «молоко» и кластер слова «нефть» взять и найти пересечение из нефтяной промышленности и молочной, то очень мало будет общих слов, соответственно, Число объектов в пересечении первом будет больше, чем число объектов во втором пересечении. Таким образом, мы можем сказать, что мы обнаружили некую меру или признак общности, то есть в какой степени вот эти два слова общие являются словами одной тематики, одного контекста. Расписать это можно так, что вот контекст молоко и сыра, он состоит из того, что мы складываем эти два контекста, а затем складываем веса объектов, которые находятся в одном и другом кластере, одних и тех же объектов, веса одних и тех же объектов, и вычитаем симметрическую разницу из этого. Ну, давайте дальше посмотрим. Если написать то же самое для контекста некой последовательности, то будет это выглядеть следующее. Вот в этом первом члене мы с вами находим пересечение кластеров всех известных членов последовательности. Для всех известных членов последовательности. Затем мы берем, суммируем. Для каждого члена последовательности, для каждого объекта частотного мы суммируем все веса. Вот это вот второе, вот это вот суммирование по r. И в конце концов мы берем и суммируем теперь все веса всех объектов, которые мы только что считали. То есть мы считаем общий вес на самом деле пересечения кластеров. Зачем мы это делаем? На самом деле это нормирующий коэффициент, который мы дальше помещаем в знаменатель и получаем таким образом вектор вероятности. Первая часть – это веса гипотез, которые нас интересовали. А когда мы делим на контекст, то есть на полную сумму всех весов этого пересечения, мы получаем, естественно, нормированные показатели, то есть вероятности. Вот сейчас мы исследуем, как себя ведет этот контекст, этот знаменатель. Возьмем с вами два текста про молочное производство и нефтяное, и посмотрим, что из этого получится. Вот мы давайте соединим эти два текста простым способом. Сначала текст о молочном производстве начнем читать, а потом, не делая паузы, перейдем к нефтяному производству. И каждый раз будем проделывать одну простую вещь. То есть, мы будем всякий раз вычислять контекст. То есть, с каждым новым объектом мы будем контекст вычислять. Вот мы ввели объект и пересчитали сумму. контекста и будем смотреть как он будет меняться вот смотреть что будет происходить поскольку тема одинаковая то поскольку мы с вами отбрасываем симметрическую разницу то есть то что не специфично для всей тематики то есть для то что не является семантическим ядром для молочного производства все это будет вырезаться А все слова, которые специфичны для семантического ядра, их веса будут расти, потому что они все время будут складываться. У нас вес общий будет расти. Но когда мы перейдем с вами к нефтяному производству, то одновременно и термины семантического ядра для нефтяного производства и молочного, они одновременно попадут в симметрическую разницу. потому что у нефтяного производства семантическое игро отличается от молочного. Что произойдет? Произойдет падение общей суммы. Дальше, поскольку за короткое время будет вырезано все, что относилось к молочному производству из семантического ядра, затем ядро опять начнет расти и достигнет где-то пика. Вот если войти этим двум пиком, а пики – это у нас множество объектов с весами, соответствующие семантическому ядру. Вот что это такое. Так вот, если теперь каждому из этих множеств присвоить идентификатор синтетического объекта, который раньше не существовал, то есть ни Cn1 не существовали, ни Cn2, у нас всего n было до сих пор объектов, то мы получим с вами объекты следующего уровня иерархии. Понятно, что они образуют тоже последовательность и могут быть представлены в памяти последовательности так же, как и все другие объекты. Таким образом, у нас с вами образуется механизм сжатия временного и смыслового и представления последовательности на все более высоком уровне иерархии. Ну, собственно, я это, наверное, уже сказал. Да. Ну, вот, собственно, этот вектор вероятности прогнозов. Мы, на самом деле, его и построили, потому что в пересечении этих кластеров могут попасть только объекты, которые одновременно могут быть продолжением для всех известных объектов последовательности. Поэтому это именно прогнозы. И это важно, кстати говоря. иметь в виду для следующего, для того, о чем будем говорить дальше. Автоассоциативность, память и последовательность. Дело в том, что когда мы с вами создаем… создали объект вот этот синтетический и присвоили ему некий контекст, то есть некое семантическое ядро, то всё, что является подмножеством этого ядра, каждый раз, когда мы будем вводить этот объект, на выходе будет создаваться кластер, который является подмножеством, то мы можем узнавать такой кластер и таким образом вспоминать последовательность или объект, который его породил, и подавать его на вход. Тогда возникнет автосоциативность. к примеру, может возникнуть так, что все последовательности, которые связаны с этой тематикой, они придут на вход и будут поданы на вход, таким образом мы перезапишем память и обострим ее. Если использовать еще и понижение тех объектов, несколько объектов, которые не попадают сюда в этой же выборке, то это будет похоже на латеральное торможение объектов. Давайте теперь к архитектуре чипа перейдем, к аппаратной реализации. Я, собственно, покажу, как это работает или может работать. Архитектура будет кроссборд. crossbar извиняюсь вот crossbar у нас с вами но здесь проблема с crossbar такая что поскольку у нас набор объектов и здесь и здесь один и тот же то у нас есть избыточность то есть относительно диагонали у нас удвоение связи каждый с каждым происходит и это нехорошо поэтому лучше брать треугольную вот такую вот, я ее накосынкой назвал, или треугольником, архитектуру. Причем теперь, смотрите, здесь принципиально нужно понимать следующее, что на самом деле каждым объектом в этой архитектуре является шина отдельная. То есть если в шине есть сигнал, значит этот объект присутствует. Если в шине сигнала нет, то его нет. То есть вот такая архитектура. Давайте теперь в пересечении каждых двух шин разместим два накопителя встречаемости в одном направлении A к B и B к A в обратном направлении. Понятно, что вот в уголках вот в этих будет сам собой, да, пересечение должно быть. Ну, вот его можно сделать вот таким вот образом. Я не хочу здесь о нем много говорить, не буду. Если будет интерес, можно об этом порассуждать, почему это нужно, если в тексте о тексте говорить, допустим, мы едем, едем, едем, трижды повторилось слово, понятно, что в этом есть какой-то, наверное, смысл, и мы это можем здесь записать. Как записывать теперь сигналы? Вот смотрите, предположим, что мы на вход матрицы подаем сразу несколько сигналов одновременно. И вот в этом как раз, когда я вам говорил про зрение, говорил, что можем подавать много сигналов. одновременно, но они могут отличаться, и их отличия мы можем интерпретировать либо как последовательность, либо как глубину, либо еще как-то. Вот в данном случае мы будем отличия интерпретировать как последовательность. Что мы для этого делаем? Делаем очень простую вещь. Берем самую последнюю самую объекту, поскольку он самый свежий как бы в памяти, в окне, внимание, вот, кстати, вот эта цепочка объектов, Она окном внимания называется. То есть это те объекты, которые мы в очереди последние имеем. Мы же не можем помнить там на 10 дней назад и все это в оперативной памяти держать. Вот окном внимания какое-то. Вот наиболее поздний введенный объект, у него самый сильный уровень сигнала. Неважно, что мы силой называем в данном случае. Там ток, напряжение, частоту, что угодно. Измеряемая характеристика – вот о чем идет речь. Сопротивление, что угодно. Каждый последующий сигнал будет слабее на некоторую одинаковую дельту. То есть это S1, деленное на N. N – это число сигналов. Таким образом, у нас равномерно будут сигналы у каждого следующего объекта убывать. Что это нам дает? А дает нам это следующую штуку. Вот смотрите, если у нас с вами есть два сигнала каких-то, вот, допустим, вот сюда, вот четыре сигнала мы подали, и предположим, что самым поздним был вот этот, а предыдущим был вот этот сигнал, то если мы будем теперь записывать во всей этой архитектуре только в то место, где самый сильный сигнал пересекался с сигналом, разница между которыми всего одна дельта, то есть это последний и предпоследний, то все остальные связи мы прописывать в этой матрице не будем. То есть мы будем прописывать только связи первого ранга. Вот в чем дело. Причем в одном и в другом направлении мы можем это делать в зависимости от того, какой был последний, а который предпоследний объект из этих двух. Таким образом, мы благодаря вот этой штуке, вводя много объектов, мы будем прописывать связи селективно, то есть мы можем прописывать связи первого ранга, второго, третьего и так далее. Очень важно, я это замечание сделал, поскольку мы вводим объекты вместе, то вообще говоря, это ансамбль объектов. Кроме того, когда мы суммировали в поисках контекстов, искали пересечения, эти операции тоже не подразумевают наличие какой-то очередности. ансамблевой операции, поэтому утверждать, что это только память последовательности не следует, хотя то, как мы ее интерпретируем в данном случае, она позволяет нам с последовательными системами иметь дело, хотя может работать и с зрительным, допустим, и со слухом, и с чем угодно. Смотрите, что можно сделать для того, чтобы записывать одновременно связи разного ранга. Вот у нас с вами есть первая косынка, в которой мы писали связь между самым сильным сигналом и сигналом, у которого разница в силе всего одна дельта. Вот здесь мы будем писать, вот во второй, точнее вот она вторая, а это уже третья будет. Вот во второй мы будем писать теперь в том перекресте, где одна из шин принадлежит самому сильному сигналу, а вторая – сигналу, который на две дельты меньше, чем самый сильный. А здесь на три дельты меньше, а здесь на четыре и так далее. Вот такая архитектура у нас с вами получается теперь бесконечная. И мы можем очень много связей записывать ранговых. Причем эту архитектуру можно сделать любой формы, ее можно закруглить, она может быть вот такой вот зигзагом, в смысле друг на другом могут находиться эти блоки. И если вы посмотрите на это с точки зрения ДНК, то отчасти это ДНК напоминает, потому что там связи каждый с каждым, если это структурой будет 3D, в 3D сделать такую вот спираль, то она будет напоминать ДНК, ну, по меньшей мере, внешне. Вот такая любопытная штука. Вот, и во всей этой, значит, матрице мы теперь можем прописывать эти связи. Считывать их достаточно просто. В любую шину подаем сигнал считывания, и сигнал этот проходит через все датчики направленные и считывается с соответствующих шин. Давайте теперь… Мы с вами иерархию знаний показывали перед этим, что мы создаем синтетические объекты некие на новом уровне иерархии и присваиваем им кластер семантического ядра. Как это сделать? Вот смотрите. Вот это вот у нас слой… Слой объектов, то есть это нижний уровень иерархии, а вот это следующий за ним, слой иерархии. Между ними есть нейрон, который, собственно, должен нам создать искусственный объект нового уровня иерархии. и должен запомнить при этом о последовательности, которая его породила. Вот мы вводим с вами, достаточно долго там можем вводить вот это окно последовательности, в смысле его увеличивать можем, долго я это имею в виду, или можем его сдвигать, поскольку это очередь, и смотреть за тем, как меняется на выходе сумма всех весов, а именно контекст. Суммарный вес, имеется в виду. И когда мы заметим, что суммарный вес упал после того, как он долго рос, ну, долго опять это все относительно, но тем не менее, он рос, срос, а потом упал или остановился. Вот в этом месте этот нейрон может запомнить все связи, которые его породили, запомнить соответствующие порог и открыть канал в матрицу следующего уровня, в слой следующего уровня. На самом деле в следующем уровне просто находится там, к примеру, может находиться одна шина в состоянии ожидания, а на ней какой-то сигнал ожидания. И как только пробивается какой-то новый сумматор, который хочет найти такую же следующую шину свободную, он соединяется просто с этой, коммутируется с этой шиной, и вот теперь у нас есть шина в общей архитектуре, которая является шиной следующего уровня иерархии. Здесь же есть обратная связь, то есть как только наш нейрон запомнил вот эту сумму, то есть контекст, и подал сигнал наверх, тут же по обратной связи он запоминает значение вот этой понижающей функции понижения сигнала на всех шинах, которые в этот момент были запитаны сигналами, вот этими сигналами, он просто запоминает, какие дельты на каждой шине были. Таким образом, мы что сделали? Мы с вами создали, во-первых, объект нового уровня иерархии, во-вторых, запомнили, какая последовательность его породила. Это достаточно важно. Как это выглядит логически? Логически выглядит так, что вот на выходе матрицы мы берем, измеряем веса и находим максимальную сумму весов для того, чтобы порог прописать для этого нейрона иерархии. Затем выход его, он находится в слое шин следующего уровня иерархии, и конец его шины, он находится на входе матрицы, где он запоминает коэффициенты, которые определили уровень сигнала каждого из объектов, для которых был контекст получен. Вот логика такая. Выглядеть это будет на матрице так же, как и все остальное. То есть никакой разницы на самом деле нет. Вот зеленые шины показаны – это первый уровень, вот синим показан второй уровень, в том красном, может быть, третий уровень показан, и так далее, и так далее. То есть архитектура при этом не меняется. Важно, чтобы просто были размещены в соответствующих местах соответствующие нейроны. Теперь параллельной последовательности. Речь идет о мультимодальности, когда у нас с вами есть некие события, которые одновременно наступают. Допустим, мы с вами научились глазами определять, где находится кошка, и на каком удалении, потому что за счет параллакса мы можем посчитать, на каком она удалении находится. Соответственно, вот у нас теперь есть изображение кошки, предположим, мы ее глазами все-таки умеем узнавать. И есть звук мяукания, а уши бинауральные тоже, слух у нас, то есть мы умеем определять расстояние и направление источника звука. То есть, если мы благодаря бинуральному слуху и бинокулярному зрению определяем, где находится кошка, где находится предмет, от которого исходит звук, и мы понимаем, что это один и тот же предмет, то мы их объединяем. То есть, если это происходит в пространстве, а мы определили пространство, и в одно и то же время это происходит. Таким образом мы можем сшивать какие-то свойства объектов воедино, таким образом усложняя поведение объектов. Я это называю многопоточным, потому что последовательность – это все-таки потоки. Они мультимодальная память последовательности. Как это сделать? Надо искать корреляции. Каким образом мы можем искать корреляции? Для этого я сделал слой измерений. В данном случае показан слой измерений строительной системы исчисления. Вот представьте себе, что на зеленых шинах работает генератор. Он запитывает сначала одну, потом две, потом три шины. Затем сбрасывается опять 1, 2, 3 шины. Каждый раз, когда три шины запитаны, включается следующая шина следующего разряда. Соответственно, после двух циклов будет две шины здесь, после трех циклов три, а если здесь уже три, то следующая будет здесь один разряд. Таким образом, мы счетчик включаем, это счетчик времени. Я позже узнал, что аналогичная структура существует в мозге, есть группа нейронов, которые циклично, значит, возбуждаются, там по кругу у них крутится такая вот, как стрелка, считаете, да? И вот эти биологические циклы, они накручивают, собственно, время. Ну, здесь аналогия получилась просто случайно. Хотя, может, не случайно. Вот генератор показан, и вот есть шины. Причем шины могут быть чем угодно. Это может быть и расстояние. Почему расстояние? Объясню. Дело в том, что если вы поворачиваете глаза, то у вас срабатывает… Я это пояснял, что вы можете посчитать, насколько сместился привод для того, чтобы глаза повернулись настолько, да, и вы можете соответствующее смещение учесть в смещении своего собственного положения, места положения. Также то же самое можно сказать и о любых других мышцах, что вот эта вся система, она очень слаженно работает и отслеживает, куда вы повернулись, насколько повернулись с тем, чтобы ваше представление о своем местоположении поддерживать актуально. Так вот, такие слои измерений могут отслеживать вот такие повороты, время могут отслеживать, и эмоции, в частности. Как эмоции? Эмоции очень просто. Каждую эмоцию мы дискретизируем, допустим, от единицы до ста, и включаем столько шин, которые отражают уровень этой эмоции в данный момент. Таким образом, мы можем запоминать некие… освещать эмоционально все последовательности, которые у нас происходят. Обжег руку, значит, придал ей такой уровень неприятия, что дальше ты этого делать не будешь. Вот, здесь показано, как эта штука работает. Модель, на самом деле, достаточно типовая, теперь уже вы ее узнаете. То есть, на входе и на выходе вот эти сенсоры стоят, они запоминают, на выходе запоминают, считывают текущее положение. А здесь начальное положение, чтобы определить разницу этих положений. Положение, я называю в данном случае, это может быть либо два момента времени, начало и конец, либо это место положения, начало-конец, ну и так далее. Либо это первая эмоция и вторая эмоция. Почему это нужно? Потому что когда мы будем искать объекты, нас будет интересовать, чтобы их длительность была примерно соизмерима с длительностью, по которой мы ищем. То есть у нас есть образец некий с некоторой длительностью, допустим, наносекундной. Было бы глупо искать объекты с длительностью в год. нам нужно чтобы длительность была соизмерима поэтому нам нужны два датчика вот примерно такое вот примерно такая история Да, и вот этот вот нейрон метки, он работает вместе с нейроном смысловым, который контекст цепляет. Собственно, он возбуждается в момент, когда возникает новый нейрон контекста. по обратной связи. Здесь это не показано, но я не доходил до уровня принципиальных схем. Тем не менее, значит, он должен активироваться в момент, когда вот этот нейрон активировался, запомнить вот эти две метки. И возбуждаться он будет, либо искать теперь можно будет, либо по метке, либо по контексту, и находить те объекты, которые точнее тем. те образцы, которые имеют сходную продолжительность, сходный контекст, и которые происходили примерно в то же время или в том же месте и так далее. То есть у нас ассоциативность будет такова, что будут подниматься по меткам не только события, которые мы видели, но те, которые слышали в том же самом месте и которые локализованы были либо по времени, либо по месту. Таким образом у нас возникает машина для производства воспоминаний многомодальных, мультимодальных. Теперь поговорим очень коротко, тоже поскольку времени у нас мало. Уже почти полтора часа, господи, быстро идет. Состояние сознания, значит. Вот смотрите, эрготические системы – это те, у которых ансамблевая вероятность – это Монте-Карло. А неэрготические системы – это временная вероятность. Представьте себе, что когда вы подбрасываете монетку в Монте-Карло, при проигрыше у вас одна десятая доля монетки. вырезается. То есть вы ее теряете. А когда вы выигрываете, одна десятая прибавляется. Но поскольку вы вырезали одну десятую, то когда вы прибавляете одну десятую, вы на самом деле прибавите не одну десятую, а 0,9. Да? 0,09. Соответственно, у вас искажается. Это не 50 на 50. И таким образом вы можете проиграться в результате таких жизненных ситуаций. То есть если вы руку потеряли, то ее уже не отрастите. То есть вы не можете предполагать, что вы снова игру начнете с двумя руками. Так вот, если у нас от времени система не зависит, то накопленный опыт можно представить связями только первого ранга. То есть нам с вами будет достаточно создать матрицу первого ранга для эрготических систем и линейным образом просто высчитывать связи любого другого ранга, которые будут линейной комбинацией связей первого ранга. Если же нет, то нам нужно будет делать как можно больше рангов и запоминать их. На эту тему есть исследования, в физтехе как-то семинар мы делали, и там вопрос этот задавали, а позже как раз подумал об этом, вот с таким выводом пришел. Давайте теперь сравним, как наша система работает, предложенная вам, и с тем, что говорят об человеческом интеллекте разные люди. В частности, давайте сравним с тем, что говорит Анохин о памяти. Вырожденность одно и то же событие хранится в виде множественных и неинцидентных копий функциональной системы. Неидентичных. Ну, неидентичных – это многопоточности или мультимодальности, прежде всего, можно сказать, первое. к тому, что если синтетические объекты не вполне совпадают, то они будут заново создаваться. То есть мы будем с вами иметь Даже на уровне синтетических объектов немножко разная, так сказать, повторяемость некоторая будет, то есть они могут являться подножиями друг к другу. Автоассоциативность, это я уже говорил, она автоассоциативна, причем она вызывает огромный рой воспоминаний, которые можно вводить в матрицу и создавать новые связи, то есть перезаписывать их. Реинтегративность. Целая система может быть заведена из памяти по возбуждению небольшой части элементов. Это, очевидно, следует из того, что я только что сказал. Репаративная система может восстанавливаться при повреждении части элементов. Здесь я не могу сказать твердо, что прямо вот так. Но, наверное, она научится через какое-то время и создаст новые связи, повторяющие прежние. То есть она восстановит память, но в другом немножко виде наверняка. То есть она не будет идентична той памяти, которая была утеряна. Нерепрезентативность. Она не является точным отражением событий внешнего мира. Точно нет. Она может заметить, что может не заметить. Реконструктивность и воспроизведение являются активным процессом самосборки системы. Во время воспроизведения, по сути, мы ее перезаписываем немного. Когда мы множество воспоминаний вернули к ней на вход, она их перезапишет, потому что любой ввод информации, неважно с какой целью, Это одновременно и процесс записи памяти, прописывания. Кстати, вспомнил просто интересную вещь. Ребята из группы спецпроектов в МФТИ исследование небольшое делали, обнаружили, что на небольшом корпусе текстов Она предсказывает память последовательности, предсказывает 99-100%. Как только не очень сильно увеличили корпус, она перестала узнавать. То есть Монте-Карло, в общем, было 50 на 50. Это связано с насыщением. Вот если у вас, а у людей это мудростью называется, если вы многократно имеете опыт, и он у вас постоянно увеличивается, то процент то есть в процентном отношении каждый следующий акт этого события он будет все меньше и меньше процесс от всего от всей вашей памяти составляет таким образом перестанет в какой-то момент влиять если все веса таким образом увеличивается то тогда какой-то момент наступит насыщение то есть они будут равными вот о чем идет речь поэтому записывать в памяти веса лучше таким образом, что вы, условно говоря, увеличиваете вес на определенное число процентов от того, которым он был, условно, на 5 процентов, на 1 процент каждый раз при каждом новом акте встречаемости. Тогда у вас будут они пропорционально расти, поскольку степенной функции окажется. Дальше давайте. Рекатегориальность. Каждая новая реконструкция в воспроизведении проходит оценку идентичности на всех других связанных с ней систем. Ну, здесь не могу сказать. Реконсолидируемость. Каждая новая реконструкция подвергается запоминанию сходному. Ну, это точно так. Меня Антон специально просил сравнить с трансформерами. Вот с трансформерами сравню с удовольствием. Хотя глубоко это делать не буду, оставлю это профессионалам. Но по декларациям самих, собственно, гугловцев, главное отличие вот этого внимания, которое они ввели, в том, что раньше они использовали сети LSTM. там или bidirectional STM, который собственно цепочку всю пытался запомнить. А здесь они создали механизм, который relationship между каждой парой запоминает, то есть связь между каждой парой. ну видно что даже по рисунку здесь что это вот типичный рисунок памяти последовательности да то есть там в каждом ранге мы просто запоминаем только это сделано безумно дорогими средствами это память последовательности вот трансформер вот и все выглядит вот как они его описывают вот так вот не знаю здесь надо Вот так вот, наверное, сделать, чтобы с самого начала. Я не знаю, видели вы эту картинку или нет. Вот как она запоминает. Каждый с каждым сначала и в нескольких слоях. Затем она переводит. Насчет перевода, кстати говоря, я не буду ничего говорить относительно того, как использовать память последовательности для создания переводчика. Над этим еще надо работать, наверное. Но я не думаю, что это слишком сложно. Просто я над этим не думал. Ну, вот примерно так. Наверное, я этим закончу с трансформерами, да, с тем слайдом по трансформерам. Сложность архитектуры. Вот смотрите, у памяти последовательность сложность следующая. Это число всех слов, ну, всех объектов, это число рангов, в которых мы хотим, с которыми мы хотим работать. А это число образцов последовательности с участием каждого ключевого слова в корпусе. И это всего лишь произведение. То есть, фактически, это линейная зависимость от числа образцов. Вот что это такое. Ну, если мы определенное число рангов берем. Или от числа рангов, если мы их увеличиваем. То есть, линейная зависимость. Сложность трансформеров, она 2 в степени n. То есть, если мы на… Допустим, на тысячи слов делаем, тысячу рангов возьмем, да, то сложность задач трансформера будет больше, чем атомов во Вселенной. Ну, даже и тысячу не можем для этого не брать, понятно, там их шестидесяти, наверное, хватит, я не помню, какое число нужно. Обучение с подкреплением и как это связано. Первое, что хочется отметить, что маркерские процессы – это процесс, который, значит, декларативно не зависит от прошлого. Память последовательности, она построена под стулатик, который как раз гласит, что оно зависит от прошлого. Поэтому, собственно, это и является моделью мира. И важно понять применимость этих двух вещей и их соотношение, поэтому нужно понять, где проходит граница применимости к обучению. Поскольку мы пытаемся изменить мир, мир, на самом деле, это мысль наша пытается, она действует через те инструменты, которые ей доступны, непосредственно путем посылки сигналов. Это прежде всего тело наше. Потом тело учится. Сначала мы учимся управлять телом. И вот это первая граница, где как раз применяется кообучение точно совершенно. То есть мы случайно посылаем сигнал, обнаружим, что вот этот сигнал оказывается ногой двигает, а вот это рукой, этот глазами, но глазами скорее нет, потому что там генетическая память есть. Но тем не менее. Таким образом, затем мы это расширять начинаем, т.е. когда мы научились управлять телом, мы начинаем управлять предметами, т.е. когда мы сидим в машине, управляем машиной, на самом деле граница вот наша, она раздвинулась, теперь мы – это не просто тело, теперь мы – это машина, т.е. мы себя ощущаем машиной и знаем, где ее края и т.д. Вывод такой, что обучение с подкреплением является средством создания мозгом драйверов управления актуаторами зоны материализации мысли. Она, все время мы ее пытаемся расширить. Возможно, целеполагание также требует подкрепления, это я не знаю. Может, а может нет. Роль памяти последователей в таком обучении. Прежде всего, мы можем создать картину мира, то есть любые закономерности, которые мы можем наблюдать, они могут стать, собственно, в памяти последовательности, стать такой памятью, которая будет предсказывать. Кстати говоря, ускоренное движение, в частности, почему? Я вам говорил, что есть разные мышцы, одни – ускоренное движение, другие – пропорциональное, то есть дифференциально-пропорциональное движение. Это инварианты, которые можно запоминать в качестве объектов. Соответственно, их последовательность, она может воспроизводить движение. То есть память последовательности, она может запоминать в частности движение. Модели драйверов-актуаторов, которые уже обучены, мы многократно повторяли, и у нас начинают прописываться связи, которые мы теперь уже твердо знаем, как управлять рукой. Вот мы можем разгружать вот это кообучение и больше не обучаться, а делать это на автомате, доставая эту модель из памяти последовательности. В памяти последовательности есть некие проблемы, которые мы с вами обсуждали. В частности, вот проблема контролёра. Я помню, что были замечания коллег о том, что в жизни всё не так, как в кообучении, которое вот мы знаем по аторигам, потому что они, в общем, делались для сравнения версий кообучения. В данном случае обращаю внимание на то, что зрение является контролером, то есть обратной связью. Когда мы пытаемся взять кружку, вы можете это попробовать прямо сейчас или любой предмет с закрытыми глазами, потом с открытыми, обнаружите, что с открытыми вы смело это делаете, потому что глаза контролируют этот процесс. с закрытыми, это сделать сложнее существенно. Из этого можно сделать вывод такой, что память последовательности может быть как раз таким контролером, который определяет вот эту дальность. В данном случае вот это простой пример, когда зрение, то есть это вроде очевидная вещь, хотя она не такая простая, но очевидная тем не менее, но можно определять для кообучения Для обучения с учителем можно либо потерю, либо награду определять благодаря вот этой обратной связи. Как это сделать? Давайте вспомним меры близости. Спасибо Антону, который недавно публиковал ссылку на эти работы. Напомню, что любое состояние описывается ансамблем объектов, которым соответствует ансамбль-композиция кластеров. Мы с вами можем создать некий кластер, который соответствует конечному состоянию. Допустим, отдать ребенка в школу. Школа, ребенок, отдать. Вот берем эти кластеры. Это конечная цель. А сейчас ребенок рядом с нами сидит. Что для этого нужно? Отвезти ребенка в школу. Берем, создаем кластер, вот ребенок сидит со мной, а должен быть в школе. И теперь нам нужно построить из одной точки в другую дорогу. Как найти путь? Казалось бы, вот эти кластеры можно рассматривать уже. Наверное, можно, а может и нужно. Но у нас с вами есть очень хороший механизм, который мы с вами обсуждали до этого, а именно прогнозы. То есть вот вектор прогнозов, если мы его создадим для начального положения в будущее, а для конечного положения мы его создадим в прошлое, то они по меньшей мере коллинеарны в конце концов должны быть. И расстояние между ними должно быть минимально, т.е. они должны быть близки в конечной точке. Т.е. мы все время можем сверять, насколько вектор вот этот, который мы своими действиями создаем, насколько он далек от того вектора, который нам нужен. Вот, собственно, мера дальности, близости и т.д. Как это сделать? По Сёренсену можно в мощностях множеств, по Жакару можно в множествах эту задачу решать, косинусное сходство, можно добиваться коллинеарности. Ну и в вероятностях, вот по кульбака Леблера можно смотреть, здесь, кстати говоря, энтропия информации незримо присутствует вот в этой формуле, я ее здесь специально написал, и вот смысл интерпретируется как количественная потеря информации при замене истинного распределения конечного, она предполагаемая, то есть на наших действиях, то есть что мы вот если поступим так, что будет? И нужен, опять, мы много дискутировали по поводу честного полигона для ко-обучения, здесь просто замечания некоторые. Он нечестный, конечно, этот полигон, JimOpenEye.com, потому что он не для AGI сделан. Он нечестный и по многим причинам, например, черлайк не способен по координатам мячика и ракетки не назвать игру, не понять ее правила, не научиться играть в нее. Почему вот ADI должен это сделать? Кто мне скажет? Я не понимаю лично. Движение рукой обычно дано игроку в виде нажатия каких-то кнопок. Задачей здесь является нахождение этих кнопок и координация движения таким образом, чтобы управлять кнопками этим шариком должным образом. Но делается совсем другое тоже. В результате пинг-понг действия игроки ракетка на самом деле не приводят к материализации мысли агента. Вот, Антон, ваша презентация как пример. Я не имею ввиду ничего плохого, а просто как пример. Там ведь от того, отбил ты мячик или не отбил, ничего не поменялось в игре, по сути-то. То есть материализация, возможно, это необходимая вещь, чтобы наши действия материализовались в нечто, кроме награды виртуальной какой-то. Ну вот, я закончил здесь. Спасибо большое за внимание. Надеюсь, что было интересно. и готов ответить теперь на ваши вопросы. Антон? 

S02 [01:33:30]  : Олег, спасибо. 

S03 [01:33:32]  : Уложился практически, смотри. 

S02 [01:33:35]  : Так, ну вот давайте пойдем сначала. Первый вопрос, который у Евгения Бабарыкина возник. Когда вы говорили про сетчатку, чем те поля, которые вы рассказывали, отличаются существенно от сверточного поля? 

S03 [01:33:54]  : На самом деле это очень хороший вопрос, потому что по сути не отличаются, только действуют существенно быстрее все это. Понимаете, какая штука? Можно ведь сказать, что и память последовательности может не отличаться от трансформера по результату. Только у трансформера. Трансформер можно там условно запустить Google. Вот Яндекс, когда запускал трансформера, они сказали, что на том процессоре, который они до того использовали с LSTM, 10 лет бы он учился, трансформер. То есть им пришлось существенно группировку увеличивать. Вот о чем идет речь. Нет, не отличается по смыслу, потому что результат похожий. Но красота. в подходе, который я проповедую, она должна и подкупать, и убеждать. Я так думаю. 

S02 [01:34:47]  : Спасибо. Следующий вопрос вот по рецептивным полям. То, что вы рассказывали, я правильно понимаю, что вот когда вы рассматриваете отдельное рецептивное поле, вот кружочек, да, это на самом деле только одно поле из многих. То есть на самом деле есть В сетчатке находится много таких рецептивных полей, правильно? 

S03 [01:35:08]  : Да, оно абсолютно виртуально, то есть оно может находиться условно говоря в одной геометрии, то есть вот в одной окружности. Они могут быть одна и та же окружность, описывать светочувствительные элементы, подключенные все одновременно к разным анализаторам. То есть одно и то же поле на самом деле к разным анализаторам может быть подключено. Одни к детектору линейной границы, другое к угловой границе, третье с другим наклоном главного направления и так далее. 

S02 [01:35:43]  : Это по этому поводу как раз будет вопрос дальше. Я вот что хотел принципиально пояснить. Когда вы говорите про рецептивное поле, которое подключено к разным рецепторам. Да, и на самом деле, как я понимаю, есть некоторая стопка разных рецептивных полей, которые выделяют разные характеристики. Вот это все-таки рецептивное поле, оно одно, и мы этим одним рецептивным полем управляем с помощью вот этих вот мышц или все-таки вот таких вот рецептивных полей много. То есть, как в Корее есть набор колонок, так точно также в Сычатке есть набор таких рецептивных полей. 

S03 [01:36:18]  : Условно говоря, мы с Вами создали мерное поле для декартовой системы координат. Оно распространено на всю сетчатку. Оно вместе с сетчаткой поворачивается, то есть эти рецептивные поля там не движутся. Затем мы создали вот эти поля, потом нам нужно создать детекторы признаков, там угла, линейные границы. Мы их создаем такое же поле тоже на всю сетчатку. Затем нам нужно создать какие-то еще какие-то признаки, отлавливать поверх этого опять, но все эти поля они виртуальные. Они просто одни и те же сигналы с одних и тех же светочувствительных элементов передаются в разные фильтры. Вот и все. 

S02 [01:37:07]  : окей ну вот все-таки вот вот эти вот кружки да то есть то что они виртуально это понятно да но реально вот таких вот зон окружных да они вот самая большая зона это вся сетчатка Или все-таки есть какая-то дискретность и можно как-то структурировать сетчатку по максимальным зонам, максимальным радиусам этих зон? 

S03 [01:37:35]  : Я, может, не вполне понял, но вот как понял, тогда отвечу. Первое, вот можете мои рисунки брать, закалькую, как это должно выглядеть физически. Вот это вот прям так. То есть одно накладывается на другое всё время. Если говорить о признаках, которые симметричны по 360 градусов, какие это признаки? Вот помните, допустим, признак пятна или кольца. то такие лучше, допустим, располагать, к примеру, везде можно расположить, но можно в центре расположить концентрические окружности, которые будут измерять их именно. Вот площадь, к примеру. То есть и да, и нет. То есть это зависит от потребностей того, кто дизайном системы занимается. Но в целом, еще раз повторяю, это виртуальное. деления, то есть есть физические светочислительные элементы, мы их просто группируем виртуально в отдельные зоны и располагаем эти зоны таким образом, чтобы решить свои проблемы, а именно проблемы измерения и выявления признаков. 

S02 [01:38:43]  : В вашем понимании эти зоны, которые выделяют признаки разных способов, их счетное количество или вы перечислили только примеры, а их на самом деле гораздо больше? 

S03 [01:38:56]  : Знаете, я исходил не из того, какое их количество, а исходил из того, что я как инженер мог воспроизвести на основании той информации, которая уже известна широко. И я не уверен, что я всю информацию знаю, кстати. Я не могу стать нейрофизиологом, ученым за короткое время. А я этим занимался, условно говоря, полгода. 

S02 [01:39:27]  : А эти зоны, они в вашем понимании расположены в сетчатке? 

S03 [01:39:32]  : Зоны расположенной сетчатки, там есть несколько слоев нейронов, всем это известно. Забыл название. Ну, там три слоя нейронов есть. Вы мне картинку, помните, присылали исследование, что обнаружили, что к одному из типов этих нейронов идут много, из разных зон как бы идут. И он анализирует разные зоны. 

S02 [01:40:02]  : Ну, в вашем понимании, это происходит именно в сетчатке, то есть это происходит не в коре. 

S03 [01:40:07]  : Я думаю, что значительная часть происходит там. Дело в том, что вы не можете без коры обойтись по другим причинам, потому что кора отвечает за сборку картины, и вы без нее обойтись точно не можете. Вопрос другой, что в какой степени сетчатка автономна. Я берусь утверждать, что для этого кора не нужна, чтобы сделать определенные функции, они будут автономны совершенно на уровне матрицы. 

S02 [01:40:33]  : Хорошо. А тогда еще такой вопрос. Вы говорили про симметрию с одной стороны, а с другой стороны говорили про вертикальные горизонтальные мышцы. А теперь смотрите. Я смотрю на дерево. Вот оно симметрично. Я лег на бок, а дерево остается симметричным вертикально. Соответственно, достигается сохранение симметрии за счет вращения глазного яблока вокруг своей оси или Или как? 

S03 [01:41:01]  : Вот там интересно, обращение яблока вокруг овощей, значит, оно до 15 градусов только возможно, причем физиологи пишут, что это причину какую-то смешную называли, не помню. На самом деле с помощью этих мышц можно кривизну, кривых, я просто об этом не говорил, там другая немножко работа, кривизну можно мерить, понимаете? В частности, но к вашему вопросу возвращаясь, не вполне так, потому что не забывайте, пожалуйста, у нас есть вестибулярный аппарат, и он в данном случае вам говорит, что вы находитесь не в том положении, когда вы можете применять вертикальные и горизонтальные мышцы по их задумке. 

S02 [01:41:46]  : Но это тогда все-таки получается, что симметрия будет обрабатываться уже в коре с учетом коррекции вестибулярного аппарата. Потому что сетчатка про вестибулярный аппарат ничего не знает. 

S03 [01:41:59]  : Да, но смотрите, тут какая штука. Вы не можете оказаться в невертикальном положении, потому что мышцы устроены таким образом, что спиной столб отвечает за авторегуляцию состояния. То есть, если нагружать вашу руку, то мгновенно придёт Система отработает, это не мозг, а это спиной мозг, отработает так, чтобы нагрузить мышцу, напрячь мышцу настолько, чтобы стабилизировать ее растяжение, чтобы оно прекратилось. Вот в чем суть. То есть эта функция, как считают физиологи, служит для того, чтобы вы сами собой не свалились, стоя вертикально. То есть вы можете нагнуться или изменить свое положение, специально. То есть это должна быть материализация вашей мысли. Вот в чем суть. 

S02 [01:42:52]  : Спасибо. Виктор Казаринов спросил, что заметил, что прозрение, то, что вы рассказываете, это похоже на работы Хьюбилла, которые, он говорит, читал в 90-е годы. Соответственно, вопрос, вы как-то можете это прокомментировать? Вы с работами Хьюбилла не знакомы? 

S03 [01:43:11]  : Знаете, я очень много читаю. Но не запоминаю кто, поэтому я боюсь, что не смогу прокомментировать. Даже если я читал. Может и не читал, кстати говоря. Меня такого чужого озарения не использовал, короче говоря. Но это может быть всего не знание. 

S02 [01:43:30]  : Спасибо. По поводу корреляции причинно-следственных связей. Как вы их соотносите? Вопрос от Бориса Новикова. уже видимо дальше по поводу последовательности. 

S03 [01:43:44]  : Ну, не вполне понимаю вопроса корреляции. 

S02 [01:43:47]  : Ну, это как бы известная на самом деле проблема, что насколько корректно выводить причинно-следственные связи из корреляционных зависимостей. Вот, может быть, Борис учился пояснить свой вопрос, но на самом деле стандартная проблема и, соответственно, я это вижу как вот с точки зрения вашей работы, как вы их соотносите. 

S03 [01:44:09]  : ну смотрите ситуация какая вот когда вы говорите про корреляции ну да каждый раз там когда люди садятся в автобус да там у них становится меньше денег но у них меньше денег становится во время инфляции тоже Корреляции вне последовательности событий, они могут быть просто шумом. И проблема с последовательностями именно в том, чтобы за счет в целом повторяемости последовательностей, которые основаны на повторяемости встречаемости, вот чтобы вот эту именно последовательность на основе встречаемости Чтобы вот эту причинную связь, эти причинные связи, их последовательности выявить и о шум устранить. Поэтому это немножко другое. То есть, это не просто корреляции. Я просто хочу это подчеркнуть. То есть, корреляции и последовательности – это не одно и то же. 

S02 [01:45:16]  : Борис, пожалуйста. 

S00 [01:45:18]  : Вопрос такой. Вы говорили, что одно после другого, и это говорит о причинно-следственных связях и корреляции. Но обычно корреляция возникает, когда два следствия одной причины. Например, увеличение освещенности на улице Иро и повышение температуры. Связано это с восходом солнца. Но если мы скажем, что это причинно-следственная освещенность температура, то включив лампочку мы будем ждать повышения температуры. Как это отличить от того другого? 

S03 [01:46:01]  : Знаете, что я тут могу сказать? Наверное, это память отличит, а может нет. Не знаю. Я бы не брался сейчас судить о том, насколько хороша память последовательности и как она решает все проблемы. Потому что лучше решать проблемы по мере их поступления. Вот я вам рассказывал про насыщение весов. Дело в том, что я об этом думал и раньше, но я не думал, что мы столкнемся с этим так быстро. мы столкнулись быстро вот понимаете есть еще какие-то аспекты которые можно о чем можно говорить то есть мы которые можно предвидеть но много есть я уверен вопросов с которыми мы столкнемся когда начнем этим заниматься плотно 

S02 [01:46:54]  : Спасибо. Олег, вот как вы прокомментируете комментарий Юрия Бабурова, которому показалось, что вот то, что вы делаете по последовательностям, это просто наивный баяс для прогнозирования? 

S03 [01:47:11]  : Ну, не берусь судить. Наверное, виднее человеку, потому что он, наверное, знает, о чем говорит. Мне кажется, что... Нет, может быть, это и наивный бальз. Я... Как сказать-то здесь получше? В общем, не буду спорить. 

S02 [01:47:31]  : Может быть и так. У меня сразу вопрос в развитии этого. Я дальше спрашивал, но вот он связан. Вот есть гипотеза, что как раз сила вот этих вот всяких GPT-3, BERT и глубоких сетей в том, что они действительно за счет иерархии Они позволяют получить глубокую структуру информации, запомнить некоторым образом и прогнозировать ее более структурированно, чем это можно делать с помощью наивного байеса на простых сетях. А у вас, с другой стороны, в докладе была заявлена иерархическая память последовательности, насколько я помню. Вот, но когда вы рассказывали про вот эти вот кластеры прошлого и будущего и проекцию прошлого на будущее, иерархия не прозвучала. Соответственно, вот вопрос такой вот, где у вас берется иерархия, которая есть в глубоких сетях просто за счет того, что они глубокие. 

S03 [01:48:35]  : Ну, я вам говорил о синтетических объектах, которые формируют, собственно, иерархию. На каждом уровне синтетические объекты, когда они возникают у вас в виде идентификаторов контекста сложившегося, когда он начал меняться. Вы, собственно, получаете последовательность смены контекстов на вот этом уровне. Но сама эта последовательность смены, она тоже является последовательностью. То есть теперь уже новых объектов, которых не существовало, ну считайте их аббревиатурами выдуманными. И когда у вас их достаточно, они начинают повторяться, они начинают возникать и возникают множество последовательностей на этом уровне абстракции, возникают новые кластеры, которые будут порождать синтетические объекты следующего уровня иерархии и так далее. Сколько этих слоев может быть, я не знаю. То есть, это вопрос хороший, на который я точно не отвечу, потому что это практический вопрос очень. Этим нужно прямо вот заниматься и смотреть достаточно, недостаточно. 

S02 [01:49:41]  : Спасибо. Вопрос от Виктора Казариного. Как, по вашему мнению, среагирует ваша предсказательная система на таком тексте? Глокая куздра, штека ободланула бакра и крутячет бакрёнка. Как это будет работать в разрезе семантических ядер, контекстов, моделей мира в рамках вашей теории? 

S03 [01:50:02]  : Хороший вопрос в том, что я сам ничего не понял. У меня естественный интеллект, понимаете? Я не понял ничего. Поэтому, наверное, память последовательности должна отреагировать очень простым образом. Если она увидит повторяемость вот этой вот штуки, я повторить ее не могу, увидит, что слагаемые вот этого длинного текста, они повторяются и что у них есть встречаемость, кроме того, что вот в этой вот, я не знаю, что это пословица, не пословица, шутка какая-то. то она их запомнит. Нет, она их и так может запомнить, если ей там повторить. Один даже раз сказать, потому что этого достаточно будет. Просто введет новые объекты, скажет, что, блин, не знала, что… Как там первое слово? Вот не знала такого слова, запомню. И будет воспроизводить эту последовательность вам просто. Ну, вес будет небольшим, но будет последовательность. 

S02 [01:51:04]  : Вот замечательный вопрос от Николая Рабческого как раз к вопросу о подкреплении. Какое подкрепление считать отложенным, какое нет. А вопрос такой. Чем практически определяется длина контекста? 

S03 [01:51:19]  : Практически ничем не определяется, кроме того, вес увеличивался, увеличивался, потом падать начал. Вот, собственно, здесь граница контекста. Других границ нет. То есть, невозможно сказать. Вот если мы будем читать о молоке и… Я не знаю, час, может и час контекст не меняться. Наверное. Это вопрос экспериментов. Не знаю, не могу вам ответить. Твердо не могу, потому что нужно экспериментировать с этим. Николай, вопрос хороший, но давайте поэкспериментируем с вами. Вот я смотрю, вы сейчас сидите спокойно, ничем не заняты, давайте экспериментировать. 

S02 [01:52:04]  : Спасибо. От Евгения Бабарыкина есть комментарий, который я не очень понимаю, но, может быть, Вы поймёте. Евгений пишет, что я так понимаю, что последовательность здесь используется в рамках анализа бесконечно малых. 

S03 [01:52:24]  : Тоже не очень понимаю. Бесконечно малых – это математический термин, величин. А я говорю про объекты. Я не знаю, как здесь связь. Слушайте, бесконечно малых… К математике можно все свести. Я могу сказать, что можно придумать сетку очень простую, допустим, для измерения производных любого уровня. Она очень простая. Не знаю, это Маугли, не Маугли. Что это? 

S00 [01:52:55]  : Не знаю. 

S02 [01:52:56]  : Спасибо. Вопрос от Эдуарда Менталоджи. Не знаю, насколько это имеет отношение к докладу. Видимо, вопрос глобальный. Как вы отличаете данные от знаний? 

S03 [01:53:13]  : С точки зрения памяти последовательности никак вообще. Там просто есть объекты. Объекты откуда берутся? Вот это был бы хороший вопрос. Откуда берутся объекты? В случае зрения мы там распознали что-то. Это не все объекты, которые нас интересуют, безусловно, но какие-то. Если мы говорим об обонянии плодовой мушки, которую обсуждали не так давно, если помните, неделю-полторы назад. Там есть вектора какие-то, они тоже объекты. То, что они векторами являются, не мешает нам представить их одной шиной всего. Каждый объект одной шиной. Вот вам объекты получились. Вот примерно так. 

S02 [01:54:05]  : Спасибо. Практический вопрос от Евгения Бабарыкина. Каково практическое применение вашей конструкции? Можете ли привести практический пример, где это можно использовать? 

S03 [01:54:22]  : Самое простое, вот и могу вам сказать следующее. Ну, допустим, можно сделать voice-to-text и text-to-voice, допустим. Как сделать? Можно сделать простой достаточно, с моей точки зрения, по меньшей мере потому, что я для себя думаю, можно сделать преобразователь голоса в слова. Загнать в память последовательности собственно язык, корпус достаточно большой, чтобы она правильно угадывала следующие слова и могла ошибки ввода с голоса отлавливать. Затем можно сделать в обратную сторону. Поскольку это двухмодальная система, через слой измерений можно связать эти две модальности и в обратную сторону все это выдавать в голосе. Но там синтезатор просто понадобится. Вот такая система. Что еще? Память последовательности сама по себе, она очень интересна в аппаратной реализации, в аналоговой, как я предлагаю, потому что она за один такт создает кластеры, трудоемость которых, вычислительная сложность, весьма высокая для обычных компьютеров, то есть она очень высокая. Вот эта штука, она должна быть супер эффективной по энергетике на операцию классических компьютерных процессоров CPU и GPU, я тоже думаю. Так что у неё очень высокая эффективность воспоминания вещей, и она учится одновременно с вводом и воспоминанием. Ну вот как-то так. Приложений много. Сразу у меня мыслей-то много было. а вот прям вот спрашивают и не могу ответить сразу не нахожусь но одного примера наверное достаточно по поводу зрения но ребята хотели сделать могу вам сказать что они хотели сделать Систему, которая будет параллельно смотреть котировки, рынка и новости. И хотели корреляцию там. Создать вот такую корреляционную машинку и посмотреть, как это будет работать. Применения могут быть самые разные. Можно делать, допустим, переводчик. Так же, как на трансформерах. Только здесь нужно будет придумать звено, которое будет связывать параллельные тексты. Я не думаю, что это суперсложно. Собственно, там в девяностых, в середине, мы с Ашманом, собственно, занимались в основном лингвистическими приложениями. Там словари и, в частности, переводы. И, в общем, тематика-то знакомая. Хоть я не люблю ее с тех пор. 

S02 [01:57:29]  : У меня тут маленький комментарий, что и в задаче speech-to-text, и в задаче перевода, и в задаче численных последовательностей и текстовых последовательностей везде возникает необходимость не только памяти последовательности, но и пресловутой связки между различными модальностями, которая, по-моему, у вас не очень сейчас проработана. 

S03 [01:57:56]  : Вы знаете, утверждать, что она хорошо или плохо проработана, я не могу. С моей точки зрения, она идеологически очень правильная. Сказать, что она практически без изменений может быть реализована, я не могу, потому что это можно узнать только на практике. Кстати, еще один вопрос забавный. Все мы знаем, что квантовые компьютеры видятся как средства взламывания асимметричной криптографии. Ну, допустим. Что такое асимметричная криптография? Это ключи, пары ключей. По сути, это последовательность, потому что там простые числа и так далее. И это закономерная последовательность. Понимаете, к чему я клоню? Может быть, и эта штука сработает, я не знаю. 

S02 [01:58:43]  : Спасибо. Вопрос по поводу переобучения, про которое вы рассказали, что наступила у модели мудрость, когда такая мудрость, что она перестала что-то запоминать. И вы говорили о том, что нужно правильно подбирать параметры, чтобы эта «мудрость» не наступала слишком быстро. Здесь нет ли такой проблемы, что проблема подбора параметров возникает в отдельный слой, то есть как в нейронных сетях нам нужно долго экспериментировать и подбирать параметры, причем для одной задачи эти параметры будут одни, на одной тоже архитектуре, на другой другие, так и в вашем случае будет такая же ситуация с подбором параметров, которую нужно выносить тоже в отдельный слой. 

S03 [01:59:30]  : Смотрите какая штука. Я во многих случаях, к сожалению, вынужден был отвечать типовым способом, что это нужно на практике смотреть, потому что из того, что я понимаю об этой штуке, она существенно проще, предсказуемее и понятнее нейросетей. То есть она абсолютно понятна. Вот если в нейросетях вам приходится изобретательством совсем, ну, знаете, слово-то даже не подберешь, что это за изобретательство, да? Потому что, ну, это в лесу, в котором и направление непонятное, вот там каким-то образом по моху, значит, определяют, где север. Здесь все понятно. Понятно, что они насыщатся, допустим, будут веса. Понятно, там ещё какие-то эффекты. Они понятны, потому что вы понимаете, как эта штука работает, и у неё есть описание. У неё подробнейшая инструкция. Неизвестных тут нет вообще. Связывать с сетками можно по разным причинам. Может быть, да. Может быть, какие-то параметры можно. А может быть, и сама память последовательности может быть использована в качестве Собственно, карты признаков, почему нет? То есть, это тоже возможно. Я не думаю, что нужно, допустим, забывать нейросетки, которые уже хорошо работают. Просто потому, что ты занимаешься чем-то еще, памятью последовательности или чем-то еще, неважно чем. Если у нас есть инструмент, который надежно работает, лучше укоротить дорогу к цели. И использовать другие инструменты. Но, повторюсь, у памяти последовательности есть очень большие преимущества. Понятность, интерпретируемость, энергоэффективность, причем суперэнергоэффективность, она на порядке лучше должна быть, чем известные способы. И вот это все нужно просто брать и делать. Моя точка зрения. И все вопросы там снимутся. 

S02 [02:01:30]  : Спасибо. Вот здесь есть вопрос у Виктора Казариного. Я, честно говоря, не очень его понял. Может быть, Виктор пояснит. Значит, про модель мира. То есть, вот что-то прозвучало про модель мира. Вот. И как вот модель... В общем, Виктор, если вы хотите, чтобы у вас вопрос прозвучал, можете его сформулировать? Ну что, я, честно говоря, не понял, что здесь написано. 

S01 [02:02:00]  : Здравствуйте. Я могу говорить? 

S02 [02:02:02]  : Да-да-да, я слышу. Слышно вас. 

S01 [02:02:04]  : Так. Ну, сейчас я камеру включу, если кому-то я интересен. Здравствуйте все. 

S00 [02:02:12]  : Здравствуйте. 

S01 [02:02:16]  : Модель мира в том плане, что у вас в основном фигурируют признаки. Вот вы работаете с приличными признаками слова, пятачки, пятна на изображении и так далее. И в итоге вы со словами манипулируете. И вот к этой глокой куздре тоже возвращаюсь. Там смысл в чём? Что человек, зная модель, зная больше, чем вот в этой непосредственной фразе находится, может, основываясь на своём опыте, сказать, что куздра — это нектое существо женского рода, что бакрёнок — это ребёнок, что куздр бодлануло. Бокр — это кто-то такой. Таким образом, не зная слов, первый раз слыша и видя что-то, если вы еще раз несколько раз перечитаете фразу, вы поймете примерно, что нечто кого-то, что-то с кем-то сделало. Понимаете, эта модель мира у человека формируется из другого опыта. И поэтому Когда вы говорите, что ГПТ-3 и так далее, они все работают в мире слов, в мире языка, то, в принципе, вы все, что демонстрировали в последовательных слов, вы тоже в этой же самой модели языка работали. А модель мира, которая стоит далеко за этим, это тот наш мир, где слуховые образы и зрительные, и 3D-объемные, объединяются в нечто более цельное, вот это более общее цельное. Ваша память последовательностей может до такой иерархии дорасти, чтобы именно вот этот более глубокий смысл понимать, то есть представлять его, получив только небольшие фрагменты, признаки. Понимаете, о чём я говорю? 

S03 [02:04:16]  : Ну, слушайте, мозг ваш, для сведения все-таки, для нашего с вами, чтобы мы на одной волне рассуждали, мозг ваш на самом деле, он получает просто сигналы некоторые от периферии и посылает сигналы. Если бы он не знал, что есть глаза, то он бы и не знал, что есть глаза. 

S01 [02:04:37]  : Я не спорю с этим, все совершенно так. 

S03 [02:04:40]  : Позвольте, я же выслушал вас, не перебивал, верно? Так вот, во-первых, я в самом начале сказал, что если мы будем изучать только слова, мы изучим ничто иное, как правила языка. И тут вы правы абсолютно. Я это с самого начала сказал. Это и будет GPT-3, только будет очень дешево стоить. Вот и все. Но будет GPT-3 тоже самое, понимаете? Если вы хотите создать естественный интеллект, то я вам встречный вопрос задам. Вот вы вот эту странную штуку произнесли, которую повторить невозможно. Представьте себе, если вам сейчас дать запись на фарси, которую, я предполагаю, вы не знаете. 

S01 [02:05:25]  : Минутку. Я говорил слова на русском языке. То есть язык известен, но неизвестные объекты. Вот в чем разница. А вы говорите о другом языке. Это разные вещи. Хорошо. 

S03 [02:05:38]  : Порить не буду. Ладно. Я предполагаю, что если интегрировать так называемую мультимодальность, в моей трактовке это многопоточная память последовательности, она будет знать о мире существенно больше. Она будет знать не только со слов, потому что в словах порядок событий изменен. Если она будет знать реальный порядок событий, то потом в описании этих событий она их будет узнавать. Если она поймет, что маленькие люди маленького размера называются ребёнок или чертёнок, как там у вас там слово-то какое-то, онок, и что оказывается… Мокрёнок. Мокрёнок, что онок – это оказывается признак маленького человечка. то она, скорее всего, будет вот в таких категориях и думать. Скорее всего. Ну, вот если я ответил на вопрос, то вот так. 

S02 [02:06:36]  : Спасибо. Вопрос… А можно еще один вопрос? 

S01 [02:06:45]  : Вот я, к сожалению, не все внимательно слушал, но, может быть, и другие также. Что из того, что у вас в памяти последовательность запоминается, можно ли сказать, что вот это новизна, а вот это закономерность? То есть две на чаши весов. Механизм ваш это все позволяет или нет? Просто, может быть, я упустил. Или оно идёт в статистику, там берёт, запоминает последовательность. 

S03 [02:07:14]  : Нет, просто понимаете, последовательность, память последовательности оперирует объектами. Объекты кодируются шинами. То есть, если вот шина такая уже закодирована под объектом, значит он существует, уже существует. Если его нет, то нет. Новизна, она скорее в применении этих объектов. Условно говоря, вы подумали, что это объект, И закодировали шину этого объекта. Но потом выясняется, что он ни с чем не встречается. Скорее всего, это не был объект, а шум. Вот он не встречается больше. Вот и все. 

S01 [02:07:47]  : А уникальный объект может тоже не встречаться? 

S03 [02:07:50]  : Ну ладно, я извиняюсь. Хорошее замечание, я это именно и сказал, что вот уникальный, там все шины принадлежат уникальным объектам, то есть это уникальный объект. Вот вы назвали слова, которые на самом деле не существуют. Вот это типичные уникальные объекты, которые по-другому и не используются, кроме как вашей вот этой скороговорки. 

S01 [02:08:10]  : И вот вашей памяти последовательности, я злоупотребляю, наверное, вопросами, но вот еще вопрос. Она вот компрессирует, удаляет вот это вот повторяемое. Причем я говорю про закономерность. Если их два или три или четыре одинаковых последовательности фрагмента встретилось, то было бы логично убрать и заменить ссылку на один такой элемент. 

S03 [02:08:38]  : Виктор, там нет ссылок, вот в чем дело, там просто запоминается встречаемость. 

S01 [02:08:43]  : Вот-вот-вот, а я как раз почему, я работаю над такой аналогичной системой, у меня есть примеры, когда она работает с новизной и закономерностью и строит террористическую сеть. Я ее называл своим, так сказать, самопально-мегатронная сеть, но о ней никогда не распространялся. 

S03 [02:09:03]  : Да, у многих есть модели какие-то. Я докладываю свою. 

S01 [02:09:08]  : У меня память, последовательность то же самое. Поэтому я очень сравниваю со своим подходом и ваш. 

S03 [02:09:17]  : Еще раз я повторю, что именно я предложил. вы вводите некую последовательность, вы предполагаете, что это последовательность, вы ее вводите, а машина просто смотрит, как часто повторяются связи этих слов дальше. То есть, если будут вводить дальше последовательность, эти слова будут возникать и в связи с этими же словами, то вес будет увеличиваться, будет реже встречаться. или не будет встречаться, точнее, он останется таким же, а другие веса будут расти, таким образом выиграют те, которые чаще встречаются, вот и все. То есть вы просто статистику получаете, она не выдумана нейронной сетью из носа, не вытащена. 

S01 [02:10:00]  : Примерно так же, я понимаю вас. 

S03 [02:10:03]  : А она – это статистика встречаемости. Ей можно доверять точно. Более того, я вам говорил про состояние сознания. Состояние сознания не зря так назвал, потому что представьте себе, мы выучили машину нормальной идеологии некой. Когда она ничего вредного не делает. Потом вдруг ей начинают скармливать фашистскую идеологию. Мы же начинаем ей скармливать. Смотрим сигнатуру, как меняются веса. Вот сняли эту разницу изменения весов. Потом по этой сигнатуре смотрим, ее вот начали видеть, что начинает она заваливаться в сторону насилия. Мы берем, обнуляем эти веса, возвращаем их в состояние сознания, которое нам требовалось. Вот все эти так называемые законы кибернетики или робототехники, там Айзек Азимов, да любые другие. которые мы захотим внедрить, они могут быть внедрены на уровне вот карты сознания, о которой я сказал. То есть вам не нужно для этого догадки строить, как вот сейчас там строят. Откуда, почему вот этот вес такой? Бог его знает. Слушай, это же градиентный спуск. Такой. 

S01 [02:11:17]  : На самом деле, ваши последовательности тоже при большом числе. Чисто технически будет очень сложно отследить все это. Последовательности будут большими и широкими, раз у вас параллельно еще идут каналы. Физически это будет, наверное, невозможно. 

S03 [02:11:33]  : Виктор, нет, не будет. Более того, еще вот многопоточная память, она хороша тем, что у вас может быть в одной и той же памяти, из-за того, что там многие потоки, у вас могут быть разные языки. У вас могут быть одновременно разные языки, но поскольку языки объясняют одну и ту же реальность, то при этом каждый язык может быть связан с изображениями кошки, с видео, которое мы снимаем и видим, что откуда звучит вот этот голос мяукания там и так далее. То есть, на самом деле, это мультимодальная машинка, там можно построить весь мир. Более того, там интересно ведь что? Представьте себе, что существуют корреляции, условно говоря, между напряженностью магнитного поля Земли и чем-то еще, и уровнем воды в реках, к примеру. Мы же это не можем отследить. 

S01 [02:12:23]  : У меня только последний вопрос. Судя по всему, получается, как бы… Алло. 

S02 [02:12:35]  : Так, Вселенная промодерировала Виктора. Ладно, пока Вселенная держит Виктора на паузе, значит, здесь ещё несколько вопросов у нас осталось. Значит, от Евгения Бабарыкина вопрос, значит, вы можете, чтобы было понятно, связать логику, значит, того, что вы говорили про зрение и память последовательности? Как-то вот, хотя оно все-таки немножечко, с моей точки зрения, тоже, так сказать... Это независимо получилось. 

S03 [02:13:05]  : Я, на самом деле, отчасти воспользовался случаем, чтобы обе темы рассказать. Но здесь связь есть, безусловно. Она заключается в следующем. Память последовательности. Когда я рассказываю, в частности, людям, она воспринимается именно как память-последовательность, несмотря на то, что на самом деле это штука, которая ансамбли объектов принимает на вход, и поэтому интерпретировать, вообще говоря, может что угодно, связь. Словно говоря, если вы в качестве ансамблей используете английские предложения и русские, В английских порядок слов определен правилами, а в русском в основном нет. Существенно ниже эти требования. Соответственно, русский язык будет выглядеть больше как ансамбль слов, понимаете? А английский как более упорядоченное множество слов. Тем не менее, эта штука может работать и с тем, и с другим. Вот, собственно, может быть пересечение вот здесь. Точнее, то, что я хотел, чтобы было ясно. Вот, собственно, что это ансамбли или последовательность, и что это, по сути, для памяти последовательность одно и то же. То есть, она может работать с любыми сущностями, с любыми объектами. Вот к этому вел. Но, может быть, немножечко искусственная связь, конечно, зрения с памятью последовательности, спорить не стану, признаю. Меня поймали. 

S02 [02:14:39]  : Спасибо. Вопрос от Бориса Новикова про реализованные практические или экспериментальные приложения. Так понимаю, их нету. Вы уже ответили. Ну и вопрос про тексты про молоко и нефть. Если эти тексты пойдут в случайном порядке, то будет ли ваш подход работать? 

S03 [02:14:58]  : Ну, случайно, если вы бессмысленный текст напишите, где будете говорить примерно следующее, что я сделал сливочное масло из нефти, ну, после этого машина это просто проглотит, потом поймет, что вес такой встречаемости ноль и не будет использовать вот еще. 

S00 [02:15:19]  : Борис, пояснись, пожалуйста. Первый текст был про молоко, второй про молоко. третий, четвертый и пятый про нефть, шестой про молоко, седьмой про нефть и так далее. Есть два множества текстов. Одно множество про молоко, другое про нефть. И в случайном порядке они используются. Сработает или не сработает. 

S03 [02:15:45]  : Она будет узнавать тематики, да и все. Потому что если она одну тематику уже узнала и составила вот этот синтетический объект узнаваемости. 

S00 [02:15:55]  : Она же его узнавала, когда падал контекст, вершина контекста падала, а тут она будет скатать по такой ломаной 

S03 [02:16:08]  : Нет, тут дело не в этом. Когда она запоминает этот контекст, она по сути что сделала? Если это было короткое сообщение и она его запомнила, то у нее просто семантический вот этот образ, то есть кластер семантических объектов, семантического ядра, он будет урезан. Вот в чем дело. Когда вы более длинный текст дадите, она его просто вынуждена будет дополнять, и перезаписывать прежний объект должна будет, или делать новый объект с большим контекстом, под множеством которого будет предыдущий контекст. Почему контексты важны? Их на самом деле вырезать нельзя, потому что вот этот вот контекст, который являлся под множеством, На самом деле он связан с той последовательностью, которая его порождала. И в этом смысле его полезно, может быть, сохранять, потому что когда вы полный, условно, кластер даете на вход, точнее, пытаетесь полным кластером семантического ядра восстановить все последовательности, которые к этому относятся, это будет очень полезно. То есть вы восстановите вот такие все контексты. 

S00 [02:17:12]  : Но вы же объясняли, что контекст выделяется автоматически. Там, где нарушается монотонность некоторой функции. Где был рост, потом резкий скачок вниз. И вот там выделяется контекст. А если нет монотонности, вот случайный порядок текста, как вы контекст выделяете автоматически? 

S03 [02:17:36]  : ну смотрите на какой длине возникает контекст это вопрос который бы следовало скорее вот которым надо было бы заменить ваш вопрос я не знаю дело в том что вот если смотреть на наш с вами общение да что ведь получается вот мы с вами допустим идем там можем разговаривать и вот обменялись фразами может быть зайдем в это кафе давай зайдем в кафе после этого молчания 10 минут ты что будешь там пирожное эти вот в частности паузы прерывание они формируют в частности представление о том что происходит и могут являться сменой контекста и восприниматься как смена контекста вот смотрите Саккады – это перевод глаз на новое место внимания, которое нужно изучить. По сути, между ними, между вот этими саккадами, мы имеем паузу. Что такое пауза или прерывание, я это называю. Это типичная штука, как из известного изречения, казнить нельзя помиловать. Вот если мы ставим паузу, казнить нельзя помиловать, это одна история. А если казнить нельзя помиловать, это другая история. Почему я это говорю? Потому что знаки припинания на самом деле это прерывание речи. Потому что речь возникла раньше, чем письменность. И вот чтобы отразить те паузы, которые мы делаем, а в них глубокий смысл есть, прерывание контекста в частности, мы создаем знаки припинания. И их используем. 

S00 [02:19:11]  : Как вы определите, что молоко и сыр это один контекст? А нефть и бензин – это другой контекст, а не четыре контекста. 

S03 [02:19:20]  : Я не буду этого делать из принципиальных соображений. Я против эвристик. Я посмотрю, как это сделает память последовательности. И если я пойму, что она что-то там делает не то, надо будет посмотреть на конструкцию этой истории и внести изменения не эвристические, а архитектурные. 

S00 [02:19:41]  : Тогда вам обучение надо давать на структурированном массиве текста. 

S03 [02:19:48]  : Нет, не надо. Это принципиально нельзя делать. 

S00 [02:19:52]  : Тогда у вас ничего не получится, так как вы объясняли. 

S03 [02:19:55]  : Получится. Может быть я объяснял. 

S00 [02:19:59]  : Когда предъявите практическую реализацию, тогда будет видно. Если нет, то это сомнительно. 

S03 [02:20:05]  : Ну вы знаете, я вот поисковыми технологиями так случилось в жизни, нам с 95 года знаком глубоко очень. Получится, как я говорю. 

S00 [02:20:16]  : Хорошо. Желаю успехов. Спасибо. 

S02 [02:20:20]  : Да, ну вот здесь вот как раз есть комментарий от Евгения Барыкина, то что надо кодить эту память последовательности и на практике все проверять. Все интересно, но надо все это проверять. Да, значит, и все тут с этим соглашаются дальше, но вот один вопрос, значит, наверное, уже последний от Евгения Бабарыкина еще по поводу забывания. А вот как ваша система будет что-нибудь забывать, если оно не нужно или оно входит в противоречие с какой-то новой информацией, например? 

S03 [02:20:50]  : Ну, это же технический вопрос. Там условно говоря, каким образом мы можем снижать веса определенные. Допустим, у нас есть сигнатура весов, которые не должны расти или которые не должны уменьшаться. ниже там определенного порога. Ну это вопрос контроля элементов микросхемы. Не знаю как это будет конкретно сделано, но это сделать можно. Это вопрос технический. 

S02 [02:21:18]  : Ну и тогда уж контрольный вопрос от Дмитрия Салихова. Какой видимый результат вы планируете показать вашей архитектурой в ближайшее время? Типа что будет показывать демонстрационная версия? 

S03 [02:21:31]  : Ну знаете Позволите я вам просто историю немножко расскажу. У меня товарищ есть, я занимался Embedded Systems довольно долго. Единственный отечественный компилятор Frontend, полностью выполненный российскими. Вот это компания моя делала его. И разговаривал с одним из партнеров как раз по поводу памяти последовательности. Он вспомнил такой случай своей жизни. Как-то их профессор в институте собрал и дал им задание. Это было очень давно. И дал им задание такое. Представьте себе, что вы можете на подложке разместить миллион, целый миллион любых транзисторов, любых, какие хотите. Вот только представьте. Вот вы придете на следующее занятие и предложите, для чего бы это было нужно. Вот он пошел домой, долго не спал, а потом решил, что зачем это нужно, ведь все задачи и так решаются. Еще один пример. А чем будет заниматься AGI? Чем он будет заниматься? Наверное, тем же, чем занимаетесь вы. А чем именно? А чем вы занимаетесь? Не знаю, чем я занимаюсь. Поэтому, что будет делать память последовательности – это хороший вопрос. Когда делают новые нейроморфные процессоры, их просто предлагают, допустим, в облако ставят, условно говоря, и говорят, ребята, тренируйтесь. Потом смотрят киллер-аппликейшнс и делают специализацию под эти киллер-аппликейшнс. Вот такой путь может быть. Нейропроцессоры, нейроморфные процессоры, или вообще специализированные большие процессоры, какие-то очень сложные. А этот процессор можно сделать на большой подложке, на полной подложке. Видели, кстати, чипы вот такого размера есть? Вот можно такой чип сделать, потому что чем больше шин, тем он мощнее, тем больше объектов он может держать. Соответственно, чем больше чип физически, тем лучше будет. Вот такой чип может стоить миллионы долларов просто для экспериментов. Что сначала сделать? Я бы сначала, на самом деле, построил модельку. И, собственно, мы этим в физтехе занимались. Хотелось бы найти единомышленников, с которыми можно построить рабочую модель, которую дальше уже раскручивать уже коммерческим способом. И деньги можно найти. Это не технология известная. На чем можно делать? Можно сначала сделать модельку, которая анализирует входящие потоки информации. и показывает тебе, что может построить фразу и угадать следующие слова с высокой вероятностью, близкой к 100%, что мы и показывали не так давно в физтехе. Что это дает? Если у вас есть такая машинка, которая работает как GPT-3, но занимает не комнаты и не здания, а занимает один компьютер, наверное, эта штука мощная, правда? Наверное, на неё следует обратить внимание. Переводы. Вот если поработать над переводами, я думаю, что там понятно, там два языка, и надо их просто научиться параллельные тексты друг другу в соответствии ставить. Может даже с использованием какой-то сетки промежуточной. Может, не знаю. Тоже замечательное применение, потому что каждому нужен переводчик локальный. По поводу Text-to-Speech, Speech-to-Text, если вы сделаете чип, а я думаю, что как раз маленький чип можно сделать, он может быть локально размещен везде, понимаете, штука какая. Как только вы создали память вот этих фонем, Как там они называются? Господи, тоже выпало из головы. И языка, и уметь одно в другое превращать, и это можно локально размещать в телефоне, в часах, еще где-то. Слушайте, это большое дело вообще-то. Это огромнейшие рынки. То есть, вот любое такое маленькое применение, оно позволяет сделать миллиардные компании. 

S02 [02:26:00]  : Спасибо. Ну и последний вопрос. На ваш призыв к сотрудничеству уже пошли организационные вопросы. Например, ваше отношение к опенсорсу, Евгений Бабарыкин спрашивает. 

S03 [02:26:13]  : Да, Open Source я поддерживаю, это хорошая штука, но вопрос-то другой. Вы поймите, что любой Open Source – это бизнес-модель, несмотря на то, что это Open Source. GitHub, купленную Microsoft, как вы думаете, почему? Почему они после этого сделали машину, нейросеть, которая учится делать программы? Везде есть коммерция, ее там, может быть, не видно для начала. Вот, к сожалению, мы живем в таком мире, где деньги существуют, поэтому нужно с этим считаться. 

S02 [02:26:46]  : Кстати, насчет вашего примера про GitHub и Microsoft, он на самом деле очень жил, потому что Дмитрий Салихов по поводу прототипа, то есть примера или демы, которую будет делать ваша программа, ответил, что вот то, что он сам делает, оно предназначено для программирования. То есть, тот искусственный интеллект, который он делает, он по замыслу будет программировать. То есть, видимо, Microsoft как раз именно для этого же купил GitHub, чтобы автоматически писать программы на основе той базы данных, которую он получил в свое распоряжение. 

S03 [02:27:24]  : еще учтите что open-source там ведь разные лицензии есть open-source то есть open-source он до того open пока вы коммерчески не используйте как правило как только вы хотите сделать на open-source какое-то бабло извините да тут же вы сталкиваетесь с тем что open-source он не open Вы должны заплатить лицензионный сбот. Хорошо в этом плане то, что я занимаюсь защитой интеллектуальной собственности очень давно, очень много лет. Вот первый патент по этой теме, на самом деле, датирован не 16-м годом, а 11-м. В частности, американский патент есть на эту тему и российский. Поэтому, понимаете, когда у вас есть патенты, это не к тому, что отдать их троллям, чтобы они деньги вышибали из ответчиков. Хотя это тоже неплохой способ заработать. Почему нет? Все хороши. Нет. А к тому, что если вы делать начнете сами, то у вас есть защита. И вот такая защита для тех, с кем я буду работать, Есть, потому что я об этом вовремя позаботился. Поэтому, милости прошу, не воспринимайте это, пожалуйста, как угроза собственной безопасности, а как защита наших общих интересов, если мы работаем. Вот о чем идет речь. 

S02 [02:28:45]  : Хорошо. Олег, большое спасибо на этой высокой ноте. Давайте поговорим до кладчика и до новых встреч. Да, спасибо вам огромное. Спасибо. 

S00 [02:28:57]  : Всем до свидания. 









https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
