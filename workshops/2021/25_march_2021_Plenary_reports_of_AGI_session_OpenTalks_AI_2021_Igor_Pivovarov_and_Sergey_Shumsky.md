## 25 марта 2021 - Пленарные доклады AGI-сессии OpenTalks.AI 2021 — Игорь Пивоваров и Сергей Шумский — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/MheTFaS62Qc/hqdefault.jpg)](https://youtu.be/MheTFaS62Qc)

Суммаризация семинара:

Семинар, о котором идет речь, посвящен обсуждению кооперации в рамках сообщества, занимающегося искусственным интеллектом. Основной темой семинара стали вопросы кооперации, которые были подняты в ходе предыдущих обсуждений. Участники семинара выделили важность координации усилий и предложили организовать отдельный семинарчик, посвященный именно этим вопросам.

Ключевые моменты семинара:

1. Кооперация и координация усилий: Участники семинара подчеркивают необходимость координации усилий для достижения общих целей. Это связано с тем, что кооперация не должна оставаться в воздухе, и для этого необходимо четкое планирование и структурирование обсуждений.

2. Серия семинаров по архитектуре AGI: В рамках семинара обсуждается идея проведения серии семинаров, посвященных архитектуре AGI. Эти семинары будут включать в себя обсуждение собственных работ участников, а также представление интересных архитектур, разработанных другими командами.

3. Структура семинарчика: Предлагается разработать структуру для семинарчика, который будет посвящен определению структуры и вопросов, которые нужно рассмотреть при изучении различных моделей. Это позволит создать стройное изложение ряда моделей в одной структуре.

4. Взаимодействие и открытость: В ходе обсуждений поднимается вопрос о недостатке открытости и необходимости более практического взаимодействия. Участники семинара отмечают, что иногда сложно описать модели и архитектурные механизмы, что может быть связано с отсутствием готовности к открытому обсуждению.

5. Обзорные доклады и направления развития: В семинаре обсуждаются обзорные доклады, которые представляют собой взгляды на текущее состояние и будущие направления развития в области искусственного интеллекта. Участники подчеркивают важность личного опыта и перспективы докладчика.

6. Механизмы разрешения неопределенности: В рамках семинара также поднимается вопрос о механизмах, которые позволяют мозгу разрешать неопределенность, например, в ситуациях, когда один и тот же сигнал может привести к положительной или отрицательной награде.

Итогом семинара является понимание того, что кооперация требует не только мотивации, но и четкого планирования и структурирования обсуждений. Участники выразили желание продолжить обсуждение в рамках отдельного семинара и начать серию докладов и семинаров по архитектуре AGI.




S05 [00:00:01]  : Да, коллеги, добрый день. Сегодня Игорь Пивоваров представит первый и второй доклад из трилогии с пленарной сессии по AGI конференции OpenTalks AI 2021. Первый доклад его собственный, второй Сергея Шумского, а третьим докладом был доклад Константина Анохина, который уже, наверное, все посмотрели. Он выложен у нас в группе. Игорь, пожалуйста. 

S02 [00:00:26]  : Да, коллеги, всем добрый вечер. Да, у нас сегодня такие два доклада. Мы считаем, что все эти три доклада были подобраны вместе. Они и правда были как такой своеобразный сериал, поэтому мы и хотели их посмотреть вместе. Предложение такое, мы сейчас включим один доклад, после него можно будет поотвечать на вопросы и потом второй доклад Сергея Шумского. И тоже можно будет уже поотвечать на вопросы на общие и сделать дискуссию. Мы примерно 45 минут и Сергей еще где-то 35 минут, наверное, так что вот мы, наверное, сделаем 45 минут доклад, потом там минут 10, максимум 15 на вопросы, потом второй и снова вопросы. И вопросы, обсуждения, комментарии, что там будет. И, соответственно, дальше. и мы пытались сделать чтобы можно было показывать видео прям потоком zoom но это не получилось поэтому буду показывать со своего компьютера надеюсь что надеюсь что это будет еще все себя выключу чтобы максимально разгрузить поток надеюсь что будет нормально видно все так ну что же давайте поедем значит и я у себя звук отключу антон ты там показывай если будет там если если что-то будет плохо видными там будем оперативно пытаться подправить так ладно хорошо 

S05 [00:02:24]  : Так, Игорь, нету звука. 

S02 [00:02:29]  : Так, я у себя выключил микрофон, а звук значит пропал, ясно. Хорошо. 

S05 [00:02:34]  : Ты можешь не выключать микрофон, будем надеяться, что кот не будет мяукать. 

S02 [00:02:38]  : Ну да, да. Хорошо, давай попробуем. Давайте мы начнем последнюю сессию последнюю официальную сессию этой конференции. Не, пока за остатку еще, еще на минутку. Я еще пару слов скажу на эту тему. Эта сессия для меня лично особенно важна. на этой конференции. Собственно, весь сегодняшний день, как для меня лично, очень важен, потому как сегодняшняя сессия пленарная большая по reinforcement learning и по AGI. Мне кажется, это те области, где у России есть сейчас реальные возможности чего-то добиться. Мы, собственно, про это напишем в нашем новом «Альманахе». Я про это и раньше говорил. Я думаю, что что касается Computer Vision, NLP, у нас слишком много ограничений, я сейчас не буду про это говорить, чтобы занять какую-то существенную позицию в мире с точки зрения научной школы. Но что касается Reinforcement Learning и AGI, у нас есть для этого все. И эта позиция пока достаточно, можно сказать, почти пустая. и сессию по и джай мы сделали сегодня как бы специальную особенную вы все понимаете что вы джай нету работающих готовых моделей ни у кого иначе бы мы про них показывали. Есть только подходы но мы думаем что есть новый путь или серия новых путей если хотите и вот эта сессия вечерняя и все три человека которые на ней будут выступать Мы думаем, что есть место для новых моделей, про которые мы хотим вам рассказать и дать возможность их увидеть, и, может быть, кто-то из вас будет их делать вместе с нами. Давайте начинать. Первый, собственно, первый доклад мой же. Я буду сам себе моделировать, что плохо, но у меня есть хронометрист. Доклад у нас обзорный, что происходит и куда идти. Конечно, он обзорный с точки зрения личной перспективы, как и любой обзор. Я скажу пару слов о себе, чтобы просто вы представили мою позицию, с которой я говорю. Я физик-теоретик по образованию и закончил МГУ у Миломаносова. Квантовая теория поля, биофизики, матмоделированные клетки. Я программист всю жизнь, самоучка и высшая школа МГУ. И пишу на кучу языков, но в последнее время на нескольких основных. Свои нейросети я программировал в 1995 году первые. Я менеджер в бизнесе на разных позициях, я много лет предприниматель, у меня много стартапов, я поднял много денег. Это как бы к тому, что я с разных сторон, может быть, немножко смотрю. Но сознание, тема сознания, мозг меня увлекали с детства. И я сделаю пару-тройку дисклеймеров для этого доклада. Первое. Слова не точны. Это то, что я ясно понимаю за свою недолгую жизнь. И если вы представите себе определение неких идеальных понятий, вот как они здесь нарисованы, видите, в форме некоторых точек таких, то для одного человека он эти слова как бы Он эти понятия в одни слова упаковывает, если хотите, а другой человек в другие слова. И есть разница смыслов в этих больших коробках слов. Поэтому слова, когда я буду рассказывать про некоторые вещи, не относитесь строго к словам, старайтесь услышать что-то внутри этих слов. К любому определению можно придраться, к любым словам можно придраться. Я сейчас не про слова и не претендую на точное определение. Второй дисклеймер. Цифры небоказательны. У меня в докладе не будет там потрясающих воображений цифр, триллион параметров, столько-то дней. Я считаю, что все это неважно. Важны некоторые сутевые моменты. Я постараюсь про это поговорить. И третий дисклеймер. Авторы незабытые. Я прекрасно помню множество людей, которые на которых я буду опираться в своем, в своем рассказе, но я не буду приводить никаких конкретных фамилий и цитат, потому что мне кажется, что это отвлекает. Я буду как бы показывать вам картинку, как я ее понимаю, она будет как сплав из множества отдельных людей и мнений, но вы, но как бы это, это, это тот сплав, как я его вижу, и на самом деле это Кумулятивные позиции очень многих людей, то, что я буду пытаться изложить. Вот с этими тремя дисклеймерами мы движемся дальше. Я попробую показать, как мы сейчас пытаемся моделировать сильный искусственный интеллект, что мы, собственно, пытаемся моделировать и вообще, что такое модель. и дать вам немножко по-другому посмотреть на это. И вообще моя задача – поговорить о том, что вы и так знаете, но с другой стороны. Какие принципы должны лежать в основе новых моделей, и познакомлю вас с такой новой моделью. Для начала, как мы пытаемся смоделировать интеллект? Собственно, первые источники наших данных об интеллекте – это интроспекция, познать себя. Сократ сказал. Наверное, были еще до него. И поэтому, познавая себя, глядя в себя внутрь, пытаясь понять, как мы устроены, собственно, это был первый источник данных. На базе него есть целый такой большой подход, он называется когнитивная архитектура. Это множество. Мы строим схему того, как устроена наша память. процедурная, длинная, короткая, как мы оперируем действия. И дальше пытаемся в этой схеме, внутри этой схемы, ну, как-то ее как бы воплотить, например, в коде или в чем-то. Кому интересно, здесь есть QR-код на статью «40 лет когнитивных архитектур». Их за это время, ну, огромное множество сделано, но на этом пути незаметно большого прогресса. И в первую очередь незаметно большого прогресса, потому что мы сильно ограничены нашими представлениями о мозге. Когда мы смотрим внутрь себя и решаем все эти блоки, это как бы то, как мы понимаем, наша внутренняя интроспекция. И она ограничивает то, что мы делаем. И хотя мы полностью понимаем, что происходит внутри, в отличие от нейросети, и можем контролировать этот процесс, Это сильная сторона, но она же и слабая, потому что мы ограничены нашим представлением о мозге. Ну и нужно переводить с окружающего, из реального мира на язык этой системы. Это сильное ограничение. В 50-е и 60-е годы появились новые данные. И, собственно, появилась модель Ходжкина-Хаксли и модель МакАлака-Питца на базе нее. И, собственно, мы получили модель вот этого самого искусственного нейрона. И не просто модель искусственного нейрона, нейрон как сумматор сигналов. Хотя, ну, в свое время Петр Кузьмич Анохин прекрасную статью написал на эту тему, Никто не доказал, что нейрон суммирует, вообще непонятно, как нейрон это делает. Это тоже непонятно, но нет единого представления сейчас о том, как это делает. Но мы говорим про модель, про математическую модель этого процесса, который, в общем, неплохо работает. И она не только неплохо работает, но она еще и прекрасно масштабируется. Мы берем много таких элементов, соединяем их с достаточно простыми правилами и получаем огромные работающие структуры. Но at the end of the day, если вы посмотрите на эту нейросеть, это просто система уравнений. гигантская система уравнений с гигантским количеством параметров, и в конечном итоге то, что она делает, это просто аппроксимация. Мы аппроксимируем входные данные выходными. No more, no less. Мы можем это делать сколько угодно потрясающе. У нас может быть как угодно много точек на входе, потрясающее количество параметров и уравнений. Но эта модель все равно будет аппроксимировать выход на вход. В этом смысле, например, GPT-3. Вы здесь много раз рассказывали. Прекрасная модель. Работа с языком, генерация языка. Как правильно сказал Григорий Сапунов, когда он рассказывал про статистический попугай. Она обучена повторять, воспроизводить некоторые знакомые паттерны. Она фактически аппроксимирует некий вход, некий выход. Там нет никакого понимания. И я поговорю чуть позже про то, что такое для меня понимание. И, на мой взгляд, главное заблуждение людей, которые занимаются нейросетями, оно вот на этом слое. Если мы будем наращивать число нейронов и весов, то рано или поздно оно как-то само собой эмерджентно, есть такое замечательное слово «эмерджентно». Это обозначает, что никто не знает как, но как-то там, it's a mystery. Вот как-то там появится, значит, мышление и сознание. и даже сознание. Но я лично уверен, что нет, конечно, не появится. Как мы не будем наращивать эту систему количественно, в ней не появится нового качества. Новое качество нужно в нее привнести отдельно. То есть здесь мой тезис состоит в том, что размер этих сетей, пусть он потрясает воображение, но он не играет решающего значения. Хотя система, безусловно, прекрасно масштабируется, но мы совершенно не понимаем, что внутри. Она прекрасно решает некий класс задач. Ну окей, это прекрасно, но это определенный класс задач. Надо понять, что она делает, а чего она не делает. Ну и для полноты картины я должен упомянуть, что есть еще один тип архитектур, еще один подход к сильному интеллекту, который, собственно, продиктован программным кодом, как такой источник вдохновения, так называемой вычислительной архитектуры. Мы понимаем, что если мы пишем программу для решения любой задачи, то почему бы нам не написать программу, которая сама напишет любые программы, которые решат любые задачи. И давайте мы сделаем такой код, который будет генерировать любой другой код. И вот есть всякие такие архитектуры, как мы можем генерировать новый код и тестировать его, и чтобы он сам себя бесконечно писал. И глядишь, рано или поздно эта программа напишет такой код, который сам напишет там любые другие коды. И, в принципе, есть даже алгоритмы, которые работают неплохо и решают тоже определенный круг задач. Но я, честно, здесь так же, как и в случае нейросети, не думаю, что наращивание Просто это алгоритмического подхода, оно к чему-то приведет. Но обучительная архитектура на самом деле является частным случаем символами архитектур. Я на этот слайд так и не успел картинку подобрать. Но вы знаете, символы архитектуры, где мы оперируем конкретными символами, сейчас, например, предикатные модели. И мы выводим там… Это может быть система правил, экспортная система, это пример символьной системы. У нас здесь были прекрасные доклады Антона Колонина, например, или Евгения Витяева. Это тоже примеры таких символьных систем. И я сейчас… Поинт моего доклада состоит не в том, чтобы отрицать все по отдельности. Вернее так, в том, чтобы сказать, что на самом деле истина где-то в совмещении этих подходов. И мы сейчас до этого дойдем. Но меня лично всегда потрясает вот это противопоставление подходов, когда, знаете, в конечном итоге разговор про архитектуру всегда сводится почему-то к этому противопоставлению. С одной стороны нейросетевые, а с другой стороны символные. И как бы люди начинают друг с другом рубиться. Ну и для меня это неосмысленно, потому как на самом деле, если мы внимательно приглядимся, я сейчас вам это покажу, в нейросетевых архитектурах уже есть, так сказать, некоторые символные истории. И они уже начинают совместить таким образом. И надо идти в это направление. Но для того, чтобы идти в это направление, я теперь отвлекусь и поговорю о том, что мы хотим смоделировать, говоря об Эджае. И, по крайней мере, мой личный, так сказать, мое личное понимание этого вопроса. Потому что мы делали недавно семинар на AGI сообществе, и первым вопросом было, прежде чем говорить про AGI, скажите, что вы хотите смоделировать. И это очень много разных подходов. Есть люди, которые говорят, я хочу сделать решатель, универсальный решатель любых задач. Окей, это как бы одна концепция. Но для меня, я лично в своей работе, когда мы с Сергеем Шумским делаем некую модель, то для меня, когда я говорю про общий искусственный интеллект, я говорю про другое. Для меня, в первую очередь, интеллект – это способность адаптироваться, ставить цели и достигать их. И делать это в любой среде, в условиях неопределенности, с ограниченными ресурсами, это определение можно еще как-то докручивать. Но важно, что фундаментальное качество нашего интеллекта, которое, собственно, позволило нам, людям, стать доминирующим биологическим видом на планете, это как раз способность гибко адаптироваться, достигать целей, которые нам нужны, какие бы они ни были, эти цели. Цели могут быть там. Это, например, может быть… Я задаю вопрос этой системе, она мне дает ответ. понимая, что я имею в виду. Или я хочу, чтобы она выиграла в игру в Atari, и она выигрывает в игру в Atari. Или я хочу, чтобы она принесла мне там стакан кофе, и она приносит мне стакан кофе. То есть цели здесь могут быть любые. Но важно, что цель должна быть. Я хотел попросить, чтобы звук выключили на моем докладе в слайдах. Это вот замечательное видео, которое показывает, что если есть цель, то интеллект может найти, как эту цель достигнуть. Но чтобы ее достигнуть, цель должна быть. И это важный поинт. Мы про него еще поговорим. В данном случае у Голубя была цель, и он нашел, как ее достигнуть, и это совершенно замечательно. И когда у тебя есть цель, есть некто, кто к этой цели идет и демонстрирует какое-то поведение, то только тогда можно говорить о понимании. Для меня лично понимание можно говорить только когда я вижу некое целенаправленное поведение. Вот представьте себе сцену, вы ее видите хорошо, там мальчик Судя по всему, играл в футбол. В окно попал в вазу, разбил вазу. И я прям ясно вижу маму, которая говорит ему, чтоб ты больше так не делал. Ты меня понимаешь? И вот если я в этом месте поставлю точку и спрошу вас, а что бы вы посчитали правильным пониманием? Вот если мальчик скажет, я понимаю, означает ли это, что он понял? Я лично думаю, что вот если он больше не будет играть дома в футбол с мячом, потому что мама сказала, что так делать нельзя, то есть он изменит свое поведение. под этой информацией, потому что он как бы увидит, что он в данном случае получил условно отрицательное вознаграждение, говоря терминами ИРЛ, он изменит свое поведение и больше не будет так делать. Тогда он понял. Но если он ничего не сказал или даже если он сто раз сказал, что он понимает, но при этом ничего не сделал, или продолжают делать так же, то о каком понимании можно говорить? В этом смысле меня, например, удивляет, когда люди говорят про GPT-3, говорят там, понимает ли она тексты? Бессмысленно говорить о понимании, потому что мы не можем это никак проверить. Вот если вы GPT-3 поставите некую цель, она должна что-то делать, Демонстрировать какое-то поведение, а потом вы будете задавать вопросы и давать какую-то информацию. Если она будет корректировать свое поведение как-то осмысленным образом, тогда мы можем говорить о понимании. В противном случае мы можем сколько угодно много эмбеддингов построить, этих фич внутри, но мы ничего не можем сказать про понимание этой системой чего-либо. Это просто математическая модель, которая апроксимирует вход на выход. Это важный момент, и для меня он кажется важным. И ЭДЖАИ для меня, когда я занимаюсь своим моделированием, это конкретно модель, вырабатывающая целенаправленное поведение. для достижения любых целей, ну и при этом в скобочках как бы демонстрирующее понимание входной информации. Ну то есть это скорее такой прагматический коммент, последние там несколько слов. Просто это проще понять, потому что вот, например, Дмитрий Салихов рассказывал про лидерборды. Как мы поймем, это бы общий тест такой, если мы делаем модель, как мы поймем, как бы, что это вот UGI. Для меня, я не верю в лидерборды, мне это кажется Это моя личная позиция во всякие эти тесты. Но я хочу увидеть, что система демонстрирует понимание. А демонстрировать понимание она может самым разным способом. В данном случае вы видели голубя, который пытался достать кусочек фрукта. Он явно демонстрировал понимание ситуации, что этот фрукт, он хочет его достать, и понимание, что ему нужно сделать для этого. Это важный момент. В общем, это то, как я определяю джайв для себя. Опять-таки, возвращаясь к первому дисклеймеру. Слова не точны. Это мое личное определение, к нему можно тысячу раз вязаться, корректировать. Для каждой из вас каждое слово здесь понимают по-своему. Слово «модель», слово «цель», слово «достижение». Это нормально. Но я понимаю это так. Но когда мы говорим про модель, надо еще понять, что такое модель. И я хочу дать вам взгляд на это определенный, потому что мы используем это слово зачастую, но очень не точно. Я физик в образовании, мы в физике привыкли к модели. Что такое модель вообще? Каждый из нас видел это много раз. Это гигантские впечатления для человека. Представьте себе первобытного человека, который на это смотрит. И мы видим это каждый день. И это солнце, оно восходит, оно идет наверху, и оно садится. И это гигантское впечатление, и мы пытаемся его как-то для себя объяснить. Почему оно там движется по кругу? Это какие-то вещи, над которыми человек начал задумываться очень давно. И отсюда возникает некая модель, это упрощенное объяснение мира, некое упрощенное описание мира, которое объясняет известные нам факты и предсказывает новые. Если моя модель работает, то я знаю, что если у меня есть модель, по которой Солнце движется, то я как бы уверен, что с ней будет. И кто из вас помнит, что было первой моделью, описывающей движение Солнца? Я думаю, что это был Зевс или Гелиос, который скакал на колеснице по небу. И это была хорошая модель для тех времен, когда весь мир был полон богов и каждый бог чем-то управлял. Была такая модель. Гелиос скачет на колеснице и проводит свою колесницу по небу. Это некое объяснение, которое как-то работало. Но оно было понятно мифическое, а первое как бы условно-научное объяснение была система оптилемии. геоцентричная, то есть Земля в центре, вокруг нее все вращается. И это же, ну, об этом говорит наш обыденный опыт. Мы смотрим на это Солнце, вот оно там встает, оно вокруг нас крутится, оно село там, очень логично. И мы делаем вывод, что вот Земля круглая, и Солнце крутится вокруг нас. Прекрасная модель, описывающая реальность с одной стороны. Но мы знаем сейчас, что это не так. Уже существует множество новых фактов. Они в какое-то время накопились в астрономии. Появилась модель Коперника, Гордона Бруна Коперника, что на самом деле мы живем в гелиоцентричном мире. Это тоже модель, описывающая реальность. Модель, еще раз, это упрощенное описание мира вокруг нас, внутри нас, какой-то части этого мира, которая позволяет нам объяснить существующие факты и предсказать новые. И модель имеет смысл только тогда, когда она обладает предсказывающей силой. Только так мы ее можем проверить. Дает она какие-то предсказания или нет? Собственно, в физике так устроено. Мы делаем новую модель чего-то, потом пытаемся объяснить новые факты, делаем предсказания, и если они оправдываются, значит, да, наша модель сработала. И это важный момент. Но у каждой модели есть область применения. Модели не универсальны. Они очень конкретны, и люди зачастую этого не понимают. И вот здесь внизу условная такая шкала в метрах от 10 в минус десятые метра до 10 в пятнадцатые метра. И в нашей области жизни, в нашем привычном макромире, скажем, прекрасно работают механиками икон. Это классическая модель. Такая траектория, вот если я брошу там кликер сейчас, я примерно представляю, по какой траектории он полетит, я не ожидаю квантовых скачков, я не ожидаю, что гравитационное поле там как-то изменится и куда-то его унесет в сторону. Я очень точно понимаю, куда он полетит, потому что у меня есть модель, здесь имеется механика Нейтона, но в микромере она не работает. В микромире, если вы посмотрите на микромасштабы, там работает другая модель, вы это знаете, квантовая механика, где траекторий нет, там есть облака вероятности и там все устроено по-другому. И в макромире, совсем на больших размерах, тоже механика не работает, там работает общая теория относительности Эйнштейна. И там работают гигантские гравитационные поля, искривление материи. И никто туда не пытается идти с механикой Нейтона, бросать мячик и думать, что он там будет летать. И тут это кажется очевидным, правда? Все же это знают, это аксиома. Почему-то, когда мы говорим про модели мозга, то про это все забывают. И мы лезем с моделями квантовой механики в макромир и пытаемся что-то там объяснить. И мой тезис в чём? Посмотрите на этот мир, некая тоже шкала, ну условно, от 10 в минус десятый метр до условно метров. Ну как бы есть атомы. на очень мелкий масштаб, где работает квантовая механика атома. Может, мы пытаемся объяснить работу нашего мозга на атомном уровне, правда? Но это будет смешно. Есть там уровень рецепторов отдельных молекул. Я видел некоторые подходы. Я видел людей, которые проектируют нейронные сети, но не просто нейронные сети, они еще проектируют отдельные рецепторы, ансамбли рецепторов. как они устроены, ансамбль молекул. Я вам сейчас скажу, как физик, это путь никуда. То есть вы можете абсолютно детально воспроизвести эту систему, но когда мы говорим про мозг, в котором 86 миллиардов нейронов и триллионы связей, то как бы вы не пытались точно все это воспроизвести на уровне рецепторов и нейронов, вам потребуются гигантские участительные ресурсы, и вы все равно не сравнитесь с мозгом. С другой стороны, если мы посмотрим на уровень метров, на уровень человека, на уровень транспекции уже целикового объекта, где работает, казалось бы, когнитивная архитектура, этот уровень слишком грубый. слишком большой. И тут другие модели работают. И мой пойнт в том, что есть масса промежуточных уровней, на которые сейчас человечество практически не смотрит. Начало немножко смотреть. И если вы обратите внимание, то реальные серьезные улучшения в нейронных сетях произошли тогда, когда люди подробнее посмотрели на то, как устроен мозг, И некоторые идеи того, как устроены нейронные ансамбли, воплотили в сетях. Конволюционные сети – это как бы кусочек подсмотра на то, как устроена зрительная кора, например. И мы сразу знаем, как прекрасно работает распознавание изображений. Но эти сети будут работать только на распознавании изображений, точка, потому что они сняты за зрительные коры. Почему мы от них ожидаем, что они будут моделировать поведение, например? Это абсолютно бессмысленно. И то, что сейчас фокус внимания сообщества на нейронных сетях и попытки масштабировать это до триллионов параметров, на мой взгляд, как физика, — это путь в никуда, потому что мы с моделью, имеющей определенный область применения, лезем в другую область применения. Вы не можете метровой рулеткой бессмысленно мерить длину зала. Для этого есть другие инструменты. Хотя можно, конечно. Ну хорошо, диаметр земного шара — бесполезно. Поэтому мой тезис здесь такой – нужны новые модели, модели среднего уровня. Потому что нейросети – это слишком низкий уровень, условно, когнитивная архитектура слишком высокая. И я хочу показать вам эти модели. Но прежде чем показать, я хочу сказать, какой должна быть эта модель сильного интеллекта. И так, я чувствую, что я перебираю время, но я постараюсь. Значит, я хочу сказать про три принципа, которые я вижу, интеллекта, и как бы три условия для них. И в первом принципе, про который я скажу, ну, мне придется прям категорически поспорить с товарищем Курпатовым, уважаемым из Бирбанка, потому что он говорит, его первый принцип, что мозг генерирует слабость. А я говорю все ровно наоборот. Главная задача мозга — это упрощение. Первое, что делает любая модель интеллекта — упрощает мир. Мы смотрим на очень сложную картинку этого мира и в голове ее очень сильно упрощаем на отдельные модели этого мира. И дальше оперируем этими моделями. Мы не оперируем гигантскими картинами, мы оперируем очень компактными понятиями. Первый пункт. Я сейчас покажу, как он работает дальше. Второй важный тезис, второй принцип – это иерархия. Любой… Да, и у меня здесь еще видно, здесь мелким текстом я еще пишу, что некоторые люди называют… Ой, пардон. Некоторые люди называют, скажем, вот это упрощение сжатием. Это то же самое. Сжатие, про которое некоторые авторы, я не буду сейчас фамилией, или там модель, или абстракция. Я сейчас не про слова, я про смысл, про принцип. Называйте это как угодно. Это вот первый базовый принцип. Мы смотрим на сложный мир, и мы его упрощаем внутри, сжимаем. Второй принцип – это иерархия. Мы не просто сжимаем, мы сжимаем это по уровням, по уровням иерархии. Сперва это один уровень объектов, потом он становится более сложным, еще более сложным. И в качестве примера, вот смотрите, я думал про концепцию этой сессии примерно в сентябре прошлого года и думал, что я сделаю доклад о ней. Это был очень верхнеуровневый план. Я вообще не занимался детализацией. Но когда-то, рано или поздно, мне придется это сделать. Я понимаю, что в октябре мы начали с докладчиками обсуждать, мы начали думать темы. И я понимаю, что где-то в январе мне нужно будет подготовить свой доклад. Ну и наконец, когда доходит до этого дела, появляется план, структура, детали, и в конечном итоге я вот сейчас здесь с вами и рассказываю этот слайд. Но в сентябре я вообще не представлял себе, как это будет. Мне это было не нужно, потому что мозг устроен иерархично. Мы сперва делаем некую верхнюю иерархию планов, потом как бы среднюю детализацию, потом она постепенно выполняется. Это фундаментальное свойство, которое должно быть в модели сильного искусственного интеллекта. И наконец, да, иерархия плюс упрощение, они уже работают. Если вы посмотрите на любую нейронную сеть, вы увидите, что она берет сложную картинку. И вот то, что вот эти фичи, про которые мы говорили, имбединги, как их ни называй, это и есть те самые упрощения. Просто это как бы на ее языке есть. То же самое на самых символических моделях происходит. Мы берем сложные понятия. Любая иерархия, любая классификация – это, по сути, упрощение. Ну, плюс иерархия. Их сложно показать по отдельности. И третье – фундаментальное качество мозга. Мозг – это машина для предсказывания. Мы, главная функция мозга и нашего интеллекта, это упростив эту картину мира и выстроив в ней иерархию событий, мы используем это для предсказания будущего. И это фундаментальное свойство. И это фундаментальное свойство можно измерить, будете смеяться, потому что если посмотреть на такую вещь, как горизонт прогнозов, вот здесь видите от одной секунды до 100 лет условно. Есть люди абсолютно гениальные, вот, например, Айзек Азимов. У меня ощущение, что он на 100 лет вперед смотрел, как минимум, когда я вырос на его книге Foundation, и он просто гениальный. А вот это, например, мой котик. У него горизонт предсказания примерно 2 минуты. Я много раз засекал, у него двухминутный план. Он их выполняет, он счастлив, потом у него следующий двухминутный план. В этом смысле у модели GPT-3 сегодняшней, у нее горизонт предсказания, если хотите, там десятые доли секунды, потому что она способна всего лишь несколько слов предсказать, которые человек налегко генерирует. То есть горизонт предсказания здесь важен. И в качестве примера есть такой тест известный, две зефирки, не знаю, много из вас знает его или нет. Был такой замечательный тест, психологом одним, американским, сделанный. Когда ребенку дают одну зефирку, сажают его в комнате и говорят, я приду через 15 минут, если ты ее не съешь, я тебе дам еще одну. И у ребенка есть выбор. Он может ее съесть сейчас, либо он ждет 15 минут, и он тогда получит вторую, он съест две. И дети делятся, конечно, на две неравные группы. Для большинства детей 15 минут – это вечность, просто он не способен на это выдержать. И оказалось, что этот тест на зефирке намного сильнее коррелируется с успешностью так называемого человека в жизни, чем тесты IQ, которые, на мой взгляд, полные вообще… я не понимаю, о чем они. И это понятно, потому что на самом деле ведь… Задача нашего мозга – предсказывать. И тот ребенок, у которого сильные выраженные эти способности, который способен понять, что да, лучше подождать 15 минут, у нас есть две зефирки, но с очевидностью, мы понимаем, что это как бы качество интеллекта. Ну, я прям перебираю время, я просто скажу, что важно, что цель должна быть обязательно, чтобы это как условие интеллекта. Если нет цели, бессмысленно говорить о чем-то. Должен быть поток внешней информации, вот у вас нет его, да, слайд пустой, и кажется, ну, как бы… А что там такое? Мы не можем воспринимать мир без информации. Нам нужен какой-то поток, к которому мы относимся, и только тогда мы понимаем, как мы двигаемся, что происходит. Ну и, наконец, важна некая граница с миром. У агента должна быть граница с миром, такой embodiment, о котором здесь много говорили. Мне тоже это кажется важным с точки зрения как бы условия интеллекта. Если нет границы, то есть, грубо говоря, у GPT-3, находящейся в компьютере, у нее нет границы, для нее не существует мир, не существует как бы она и мир, модель и мир. А для настоящей модели это должно быть. Ну и за оставшиеся, я знаю, что… Включите мне все равно еще, чтобы я видел. Страшное время, да. Я хочу вам показать модель, которую мы делаем с Сергеем Шумским. Мы называем ее Марти. На самом деле мы делаем две реализации. Это модель, которую придумал Сергей Шумский. Я немножко допиливаю именно в цифровом виде по ДРЛ. И мы делаем две параллельные реализации. Он свою реализацию называет Адам, я свою Марти. Значит, MARTI – это гибридный, в том смысле, что он субсимвольный-символьный, иерархический, модульный, асинхронный, предсказывающий искусственный интеллект, который просто сделан для моделирования целенаправленного поведения. Это его главная задача. И это модель среднего уровня. Вот как она устроена. Я вам покажу, как она устроена на примере OpenAI Gym. А поднимите руки, кто знает, что такое OpenAI Gym. Ну вот, да, примерно пола аудитории. Круто. Значит, это такая графическая среда, в которой можно тренировать RL-агентов. RL-агентов, в данном случае это игра Atari. То есть это как бы игра Atari, к которой подключается агент, который ничего про нее не знает. В данном случае на вход просто приходит числовой вектор. И Марти ничего не знает про эту игру, он не знает правил. К нему приходят только векторы и иногда приходит плюс балл, условно говоря, или минус балл, если он проиграл или выиграл, в данном случае в реатаре. Марти – это новая модель среднего уровня. Это модель как бы критикальной колонки, не нейронов, а модель нейронного ансамбля, которая действует уже, ну как бы, который не то, что не сказать имитирует, она ничего не имитирует. Это именно модель нейронного ансамбля, работающая, это не нейронная сеть. И это как бы модель колонки, которая есть определенный функционал. На входе она принимает цифровые вектора любого размера. содержащий информацию о входе и выходе, начиная с чистого листа. Нет никакой генерации триллионного параметра в случайных числах, а потом мы начинаем ее как-то считать. Нет. Мы начинаем просто с нуля. Марк начинает с нуля. Он делает там сколько-то случайных движений, как бы создает себе первичное такое векторное пространство движений, и дальше в нем, на этом пространстве, как бы строит некоторый свой первый слой этих колонок, и дальше они уже начинают работать. И дальше он начинает как бы расти вверх в иерархии. Как это выглядит? Представьте себе, что у нас на входе есть такие цифровые… Дальше немножко будет схема, но я надеюсь, что я быстро прибегу. На входе такой цифровой вектор. И вот есть один такой модуль Марти, который получает много-много цифровых векторов. И он через некоторое время, когда их много набрал, он их кластеризует, разбивает их на сектора. И вот здесь условно он разбил их на сектора, и, допустим, этот вектор, который к нему пришел, он внутри себя вот этот модуль обозначил как А. Появляется некий символ. Цифра перешла в символ. Цифровое пространство, кластеризовавшись, перешло в символы. И дальше из этих символов появляется последовательность. Он просто смотрит на эту последовательность, как она устроена. И из этой последовательности строят слова. Какие-то свои внутренние слова на каком-то его языке. Все, что он пытается делать, это он пытается анализировать эти последовательности. С одной стороны, предсказать следующую букву, какая она наиболее вероятна, а с другой стороны, что важно, предсказать, а какого продолжения бы ему хотелось. Хотелось слово условное, но какое продолжение для этого модуля предпочтительное с точки зрения реинфорсмент тренинга для того, чтобы не получить штраф. А дальше представьте себе, что таких модулей много. Ну, допустим, 10, 100. И каждый из них работает с этими цифровыми векторами, причем они работают у нас реально не совсем цифровым вектором входа, а только с частью ее. Это начинает походить на нейросеть, но крупноблочная, если вы посмотрите внимательно. И каждый из этих модулей переводит это в некоторые свои символы и строит свои символные последовательности, из них свои слова, и пытается предсказать некоторую следующую букву, следующее слово. И дальше вот этот слой работает как некий ансамбль. пытаясь понять, что дальше. И вот этот слой, будучи неким ансамблем, сам формирует некоторое новое цифровое пространство, которое является цифровым пространством для следующего уровня, которое в свою очередь строит свои слова, их анализирует, возвращает обратно, смотрит такие буквы. Тем самым получается, что мы как бы строим некую Марти — это многоагентная модель, много модулей. Каждый из этих модулей анализирует свой вход, строит свою картину мира и пытается сказать, какое продолжение ему кажется самым лучшим. Дальше он исследует причинно-следственную связь событий фактически и предсказывает на лучший исход. Дальше на каждом уровне решения принимается ансамблем этих агентов, и чем выше слой иерархии, тем он как бы накапливает тем более абстрактные становятся эти категории. То есть на самых нижних уровнях это просто непосредственно состояние условно этого РЛ'ного агента, а на верхних уровнях это уже нечто другое, это уже какие-то куски траектории и прочее. И вот, ну, это реально кусочек того, как Марти сейчас выглядит и как он играет. Я просто кусочки с экрана снимал, я ссорюсь за качество, оно, наверное, не очень хорошее, но здесь видно, что вот он там, проиграв примерно Здесь 500 игр в данном случае. Но это не показатель. Иногда больше, иногда меньше. В общем, он учится в реальном времени. Он отстраивает свою архитектуру. Я не могу, к сожалению… Я очень хотел к этой конференции показать, как он выигрывает в 10 игр Atari лучше, чем человек. Но, к сожалению, нет. Мы пока ограничены сильно вычислительными ресурсами. И это сейчас текущее состояние, это прототип на Ява, ну Ява плюс Питон, на самом деле, там кусочек. Это CPU-шная модель, она учится в реальном времени, занимает примерно 8 гиг времени. И проблема, в которую мы сейчас стыкались, как, собственно, все в RLE, это Exploration vs Exploitation. То есть что ты ни делай, какая бы у тебя ни была модель, гигантская нейросеть или кулыни, что угодно, ты всегда все равно упираешься в то, каким образом проектировать новые движения. Но маркетинг – это абсолютно рабочая, обучающаяся модель машинного интеллекта, не являющаяся нейросетью, относящаяся к новому типу моделей среднего уровня. Сейчас реально не хватает вычислительного ресурса, потому что когда панов показывают, что они сделали миллиард шагов, У нас пока нет такой возможности. И, собственно, мой тезис в конце и на чем я закончу. Мы видим, что есть гигантская возможность сейчас для вот таких новых моделей среднего уровня. Я совершенно не претендую на то, что эта модель – это вот именно оно, то, что там вау, то, что станет моделью сильного искусственного директора. Я говорю про то, что есть целый класс таких новых моделей. Вот мы там в нашем маленьком сообществе, которое там Антон Калунин ведет, мы пытаемся там что-то обсуждать, какие-то вещи, и мы сейчас сделали такой маленький Независимый сетевой институт IJI. Он пока в зародочном состоянии. Это попытка объединить исследователей и объединить заинтересованные компании с тем, чтобы вести совместные исследования. Не в какой-то одной большой выделенной корпорации, которая хочет это сделать внутри себя. А мы считаем, что это должно быть сделано сообществом. для всех, между всеми, и что это может фондироваться из разных компаний, поэтому я призываю любые компании, которые могут быть в этом заинтересованы, которые увидели какой-то смысл в том, что я рассказывал, мы с удовольствием сейчас с радостью поговорим, обсудим, как это можно сделать, как можно организовать совместные исследования, и на этом, собственно, хочу благодарить за внимание сори что перебрал немножко время но это первый раз такую тему рассказывают огромное спасибо что слушали спасибо мы что мы можем наверное так антон мы можем здесь прерваться с тем, чтобы те вопросы не слушать, а просто с нашими вопросами пойти. 

S05 [00:43:03]  : Игорь, давай, это ты сам смотри. 

S02 [00:43:06]  : Там был только один хороший вопрос интересный про мотивацию, но мы можем не досматривать, я считаю. 

S05 [00:43:13]  : Ты имеешь в виду про эти? Нет, конечно. Ну вот, там, смотри, вопросы пошли, значит, с конца. Я сейчас смотрю, как это все увязывается с РЛ, и как целевой сигнал влияет на репрезентацию. Ну, может, сначала пойдем? Ну, давай, хорошо, давай пойдем. Хорошо, ладно, тогда смотри. 

S02 [00:43:34]  : Ну да, мне проще, потому что я тут... Давай я, наверное, остановлю шаринг экрана. чтобы видеть, по крайней мере, людей. Нет, проще, я на двух компьютерах сразу это делаю и смотрю... На одном смотрю, на другом транслирую. Значит, что касается вопросов... Ну, комментарии я пропущу всякие, да. Комментариев там много. 

S05 [00:44:23]  : Ну вот первый вопрос по-моему содержает на это просите Каханина. 

S02 [00:44:27]  : Да, я только должен откомментировать, народ, там кто-то рисовал на видео, я успел там где-то в середине остановить. Вообще, конечно, мне кажется невежливо так делать. и в записи будут эти стрелочки на видео, на кадрах. Ну ладно, будем с этими стрелочками. В оригинале их, конечно, не было, этих зеленых стрелочек. Я тоже не мог понять, откуда эти стрелочки взяли. Кто-то их просто рисовал там. Один кадр, честно, оказался испорченный, потому что там должен был быть просто пустой черный экран. где отсутствие входной информации, это был такой эффект, но тут были зеленые стрелочки, ну ладно, что ты делаешь. Да, по поводу сетей Кахонина, мне тоже, кажется, был первый вопрос реальный. Ну вот про зефирки могу откомментировать, что на мой взгляд, зефирный тест работает. И недавно была статья, буквально несколько дней назад, по-моему, чуть ли не две недели назад, про то, что осьминоги могут проходить зефирный тест. Ну, там, конечно, в своей постановке. Причем статья там очень в хорошем журнале. И это прямо показывает, что у животных есть определенный уровень интеллекта. Собственно, я в этом не сомневаюсь. Просто горизонт прогноза другой, уровень поменьше, но тем не менее он есть. но они в своей форме вполне себе этот зефирный тест могут проходить значит что касается сетей кахонина ну как бы и да и нет вот особенность модели то есть как бы мы опираемся на модель которую придумал сергей он там когда будет рассказывать, ну правда он про свою модель в этом докладе рассказывать не будет, но в принципе он рассказывал, и у него целая книга на эту тему написана «Машинный интеллект». Как бы в каком-то смысле да, это как сеть Кахонина, но реализация немножко другая. то есть оно там идейно похоже, но математически это просто выглядит немножко по-другому. Я в принципе на схеме изложил там буквально как это выглядит. То есть это такой своего рода ансамбль парсеров, если хотите, парсеров цифрового пространства, которые дальше его переводит символьное пространство и дальше они оперируют своими внутренними языками и у каждого некий свой такой язычок и и они на нем как бы работает но в общем как-то так В основе механизма Адама, похоже, я не очень понял. 

S05 [00:47:22]  : Видимо, имеется в виду, что Адам Шумского. 

S02 [00:47:27]  : Ну да, ну да. Идея такая, математика, математически это алгоритм Сергея, просто когда этот алгоритм разворачивается в код, для немножко разных применений, немножко по-разному разворачивается. Мы его просто параллельно делаем эти реализации. Я вообще большой любитель конкуренции. И просто это немножко по-разному реализовано. Сергей делает на Джулии, я делаю там на Яве. Иерархия ансамбля из Random Forest уровень обобщения будет низким. может быть я бы сказал только практика покажет мы сейчас пока но я не могу сказать что мы ну что что что что что что Сергей своих экспериментах что там я в своих экспериментах что мы дошли до какого-то реально высокого уровня обобщения но то есть видно что как бы терминами смотрите терминами значит там модели на которых мы тренируемся но в частности опаны айджи где там игры правда я должен сказать что мы тренируемся на на сложных моделях то есть игра против противника который целенаправленно старается чтобы ты проиграл это не это не это не просто пинг-понг где ты мячик там просто пытаешься поймать мячик это как бы немножко более там сложная история но тем не менее то есть там уровень обобщения грубо говоря выглядит так что есть просто мелкие движения потом условно этот агент должен понимать что если там условно позиция такая то ему там стратегически нужно сдвинуться сюда потом он начинает понимать что ему значит там смысл делать какие-то связки и вот определенный уровень обобщения есть но мы пока реально там у нас двух максимум трехслойные модели ощущение такое что ну для хорошего вообще не нужно делать там где-то так 4 5 слоев наверное но мы еще просто физически до этого не дошли к сожалению поэтому показывали то что есть не пробовали бы на нижнем уровне совместить простой персептрон и на среднем марте не пробовали можно попробовать наверное будет работать я вообще готов поспорить что даже даже думаю что надо просто совмещать разные модели надо быть смелее в этом смысле ну как бы там экспериментировать, делать там какие-то разные связки, это безусловно будет работать. Вот хороший вопрос, как MARTIC RL присоединяется, как целевой сигнал влияет на репрезентацию или просто нейросеть по этой репрезентации RL делает? но это всегда сложно понять как устроена чужая модель это такой момент не тривиальный но грубо говоря смотрите что такое rl для нейросети есть нейросетка которая смотрит там состояние но условно у нее состояние на входе это значит там картинка на выходе у нее действия допустим какие-то да ну реально допустим нейросети которые в атаре играют и выигрывает у них на входе ни одна картинка там 4 как бы 4 последних состояния с одной картинкой ничего не получается то есть у них там 4 5 последних кадров как бы есть то есть последние траектории контекст такой своего рода но это я про дипмайн говорю и вы получаете реальный сигнал и что она реально делает она подстраивает свои внутренние веса то есть все что она может сделать когда получила сигнал значит алгоритм подстраивает внутренние веса чтобы ну как бы делать другой выход на другой вход в этом смысле марте точно также подстраивает свои внутренние там не то что веса оценки если так можно выразиться подстраивает внутренние оценки и вот здесь есть такой интересный сложный момент вот какой если вы помните был такой доклад от артура франца прекрасный совершенно доклад мне очень понравился который рассказывал про сжатие и про сжатие там последовательности я думаю что он он тоже как бы идет то есть вообще в нашем сообществе очень многие делают примерно одно и то же по моим представления но только каждый держится за свои слова там за свои названия свою терминологию там за свое авторство но мы все дети такие только большие уже артур делать очень правильные вещи на мой взгляд с этими сжатиями единственное что в чем эта модель не очень на мой взгляд будет работать и видимо не очень работает судя по его там ответам что она идеально будет предсказывать то что уже было как бы идеально модель сжатия абсолютно идеально анализирует то, что было, и предсказывает то, что будет из того, что было. Но в данной ситуации нам не нужно то, что было. Нам нужно что-то другое, что приведет к тому исходу, который нам будет нужен. Здесь есть ключевой момент. Либо надо делать 2 миллиарда шагов, и просто как бы брутфорсом таким перебирать очень много комбинаций, на что у меня просто нет вычислительной мощности, либо нужно искать какие-то как бы способы, искать какие-то новые траектории. Но смысл в том, что, грубо говоря, если агент 200 раз подряд проиграл, проиграл в смысле не выиграл, а в смысле проиграл игру в компьютеру, то он блестяще запомнил вот эту последовательность, и он ее блестяще воспроизведет последовательность, как нужно проиграть. Но нас-то интересует, чтобы он научился выигрывать. Вот в этом смысле, когда мы получаем сигнал Ирельный, для марта здесь тоже так ну там челлендж состоит в том что с одной стороны мы должны подстраивать некую внутренние там свои оценки но с другой стороны как бы логика на который который работает на текстах изначально это архитектура на текстах была отработана когда по текстам всегда можно жутко 3 блестяще предсказывает следующее слово в тексте это вообще не вопрос это сделать потому что там у тебя нет задачи какой-то специальной текста предсказывать как раз абсолютно понятно а вот а вот как бы в жизни когда нужно предсказать новое развитие событий который приведет к другому исходу который желателен это оказывается ну как бы вещь нетривиальная вот я сейчас в этом месте немножечко там ну то есть вот в этом основная пока битва но это по сути exploration with exploitation если говорить вообще принятыми терминами так у марти закрытый код ну да пока это просто закрытый код это пока как бы это вообще большая дилемма для меня там как дальше с этим жить но но но пока это когда-то когда это станет дойдет до какого-то уровня работающего это можно будет сделать там некая открытая библиотека на которой люди дальше начнут настраивать что-то сейчас пока но просто я не считаю что он в той форме которая ну там в которой можно кому-то отдавать Тем более, тем более, в open source. Только позорится. Ну, не то, что позорится, но, в общем, это пока не тот уровень совершенно. Так, вот вопрос Евгению Евгеньевичу. Какая сейчас ситуация со распределенным институтом искусственного интеллекта? Это хороший вопрос. Давайте я... Ну, то есть... Он рождается, я так скажу коротко. Я боюсь, что мы начинаем перебирать время, но, в общем, мы с Антоном там иногда его обсуждаем. Скажем так, это отдельный хороший правильный вопрос, который я бы предложил даже, знаете, поглядев на некоторое количество обсуждений в нашем вот этом бесконечном чате, который иногда даже дочитываю до конца. я бы на эту тему просто отдельно даже поговорил потому что это вопрос очень фрактальный если сейчас на него отвечу тут же нарисуется там еще три четыре других вопроса и мы в него как бы фрактал пойдем просто вы понимаете было очень интересно да но вот но это тема точно заслуживающая не то что обсуждение совместной работы я считаю что вот ну как бы все мыслящие люди но это такой мой пешин да там еще нескольких людей которые в этой группе мы, мыслящие люди, мы должны сами себе сконструировать нечто интересное. Но, в общем, эта задачка нетривиальная, к сожалению. Так, Антон, скажи, пожалуйста, тут еще, я еще даже до середины, наверное, не дошел. Там есть еще вопросы. Времени у нас 19.09, мы что, я быстро доотвечаю? 

S05 [00:57:52]  : Давай доотвечаем, у нас как бы у Сергея же сколько где-то, ну у Сергея 30 минут, ну давай еще минут 20 у тебя есть. Ну 15 точно есть. Минут 15, да. 

S02 [00:58:07]  : так модули одного уровня как решают чей выход пойдет на уровень выше мне там картинка не такая значит модули нижнего уровня значит знаете значит группы модулей можно воспринять как в некотором смысле они как как ансамбль экспертов не ансамбль, как это выразится, как группа экспертов, если хотите. Значит, группа экспертов, которые как бы все высказываются. И те, кто выше, каждый из тех, кто выше, он видит, ну как бы там разные есть варианты реализации архитектуры, либо он видит всех экспертов внизу, все модуль, либо он видит часть их там по-разному. но если хотите каждый начальник ну как бы может выслушать всех экспертов но решение он принимает сам но но при этом он как бы слушает всех и в эту сторону как бы он он пока не имеет возможность ну строго говоря как он уже смотрит на цифровой вектор считайте что он как бы всех видит другой просто какими весами он их учитывает но это вот там дальше идет балансировка внутренняя как бы выявляются те те эксперты у которых более адекватное виде нереальности если хотите потому что каждый из них сможет видеть какой-то свой кусочек один устом нащупывает слона там хобот другую ногу третий там еще что-то и каждый по-своему описывает эту реальность, а начальник слушает их, если хотите, и пытается понять, какие комбинации из этих экспертов оказываются, кто из них там наиболее адекватен при всяких ситуациях. А начальники между собой, верхний слой, пытаются договориться. и там тоже есть ну как бы разные реализации мы разные пока пробуем потому что пока нет очевидно правильно так нет пока едит такого настолько более там правильного подхода чтобы он был чтобы он ну как бы чтобы он победил то есть можно делать либо там по принципу там один выиграл все остальные тормозятся это вот как мозги вроде это делается это одна реализация другая реализация это выбирать там несколько колонок которые голосуют за одно решение и чей как бы вес там но сильно отличается от всех остальных ну грубо говоря если там есть там 15 вариантов решения и там и из них все проголосовали за разные, а трое за одно, то эти трое выбираются. Я бы сказал, что, к сожалению, я пока еще не могу сказать, как надо делать, это просто вопрос экспериментов. Это пока открытые вопросы. Бритва Кама. так усложнение и затем упрощение вот по поводу усложнения я с этим не согласен вот это как раз тезис который я специально про него говорил то в чем я не согласен с там с доктором курбатовым который у которого первый принцип как бы интеллекта это что значит мозг это порождение сложности но вот я категорически не согласен. Я считаю, что первая функция нашего мозга это упрощение, это предельное упрощение. Мы смотрим на сложную картинку, если хотите, миллиарды, триллионы пикселей вокруг нас, ну прямо скажем, ну как бы не вокруг нас, окей, у нас на сетчатке там просто, ну там, я не знаю, читал, не помню сколько там, ну там миллиарды этих палочек, колбочек, которые воспринимают картинку. Но как бы представьте, я смотрю, сейчас вижу экран, вижу отдельные лица и не вижу всех этих миллиардов пикселей. То есть в этом смысле мозг упрощает все-таки. И вот в этих упрощенных терминах строят наши модели предсказательные. А потом, когда мы уже что-то хотим сделать, мы это не то что разворачиваем в сложность, так нельзя сказать, но мы как бы возвращаем упрощение облекается обратно. Сложность физического мира, если хотите, постепенно она спускается обратно. Так. Справка про Кахонина. Да, прекрасно. Да, модули одного уровня смешивают свои выходы, но фактически они как бы образуют. состояние модуля вы представьте себе колонку значит ну там это модуль марки это это модель колонки нейрокортекса колонка это значит там примерно в нашей модели сейчас это примерно 30 таких как бы мини колонок и они себя представляет некое состояние ну грубо говоря совсем там предельно упрощая там она возбуждена или или нет то есть как бы какой-то код если хотите то есть как бы у каждой колонки есть какой-то с как бы она в каждый момент времени и ее состояние представляется но если хотите неким неким таким большим развернутым вектором вот эти большие развернутые вектора совместно как бы объединяясь они дают некое новое цифровое пространство в некотором смысле как бы колонки верхнего уровня смотрят на то в каком состоянии находятся колонки нижнего уровня как они с точки зрения там этого потока данных в каком они состоянии находятся такая для интеллекта для интеллекта характерно рефлексия в ваших моделях рефлексия как-то моделируется или нет ну вопрос что такое рефлексия рефлексия это так я понимаю это слово это некая способность как бы отразить своей внутренней картине мира состояние внешнего мира и что-то потом как бы на базе этого сделать в этом смысле как бы это одна часть а другая часть это ну как бы то что есть некто некий субъект который осознает что вот это внешний мир а вот он как бы у себя внутри отразил от рефлексировал это вот ну как бы я сейчас свое понимание этого слова описываю и как бы там от рефлексировала дальше там ну что-то с этим сделала второго конечно в марте нету там пока нет никакого субъекта а первое ну конечно есть то есть некие события и состояние внешнего мира они как-то отражается вот в этой внутренней картине мира этого Марти. Если хотите, он похож на ну вот каждый модуль похож своего рода на акына такого, который как бы поет какую-то свою акынскую песню. Знаете, как вот что вижу, то пою. Он какие-то свои, значит, бормотания такие делает, если терминами этой модели. Это смешно, но просто как аналогия. И вот он постоянно он видит что-то и говорит. то есть он как-то все время какие-то вот и вот это вот вот это текущее слово если хотите это есть его рефлексия того что происходит сейчас со временем эти слова становятся как бы длиннее из них появляются некие предложения то сейчас в каком-то смысле все время рассказывает если хотите как бы что он видит каждый этот модуль но вот это такая своего рода рефлексии безусловно так Нужны ли, неужели только аппаратные ограничения есть у Марти для полноценного AGI? Может понадобится что-то из архитектурных новшеств? Планируется химоноделуп обучения? Это сразу несколько вопросов. Только в моменте это аппаратное. То есть не то, что просто аппаратное ограничение. Это просто прототип пока, который там как-то написан. не оптимально, не в расчете на правильное железо и так далее. Он как бы написан идеологически, с некоторым замахом. Он может быть переписан впоследствии на C++, на GPU ядра, выполняться параллельно. Все эти модули могут выполняться с высокой параллельностью, асинхронно. потенциально это может полностью очень быстро сейчас в моменте просто не до этого и поэтому сейчас пока вот до аппаратное ограничение есть что касается human the loop в обучении то это как раз то что мы в общем с Сергеем там и и думаем, что этим все должно закончиться. То есть мы считаем, что в конечном счете все обучение роботов, для которых мы, собственно, делаем мозги, оно будет в конечном итоге осуществляться не программированием, а обучением человека. То есть человек будет показывать нечто, что нужно делать, а робот будет повторять. со временем будет делать это все лучше и лучше в этом смысле правильная модель обучения на мой взгляд она не никакие не разметка там не что это воспитание в идеале но пока еще до этого далеко честно нам далеко идеологически мы за практически далеко так рука владимира смолина владимир 

S07 [01:08:49]  : Ну, собственно, значит, вот эта справка Потеова-Коханина была дана для того, что я не против того, что вы, когда получали свое прекрасное образование, у вас было много друзей из Малороссии, у вас, как бы, такой южно-российский голос при произношении всех фамилий. Но, как бы, это демонстрирует то, что, значит, по Коханину вы, так сказать, очень косвенно что-то представляете, как и задававший вопрос. И, собственно, это удивительно. То есть, я обязан быть по всем моделям. То есть, прочитайте еще раз, как там ударение ставится. Может быть, вам когда-то это поможет, предмастерить свою грамоту. Не каждый человек, конечно, должен во всех областях быть грамотным, и я, естественно, много есть областей, которые я не знаю. Но вот с Вашим докладом в целом я согласен со многим. Но здесь, значит, вот как раз этот момент о том, что у высокого интеллекта, которым человек обладает, главное то, что он способен легко вкусно приспосабливаться, На самом деле тот же голод и многие животные тоже хорошо приспосабливаются. А основное преимущество человека то, что он научился разделять труд и регулировать конкуренцию. Вот это то, что мы работаем не по одному, а совместно с сообществом и именно делает нас цивилизацией. Из этого следует то, что нельзя бы себе представить, что искусственный интеллект будет построен одним гениальным ученым без контакта с другими. И это как раз возвращает нас к началу вашего доклада, который сделает, что вроде бы в России всё есть для того, чтобы создать сильный искусственный интеллект. Так вот, самое главное, чего нет, и в том числе вы можете исправить что-то в своем поведении, и, может быть, это поможет в строении сильного искусственного интеллекта, состоит в том, что у нас нет организаторов, которые могли бы организовать этот труд большой группы ученых по созданию сильного искусственного интеллекта. Каждый, в том числе вы, стремитесь занять лидирующую позицию, и чтобы к вам кто-то подключился вам помочь в ваших каких-то идеях. Использовать чужие идеи вы не настроены. Это не только к вам относится. Это и Греев, и Курпатов, и Крайнов, и можно давать длинный список фамилий. Вот этого в России очень не хватает. Нет хороших организаторов, которые могли бы организовать совместную работу многих ученых для создания сильного искусственного интеллекта. Вот на это хотелось бы обратить ваше внимание. 

S02 [01:11:10]  : Владимир, спасибо огромное за комментарий и огромное спасибо за то, что вы поставили меня в один ряд с Грефом и Крайновым. Мне очень лестно. 

S07 [01:11:18]  : Я не хотел вас обидеть. 

S02 [01:11:20]  : Нет, нет. Меня вообще сложно обидеть. Я человек не обижающийся. Но вот я вам так скажу, чтобы вы просто понимали. Вы ведь это говорите мне, хотя я человек, который сделал единственную независимую конференцию по и которая но там фандразит со всех понемножку но никому конкретно не принадлежит в этом смысле уж что что а И на этой конференции я четыре года сам докладов не делал, заметьте так, на минутку. Это был первый раз. Наконец вас порвало, да. Нет, я просто, ну, я посчитал, что мне есть что сказать. Да, я рад. Я большой специалист потому, чтобы находить людей и их идеи, к вашему сведению. И точно, заметьте, я в своем докладе никого не призвал присоединяться ко мне в моих исследованиях. Нет, я пока занимаюсь... Только как институт, который вы организуете. А подождите, институт я организую с другими целями. Я как раз его организую для таких людей, как вы, которые думают, что все по одиночке сидят. Это так и есть. И это отдельная тема для обсуждения. Но тоже вам скажу еще просто, как факт такой любопытный, значит, что в свое время мой главный стартап, который я поднял, ну, самый большой стартап, который был сделан, это была такая компания Гемакор, на которой я поднял много денег, и из разработки в лаборатории, из маленькой, смешной, там, крошечной машинки, сделал большую компанию с миллиардной капитализацией, которая сейчас работает, в которой в пике было 130 человек ученых. очень разных таких вот все очень как бы со своим мнением но это был абсолютно работающий коллектив который за два года сделал работающий продукт и он был на рынке как бы через через два с половиной года был на рынке в этом плане если было бы финансирование и правильно это сделать то можно Но проблема в России сегодня в том, что деньги как бы, как любит говорить, финансирование есть у одних, а как бы идеи у других. И вот как совместить одних со вторыми, пока не очень понятно. Но в целом, давайте так, это вообще как бы не к этому докладываю. Вообще-то ваши вопросы, комментарии, я предлагаю с ним сдвинуться, к тому же у нас сейчас... Нет, это прямо к вашему заявлению в начале доклада, что в России все для оди есть. 

S07 [01:13:50]  : Это вот по поводу того, что не все, вот это не все. 

S02 [01:13:53]  : Ну, каждый, опять, каждый слышит, конечно, да, по-своему. Ну, хорошо. 

S07 [01:13:57]  : Вы тоже меня слышите по-своему, безусловно. 

S02 [01:13:59]  : Верно. Это, короче, отдельная дискуссия. Давайте мы просто оставим это на отдельном... Я не расстаиваю, я как бы все высказал. 

S05 [01:14:06]  : Игорь, я предлагаю ответить еще очень коротко на вопрос, есть ли в вашей схеме механизм внимания? Вот. И похоже ли это на то, чем занимается Хокинс в Нью-Менто? Вот на эти два вопроса коротко ответить и дать слово Сергею Шумскому. 

S02 [01:14:22]  : Да. Ну, похоже на то, что занимается Хокинс в Нью-Менто. Просто они занимаются этим... как бы опять-таки идеологический подход такой же, но они у них другая модель, они там делают по-другому. Я детали не знаю, но мы с Сергеем пару раз обсуждали. Сергей больше меня про это знает, а что касается внимания. Внимание это не сигнал сверху вниз, нет. Внимание это нечто другое, как бы в терминах сегодняшних, если я правильно понимаю нейросетей. в этом смысле как бы внимание сейчас как сказать и да и нет если хотите как бы вот давайте так сформулирую внимание в сегодняшних трансформеров как я его понимаю сродни некоторому саккадному движению глаза если хотите которые как бы там ощупывает пространство точно как бы фокусируясь на отдельных точках выстраивает за счет этого как бы лучше общую картинку но только в случае текстовых трансформеров он как бы ощупывает это предложение там по одному слову но смысл примерно такой же в этом смысле в марте есть ну как бы некий можно сказать что некий динамический механизм внимания но в прямом виде он он он не моделирован это но скорее такой не то что побочный но просто эффект архитектуры своего рода точно такой постоянно как бы такой некое сакадное движение так допустим сформулируем есть вот 

S05 [01:16:16]  : Ещё один вопрос, он тоже близкий, давайте на него коротко ответим. Есть ли какой-то механизм выявления новой ситуации, так сказать, удивления? 

S02 [01:16:26]  : Хороший очень вопрос, он мне нравится, да. Пока механизма как такового нет, хотя вот эта история мне очень нравится, она правильная. 

S05 [01:16:41]  : Так, давайте Сергею дадим 100. 

S02 [01:16:44]  : Да, я вот тоже за. Тут, кстати, кто-то пишет, можно ли сделать двухминутный перерыв после доклада. Да, ну да, а ты уже ответил, да, что устраивать софию, логично. Так, все, значит, я тогда сейчас запускаю шаринг. Сергей, ты с нами? 

S06 [01:17:03]  : Да, с нами. 

S02 [01:17:04]  : Ага, отлично. Я тогда запускаю видео или ты как-нибудь вступить там хочешь? 

S06 [01:17:15]  : Да нет, тут время уже не позволяет. Поехали дальше. 

S02 [01:17:20]  : Ага, так, сейчас я. так да у меня же не появилось ничего блин я ожидал что я на что я надеюсь что у меня появится та опция нету Давайте мы к следующему докладчику перейдем. Я очень извиняюсь, что я время перебрал, и это нехорошо с моей стороны было. Так, Сергей, Сергей Александрович, ты с нами? 

S06 [01:18:11]  : Да, надеюсь, что да. 

S02 [01:18:14]  : Да, я не ожидал тебя услышать отсюда, но это было очень логично, да. А ты оказался куда ближе, чем я думал. Ты прям тут, знаешь, у меня прям из-под носа, можно сказать, сказал. Класс. Рад тебя видеть, дружище. Всё, значит, тебе слово. Постарайся уложиться в 30 минут, хотя вот, как видишь, я просто взял и выдвигался из тайминга. Хорошо, я понимаю. Шарю экраном. 

S06 [01:18:39]  : Мне не разрешат. 

S02 [01:18:43]  : А включите, пожалуйста, там Сергею экраны. Да, подожди. Вообще надо было про тебя представление сказать. У нас такая сессия, это же я без всяких степеней. Я про тебя межпроволоку сейчас найду. Так, Сергей Александрович Шумский. Президент Ассоциации нейроинформатика, автор 75 научных публикаций, кандидат в физмуфизмат наук, директор научно-координационного центра науки и технологии искусственного директора МФТИ, руководитель направления нейроассистента и национально-технологической инициативы, ну и шлем программного капитета опыток САИ, конечно, и главный редактор нашего замечательного балимонаха искусственный интеллект. Я прошу приветствовать Сергея Александровича Шумского. 

S06 [01:19:33]  : Спасибо большое, Игорь. Я хочу узнать, меня слышно и экран видно? 

S02 [01:19:40]  : Отлично, слышно. Тебя сейчас видно в видео. Ребята, мы видим слайды? Да, я вижу. Да, все, слайды пошли. 

S06 [01:19:48]  : Хорошо, тогда я начинаю. Очень удачный у меня доклад дополняет то, что говорил Игорь. Я просто присоединяюсь. У нас одинаковые взгляды на удивительные вещи. И я хочу тогда просто дополнить, что знание человеческого мозга, я верю, что мы знаем, но как это может помочь нам в конструировании системы искусственного интеллекта? При том, что я, конечно, не являюсь знакомым мозга, Я тоже учитель по образованию, но я имею в виду, что у нас есть третий доклад, кстати, Инна Ноклина, и он у нас здесь как раз дата уровня произведен. А я хотел сосредоточиться на том, какие вопросы мы должны задавать с точки зрения конструкторов искусственной инфедерации, потому что это правильно заданный вопрос, Вот Дэвид Марк, один из инженеров в нейроинформатике, говорил, что из трех уровней характеризования сложных систем, это архитектурный уровень информатики, уровень реализации, верхний уровень является наиболее важным. Но исторические знания о мозге, они как раз развивались снизу вверх. То есть вот понимание каких-то до понимания алгоритмов работы каких-то эвакуативных подсистем, и, наконец, ну это уже совсем недавно, понимание, системное понимание музыки, понимание науны, архитектуры музыки. Давайте посмотрим, как это было. Ну, чтобы не ходить далеко, 19 век, в физике царит со всех наук, и все физические явления, конечно, старались объяснить их в контексте концов софт-физики. Вот, например, Иван Михайлович Сичинов в книжках головного мозга, он сводил, просматривал мышление как такую длинную цепь рефлексов. Ну, ассоциации. Ну, то есть это было как такие шестрелки, которые друг за другом цепляются, и, можно сказать, гигантские такие часы. То есть мозг просматривался как эта машина, и вопрос был за том, как устроена эта машина. И к концу 19 века, в общем-то, стало понятно, что это электрическая машина, физическая природа была понятна, и Рамона Кахара выяснил, что это не просто нерасситить, а это нерасситить сосредоточение нейронов. Именно нейроны являются базовыми элементами, которые ответственны и за передачу сигналов в этой сфере, и за влияние социальной памяти. Ну, в XX веке эта физика нейрона стала центральной темой, и были созданы теории распространения сигнала по передаче сигналов между клетками и теории долговременной памяти. Ну, вот здесь надо отметить Дональда Хербера, который предложил главный принцессу связанного нейрона Файя Тубер, Ну, уже в 21 веке, в веке безумных компьютерных мощностей, возникает соблазн просто смоделировать, можно сказать, вместе с первым принципом. Ну, слава богу, действительно мощности хватает, и теперь мы очень много знаем, действительно, о ноте, о том, как построены конкретные нейросети. И, в общем, давайте тогда сделаем полную модель мозга, и с первых ликсисов таким образом решим проблему. Вот такая идея лежала за проектом Blue Brain, который в 2010 году начал Генри Моран, то есть начиная там с описания детского нейрона на очень низком уровне и как описание все более крупных реалистичных ансамбль нейронов, и вплоть до… целого мозга, то есть конечной целью проекта BlueBrain, который еще идет, — это было создание цифровой модели мозга в мышках. Пока это не сделано, но тем не менее, в сентябре 2013 года Markram запустил еще более интенсивный проект, HumanBrain впрочем, обещая за 10 лет создать такую цифровую модель, лет на корабле произошел бунт. Многие ученые выступили против вот такой вот физикальности программы. В общем, проект реализирован на более реалистичные и прагматические вещи. Конечно, мы бы просто заменили одну безумно сложную систему, другую безумно сложную систему, и это не прибавило нам никакого внимания тому, как мозг работает. Чтобы понять мозг, мы все-таки должны рассматривать его не как физическую систему, а как вычислительную машину, и интересоваться алгоритмами, которые эта машина выполняет. Потому что именно алгоритмы, конкретный тип алгоритмов, которые обучаются решать задачи, обучаются целенаправленному ведению, эти алгоритмы лежат за феноменом интеллекта. Вот тут я просто привёл некоторые отделения не для того, чтобы на них останавливаться, а чтобы вычеркнуть, что интерес и всё, что связано инвариантно относительно носителя. И раз так, то, значит, мы должны сменить вот эту вот редукционистскую исследовательскую программу, смотрящую вниз на физику явления, на программу, как говорят, энерджетную, присмотрящую вверх, как ансамбли нейронов приобретают новые свойства решения каких-то интеллектуальных задач и, в конце концов, как они умудряются сорганизоваться так, чтобы у нас возник интеллект. Таким образом, логически возникает следующий шаг к исследованию и пониманию мозга — это исследование алгоритмов вычислений в модельных нейросетях, которые объясняют нам, как работают различные подсистемы мозга. Ну вот Дэвид Марк был тут пионером в его серии таких очень ярких работ 1969-1971 годов, Он вскрыл алгоритмы работы мозжечка, неокутницы и гиппокампы, конечно, на уровне понимания того времени. Давайте мы тоже сейчас пройдемся по основным модельным нейросетям, которые вскрывают принципы работы нашего мозга. Начать логично с персептрона. поскольку вот это первая модель, которая реально претендовала на объяснение принципов организации мозга. Ну, даже Розенблаг так назвал свою книжку. Ну и действительно, пресептрон является очень хорошей моделью условных рефлексов. В общем, в совокупности условных рефлексов, как мы помним, Сеченов объяснял мышление. При этом сам персептрон является очень простой моделью. Это три слоя нейронов, причем первое — это просто поле рецепторов. Второй слой — это ассоциативные нейроны, это набор признаков, каждый из которых отражает только какую-то часть информации о внешнем мире. Ну и, наконец, решающий слой — в том смысле, что это нейроны, которые принимают решения в зависимости от наличия этих или иных признаков. При этом Розенбаум нашел алгоритм обучения такого персептрона и количество условных рефлексов, которые может персептрон который может обучаться, оно прямо пропорциональное количество ассоциативных нейронов. То есть чем их больше, тем больше условных рефлексов может такой искусственный мозг Розенблата себе воплотить, тем сложнее поведение, которым он может управлять. Оказалось, что действительно Вот такой персептромерозимплатой является очень хорошей моделью мозжечка. И более того, самое большое количество клеток в нашем мозге – это гранулярные клетки мозжечка. Они как раз и являются теми самыми ассоциативными клетками в этой модели. А выходными клетками являются клетки Пуркинье. У человека более 60 миллиардов таких гранулярных лет. То есть можете себе представить то разнообразие условных рефлексов, паттернов поведения и паттернов мышления, кстати, которые мы приобретаем в течение своей жизни. Мы просто не представляем себе, какую большую роль действительно в нашей жизни играют условные рефлексы. Вторая очень важная модель – это сеть Хоккинга, которую Хоккинг предложил в 1982 году. Это полносвязная сеть нейронов, которые связаны симметричными связями с Хайбургским обучением. То, что связи симметричны, гарантирует наличие у такой сети энергии и, значит, сходимость динамики к устойчивым То есть эта сеть является памятью. Запоминание в такой сети тоже очень просто. Достаточно в этой сети возбудить какой-то паттерн, и он мгновенно продавит ямку в этой энергетической поверхности и превратится в одно из ее устойчивых состояний. является ассоциативным то, что у каждого такого стационарного состояния есть свой бассейн притяжения, и поэтому размыто, или даже по кусочку образа можно восстановить образ целиком. Так вот, сеть Hopfield является очень хорошей моделью нашей эпизодической ассоциативной памяти, которая сосредоточена в гиппокампе. В гиппокампе как раз имеется вот такая фракция нейронов, которые тесно связаны между собой рецепторными связями. Фактически представляет для себя такую сеть Hopfield. Гиппокамп формирует такие локальные ключи к глобальным паттернам активности в коре и соединяет отдельные признаки, которые выделяются разными частями коры, в единое восприятие, в единый образ, который запоминается у нас и который мы потом можем вспомнить по каким-то его фрагментам. И гиппокамп называется архикортексом, то есть это самая древняя часть коры, потому что для животных очень важна способность запоминать любую ситуацию. И вообще любой животное, попав в незнакомую ситуацию, мгновенно начинает составлять карту местности, включая человека. И делает это гиппокамп. Но, как вы видите на этом рисунке, всё-таки наш мозг развивался не за счёт развития гиппокампа, а за счёт развития неокортекса, где сосредоточена большая часть нашей памяти. Но эта память делает совершенно противоположное. Если гиппокамп запоминает особенности ситуации, то новая кора, неокортекс, запоминает типовые типовые свойства многих ситуаций, то есть представляет из себя категориальную память. А математическая модель неокортекса с саморганизующейся картой признаков была предложена Коханиным в то же время, как и сеть Кохана в 1982 году, и тоже представляет из себя полносвязанную сеть с симметричными связями, но эта сеть имеет свою топологию, то есть это слой нейронов, и там есть понятие близости. И обучаясь по ХЭБУ, такая сеть формирует у себя топографические карты признаков. Каким образом? Любой входной сигнал возбуждает на карте локальную группу нейронов, которые являются признаком этого сигнала. А близкие сигналы имеют близкие признаки на этой карте, то есть сохраняется топология сигналов в многомерном пространстве, и она отображается вот на такую двумерную сеть. В нашем мозгу такая двумерная сеть — это, собственно, есть новая кора, и наличие таких карт, в частности, в сенсорной коре, в моторной коре, в первичной зрительной каре оно было известно и таким образом объяснялось. Монкассу показал экспериментально, что вообще говоря вся кара является набором вот таких карт Кахонина, то есть состоит из однотипных элементов-детекторов, и таким элементом является колонка, размером где-то 1,3 мм, это примерно разброс гендритов нейронов, и колонка содержит около 10 000 нейронов, и срабатывает как целое. Колонки объединяются в более крупные модули, гиперколонки, размер которых определяется разбросом уже не гидридов, а аксонов. И поскольку активность колонок, колонки активно конкурируют друг с другом, то каждый момент в гиперколонке может быть активна только одна колонка. разбивает все входящие сигналы в неё на набор категорий. Причём таких категорий не так много, там порядка 30 символов. Фактически это вот символный способ мышления в нашей каре. То есть вся наша память представляет из себя набор вот таких категорий. Но поскольку таких категорий немного, то одна гиперколонка не способна распознать там какой-нибудь сложный образ. Но несколько гиперколонок могут, особенно если они смотрят на разные черты входящего образа, то есть работают с разными подпространствами. Тогда достаточно, в общем-то, небольшого количества, там меньше десятка гиперколонок, и такой вот модуль будет способен различать сотни, тысячи и даже миллионы образов. Точнее, n в степени k, где n – это количество колонок, гиперколонки, а k – это количество гиперколонок в таком модуле. Известно, что мозгу человека в нашей сочной Имеется такая область, например, размером примерно такого, в доле квадратного сантиметра, размером с горошинку, которую распознают лица. Если там с ней что-нибудь случится, то человек просто теряет способность распознавать лица, оставаясь вполне нормальным во всех отношениях других. То есть вот такими модулями у нас кора и заполнена. Распознавание лиц можно уподобить составлению такого фоторобота. Это такой общий принцип, как кора запоминает сложный образ. Известно нам, что мы распознаем не только пространственные образы, но и временные последовательности, то есть мы можем распознавать последовательности нот, последовательности букв. Как это может быть? Это легко может реализовываться с помощью небольшой модификации вот того модуля. Если одна гиперколонка, которая смотрит вовне и различает по входной информации набор какой-то категории или символов, окружена несколькими гиперколонками, которые смотрят уже на неё и друг на друга. Вот такой модуль, я называю рекурсивным модулем, он способен… он как бы переводит временные паттерны в пространственные. То есть каждая мелодия или каждое слово, оно кодируется вот такой картинкой на таком модуле. И такой модуль может запоминать большое количество последовательностей, каждое длиной К. То есть, ну, можно себе представить, что в мозгу должен быть тоже там какая-то размер коры, там, размером с горошину, где мы запоминаем все мелодии. Эти мелодии короткие, но более длинные мелодии мы можем запоминать с помощью ассоциативной памяти гиппокампа, так что, в общем, поначалу Следующий очень важный принцип – это глубокие нейросети. Глубокие нейросети, которые формируют иерархию все более сложных признаков. Сегодня вы знаете, что это просто мейнстрим искусственного интеллекта, так что я долго не буду останавливаться на этом. Просто сегодня работающие нейросети обучаются с помощью backpropagation. Что касается мозга, то там более релевантны другие модели глубоких нейросетей, которые основаны на хайбовском обучении. Например, модель глубоких сетей доверия Deep Belief Methods, предложенная Хинтоном. которая тоже двуслойная. Она основана на ограниченных моделях Больцмана. Это двуслойная такая модель, простая, тоже с симметричными связями, где верхний слой пробует все время предсказать сигналы с нижнего слоя. тем самым формирует набор необходимых признаков. Если мы будем поэтапно такую сеть наращивать, то у нас будут признаки все более и более сложные. Как показала впервые в 2013 году команда из Google, наверху такой сети мы увидим очень сложные признаки. В частности, они нашли сети, которая обучалась на картинках. человеческих лиц и кошачьих, кошачьих мордочек. То есть глубокие сети могут быть хорошей моделью не только нашего зрительного аппарату, ну и вообще моделью устройства коры, то есть кора устроена иерархически, и в ней существует два типа сигналов. Это входные сигналы, которые вот идут от слоя к слою, и сигналы, которые… то есть каждая более высокая область иерархии пробует объяснить сигналы на более низкой То есть кора таким образом строит иерархию предсказательных моделей, которые и являются моделью мира нашего и позволяют нам предсказывать, что будет. Вот резюмира и кора представляют из себя нашу всю внутреннюю оптику, через которую мы смотрим на внешний мир, и вся информация из внешнего мира рассыпается на громадное количество категорий миллионы. И эти категории разномасштабные. Они объясняют нам, что сейчас происходит в мире, что там будет. Это в задней половине мозга, в передней половине мозга таким же образом вырабатываются планы, что будем делать. И раз мы заговорили про поведение, то тут нельзя не сказать про обучение с подкреплением, поскольку это наиболее общая модель обучения целесообразному поведению. Её вообще можно, как Сатан предлагает, считать математической формулировкой интеллекта, поскольку обучение с подкреплением — это фактически универсальный способ самостоятельного обучения достижению любых целей. То есть от внешнего мира не требуется никаких примеров, требуется просто минимум миниморум, только сигналы о том, что цель достигнута. В функциональных системах Ханохина для этого имеется акцепт результата действия. Фактически функциональные системы являются биологическим эквивалентом обучения с подкреплением. в общении с подкреплением, а главной функцией является функция ценности, которая представляет для себя сумму всех ожидаемых подкреплений в будущем. То есть она смотрит далеко вперед, и это и есть модель обучения достижению далёких целей. То есть это модель нерефлекторного поведения, модель целесообразного поведения, планирования, возможно, больших. Поскольку никаких подсказок нет, то обучается эта модель так или иначе с помощью метода проб и ошибок, и корректируется функция ценностей, корректируется, корректируется пропорциональная разница между реальным подкреплением и ожидаемым. Такая простая идея. Посмотрим, как она реализована в мозге. Прежде всего, где сидят наши ценности? Они находятся не в коре вовсе, а находятся в более древних и глубоких разделах мозга, в базальных ганглиях. которые были еще у динозавров. И, собственно, вот развитые базальные ганглии, которые впервые у динозавров развились до нынешнего своего сложности, они и позволили динозаврам править на Земле в течение сотен миллионов лет. Ну, а на самом деле в виде птиц они до сих пор контролируют. в воздушное пространство. И у млекопитающих решение, в конце концов, принимается в базальных данных, но вот варианты решения у млекопитающих готовит Коран. они поступают в базальные ганглии, те принимают решение, а оценивают это решение в самые древние области, там стволовые отделы среднего мозга, куда стекается информация о состоянии жизненных систем организма, то есть который реально знает, насколько ему сейчас хорошо или плохо оттуда, и поступает допаминовый сигнал, подкрепление в базальной ганглии, В этот момент, в момент выброса допамина, мы испытываем ощущение радости, и базальная ганглия корректирует функцию нашей ценности так, чтобы в будущем мы этой радости испытывали еще больше. Вот как это происходит на модельном уровне. Информация из коры из-за ее сенсорных и моторных разделов поступает в базальные ганглии, а именно во внешний раздел бальзаминых ганглий, в стриатум, и там возбуждается функция ценностей, вернее, две функции ценностей, к плюс к минус, потому что там есть две системы клеток, по-разному реагирующие на допамины. Одних он усиливает, других он, наоборот, подавляет. Это сделано специально, потому что вот разница между этими двумя функциями как раз и представляет из себя то самое предсказание, ожидаемое наградой, но оно формируется потому, что сигналы от этих двух фракций идут разными путями и приходят с разными знаками на допаминовые нейроны, субстанции негра. И сюда же поступает реальная награда через структуру промежуточного мозга. И если оказывается, что реально награда более ожидаемая, то возникает неожиданная награда, и тут возникает выплеск мы испытываем радость и функция корректируется, то есть одна фракция усиливает свои связи, а другая ослабляет, и если мы посмотрим методом индукции, то мы увидим, что действительно одна из них представляет из себя накопленную радость, а вторая – ожидаемую радость. Они немножко, чуть-чуть отличаются, но главное, что они оба являются интегралами. То есть вот такой способ обучения, он ориентирует нас на далекие цели и заставляет сегодня учиться, а не веселиться, чтобы потом поступить в институт и так далее. Итак, мы рассмотрели базовые принципы обучения основных подсистем мозга, и теперь самое время собрать все вместе, и мы тогда перейдем с уровня подсистем на системный уровень. Давайте представим себе, как работает мозг в целом, вся эта конструкция. Ну, во-первых, то, что мы не говорили, потому что таламус не обучается, но у коры очень тесные связи с таламусом, то есть любая колонка коры посылает свой сигнал в таламус и получает оттуда ответный сигнал, и таким образом формируется контур автоколебательный, и это позволяет длительным возбуждением, и это фактически механизм нашего внимания и механизм нашего сознательного, ну, сознания фактически. Базальные ганглии управляют этим вниманием, то есть базальные ганглии – это механизм управления вниманием, они усиливают одни мысли наши и ослабляют другие, и направляет наше мышление в сторону достижения длительных целей. Жемчуг перенимает решения, найденные базальными ганглиями с большим трудом, и переводит их на подсознательный уровень, на уровень условных рефлексов. Он найденные решения запоминает и потом разгружает базальные ганглии для того, чтобы они могли заниматься поиском новых решений. Ну и, наконец, Гиппокамп собирает отдельное возбуждение в разных частях Корнеева, целостные органы. Вот так примерно это работает. Но я перерисовал эту схему на следующем слайде немного по-другому, чтобы подчеркнуть очень важный принцип, который был, но чтобы он более смотрелся, что фактически вся наша гора, весь наш мост, вернее, состоит из таких типовых управляющих модулей. И каждый такой модуль решает какую-то… управляет одним аспектом нашего поведения. Ну, это, собственно, и есть функциональная система. То есть мы, фактически, предложили модель функциональной системы. Как мы убедились, что там… тут есть два контура управления, первичный контур с подкреплением и вторичный контур с учителем. И на коле вот такая функциональная система занимает доли квадратного сантиметра, чуть меньше квадратного сантиметра. То есть все млекопитающие в этом смысле устроены одинаково, но только вот у мышки таких модулей растворяющийся у человека, у которого в тысячу раз больше мозг. У нас их несколько тысяч. И поскольку мы знаем, что кора устроена иерархично, то значит и все эти модули устроены иерархично, и значит управление нашим поведением тоже устроено иерархично. И чем больше у нас этих модулей, чем больше размер коры, чем больше нейронов в коре, поскольку каждый модуль — это фиксированное количество нейронов, тем глубже вот эта вот управляющая нейросеть. А мы знаем из теории глубоких нейросетей, что главная – это глубина. Чем глубже нейросеть, тем больше у неё возможностей. Ну, вот примерно так всё устроено у нас. И остаётся один последний вопрос. Я фактически перехожу уже к вопросам, но только я первый вопрос задам себе сам. А почему мы такие умные? Ну, собственно, у нас же не самый большой мозг, там, у слона, 3 раза больше, где-то 6 раз больше. В чем уникальность человека? Есть ли вообще она? Оказывается, есть. но только не у человека, а у приматов. У всех приматов количество нейронов в коре растет быстрее с ростом мозга, чем у остальных микропитающих. Существенно быстрее. И поэтому мы просто воспользовались этим и приобрели самый большой мозг ради приматов. Но в итоге у нас больше нейронов в коре, чем у китов и чем у слонов. Если бы мы не были приматами, то наш мозг должен был бы весить 30 килограммов, а тело – 80 тонн. Ну, спрашиваются, а почему мы такие умные, мы приматы? Ну, никто не знает, конечно, но у меня есть рабочая гипотеза, что именно приматы изобрели вот те самые рекурсивные модули и научились работать с последовательностями более эффективно, чем остальные млекопитающие. большинство связей короткие, то так и должно быть, что количество нейронов пропорционально массе коры, а у других микропитающих основная масса сосредоточена в белом веществе, там, в межсоединениях длинных, ну и, соответственно, количество нейронов растет медленнее. заканчиваю ну заключение просто хочу сказать что вот ответ на тот вопрос который я задал в начале что мы должны спрашивать и мозга сегодня мы должны скорее всего, нас должна интересовать именно вот эта вот крупномасштабная архитектура, потому что вы видели, что глубокие нейросети — это один из пяти принципов, которые у нас в мозге задействованы, то есть наше мышление устроено гораздо гораздо сложнее, чем обычно нейросети. Действительно, я согласен с Игорем, что усложнение нейросети – это не есть правильный путь к сильному интеллекту. Правильный путь – это понять архитектуру мозга и воссоздать ее в сильном искусственном интеллекте. 

S02 [01:53:34]  : У нас с тобой практически синхронотаймер сработал. Сергей, спасибо тебе огромное. Это было замечательно. Просто просто замечательно. По-новому, кстати, все схемы перерисованы и очень получилось классно. Друзья мои вопросы. Антон, но я останавливаю видео и у нас тут своих вопросов. Дальше ты веди, а я веду. 

S05 [01:54:08]  : Я думал ты видишь. Ну хорошо, ну давай, ладно. Владимир Смолин. Полносвязности, особенно двунаправленной в гиппокампе, нет. Что у топологии гиппокампа общего с сетью Хопфилда? 

S06 [01:54:27]  : Там просто все переплетено, но действительно там неполносвязное. У сетей Хоффилда есть очень большая литература. Начинается все с работы Хоффилда, где все полносвязное, но дальше там есть сети с разреженными связями. у которых побольше ёмкость, а недавно возникли связи с экспоненциально большой ёмкостью. Более того, даже сейчас в трансформерах используют сети Хофелда, и вот буквально там этого года, ну или 2020 года, статья называется и где показано, что внимание в трансформерах можно свести к сетям Хоффилда. У Хоффилда это базовая модель, а дальше ее можно развивать. Понятно, что она не отражает всех особенностей. человеческий или приматовского мозга или гиппокампа. Как всякая модель, она отражает какие-то существенные черты. 

S05 [01:55:49]  : Следующий вопрос от Владимира Осмолина. Что общего у статистических минимумов энергии в сети Hopfield с эпизодической памятью? 

S06 [01:55:57]  : Ну так вот просто это и есть состояние эпизодической памяти. Это то, что мы вспоминаем. То есть мы по какому-то ключу, по какой-то частичке мы вспоминаем целое. Это и есть модель памяти. 

S05 [01:56:12]  : Сергей, а можно я тогда сразу же ставлю вопрос. А как это, с вашей точки зрения, связано с неподвижными точками, про которые Евгений Евгеньевич рассказывает? 

S06 [01:56:27]  : Это от трактора? 

S05 [01:56:28]  : По-моему, это не то же самое. Оно как-то очень похоже по сути. 

S06 [01:56:37]  : Лучше скажет. 

S01 [01:56:38]  : Оно на самом деле очень похоже. В неподвижных точках происходит замыкание фактических предсказаний или энергии. Они на самом деле очень похожи между собой. Просто другой аппарат используется. 

S02 [01:56:55]  : О чем я и говорю. Многие в сообществе примерно об одном и том же думают и говорят и делают, только немножко по-разному, разными терминами. 

S05 [01:57:04]  : Хорошо. Следующий вопрос от Владимира Расмолина. Это Deep Q-Learning Network, имеется в виду, видимо, современная форма RL. Как она связана с динозаврами и базальными ганглиями? 

S06 [01:57:26]  : У современных сетей нет такой цели связать ее с динозаврами, но просто Deep Q-Learning – это там, где у вас эта функция ценности представлена глубокой сетью. И это позволяет… фактически в этой сети вырабатываются те признаки, которые у нас вырабатываются в коре. То есть можно сказать так, что кора совместно с базальными ганглиями – это такая вот иерархическая управляющая система, но она устроена просто не так, как современный Deep Learning. Архитектурно не так. Есть несколько этажей, где на каждом есть движение информации вверх и вниз, а в Deep Learning есть только одно движение информации, однонаправленно, снизу и вверх. 

S05 [01:58:24]  : Спасибо. От Евгения Евгеньевича Витяева вопрос. В какой структуре находится мотивационное возбуждение и санкционирующая афферентация? 

S06 [01:58:35]  : Ну... Возбуждение идет... Ну вот это... Есть система, которая дает энергию всему мозгу. Если ее отключить, то мозг отключается. просто о каких возбуждениях идет речь. Но мотивация идет от передней части базальной ганглии. Помните, вот такая мышка, которая вживляет электрод? И она потом нажимает лапкой на этот электрод, возбуждая себя и постоянно возбуждая вот это вот поведение. Так вот, этот электрод внедряет как раз в путь дофаминовых нейронов, которые идут в самый-самый передний кусочек базальной ганглии. Там у нас самое сильное удовольствие, оттуда распространяется самая начальная мотивация. Самый верхний уровень мотивации – это там. 

S05 [02:00:15]  : Спасибо, хорошо. Дальше вопрос от Владимира Смолина. Что такое экспоненциальная емкость? 

S06 [02:00:25]  : Количество образов, которые может запомнить сеть Хопфилда, пропорционально количеству синапсов у отдельного нейрона. В полносвязной сети это равно... количество нейронов, а разреженная сеть – это полносвязанная сеть, это количество синапсов. Поскольку синапсов у нейрона десятки тысяч, всегда возникал вопрос, а как же так, мы помним гораздо больше, чем 10 тысяч образов. И вот оказалось, что если взять энергию не в виде квадратичного, а в виде кубичного или еще более высокой экспоненциальной функции, тогда и количество образов, которые может запомнить сеть Окилда, оно экспоненциальное. Тем самым снимается вот это ограничение, что человек или животное может запомнить только очень маленькое количество образов. 

S07 [02:01:27]  : Можно я задам вопрос? Да, конечно. Вы в курсе, наверное, учили, что теория информации равна логарифму от числа состояния. У меня все время такое впечатление, что ребята забыли взять логарифм от этого экспоненциального числа, который они там считают. Вам так не кажется? У меня такое твердое впечатление, что этот логарифм они забыли взять, что экспоненциальное число состояния, а информацией запоминается логарифм от этого числа. Нет, это количество образов. Вы подумайте на досуге над этой проблемой. Она серьезная. 

S05 [02:01:59]  : Хорошо. Вопрос от Виктора Казариного. Сергей, допускаете ли вы возможность того, что ЭДЖИА удастся создать без полного понимания принципов работы мозга человека и животных? 

S06 [02:02:12]  : Ну, наверное, можно. Но просто у нас есть подсказка под носом. И раз мы в общем-то блуждаем в потемках, то почему не воспользоваться фонариком? 

S05 [02:02:25]  : Евгений Бабарыкин. Я так понимаю основную идею доклада, что чтобы смоделировать работу мозга человека, надо использовать комбинацию разных сетей, которые моделируют тот или иной участок мозга. 

S06 [02:02:39]  : Дело в том, что мозг всегда как какую-то машинку рассматривали. Но мой пойнт, как говорит Игорь, в том, что эта машинка для обучения. И нам надо понимать архитектуру обучающихся под системой. Не просто то, как устроена анатомия мозга. а как устроена анатомия обучения в мозге. И разные типы обучения, они совместно действительно необходимы для того, чтобы возникло такое сложное поведение, которое мы можем демонстрировать. Наверное, это не зря, что у нас в мозгу есть и с учителем, и без учителя, и с подкреплением, и все это нетривиальным образом соединяется в модули, в иерархические. Наверное, это не зря. 

S02 [02:03:43]  : Сергей, разреши, я добавлю, что к этому вопрос был такой, нужно ли использовать разную комбинацию сетей. Но все-таки идея-то в том, что нужно использовать действительно разные модели. Не обязательно какая-то конкретная модель будет сетью. Вот здесь надо не ограничивать себя конкретно сетями, коллеги, все-таки. Сергей не говорил про сеть в каждом конкретном случае. А именно могут быть просто комбинации разных моделей. Важно понимать, как они устроены, общую архитектуру из них построить. 

S05 [02:04:17]  : Обучающийся под системой. А вот прекрасный вопрос, по-моему, хотя спикер он показался не совсем по докладу. Вопрос такой, какие в мозгу есть механизмы разрешения неопределенности? Например, если один и тот же сигнал, приходят и положительные, и отрицательные награды. 

S06 [02:04:39]  : Но она же приходит почему-то… Она приходит. 

S05 [02:04:43]  : Я дополню от себя. То есть, на самом деле, непосредственно отношение идет к нашим экспериментам с Игорем с reinforcement learning. Когда у нас есть неизвестная среда, где действуют многие факторы, И когда вроде как мы что-то делаем, а в итоге мы получаем награду, и она может положительная и отрицательная. Но она положительная и отрицательная не только потому, что мы что-то делаем правильно и неправильно, а просто потому, что вот так вот. Есть еще случайный фактор. И не всегда то, что мы делаем, однозначно приведет к успеху. То есть, мы вроде как все делаем правильно, но иногда может получиться отрицательная награда. 

S06 [02:05:25]  : Вы же знаете, что обучение – это некорректная задача. Соответственно, решений у нее очень много. И для этого на Земле есть десять десяток человек, Каждый находит одно частное решение, это некорректная задача. 

S05 [02:05:47]  : Есть только комментарии от Юрия Бабурова, что Deep Culling, эти этажи внутренней нейросети уже сложные, а в более сложных системах выделяют Policy Network, Valid Network. Это примерно уже эквивалент разных частей мозга. И вопрос от Владимира Смолина. А знаете ли вы иерархические системы с ДКН? Это уже последний вопрос, судя по всему. 

S02 [02:06:12]  : Это вопрос, по-моему, Юрия Бабурова, мне кажется. Да. 

S05 [02:06:17]  : Ну, в общем, уже началась свободная дискуссия. 

S06 [02:06:20]  : Есть работы Ботвинника, которые строят иерархические системы, но они, как правило, двухуровневые. То есть нет систематической глубины иерархичности. В современном Deep Learning нет глубины по иерархии. Закончились вопросы? 

S05 [02:06:51]  : Да, Игорь, большое спасибо. Вопросы закончены. 

S02 [02:06:54]  : Но я думаю, что может быть, кто-то хочет. Да, вот я не сомневался, что Николай захочет комментарий сделать. Я даже удивился, что до сих пор ты мне написал. 

S08 [02:07:05]  : Да, Николай, пожалуйста. Да, ну у меня два комментария на самом деле очень маленьких. Во-первых, о том только что говорили. Когда говорится об иерархичности, то можно говорить о двух совершенно разных вещах. Иерархичность структуры системы, то есть иерархия элементов, из которых мы строим систему. А вторая – это иерархичность модели которые используются системой и она может быть совершенно независимой другой то есть в рамках не иерархичной структуры системы можно использовать иерархичные модели да конечно И второй момент, который почему-то мне кажется как-то мало оцениваем тем, кто занимается невральными сетями. Если состояние нейронов или синапсов, или чего бы то ни было в модели, нейронной сети определяется действительными числами, и потом строится числовая компьютерная модель такой системы, то свойства этих двух систем будут сильно отличаться. По какой причине? Физическая система, состояние которой определяется действительными числами, не может гарантировать постоянство их величины, они будут плыть все время. Единственный способ гарантировать постоянство в физической системе – использовать элементы с стабильными различными дискретными состояниями. Когда мы переходим к непрерывной модели, это теряется в физической реализации. но сохраняется в компьютерной реализации, потому что компьютер может действительные числа хранить как угодно долго, не меняя, по той причине, что он сводит их опять-таки к бинарному представлению. То есть, если мы имеем реальную модель, в которой величины являются физическими величинами, а не дискретными состояниями, а потом моделируем её компьютерной, которая этим свойствам не обладает, то мы компьютерную модель сделаем не похожей на оригинальную. Ну вот у меня всё. 

S06 [02:10:23]  : Спасибо. 

S02 [02:10:25]  : Можно я, мне кажется, что здесь, вот я по поводу первого комментария вашего Николая, мне кажется, что сегодняшний мейнстрим, по поводу иерархии систем и как моделей, которые строятся внутри, у меня ощущение, что сегодняшний мейнстрим и, собственно, ну и направление, в котором мы с Сергеем тоже работаем, оно как бы такое, что если хотите как бы система динамическую иерархию на отстраивает внутри и это как бы отражает но некую иерархию моделей внутри то есть в каком-то смысле и сегодняшние глубокие нейронные сети они ведь тоже про это то есть как бы много слоев и благодаря этим многослойным конструкциям возникает внутри ну как бы иерархичное некое представление то есть мне кажется что эти вещи они видимо наиболее естественным образом взаимосвязаны это как бы такой ответный комментарий а по поводу второго тезиса вот я не совсем понял вас когда вы говорите про внешний мир он безусловно непрерывный сто процентов но как бы там но мы понимаем Но когда мы говорим про мозг и про то, как мозг функционирует, у меня нет полного ощущения, что там сплошная непрерывность. Скорее, там как бы нейроны ведь они все-таки как бы штучными зарядами работают. И даже я бы сказал, что вот мое понимание как биофизика, когда я на все эти вещи смотрел, что вы очень хорошо сказали что нужны как бы эти четкие физические элементы которые работают стабильные вот в жизнь как бы в конструкциях которые природа бы создала и которые у нас внутри там везде работают в клетках там везде те же белки или там какие-то клеточные вещи они как бы работают обычно с жесткими переключениями то есть там нету непрерывных как бы там там нету непрерывных спектров там есть как бы четкие переключения причем они настолько знаете жесткий и как бы я могу много примеров на примере белков рассказывать на примере насосов там или чего угодно то есть я думаю что строго говоря когда мы смотрим на эту внешнюю картинку мира и как только мы внутрь как бы как только она пошла по на этапе колбочек она еще в каком-то смысле там непрерывно то можно поспорить но когда она идет дальше туда как бы сигналами я думаю что ну высокой степени уверенности мы можем утверждать что сигнал уже как бы стал дискретным но правда просто очень как бы там ну там высокой размерности сигнал но все равно он уже как бы дискредит, ну как бы да. В общем, это уже не непрерывный сигнал. Дальше он только сильнее дискредит дискредит. Не дискретизируется как правильская семья. 

S08 [02:13:32]  : Прямо дискретизируется. Да, да. Дело в том, что модели те, которые используются в классических на сегодняшний день невиральных сетях, они описывают ситуацию, то есть состояние нейрона или связи действительными числами, не дискретными, перестраивают их. Это правда, да. Таким образом. 

S02 [02:14:11]  : У них есть масса промежуточных состояний, это правда, это не похоже на то, как оно внутри реализовано, это факт. 

S08 [02:14:24]  : Если мы хотим адекватности моделей полной, то мы должны либо признать, что в действительности у нас дискретная ситуация, и ее дискретно моделировать, либо признать, что у нас физическая, непрерывная, и тогда для ее моделирования добавлять шум какой-то. временной, который будет менять эти характеристики в моделях. 

S06 [02:14:52]  : Мы же знаем, что язык дискретный, мы же различаем слово как некий символ, то есть дискретность есть, и мы понимаем ее физическую причину, это самовозбуждение, вот эти контуры самовозбуждения в коре, которые на четвертом слое, куда приходят сигналы, там входных сигналов порядка 10%, а 90% – это внутренние, самовозбуждающиеся. Дальше он либо возбуждается, либо не возбуждается. То есть, вот такие механизмы дискретизации в мозгу есть, они встроены. Они отсутствуют в современных глубоких нейросетях, это да? 

S08 [02:15:37]  : Да, да. Дело в том, что, насколько я понимаю, Для большинства моделей нейрона из двух нейронов сделать один триггер и дальше как бы строить компьютер на этой основе. 

S05 [02:15:55]  : Коллеги, насчёт дискретных и непрерывных сред, на самом деле, мы, насколько я понимаю, собираемся в воскресенье по этому разговаривать, поэтому можно будет продолжить дискуссию. Антон, а можно... А сейчас, секундочку, Олег, секундочку, тут есть несколько вопросов уже, да, значит, Олег, давайте мы запомним, что у вас есть вопрос, но здесь есть несколько хороших вопросов от Ивана. Значит, я Сергея Терехова потом еще. Значит, от Ивана. Что принципиально меняется, когда мы часть нейронов обменяем в колонки? Это первый вопрос. Давайте по порядку. У нас, смотрите, мы уже как бы подкрались вплотную, поэтому я предлагаю сейчас уже коротко отвечать. 

S06 [02:16:42]  : В колонке принципиально ничего, что считать просто моделью искусственного нейрона, либо реальный нейрон, либо колонку. В нашей модели искусственные возбуждающие элементы – Это колонки. У колонок есть внутреннее взаимодействие между собой, чего нет в современных нейросетях. Внутри слоя все нейроны независимы друг от друга. А колонки в этом смысле друг с другом взаимодействуют. И отсюда появляется эта динамика «победитель забирает все» и категоризация. 

S05 [02:17:26]  : И следующий вопрос от Ивана. Правильно ли можно понимать, что в одной колонке неокортекса полностью реализована функциональная система? 

S06 [02:17:37]  : Нет, как раз функциональная система включает в себя кусочек неокортекса, кусочек базальных ганглий, кусочек кусочек мозжечка, ну и вот еще связь с гиппокампом. То есть функциональная система, они пронизаны, то есть все подсистемы мозга, они состоят из кусочков. И вот функциональная система объединяет все кусочки от всех подсистем. Вот в этом смысле. Отдельно кора не работает. Отдельно базальные гандли не работают. Отдельно мужичок не работает. 

S05 [02:18:21]  : Спасибо. А можно тогда попробую уточнить этот вопрос. А нельзя рассматривать эту всю историю? Мне казалось, что она так и выглядит, когда Игорь рассказывал про эту многокомпонентную архитектуру. Мне сразу вспомнился Минский. Society of Mind. Мы не можем говорить о том, что каждый агент представляет собой функциональную систему. Объединение агентов дает функциональную систему высокого уровня. Объединение систем агентов дает функциональную систему еще более высокого уровня. Вот так вот не может получаться? Так оно и есть. Окей, спасибо. Так, вопрос от Сергея Терехова. 

S03 [02:19:03]  : Да, меня слышно. Добрый день. Во-первых, спасибо большое Игорю и Сергею. Мы сегодня видели двух Гарри Сэлденов в живую. Причем это было очень красиво, потому что сначала Игорь в живую что-то ответил на свои вопросы, а потом Сережа, будучи Гарри Сэлденом номер два, сказал, что он согласен с Игорем. Это было очень естественно. И потом изображение практически то же самое. Такой джаз. Это очень классно. Спасибо большое, ребята. Классно, здорово. Отличная идея. Серёж, у меня вот такой вопрос. Есть ли место априорной какой-то информации, структурной или какой-то вот той информации, которая как бы дана до нефтообучения? Потому что всё, что говорилось, касалось адаптации, адаптационных каких-то вещей, приспособлению, изменениям, реакциям на внешние возражения. А есть ли вот в этой системе что-то такое, как бы, есть ли место прайеру какому-то, которое тут есть? Почему этот вопрос задаю? Потому что результаты обучения и адаптации людей, которые вот мы наблюдаем, эти самые 10 миллиардов, они же очень разные. и люди очень-очень сильно разными, до разных, так сказать, добираются каких-то в своих обучениях особенностей. Есть там олимпийские чемпионы, есть там и так далее. То есть очень большая гетерогенность в возможностях. И вот отсюда возникает вопрос, как это соотносится с адаптацией и с априорностью. И вот чего больше, чего меньше, или может быть этого нет. 

S06 [02:20:35]  : Конечно, априорность есть, потому что до определенного возраста ребенок и шимпанзе почти не отличимы друг от друга, но с какого-то возраста уже вполне себе шимпанзе упирается в некую некий барьер, и дальше не идет. Но вот если ты посмотришь просто, как растут нейроны в коре, вначале это как саженцы. Они маленькие-маленькие, там у них корневая система совсем никакая. А у взрослого человека это такие могучие дубы, у которых переплетены И ветви, и корни друг с другом сплетены. И понятно, что вся эта корневая система – это, конечно, результат опыта. А априорности там не может быть особенной никакой. Априорность на уровне архитектуры задана. Почему-то у всех людей, почти у всех, речь в левом полушарии обрабатывается. Значит, есть какая-то приорность. Она не зависит от опыта. Вряд ли право от лева так сильно отличается в нашей жизни. Какие-то пути, по которым растутся. Вот эти вот длинные связи в мозгу, они же растут как грибницы, по запаху они идут, вот эти кусочки нейроны, по запаху, какие-то в мозгу выделяются нейромедиаторы, и вот на эти нейромедиаторы ползут от нейронов отростки. Структура этого поля, его даже в эмбриологии так и называют биополем, это химические поля, которые как раз заданы генетически. И это есть априорная информация для нас, это наша генетическая память. Она задает структуру связей длинных между областями мозга. Наверное, это важно для чего-то, конечно же, обязательно. Но дальше все развивается уже. Конкретика связей между самими нейронами, а не между областями мозга, она задается уже опытом конкретным, то есть апостериорным. 

S08 [02:22:59]  : Спасибо. 

S05 [02:23:00]  : Спасибо. Юрий Бабуров что-то хотел спросить. Юрий, у вас еще есть вопросы? 

S00 [02:23:06]  : Спасибо. Все очень менялось. То мне хотелось прокомментировать, то, наоборот, вроде все отвечали. У меня был маленький комментарий и вопросик был. давайте вопросик только задам, а то времени уже много. значит вопросик такой. хотелось бы видеть еще работу task positive network, ну и default network в вашей схеме следующей версии. потому что поведение... то есть одной мотивации целенаправленной не объяснить поведение человека, иначе бы он постоянно все больше и больше фокусировался на чем-то, на какой-то задаче, никогда бы из этого состояния не выходил. а там работают параллельно, похоже, две сети. Одна постоянно делает ориентировочное поведение, чтобы следить за внешним миром, а вторая делает именно целенаправленное поведение. Они еще переключаются порядка 10 раз в секунду с одной на другую, то есть обе одновременно не работают. И вот это тоже, мне кажется, сильно влияет все же на поведение человека. Хотя, конечно, в искусственной сети, наверное, это можно разделить как-то на две части. 

S06 [02:24:54]  : Я согласен. Это очень хорошее замечание. На самом деле у нас не хватает в нашей модели, пока действительно нет переключений между этими крупными состояниями мозга. Их там даже не два, а больше. Но да, тут надо думать. 

S00 [02:25:14]  : А так огромное спасибо за доклады. И вам спасибо. И Игорю спасибо. Игорью модель я недопонял, но будем еще, может быть, еще как-нибудь обсудим. я не понял, откуда у него мотивация возникает, ну и как эта мотивация, собственно, влияет на модель, то есть целенаправленное поведение. у вас все прям вот классно. 

S05 [02:25:43]  : хорошо, спасибо. вопрос еще от Олега Серебренникова. 

S04 [02:25:49]  : Я вернусь к замечаниям Игоря Пивоварова относительно кооперации, смысл этой кооперации и возможности. Там замечания были, слова о том, что, мол, вот денег нет и, мол, потому нет кооперации, ну так, лейтмотив такой, с чем сложно согласиться с учетом того количества людей, которое, ну и группа большая, но и те люди, которые вот активно участвуют в работе группы. Их достаточно много, чтобы они могли там какой-то свой ресурс для чего-то использовать, как мне кажется. Первое. Потом, я замечание это делал у тебя, у вас в группе в AGI, в нашем, в Телеграме, по-моему. по поводу того, чтобы более практическим было взаимодействие наше. И вот честно говоря, когда разговоры об этом заходят, я все время наталкиваюсь на одно и то же. Я вижу, что у нас, первое, у нас есть недостаток открытости определенной, да? Ну, объяснения могут быть самые разные, но вот Игорь говорит, что у него не готово, то есть он не хотел бы ударить грязь лицом, показывая там, что понятно на самом деле, хотя описать модель принципиально, наверное, можно и какие-то о самополагающие вещи архитектурные, выписать и их показать. Наверное, не просто можно, а может быть даже и нужно. То же самое о докладе Сергея Шумского. Интересный очень, кстати. Спасибо, Сергей. Доклад. И я его слушал раньше, потому что Кивоваров предоставил доступ спикерам, и спасибо ему за это. Интересные доклады. Тоже ощущение возникает. Возникает оно вот почему. С одной стороны, там структура очень хорошо структурированная информация биологического характера и ее как бы аналог и приведены аналоги решений которые так или иначе существуют на рынке или научных подходов которые позволяет эти биологические механизмы воспроизвести. И из этого делается хороший вывод о том, что есть некий конвейер. Но есть ли это все, описано ли это где-нибудь, опять же, на уровне концепции. Игорь говорил, это в книге написано. К сожалению, я книгу не нашел времени дочитать, прочесть точнее. Начал ее читать, там очень много общих вещей. зерна мало было в начале может поэтому в общем вопрос о кооперации я вот не хочу растекаться по древу мыслью но я везде вижу вот что-то как будто вот этого не хватает не хватает вот даже разговор о там каком-то институте 

S02 [02:29:18]  : Олег, я мысль понял. Времени много. Смотрите, мне ваш поинт очень понятен. Я никоим образом не сомневаюсь в кооперации. Я могу сказать так. Я много лет занимаюсь тем, что организовываю людей. Давно потерял интерес делать это как бы в бизнесе за деньги, потому что это просто. как бы занимаясь более нетривиальными задачами и что касается этого сообщества то ну вот как бы некоторые здесь присутствующие там с контакт с некоторыми здесь присутствующие мы уже проводили не одно обсуждение на эту тему как это можно было бы организовать и коллеги знают что мы эту тему она нетривиально и дело не только в деньгах я просто как бы вы меня слышите вот мой доклад был 45 минут потому что когда я хотел высказаться по двум простым мыслям я понял что я не могу этого сделать в трех словах это вот как бы мой поэт я сейчас про некоторые вещи книгу пишу я уже почти написал ее но там еще надо кое-что добавить потому что когда я помню что мне нужно высказаться ну как бы это выливается в некую в некий большой текст поэтому Ответ на ваш вопрос такой. Координация точно нужна. Я на эту тему просто предлагаю отдельно поразговаривать и сделать отдельный семинарчик такой. И обсудить именно вот разные вопросы кооперации. Но чтобы этот семинар конкретно не повисал, ну как бы как некоторые, и вопрос кооперации не повисал в воздухе, как раз мы с Антоном договорились. Я специально хочу сказать, что мы с этого семинара, который вот с этих докладов, мы хотим начать еще одну серию докладов, серию семинарчиков по архитектуре AGI. Вот эти наши четверговые семинары, они посвящены нашим собственным работам. На них рассказывают люди, кто что делает. Кот очень любит видеозвонки, не пропускает ни один. И это наши собственные работы, кто что делает, и это важно, но у нас всех не очень много сил. А помимо этого есть множество интересных разных архитектур, которые делали другие люди, а мы все мало про них знаем. Я согласен с Владимиром Сможным, что я далеко не все читаю, у меня не хватает кругозора на все. Но было бы интересно это посмотреть. и он такой что один из таких конструктивных моментов я хочу сделать такой серию семинаров на которые мы просто порассказываем друг другу про то как устроены другие архитектуры других команд и это своего рода журнальный клуб, но просто не по датам статей, не по новым статьям. Нет, нет, это не то, что Гриша. Гриша Спунов, это тоже, кстати, с моей подачи был семинар сделан. Это журнал Клаб. Это как бы семинар про статьи, про последние статьи, чтобы быть up-to-date. А, например, там условно Мадерик Хопфилда или Карта Коханина, это очень старые модели, но как бы, с одной стороны, по ним смешно делать журнальный клуб, а с другой стороны, ну, у меня ощущение, что это может быть полезно, если вот там некоторые вещи пообсуждать и порассказывать, это может оказаться вполне Вполне интересно. В общем, пойнт такой. Я собираюсь делать такой семинар. Мы с Антоном будем писать в чате, когда там что-то будет. И особенность этого семинарчика будет еще в том, что мы для каждого из докладчиков, кто захочет разобрать какую-то модель, просто предложим некоторые стандартные вопросы, чтобы каждый рассказывал не как, ну не как на душу ляжут, а чтобы у нас было некоторое стройное изложение ряда моделей в одной как бы структуре. То есть вот эту структуру некую набросать. Вот собственно первый семинарчик будет посвящен как раз этому. какую структуру набросать, что нужно, какие вопросы должны быть, что мы хотим про каждую модель узнать и какие вообще нам модели, какие мы хотим посмотреть и рассмотреть. Может быть, окажется, что их будет очень много, а может быть, окажется, что их будет очень мало, это будет очень короткая серия, но мне кажется, что это будет позитивно. Считайте, что это одна из тоже линий такой кооперации, где мы, встречаясь друг с другом, 

S04 [02:33:50]  : и в начале какой-то лет, то друг друга взаимно... Игорь, извините, что перебиваю, но это не ответ на вопрос. 

S05 [02:33:56]  : Давайте, на самом деле, уже мы... Давайте организационные вопросы. У нас сейчас заквопросы докладчику. Если нужно обсуждать организационные вопросы, то можно либо их в воскресенье продолжить, либо отдельный семинар, как Игорь предложил. Просто сейчас уже мы... Скоро уже три часа будет, как мы заседаем. Не, надо уже заканчивать на сегодня. Вот, хорошо. Ну тогда, Игорь, вопрос все-таки, мы это публикуем или нет, сегодняшнее? 

S02 [02:34:29]  : Ну, я уже подумал, что у нас почти полтора месяца прошло, и раз уж мы записали, я думаю, что да, можем вполне это опубликовать. 

S03 [02:34:38]  : Хорошо. Вот в этом сочетании эта авторская вещь не связана с OpenTox, поэтому можно опубликовать другое произведение. 

S05 [02:34:45]  : Хорошо. Хорошо. Ладно, коллеги, тогда всем большое спасибо. В первую очередь докладчикам. До новых встреч. Игорь, ждем тебя от установок и подводки к следующему четвергу. Спасибо всем, до свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
