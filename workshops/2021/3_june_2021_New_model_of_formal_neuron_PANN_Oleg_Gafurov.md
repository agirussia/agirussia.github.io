## 3 июня 2021 - Новая модель формального нейрона PANN — Олег Гафуров — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/2_sUB4qcG60/hqdefault.jpg)](https://youtu.be/2_sUB4qcG60)

Суммаризация семинара:

Семинар посвящен обсуждению новой модели формального нейрона, известной как PANN. Докладчики представили свои исследования и результаты тестирования сети, подчеркнув её высокую скорость обучения и эффективность.

Основные тематические блоки


1. Математическая модель нейрона
- Модель PANN использует интервалы для обработки входных данных, что позволяет сети обучаться на основе заранее заданных интервалов.
- Вопросы о построении и изменении интервалов в ходе обучения были подняты, подчеркивая сложность задачи.

2. Обучение на уникальных и шумных данных
- Обсуждалась возможность обучения сети на данных, которые в некоторой мере уникальны, например, в играх с сложной логикой.
- Важность отбрасывания большей части данных для обучения сети была отмечена.

3. Сравнение с традиционными сетями
- ПАНН демонстрирует значительное уменьшение количества итераций обучения по сравнению с традиционными сетями.
- Было подчеркнуто, что PANN обучается линейно, в отличие от большинства нейронных сетей, что позволяет значительно ускорить процесс обучения.

4. Вопросы и обсуждения
- Спикеры обсуждали различные аспекты модели, включая её потенциал для обобщения предсказаний на новых данных.
- Вопросы о связи модели PANN с моделью Алексея Рядозубова и о возможности обучения сети на данных с большим количеством шума были подняты.

Итог

Семинар показал, что модель PANN представляет собой инновационный подход к обучению нейронных сетей, который может значительно ускорить процесс обучения и улучшить эффективность сети. Однако, как и в любой сложной задаче, существуют и вопросы, требующие дополнительного изучения и разработки.




S03 [00:00:17]  : Меня слышно хорошо. Просто проверка. 

S04 [00:00:22]  : Да, сейчас убедимся. Трансляция пошла. Коллеги, всем добрый вечер. У нас сегодня Олег Дафуров и Антон Беляков расскажут про новую модель формального нейрона PNN. Олег около месяца назад уже делал общее описание этого проекта, но сегодня по просьбам аудитории, как я понимаю, будут подробности технические. Так что, пожалуйста, Антон, Олег, вам слово. 

S01 [00:00:53]  : Ну давайте начну я. Я, во-первых, представлю вам Антона Белякова. Антон – наш технический специалист, технический консультант и партнер проекта Омега-сервер и партнер компании Прогресс Инк. Антон у нас является профессор, кандидат технических наук, профессор кафедры прикладной математики Ивановского энергетического университета. Я сейчас чуть попозже скину еще ссылочку в чат, чтобы вы могли посмотреть его резюме, его труды там, если кому интересно. Мы договорились таким образом, что Антон сейчас как раз пишет, расскажет всю математику этого процесса там, то есть того, что происходит. Покажет то, чего мы уже добились, как я уже говорил вам. У нас уже есть конкретные записи. И проведет, я думаю, надеюсь, сравнительный анализ какой-то, чтобы вы увидели все возможности именно PAN, и чем они отличаются от того, что сейчас вообще существует нынешнее. Ну, а дальше мы обсудим, может быть, еще какие-то процессы применения этой нашей системы. На этом передаю слово Антону. Антон, пожалуйста, не прерываем. 

S03 [00:02:05]  : Спасибо. Всем добрый вечер. Значит, пару слов Олег уже сказал обо мне, да я немножко еще и повторю и расскажу, как я вообще пришел к нейронным сетям. Занимался я им еще, будучи аспирантом, более 20 лет назад. Реализовал какие-то Пробные варианты для решения своих научных задач, когда писал диссертацию. Но компьютеры того времени оставляли желать лучшего. Во всяком случае, те, которые у меня были в доступности, они считали какую-то прочную сетку примерно ночь. Потом у меня через год появился более мощный компьютер, который делает за 20 минут. Но все равно какие-то серьезные задачи, которые тогда перед нами были поставлены, мы решить не смогли. Из этой области я на какое-то время выпал. Сейчас я обратно в нее вернулся уже. Около двух лет занимаюсь в университете преподаванием курсов по мат.обработке информации. Ну и в том числе искусственным интеллектом. И обратно вот вернулся в нейронные сети. И вот, значит, стал контактировать с Омега сервером, с Прогрессом. И, в общем-то, познакомился с этой сетью ПАН. Видно, да, презентацию вернулся? Хорошо. Значит, о чем мы будем говорить? Как вообще появилась эта сеть, ее математика, обучение этой сети и некоторые результаты я покажу тестированием. На самом деле результатов достаточно много. Думаю, мы их тут все не охватим, но основные, которые покажут нам преимущество этой сети перед классическими нейронными сетями, я покажу. Поехали дальше. В общем-то, предыстория, с которой большинство знакомо, я здесь остановиться подробно не буду. И нас интересует в этом всём два момента. Первое – это Генри Дэйл. Вот он, который уже почти 100 лет назад открыл нейромедиаторы, получил за это Нобелевскую премию. И на его базе сформировалась доктрина Дейла, что на одном синапсе один нервный медиатор. Это первое. Второе. Фрэнк Розенблат, чуть позже, используя эту доктрину, сформировал вот такое правило. Один синапс, один синаптический вес. Дальше на основе вот этих доктрин строилось все развитие нейронных сетей. Нейронным сетям сначала был достаточно бурный интерес и хотели получить достаточно много хороших результатов. Были большие ожидания. которые тоже хорошо всем известны, повторяться я здесь не буду, это на самом деле известная информация, читать даже не буду, тут плакатик есть, но это все понятно. Дальше появился Марвел Минский, который доказал, что Время растет экспоненциально в зависимости от количества нейронов и числа задач. И это привело к тому, что большие задачи, сложные, стали требовать большое количество машинного времени, должны быть мощные компьютеры и должна быть затрачена большая энергия. Простеньких задач проблем нет на современных компьютерах. Простенькие задачки решаются с помощью готовых инструментов. Я, например, сам пользуюсь MATLAB. Там есть встроенный механизм NN Toolbox. с которым можно решать задачки опять же вот я со студентами делаю какие-то задачи те компьютеры которые стоят у нас на кафедре они слабенькие у нас задачка более-менее там по распознаванию каких-то картинок занимает там 10-20 минут может занимать очень тоскливо мы не успеваем там за пару например что посчитать вот Далее. Основные проблемы классических нейронных сетей, которые были построены на доктрине Дейла. Один синапс, один нейромедиатор, либо один весовой коэффициент, который мы используем в наших нейронных сетях для обучения. Для обучения нейронных сетей требуется огромное количество математических операций. которые занимают много энергии и времени. Это я уже, в общем-то, повторил. Большинство нейронных сетей, не все, есть, конечно, нейронные сети, которые уже этого лишены, после завершения цикла обучения не допускают доучивания или частичного переучивания. То есть, приходится переучивать всю сеть целиком, что, опять же, занимает время. Есть проблемы с зависанием сетей, с точностью обучения. Проблемы переобучения существуют. Линейные делимости, кричительность к порядку обучения. Дальше расширение, масштабирование сети. Тоже это все известные проблемы, с которыми борются. Иногда успешно, иногда безуспешно. Но, опять же, все занимает очень большое количество времени. Теперь, собственно, проблемы обозначены. Они широко известны. И теперь переход к этой новой нейронной сети по строению нового формального нейрона. Эта нейронная сеть получила название PAN – Progressive Artificial Neural Network или как прогрессивная искусственная нейронная сеть. Ее предыстория. Значит, ее разработал Дмитрий Песчанский, ученик Кишиневской школы ТРИС. Это было в начале 21 века, то есть порядка 20 лет назад. И которое позволило обойти ограничения Минского, связанные с экспоненциальным ростом времени на обучение. То есть, у нее получились следующие способности. Быстро обучаться, доучиваться, не зависать, не переобучиваться. А еще она может быть реализована на обычных компьютерах. То есть, без использования каких-либо дополнительных видеокарт и тому подобного распараллельного процесса. Значит, в чем суть? Дмитрий обнаружил основную причину проблем классических сетей, как несоответствие формального нейрона, который используется в классических сетях, особенности биологических нейронов, идея которых была открыта еще в 70-80-е годы. которая говорит о том, что у нас используется не один нейромедиатор для передачи сигнала, а несколько нейромедиаторов на одном синопсе. Вот такая вот идея, которая позволила реализовать на основе формального нейрона, всем нам привычного, систему с несколькими весовыми коэффициентами, с несколькими весами. Давайте далее я покажу. Вот это классическая искусственная нейронная сеть. Опять же, она всем хорошо известна. Используется синапс, у него один нейромедиатор, и у нас сформировался Формальный нейрон Маккола-Купитца с этим одним синаптическим весом. Стоит сумматор и используется устройство активации, активационная функция. на основе вот этого формального нейрона строится обычная нейронная сеть ну пусть это будет backpropagation сеть обратного распространения в качестве примера вариантов много с использованием вот такого вот формального нейрона и теперь наша задача вместо одного вот этого вот синаптического веса мы захотели реализовать несколько весов. Что у нас получилось? Получилась вот такая вот картинка. Вот такая вот картинка. Это вот наш синапс, у которого уже несколько нейромедиаторов. Каждый нейромедиатор – это отдельный синаптический вес. Как это реализовано? Реализовано это следующим образом. Входной сигнал подается на некоторый на некоторое устройство, которое назвали дистрибьютором. И в зависимости от этого входного сигнала, от какой-то характеристики этого входного сигнала, мы активируем один из весов. Какая характеристика входного сигнала? Самое простое, что приходит в голову – это его амплитуда, то есть его величина. Мы используем градацию серого от 0 до 255. Разбиваем. Здесь 6 интервалов. Давайте разобьем ее, предположим, на 8 интервалов. Тогда каждый интервал у нас будет содержать 32, ровно 32 значения. Первый интервал у нас с 0 до 31, второй с 32 до 64 и так далее. Получаем 8 интервалов. В зависимости от входного сигнала мы как бы замыкаем вот этот вот ключик. И активируется в данном случае для текущего нейрона только вот этот вес. Такая вот идея возникла. И на основе этой идеи, такого вот представления нейрона можно строить аналогично нейронную сеть вот такой вот слайдик это вот классическая нейронная сеть где для каждому входному сигналу соответствует свой синаптический вес и он идет на каждый нейрончик это классическая нейронная сеть в общем то с ней все знаком теперь здесь что мы Теперь вместо одного классического весового коэффициента у нас стоит дистрибьютор, который переключает модуль входного сигнала и активирует только один синоптический вес, который участвует уже в нейронной сумме. И так для каждого входа, для всех нейрончиков. Такую же сеть можно представить в виде некоторой электрической цепи. некоторые сопротивления переменные которые мы регулируем и настраиваем систему причем в таком виде системы даже реализованные я буквально недавно смотрел не именно вот с переменным как здесь а вот на классическом классическом варианте электрическое устройство которое зависит от входного сигнала зажигает тот или иной Диодик. Это вполне реализуемо даже с помощью паяльника. Такая система. Это просто крупная наша нейронная сеть. Теперь вопрос. Как эту сеть обучать? То есть, наша самая сложная задача – это обучение нейронной сети. Если в классических нейронных сетях мы используем, как правило, методы градиентного спуска с каким-то небольшим шагом, за счет того, что у нас небольшой шаг или скорость обучения, мы получаем очень большое время обучения. Здесь же в этой сети удалось в ходе экспериментов отказаться от активационной функции в принципе. Здесь на выходе сети просто сумма весов. И вот дальше давайте перейдем к примеру обучения нейронной сети. Вот нейронная сеть, здесь указаны Весовые коэффициенты. Для удобства представления и для удобства программирования стоит у каждого три индекса. Первый индекс отвечает за номер входа. Второй индекс – за номер интервала. Здесь на примере 16 интервалов используется. Я потом о них еще скажу. И номер нейрона, с которым связан вес. Три индекса – номер входа, номер интервала, номер нейрона. В итоге, если мы имеем 10 выходных нейронов, Имеем картинку на входе 32 на 32, то есть входов у нас 1024. Получаем 1024 умножить на 10 – это 10240 весов. И умножить на 16 интервалов вот такое вот количество весовых коэффициентов – 163840. Дальше. Так, это я не буду, я пропущу сравнительные характеристики, как обучается классическая нейронная сеть и наша сеть. Перейдем сразу к шагам обучения вот этой нашей сети. Смотрите, как она происходит. На вход подается образ. Давайте говорить, что это картинка 32 на 32 пикселя. Предположим, пусть это будет в градациях серого от 0 до 255. Подаем на вход сети первую картинку. В зависимости от уровня входного сигнала у нас активируются те или иные веса, то есть замыкается ключик. Например, для первого входа это был второй интервал, для второго входа это четвертый, для N входа это первый интервал. Вот они подсвечены. Мы их выбрали. Теперь нам нужно просчитать выходы сети. Для первого нейрончика мы складываем веса. Обратите внимание, мы не перемножаем амплитуду входной сигнал на сам вес, как это принято в классических нейронных сетях. мы просто складываем весовые коэффициенты. В данном случае весовой коэффициент вот этот, вот этот и вот этот. Сложили, получили нейронную сумму. Дальше у нас есть желаемые выходы сети. Например, если мы говорим, Этот входной образ у нас должен соответствовать выходу первого нейрона. То есть он должен активировать первый нейрон. А все остальные, естественно, не активировать. Мы предполагаем, что вот здесь у нас желаемый выход единица на всех остальных нули. Что у нас получается? Эта сеть, еще одно ее преимущество, может обучаться с любых начальных весов. Поэтому на первой итерации их можно задать нулевыми. Подаем мы на вход первое изображение, считаем выход сети. Выход сети будет соответственно нулевой. Выход сети будет нулевой на всех нейронах, потому что веса у нас нулевые. Здесь выход 0. Желаемый выход 1. Считаем ошибку между желаемым выходом и полученным выходом. 1 минус 0. Получили единичку. Теперь наша задача подстроить веса таким образом, чтобы у нас На этом нейроне образовывалась единица. Мы знаем величину ошибки единицы. Мы знаем, что она складывается из суммы весов вот этого, вот этого и вот этого. Мы берем эту единичку, делим на количество входов. Я уже говорил, пусть их будет 1024. Значит, мы берем эту единичку и делим на 1024. Поделили. Получили там примерно одну тысячную. И на эту одну тысячную откорректировали вот эти веса. Одинаково. То есть ошибка одинаково корректирует все веса, участвующие в этой нейронной сумме. Что у нас получилось? Каждый вес раньше был нулевой, теперь он стал однотысячным. О чем это говорит? Это говорит о том, что при подаче на вход в сети того же самого образа мы однозначно получим здесь единицу. То есть мы сразу скорректировали сеть на полную величину ошибки и обучили этому образу. знаем сразу образ за один проход. Дальше что мы делаем? Дальше мы подаем второй образ и аналогичным обучаем второй нейрон. Подаем третий и так далее. Здесь этот алгоритм расписан, где выход нейрона N складывается чисто из суммы весовых коэффициентов. Здесь нет входного сигнала. Входной сигнал у нас только определяет, какой именно вес будет в данном случае корректироваться из интервала. Вычитаем число ошибку. вычисляем корректировку весов. Дальше мы эти шаги повторяем для всех входных образов и получаем некоторый выход сети. Если точность нас устраивает, мы на этом останавливаемся и говорим, что сеть обучена. Потому что если мы все-таки хотим обучить каждый нейрон не по одной картинке, а по набору картинок, естественно, у нас будут плыть эти корректирующие веса. При этом они плывут незначительно и нет таких обратных связей, как в классических нейронных сетях, где корректировка одного веса сильно портит общий результат и приходится корректировать все веса. Здесь мы от этого уходим. И дальше на шестом шаге мы повторяем до тех пор всю эту систему, пока не достигнем определенной точности обучения. Это один из приемов обучения вот этой сети ПАП. Дальше есть такая табличка – сравнение обучения разных нейронных сетей. Желтеньким выделено то, что вроде бы как хорошо, сереньким – то, что вроде как плохо. Здесь у нас биологические нейронные сети, то есть это мы с вами. Здесь классические нейронные сети и здесь сети ПАН. Новые сети ПАН. Смотрите, что получается. За счет того, что мы практически за несколько эпох обучаем нейронную сеть, у нас сеть получает малое время обучения, то есть быстрое обучение. Количество операций математических, которые делаются в классических нейронных сетях – это методы градиентного спуска, достаточно трудоемкая задача. Хорошо, сейчас есть инструменты, которые можно использовать, и тому, кто создает нейронную сеть, не надо заморачиваться по поводу этих методов градиентного спуска, которые уже реализованы, мы им пользуемся как инструмент. Но при этом она все равно отрабатывается, может быть, намного быстрее, чем если бы мы сами это дело программировали. В этой сети она получается практически линейной, нет никаких сложных вычислений, не надо считать непроизводных, не надо вот этот градиентный спуск, не надо шагать маленькими шажками, чтобы у нас сходилось все в нужный глобальный минимум, например. То есть, малое количество вычислительных действий. Соответственно, это влияет и на класс тех компьютеров, на которые мы решаем эти сети. То есть мы можем использовать обычные рабочие машинки без всяких дорогостоящих видеокарт, тем более можем даже не использовать какие-то сервера, например, которые предоставляет Google. Для большинства классических нейронных сетей также есть невозможность доучивания. Здесь же без проблем можно доучить в режиме реального времени эту нейронную сеть, предоставляя ей новые образы. Масштабируемость тоже, я уже об этом говорил. Эту сеть легко масштабировать, добавляя в нее новые нейроны. Получается набор новых весовых коэффициентов, в которые также добавляется новый класс нейронную сеть. В классических нейронных сетях она может зависать в процессе обучения, что увеличивает вероятность зависания, увеличивается рост размеров сети. В сетях GPAN ни разу не удалось ее повиснуть, во всяк случай для тех тестовых задач, которые делают. При этом использовали и большое количество исходных образов, большое количество классов. Она ни разу не повисла при тех условиях, при которых были проведены тесты. И еще одним достоинством такой сети является возможность пакетного либо матричного обучения сети. Получается, что в принципе можно не строить сеть как таковую, а просто использовать матрицы. Я сейчас покажу, как это делается. Просто использовать матрицы входных образов, матрицы весовых коэффициентов, матрицы желаемых выходов и определенными матричными операциями обучать эту сеть. Тем более с матричными, матричные операции есть во многих языках программирования уже реализованы, то есть их не надо писать, не надо организовывать циклы, потому что обработка цикла идет дольше, чем матричная операция в любом из пакетах. CUDA, MATLAB, MATCAD, математика, ну и есть еще естественно другие. В питоне тоже есть реализация матричной алгебры. Как проходит обучение в данном случае? Смотрите. Матрица весовых коэффициентов. Давайте начнем с матрицы С. Я назвал ее матрицей коммутации, которая обеспечивает нам для каждого входного образа выбор того или иного образа. весового коэффициента в зависимости от входного сигнала. То есть в каждой строке этой матрицы, каждая строка этой матрицы соответствует входному образу. Входному образу. Вот это вот. Не входному, выходному. Выходной образ. Выход сети. Все уже запутался немножко. Сюда мы подаем первый пиксель нашего изображения. Здесь у нас разбиты интервалы. И в каждой строке, вот этот кусочек с первой по 16, если у нас 16 интервалов, присутствует только одна единичка, а все остальные – нолики. эта единичка нам показывает, какой из 16 весов мы будем использовать. То есть, представляете, это очень сильно разреженная матрица, состоящая из нулей единичек. Единичка стоит там, единичка соответствует тому интервалу, то есть амплитуде входного сигнала. Все. Больше входной сигнал у нас нигде не участвует. То есть, мы вначале, имея исходные пары для обучения сети, с помощью исходных образов формируем матрицу коммутации. Дальше матрица весовых коэффициентов. Она имеет точно такую же структуру, как и матрица С, потому что каждая матрица коммутации соответствует одному весу. Каждая ячеечка матрицы С соответствует конкретному весу. Единственное, что мы делаем, мы ее разворачиваем, транспонируем. Это точно такая же матрица. Зачем мы ее транспонируем? Для того, чтобы потом мы их будем перемножать. матрицу С на матрицу месовых коэффициентов. Чтобы не тратить время каждый раз на транспонирование, мы ее сразу формируем повернутой относительно матрицы С. Теперь смотрите, что у нас происходит. Для расчета выхода сети нам достаточно матрицу С умножить на матрицу W. То есть матрицу коммутации умножаем на матрицу весовых коэффициентов. Мы сразу же получаем матрицу нейронных сумм, а нейронные суммы – это выходы нашей нейронной сети. То есть одна матричная операция дает нам матрицу нейронных сумм для всех входных образов. Матрица нейронных сумм квадратная. Каждая строка соответствует входному образу. Например, для первого входного образа мы считаем, что должен активироваться первый нейрон. Она будет содержать единичку в первой позиции и нули в оставшихся позициях. Дальше нам нужно корректировать весовые коэффициенты. Для этого мы считаем ошибку. Мы имеем матрицу желаемых выходов. Вычитаем из нее полученные только что на предыдущем этапе матрицу нейронных сумм, получаем матрицу ошибок. Теперь вычисляем поправки ко всем весам. Берем нашу вот эту вот ошибочку. и делим ее на число весов, участвующих в нейронной сумме, а оно равно количеству входов. Получили матрицу поправок. И на эти поправочки корректируем матрицу весов. Это предыдущее значение матрицы весов. Прибавляем к ней поправочку. Получаем новое значение матрицы весов. И дальше для всех обучающих пар мы это дело повторяем. Получаем скорректированную матрицу весов. На этом заканчивается эпоха обучения. И после этого оцениваем ошибку общую. устраивает она нас или не устраивает ну и повторяем шаги обучения вот я могу показать вот на матлабе я собирал эту сеточку вот у меня вот здесь вот это одна эпоха обучения я у меня я сел запрограммировал эту минуту за 15 всю эту нейронную сеть и Ну, отлаживать, конечно, часа полтора, наверное, вокруг не поплясал. Вот, она в итоге превратилась вот в такую вот программку. То есть, вот это вот эпоха обучения. Вот этот цикл проверяет ошибку обучения, пока она не достигнет вот здесь, в данном случае, одного процента. Мы повторяем эти эпохи. Вот эта строчка – это расчет сети, вот эта строчка – это расчет ошибки сети, а вот эта строчка – это расчет новых значений коэффициента. Это одна эпоха обучения. в матричном виде. Три строчки. То есть у меня, когда все это получилось, я был прям такой возбужден, скажем так. Очень красиво, очень быстро все это реализуется. Так вот она обучается. Значит, можно делать так называемое поэлементное обучение, а можно делать матричное обучение. Теперь результаты тестирования этой сети. Смотрите, здесь не зря это написано. Результаты тестирования. Это компьютер, на котором тестировалась сеть. Это обычный ноутбук, ну вот и пятый процессор, да, не самый последний в линейке. но достаточно средний ноутбук. Было приведено сравнение CT-PAN с классическими нейронными сетями Neural Solution Data Manager и IBM Statica 22. которую просто взяли для тестов и попробовали посчитать с помощью готовых инструментов и с помощью сети PAD. Результаты тоже сильно вдохновляют и радуют. Смотрите. Neural Solution Data Manager. Условия одинаковые. Распознаем одно и то же количество образов. с какой-то заданной точностью. Что получилось? Значит, это сеть PNED. Мелковато, конечно, но нас интересует больше вот эта циферка. Это время обучения Neural Solution Data Manager. 32 минуты 18 секунд. И вот здесь в табличке это сведено. для пинет это составило 3,3 секунды вместо вот 1938 секунд или 32 минут впечатляет это ошибка то есть здесь получилось 3 тысячных да здесь одна сотая то есть это сеть точнее это сеть быстрее не просто быстрее ну какие-то неимоверное количество раз быстрее количество эпох для обучения 8 44 тысяч впечатляет едем дальше вот опять же этот Другие немножко результаты. Подавали разное количество образов для обучения. И как растет время обучения в зависимости от разного количества образов. Видно, да? Вот этот график. Экспоненциальный рост мы получаем для вот этого вот пакета. И для нашей сети мы получаем линейный рост. И он очень-очень-очень незначительный. То есть мы от полсекунды на 500 образов до 4 секунды на 7000 образов. Дальше уже просто сравнить не получилось. Увеличить на 7000 образов этот пакет подвис. ПАН сеть продолжала обучаться и вот на 100000 образов за время порядка 1 минуты. Ну и после всех экспериментов, которые были проделаны, Получились такие вот иногда ожидаемые, иногда неожидаемые результаты. Резко уменьшилось количество эпох при заданной точности обучения. Как правило, эпох хватает. Меньше десятка эпох достаточно для обучения сети. То есть не надо ни сотни, ни две тысячи, ни три тысячи эпох, а десятки, то есть даже меньше десяти эпох. Также изначально сеть тестировалась с акселюционной функцией, потом от нее отказались, но собственно я уже показал результаты при которых активационная функция отсутствует. Дообучить сеть тоже оказалась не проблема. Добавление новых образов ни в коем не очень-очень мало влияло на уже обученный результат. Ну и она оказалась масштабируемой. К ней можно добавлять новые нейроны и новые классы. Уже к обученной сети. То есть не надо создавать отдельно новую сеть под новые классы и все переучивать. Можно добавить новые нейроны и добавить новый класс в эту сеть. Ну и еще тоже я уже повторялся. То есть ее ни разу не удалось зависнуть. Дальше давайте покажу примеры реализации уже непосредственно с картинками. Это картинки 32х32 в градации серого. Исходные образы. Вот это котенок первый, Эйфелева башня, девушка и сова. Вот они здесь показаны. Процесс обучения. в начальный момент на первом этапе у нас ошибка обучения максимальная единица потом со второй эпохи мы уже получаем 8 процентов тут не совсем корректно здесь абсолютная ошибка указана абсолютная ошибка их можно перевести в проценты но неважно все равно видно то есть от единички мы падаем к нулю на пятой эпохе мы тут получаем уже практически нулевые ошибки Теперь тест. Обучили вот этим четырем образом. И вот у нас есть еще три котенка, которые мы показывали нейронной сети. Давайте посмотрим, как она на них отреагировала. Сначала мы тестируем на том, на чем обучали. То, на чем обучали. Подаем им первую картинку. Да, у нас определилось, что это киска. То есть, это выходы всех четырех нейронов. Выход первого нейрона почти единицкий, а у остальных нулей. У эйфелевой башни, соответственно, ее соответствующий нейрон. Все остальные нули. Девушка и сова. Теперь мы берем вот эту картинку. Тестовый образ. Подаем. Вот результаты. Получили 0,27 для первого, для второго вот столько, для третьего столько, для совы близкое значение, да? Близкое значение, но все равно не максимальное. Аналогично для всех остальных картинок. Я сейчас на следующем слайде, там более это удобоваримо представлено. Вот. Вот это обученная сеть. Вот сюда подаем мордочку. Котенка, которая тестовая, она не участвовала в обучении. И смотрим вот по этим вот столбикам, к какому классу она относится. То есть максимальное значение все-таки у киски. Хоть сова и похожа, но значение меньше. Делаем вывод, что это киска. При этом мы можем здесь еще такую задачу решать. мы можем говорить о схожести объектов. Мы говорим, что эта киска похожа на киску, а еще она очень похожа на сову. И можем говорить, насколько похожа на сову. Например, в два раза больше, чем на девушку. А вот на эйфлюбашню она вообще не похожа. Ну и аналогично для всех других котят. Как видно, Мордочки кисок везде соответствуют классу кисок. Вот эта киска похожа только на киску и чуть-чуть на сову. А на остальные вообще не похожи. При этом, смотрите, мы обучались сеять всего лишь на одном образе. Всего лишь на одном образе. Мы не подавали кучу разных кисок. говорить, что это киска, а уже получили такие достаточно, я считаю, хорошие результаты. То есть, если мы обучим сеть на множестве кисок, то, подавая тестовый образ киски, мы здесь получим уже что-то близкое к единице, 0.6, 0.7, 0.8, а может и где-то к единичке, в зависимости от самих картин. Дальше, следующие тесты со скоростью обучения. Вот, смотрите, здесь 10 образов. Это программка, которая тестовая. Она еще развивается, но здесь можно оценить хотя бы просто время обучения, которое это занимает. Значит, вот просто 10 совершенно независимых картинок, 10 классов. Обучилась она за три эпохи. за время 0,2 секунды и получил ошибку менее одного процента теперь усложняем задачку усложняем задачку подаем на вход 60 образов ну они тут тоже так в разнобой сформированы эти образы но вот пицца сделана не случайно я чуть дальше покажу Но опять же, мы получаем 60 выходных нейронов, 60 классов образов. Мы не говорим, что это к одному классу относятся все пиццы. Мы говорим, что они относятся к разным классам. То есть, каждый образ свой класс. Обучили. Здесь уже время солидное. 20 секунд, но 60 образов. Вот это количество интервалов, которые используются. Если на картинках я показывал, что их 16, то здесь использовалось 8. Я тоже обещал вам про них рассказать. Как оказалось, исследование это проводилось. У меня здесь нет этих графиков. Поверите мне на слово. Значит, количество интервалов от 8 до 16. Никак увеличение количества интервалов от 8 до 16 никак не влияет на точность распознавания. Поэтому остановились на 8. Система еще исследуется, но пока 8 дает очень хорошие результаты. При этом, естественно, она учится в два раза быстрее, чем если при 16 интервалах. Теперь смотрите. Обучили вот эту сеточку вот этим образом. Едем дальше. Теперь хотим протестировать эту сеть. Видите, один PNG. В обучающих образах нет этой картинки. То есть, это тестовая картинка, на которой не обучалось. Мы ее подаем на вход обученной сети. И смотрите, что у нас получается. Здесь результаты. Я напомню, каждый нейрончик у нас соответствует какому-то образу. Здесь, в данном случае, 60 классов. И смотрим похожесть вот этого нового образа на все другие картинки. И если мы посмотрим, то впереди как раз оказались все наши пиццы, которым мы обучались. Это они не сильно похожи, да? Мы не получаем вот здесь единички. Мы получаем 12, 12, 11, 10, 8, 6, 6, 5. Это степень похожести. Вот это пицца, вот на каждую из этих пицц. Но все равно система нам говорит, что это пицца, и мы можем сказать, да, это пицца, никаких вопросов. Другие все уже видите маленькие. Но вот эти тоже достаточно маленькие значения. Они тут сравнимы. Можно найти, что это за картинка. Не буду упражниваться. Теперь еще один момент, который я и дальше тоже скажу. Время распознавания. Время распознавания, то есть это скорость работы сети. 0,12 секунды, наверное. Достаточно быстро. Время работы сети – это тоже очень важный момент. Чем быстрее отклик сети, тем быстрее мы можем принять решение на основе этого отклика. Если мы используем сеть, например, в Автопилоте, то решение нам нужно принимать очень быстро. Если оно у нас будет 2 секунды считать, что, во-первых, получать результат от картинки, и потом еще и принимать решение, можно уже поздно будет принимать решение, если этот автомобиль едет со скоростью 40 км в час, ему тормозить, выворачиваться или что-то делать. То есть время отклика – это, в общем-то, жизнь и смерть в некоторых задачах. И, опять же, вот тоже пример времени отклика. Известная всем база MNIST для распознавания рукописных цифр. И тесты на этой базе построены. Значит, этот мне буквально недавно сбросили. Но, в общем-то, здесь тоже можно проследить. Сначала использовались слои свёртки. Вот они, керосин. В первой модели стандартные слои свёртки и последний слой полносвязанный. Во второй модели те же самые слои свёртки, которые уже подавались на пинелл, на последний слой. Условия были одинаковые. Единственное отличие это в последнем слое. В первом слое полносвязанный слой, во втором это предложенная модель PINEP. Скорость отклика 0.59, 0.11. То есть в 5 раз быстрее скорость отклика для этой задачи. точность ну точности сопоставимы но все равно пенет чуть-чуть но лучше и время обучения а вот время обучения да уже видно что время обучения сильно разное до сколько в 10 раз быстрее тренинг там Еще одни варианты. Вот здесь вот есть условие. Картинки 14 на 14. 10 входов. Собственно, даже сама постановка задачи здесь не важна. Нам важны результаты. Вот опять же, время отклика. Время отклика сети. Смотрите. 2.41 миллисекунды. Здесь 0.04 секунды. Сквозковка в 55 раз она быстрее. Она быстрее выдаёт нам результат. Точность. Точность спинетти в данном случае здесь ниже, чем керосин. Насколько это критично, надо обдумывать и надо смотреть, о чём вообще идёт речь. И время обучения. 7 секунд 38 минут. то есть время обучения в 313 раз быстрее. Вот. Это, скажем так, небольшая часть примеров, которые есть. И у меня есть еще подборка видео, где показаны уже прям реализованные задачки, простенькие, но в качестве демонстрашек очень-очень даже такие познавательные. Я сейчас запущу видео, оно в общем сложности идет 10 минут. Там 10 разных задач, где-то по минутке на каждую задачку, как они реализованы. Сейчас я попробую запустить. Надеюсь, что будет видно и слышно. 

S04 [00:51:30]  : Так, мы ничего не видим, по-моему. 

S01 [00:51:32]  : И звук пошел слабый. Надо просто поменять экран, наверное, на этом. 

S04 [00:51:36]  : Да, или делать sharing всего экрана. Сейчас-сейчас, секунду. 

S03 [00:51:42]  : Секунду. Так, мне надо выйти и выбрать тот экран, наверное, да? 

S04 [00:51:47]  : Да-да-да, именно так. Или сделать sharing, в общем, весь экран sharing сделать. 

S03 [00:51:54]  : У меня весь экран был, сейчас одну секунду. 

S04 [00:51:56]  : Ну тогда значит проблема в разных экранах, это я не знаю как достигается. 

S03 [00:52:01]  : Так, сейчас момент. Мы как-то это реализовывали. Сейчас я остановлю демонстрацию и запущу заново. Уже с видео. Хорошо? Попробую так. Так, запущу пример. Так. Здесь демонстрация экрана. Я хочу... Так, стоп, я её не вижу в предложенных презентациях. А где же она у меня? Почему она не видится-то? Так. А почему-то я её не вижу, как окно. 

S04 [00:53:27]  : Так, вот видим Мега. 

S03 [00:53:30]  : Так, это, наверное, Олег. 

S04 [00:53:32]  : Да, Олег зашарил экран. Запускает видео. 

S03 [00:53:42]  : Так, но у меня вот звука нет. 

S04 [00:53:55]  : Олег, должен был быть звук? 

S03 [00:53:57]  : Да, да, должен быть звук. 

S04 [00:54:13]  : ну вы может быть олег это если вы запускаете то видимо нам не он бьют он бьют олег надо сделать олег сделайте он бьют просто включить звук да вот включил звук так теперь вы мне дали звук да сейчас есть звук звук есть нет 

S12 [00:54:53]  : Нет, нет, Олег, нет звука. 

S01 [00:54:57]  : Нету звука, сейчас хорошо. Так, а что мне надо сделать, вы говорили, так, демонстрация? Ну, share screen. Демонстрация экрана, вот я включаю его, так, открываю, и открываю вот это окно этого, так, где он у меня? А, его надо запустить в сервер там. Сейчас, секунду. Секунду, секунду. Я просто, наверное, закрываю этот плеер сам. 

S03 [00:55:31]  : У меня тоже не было никакого звука, или был тихий? 

S01 [00:55:35]  : Вот сейчас попробую, сейчас, секунду. 

S04 [00:55:39]  : По идее, если у вас микрофон, если вы слышите звук, то и мы его должны слышать, если у вас микрофон включен. 

S01 [00:55:45]  : Я слышу звук замечательный. 

S03 [00:55:48]  : Как правило, он не дает звук компьютера в микрофон, он его глушит. Иначе будет какофония. 

S01 [00:55:56]  : Нету звука? 

S04 [00:55:58]  : Нет, сейчас вообще ничего не видно. Так, секунду. 

S01 [00:56:04]  : А вот я просто так, секунду. 

S03 [00:56:07]  : Вы не включили демонстрацию, Олег. 

S01 [00:56:16]  : демонстрация экрана проигрыватель сейчас видно картинку да видно картинку попробуем звук звук есть у нас нету я вы слушайте да да я слышу замечательно а как это сделать кто напомнил подскажите так Звук пробулькивается, но очень тихо. 

S04 [00:56:56]  : У вас есть ссылка на видео? Может быть я у себя попробую запустить? 

S01 [00:57:01]  : давайте я вам сейчас скину ссылку на этот видео здесь это не секрет сейчас секундочку одну просто в папке его быстренько найду специалистом сейчас секунду. в чат вам сюда да Антон? 

S03 [00:57:40]  : давайте сюда попробуйте. 

S04 [00:57:50]  : Пока Олег копирует, Антон, можно вопрос вставлю? Вы там упомянули дрозофилу и Хинтона. Можете пояснить, что там про дрозофилу и Хинтона было на одном слайде вашем, где-то ближе к концу? 

S03 [00:58:09]  : Так, момент. Я об этом не говорил. 

S04 [00:58:12]  : Нет, вы не говорили, у вас было на слайде. Я под слайдом внизу написал упоминание Дарзахи. 

S12 [00:58:16]  : Это Борис Злотин там писал. 

S03 [00:58:20]  : Я взял его презентацию. Ничего не могу прокомментировать. 

S12 [00:58:25]  : То есть, это не моя информация. Я ее просто не убрал. А что там написано? 

S03 [00:58:30]  : Можете сказать? Сейчас я посмотрю. Секунду. Так. 

S12 [00:58:39]  : Что у нас тут было? Еще вопрос. 

S03 [00:58:45]  : Так, а где это было-то? 

S04 [00:58:47]  : А вот где-то внизу, слева внизу, перед одним из последних слайдов, которые вы показывали. Вот вниз у нас дальше были пиццы. Вот где-то вот после пиццы, в районе вниз, где-то внизу слева было написано про Дрозофилу Вихинтона. 

S03 [00:59:06]  : Нет, у меня такого нет. Там на мне стены нет. Давайте я сейчас включу еще раз. Хорошо. 

S04 [00:59:15]  : Давайте тогда пока... Посмотрите, может быть, давайте пока вопросы позадаем, чтобы время не терять. Сейчас давайте я пойду. Во-первых, вопрос. Антон, можете пояснить, что такое проблема зависания? Просто, видимо, не все в курсе с этой проблемой. 

S03 [00:59:38]  : Получается коллапс. Иногда это связано с проблемой XOR еще. То есть, сеть перестает обучаться, она висит, ничего не происходит. Это уже вопрос к программированию и к тем системам, которые ее обучают. Как-то больше, наверное, я здесь четко-то не скажу. То есть, когда начинаем обучать, ничего не происходит. То есть, получается к тому, что есть название. Мы не получаем ни глобального, ни локального минимума в сети. Мы болтаемся как бы на некотором плато и никуда не выходим. То есть сеть не учится. У нее получается туда-сюда на нашем шаге и нет остановки. То есть происходит оптимизационная функция, мы считаем, по ошибке. Когда у нас ошибка не растет, не уменьшается. Она остается на этом уровне и мы ничего сделать не можем. Мы должны это дело остановить и запускать другой порядок обучения, строить другую сеть, либо начальные веса менять. То есть вот об этом речь. 

S04 [01:00:58]  : То есть, у нас градиентный спуск не спускается никуда? Не спускается, да. Понятно. Не спускается. Хорошо. Ещё вопрос. Я на самом деле задал его, когда вы стали говорить про множество нейромедиаторов. И у меня возникло ощущение, что пойдёт речь о чём-то похожем на модель Алексея Рядозубова. которая построена тоже на основе бокодирования информации большим числом медиаторов, но потом я увидел, что все-таки немножко не то. Вы вообще как-то знакомы с его моделью? Можете прокомментировать? 

S03 [01:01:30]  : Нет, с моделью не знаком. Дело в том, что буквально вчера посмотрел его видео как раз. где он об этом говорит про работу мозга, а услышал-то я о нем только когда Олег на прошлом вашем вебинаре представлял вот эту сеть, и вы как раз на него сослались, и я ради интереса послушал. Или именно с моделью редкозубов я не знаком. 

S04 [01:02:01]  : Понятно. Но у него немножко по-другому все. Тоже, на самом деле, уходит, по-моему, еще дальше от стандартного подхода, чем вы. Хорошо. Теперь вот здесь есть комментарии от Игоря, но у Игоря еще была рука. Игорь, пожалуйста, у вас рука была поднята. Может быть, вы сейчас зададите те вопросы, которые у вас есть. 

S07 [01:02:23]  : Да, добрый день. 

S04 [01:02:25]  : Слышал? Да-да-да, слышно, пожалуйста. 

S07 [01:02:29]  : Спасибо за презентацию. Одной из главных задач мощного обобщения — это обобщение предсказаний на новых данных. В обычных сетях для этого используется синапс, который смотрит на наши разные входные инпуты. И в теории он должен найти такие коэффициенты, которые будут использоваться для правильного обобщения. И когда нам придут уже новые незнакомые данные, на основании этого веса мы знали, что с ними делать. Например, если наш результат не меняется от данных 2 и 10, то мы вышли на какой-то большой вес. отсекли результат активирующей функции, и потом нам неважно, приходят туда 20, 30 или 100, у нас есть предсказание, обобщение, и мы знаем, что с этим делать. Насколько я понимаю, у вас в сети используются интервалы для этого. То есть мы… наш заданный, грубо говоря, input, наши данные, они уже развиваются на какие-то заранее заданные интервалы, и для каждого из этих интервалов идет какой-то процесс обучения. То есть в зависимости от того, в какие интервалы попали наши данные, эти веса будут использоваться. Тогда вопрос в том, как, например, эти интервалы строятся, как они определяются, и будут ли они меняться в ходе обучения. Потому что, как по мне, то мы тогда уходим в такую задачу, что мы должны правильно определить какие интервалы, потому что если мы будем работать над задачей, где, грубо говоря, от этих интервалов может как-то зависеть наш результат, то мы можем упереться в ту же задачу, если наши данные будут попадать в этот интервал. И это пока один вопрос. Следующий вопрос. Как пробовали ли вы обучать вашу сеть на данных, которые в некоторой мере являются уникальными? То есть хорошее средство для такого есть игры. Когда к нам есть игры, там, грубо говоря, данные рассчитываются на основании какой-то сложной логики, физики или тому подобное. И каждый раз у нас хоть данные и уникальные, но они похожие. И, грубо говоря, можно туда же отнести задачу, когда к нам приходят данные с большим количеством шума. То есть в данном случае наша задача будет тогда не, грубо говоря, запоминать данные и строить веса для этих данных, а наоборот большую часть данных отбрасывать. И лишь для небольшого участка данных делать, грубо говоря, Наша сеть должна работать, делать результаты и обучение лишь на небольшом количестве данных, а большую часть она должна откидывать. Пробовали ли вы обучать вашу сеть на таких данных? 

S03 [01:05:20]  : Хорошо, Игорь. Давайте по порядку. Первый вопрос, что касается интервалов. Это процесс исследования. То есть он идет. Пока то, что удалось добиться, что, например, восемь, девять, десять, одиннадцать, двенадцать, и до шестнадцати интервалов никак не влияет на результат под тем задачей, которые мы пробовали делать. Поэтому мы остановились на восьми. И шесть иногда тоже хорошо работает. Исследования ведутся. Тут еще работы очень много. там какие-то максимально сложные задачи еще не решали. пока тестируем и пытаемся понять вообще как это все работает, но результаты такие обнадеживающие. 

S07 [01:06:06]  : Извините, можно вопрос? Пробовали ли вы те же самые данные, для которых ваша сеть обучается, переводить в разные системы обучения и смотреть, как это работает на интервалы, в разные системы исчисления смотреть, как в зависимости от этого работает количество интервалов? Потому что, грубо говоря, как по мне, если мы работаем с двойничной системой исчисления, максимум количества интервалов нужно нам 2. Для десятиричной – 10, для не десятиричной – 16 и так далее. Так как мы должны строить данные в зависимости от состояния нашей системы исчисления. Ну, в общем, грубо говоря, пробовали ли вы проводить такие тесты? 

S03 [01:06:46]  : я сам не пробовал, это тогда надо задать вопрос коллегам, потому что те тесты, которые делали мы, мы делали это вот, например, для картинок, это были градации серого, например, 255, соответственно, это была, ну, десятичная система, интервалы разбивались, да, если вы там будете подавать нули к единичке, естественно, вам достаточно будет двух интервалов, но это опять же вопрос тестов. Работ на будущее достаточно много. Ответил на вопрос, да, Игорь? Ну, как смог, да. 

S01 [01:07:23]  : Можно я добавлю просто какой-то момент? Смотрите, сама политика, скажем так, компаний, руководства и, в принципе, создателей. Во-первых, надо понимать, что компания Progress Inc. это, наверное, первая такая уникальная компания, когда компанию руководят именно создатели технологий, а не кто-то из менеджеров там типа, то есть именно люди, которые придумали эту систему, создали и так далее. Вот. И суть вопроса, точнее, ответа, ну, самого руководства, то есть Бориса Злотина и генерального директора Владимира Просяника, да, заключается в том, что если, то есть мы не против, скажем так, экспериментировать с любыми там, да, но опять же мы прекрасно понимаем, что нам все это нужно время там, да, чтобы проводить какие-то эксперименты и так далее. Мы просто предлагаем, во-первых, открытые, то есть, ну, открытые уже доступы, то есть, ну, на минимальной лайт-платформе, чтобы вы могли в любой ситуации испробовать нашу систему. Там, да, ссылочку я на нее отправил в общий чат, да, то есть, вы можете посмотреть, авторизоваться там, да, и... То есть спокойно опробовать её на любых своих задачах, то есть какие решайте вы. Сравнить её с вашими машинами, с вашими лошадками, с вашими там, я не знаю, кем ещё там, да, то есть с ракетами, там, космическими кораблями там, да, просто всё познаётся в сравнении. Но просто надо понимать, что у команды именно на то, чтобы вот развлекаться, там, ну, помимо Ирисы, делать ещё какие-то на фотках, на эйпегах, там, так далее, не особо много времени. То есть есть, когда вот конкретно мы там связываемся, да, вот сейчас, предположим, есть у нас идет переговорный процесс с одной крупной американской медицинской компанией, я сейчас не буду рассказывать, да. Если они откроют свои, скажем так, задачи, которые они решали с помощью нашей сети, тогда это станет действительно, в общем, доступным данным. сведениями. Пока у нас нет возможности передавать эту информацию, соответственно, мы не можем это светить. Но всю жизнь проводить вот в этих экспериментах, то есть что-то кому-то доказывать по скорости, да, это тоже, по-моему, не сильно хорошее, то есть, ну, именно использование времени своего. Мы делаем платформу, сейчас мы как раз вот платформа Омега Сервер на ее основе. мы создаем непосредственно нейрокомпьютеры. То есть он будет нейросуперкомпьютер. То есть у него будет вычислительная мощность такая же, как у стадионного, который занимает стадионы, но он будет умещаться в двух ящиках. Об этом заявлено, это будет сделано. И все идет сейчас по плану, который как раз отражается на этом сайте. То есть вы можете спокойно, в качестве, если вам спортивного интереса, следить за этой ситуацией, участвовать в этом и так далее и тому подобное. Что-то взять из кода, попробовать там на своих каких-то задачах порешать. Сейчас у меня один вопрос. Антон, я имею в виду Колодин, видео прошло у вас? 

S04 [01:10:02]  : Да, видео прошло, вот давайте сейчас, значит, я попробую запустить, и вы посмотрите, и пойдет ли звук или нет. Да, давайте. 

S01 [01:10:11]  : О, слышно звук отлично, замечательно, экрана не вижу. 

S04 [01:10:15]  : Да, хорошо, ну давайте тогда, значит, сделаем небольшую паузу, посмотрим чуть-чуть видео. Да, давайте перейдем к видео. А потом, может быть, вернемся к вопросам. Так. Так, видно мой экран, да? 

S01 [01:10:35]  : Да, видно. 

S02 [01:10:39]  : Перед нами пример PWA-приложения с чат-ботом с использованием нейронной сети типа PNET. Обучающая выборка состоит из семи текстовых документов. Каждый из них определяет тематику диалога и представлены в формате вопрос-ответ. Процесс работы приложения разделяется на два этапа. На первом этапе происходит классификация вопроса по заданным тематикам. После этого нейронная сеть типа Пинет в связке с рекуррентной нейронной сетью формирует сигналы, которые впоследствии будут преобразованы в готовый ответ на вопрос. 

S05 [01:11:22]  : Продолжение следует... 

S04 [01:12:14]  : Ну, у меня, если позволите сразу комментарий, что на самом деле вот такое видео, наверное, можно создать большим числом способов, в том числе за хардкодить, то есть, как таковое видео, наверное... Вы посмотрите другие просто. 

S01 [01:12:28]  : Да-да-да, давайте-давайте. Мы сейчас будете именно обвинять создателей видео в том, что они липу слепили? Что-что? Вы создателей видео будете обвинять в том, что они липнут? 

S04 [01:12:42]  : Не-не-не, я просто говорю, что на самом деле, мне кажется, я допускаю, что как бы вопросы по существу, они будут более интересные. Ну давайте еще одну посмотрим. Я просто даю Рима комментарий на вот впечатление от этого видео, что вот в данном случае... Перед нами PVE-приложение для распознавания жестов и изображений. 

S02 [01:13:00]  : Изображения создаются путем рисования с помощью модели, которая определяет положение ключевых точек пальцев руки и нейронной сети типа PINED. Процесс обучения моделей происходит в два этапа. На первом этапе обучается модель для распознавания жестов START и STOP, а на втором этапе происходит обучение модели для распознавания изображений на базе обучающей выборки MNIST. Для начала отрисовки изображения рукой необходимо показать соответствующий жест, который будет сигнализировать о начале рисования. После этого модель начнет следить за ключевой точкой указательного пальца, и таким образом траектория движения пальца будет повторяться на самом изображении. После окончания отрисовки числа необходимо подать жест в виде знака STOP, после чего автоматически начнется распознавание нарисованного изображения нейронной сетью типа PINET. 

S03 [01:14:06]  : Пока идет тут видео, я добавлю, да, Антон? Меня слышно? Да-да, слышно. Все вот эти примеры, их можно реализовать, все правильно вы говорите на других, нет проблем. Да, это простенькие задачи для нейросетей, и с ними справляются и другие нейронные сети. Но вся фишка-то здесь не отражена, вся фишка в том, что они работают быстро. То, о чем я говорил. Да, реализовать можно, проблемы нет. Тут задачки прямо простенькие. Но возможности показаны, да, что мы можем делать то же самое, что могут другие идеи. 

S02 [01:14:42]  : Перед нами пример PVE-приложения для классификации эмоций человека с использованием нейронной сети типа PNET. Обучающая выборка состоит из семи классов, которые отображают эмоции человека. Среди них страх, удивление, счастье, грусть, злость, отвращение и нейтральное состояние. Процесс анализа эмоций заключается в определении ключевых точек лица, после чего положение основных частей лица анализируется нейронной сетью типа Пинет и классифицируется согласно тем классам, которые были определены. 

S01 [01:15:30]  : Это сотрудник компании. 

S02 [01:15:42]  : Перед нами пример PVA-приложения для распознавания лиц с применением нейронной сети типа PinNet. В качестве обучающей выборки использовано изображение лиц 7 персон в количестве 30 штук для каждого класса. Процесс обучения модели происходит путем вырезания самого лица из изображения, после чего само изображение лица уже подается на саму модель для классификации изображений. PWA-приложение может работать в двух режимах – в режиме классификации изображения и режиме анализа видеопотока. Перед вами примеры использования PWA-приложения в этих двух режимах для разных персон, представленных в обучающей выборке. Перед нами PVA-приложение для классификации позруки, в котором используется нейронная сеть типа PNED. В качестве обучающей выборки было выбрано 5 классов. 1, 2, 3, 4 и 5. В каждом классе по 30 изображений для определения позруки соответственно этим номерам. Приложение может работать в двух режимах – в режиме анализа изображения и анализа видео. Для того, чтобы начать анализировать видеопоток, нужно нажать кнопку «Webcam» и кнопку «Start». После этого приложение начнет извлекать из видеопотока с определенной периодичностью изображения, которые будут анализироваться соответственно тем классам, которые были определены в обучающей выборке. В данном конкретном примере мы видим, как при изменении положения пальцев руки и позы руки изменяется класс, определяемый приложением. Соответственно классом 1, 2, 3, 4 и 5. 

S04 [01:17:51]  : Подскажите, трассировка, которая там происходит, эта трассировка делается Вашей сетью, правильно? Опс, сейчас извиняюсь, что-то закрылось. 

S03 [01:18:07]  : Трассировка это отдельная история. 

S04 [01:18:12]  : То есть трассировка делается не сейчас, я пока возвращаюсь в документы. 

S03 [01:18:16]  : В данном случае нет. 

S04 [01:18:20]  : На вход подается уже протрассированное изображение, да? Да, да, да. Так, документ сейчас куда-то... Так, сейчас куда-то я вывалился. Так, Show in Finder. Так, значит, было Show in Headline. 

S02 [01:18:35]  : Обратите внимание... Для распознавания тональности текста с использованием нейронной сети типа PINED. В качестве обучающей выборки используются новостные заголовки в области финансов. Новостные заголовки разделены на три класса – позитивные, нейтральные и негативные. Для того, чтобы начать анализ заголовка, необходимо скопировать его и вставить в само приложение. После этого нажать кнопку Start. Приложение проанализирует заголовок и определит его тональность. Также можно увидеть, как изменяется класс текстового заголовка при изменении хотя бы одного слова негативного характера на слово позитивного характера. Далее можно увидеть работу приложения для разных текстовых заголовков. Перед нами пример PVA-приложения для классификации позы тела с использованием нейронной сети типа PNED. В качестве обучающей выборки использовано 4 позы по 20 изображений каждая. PVA-приложение может работать в двух режимах – в режиме распознавания изображения и в режиме анализа видеопотопа. Модель распознаёт ключевые точки частей тела и на основании этой информации распознаёт саму позу. Перед нами пример использования PV приложения в режиме анализа видеопотока, где изображения извлекаются с определенной периодичностью и анализируются по вышеизложенному алгоритму. Перед нами пример PVE-приложения для классификации изображений, в котором используется нейронная сеть типа PNED. В качестве обучающей выборки выбрано 4 класса – перец, яблоко, киви и лимон. Для каждого класса использовано по 20 изображений, процесс обучения происходит менее чем за одну минуту. PVA-приложение может работать в двух режимах – в режиме анализа изображений и в режиме анализа видеопотока. Перед нами пример использования PVA-приложения в режиме анализа видеопотока, где перед веб-камерой мы показываем объекты, которые были обозначены в самой обучающей выборке. 

S04 [01:21:33]  : У вас проблема инвариантности к размеру решается? 

S02 [01:21:43]  : Перед нами пример приложения для распознавания голоса и тональности человека с использованием нейронной сети типа PINEC. В данном приложении PINED используется в качестве единственной нейронной сети. Обучающая выборка состоит из голосов 6 человек, где для каждого класса использовано по 500 сэмплов примеров голосов выбранных людей. Процесс анализа голосов происходит с помощью алгоритма быстрого преобразования Fourier. Результаты преобразования напрямую подаются на нейросеть типа Pinet. В свою очередь, Pinet анализирует эти данные и классифицирует звуковой файл согласно тем классам, которые были определены в обучающей выборке. Посмотрим, сможет ли приложение определить мой голос. Перед нами пример PWA-приложения для распознавания речевых команд с использованием нейронной сети типа PNET. В качестве обучающей выборки используется 8 классов речевых команд. В каждом классе используется по 500 звуковых файлов длительностью в одну секунду. Процесс анализа звука происходит с помощью алгоритма краткосрочного преобразования Fourier. Результатом этого преобразования являются изображения спектрограмм, которые подаются на модель для распознавания изображений. Yes. Up. Stop. 

S04 [01:23:37]  : Хорошо. Наверное, мы можем вернуться к вопросам, да, Олег? 

S03 [01:23:42]  : Можно я прокомментирую сейчас вот эти все видео, да? самое интересное, что здесь есть я понимаю, что все это можно реализовать это не сложная задача, не реализуемая еще раз повторю это сделано однослойной сетью однослойной сетью другими однослойными сетями вы таких показателей не добьетесь за счет того, что у нас сеть однослойная за счет очень простой математики она работает быстро Это вот самое главное, что надо делать. Она работает быстро. 

S01 [01:24:17]  : И масштабировать вы её можете в сколь угодном количестве. 

S04 [01:24:20]  : А вот хорошо, может быть, Игорь Пивоваров, может быть, вот у тебя было много замечаний, вопросов, может быть, ты сейчас их сможешь изложить? 

S09 [01:24:35]  : Да, Антон, спасибо. Добрый вечер. Антон, велико вам спасибо за доклад. Антон, спасибо за слово. Я, конечно, не уверен, что я... Ну, кто-то должен... Прошу прощения, честно скажу, цель нашего визита сюда, я уговорил Антона потратить на это время, чтобы как раз это все услышать. 

S01 [01:25:12]  : Чем больше, тем лучше. Во-первых, мы хотим видеть картину реальной, как вы ее видите. Вы профессионалы в этой теме. Предположим, для меня большинство того, что рассказывал Антон, я знаю немножко по презентации, все остальное, как говорится, птичий язык. Честно, я не ученый, я не специалист по искусственному интеллекту. Я просто инвестор, да, и я вижу просто, как выполняется это. Поэтому я хочу услышать максимум того, что вы скажете об этом. Понятно. Спасибо. 

S09 [01:25:42]  : Смотрите, на мой взгляд, здесь ситуация такая. Модель понятна математической, но мне кажется, что она более-менее понятна. И я там, собственно, в чате написал, что, на мой взгляд, это ничем не отличается, это немножко отличается от того, если бы вы вместо одного нейрона со многими, как вы называете, со многими уровнями, интервалами, нейромодуляторами, Взяли бы просто несколько нейронов параллельных. Ваша концепция такая, что недостаток сегодняшних сетей, что там есть одна один вес, как бы одна связь, один вес, и его там надо корректировать. Вместо него выводите много весов. Ровно то же самое можно было сделать вместо одного нейрона, сделав несколько параллельных. У каждого из них одинаковый выход, а от каждого из них разный вес. Ну, Юрий Бобуров это называет то, что сделали, условно говоря, не однослойную сеть, а там N-слойную. Как бы вместо одного слоя у вас реально N слоев. Но существенное отличие вашего, в отличие от того, что делаете сейчас, это фактически у вас бинарная сеть. У вас там нету тонкой настройки. У вас, грубо говоря, эти ветки работают типа 0,1. И это вполне понятно, но это резко сужает класс задач. То есть, фактически, как мне кажется, ваша модель Это такой, если хотите, быстрый классификатор, который будет очень быстро настраиваться на какие-то конкретные признаки и действительно без большого обучения. Но выучившись каким-то очень простым признаком, эта модель нельзя больше нигде применять. Вот в чем главное достоинство сегодняшних сетей глубокого обучения, при том, что я большой критик глубокого обучения, но тем не менее, в том, что во внутренних слоях, начиная с третьего, третий, четвертый или предпоследние слои, давайте так назовем, в сети глубокого обучения, это, как правило, слои, в которых нейроны представляют из себя так называемые фичи. фича – это, грубо говоря, вы показываете картинку кошечки. В вашей модели у вас очень однотипные картинки кошечек, скажем, были, те, которые я видел, и в этом сила и слабость. За счет того, что вы не перемножаете амплитуду на веса у вас, ну, грубо говоря, это сильно зависит от входного изображения, но если входное изображение, условно, ушки там есть, вместо Эйфелевой башни, у которой шпиль, значит, то эти ушки, они плюс-минус там всегда будут давать примерно свой этот свой этот сигнал но кошку от собачки таким образом отличить будет невозможно а в глубоких сетях там многослойных как раз там формируются некоторые нейронные но на этом я не буду теорию глубокий стиль рассказывать просто принципиально разницу в которых которые грубо говоря и включаются там если это условно там кошка уха там и типичная кошкина или ухо собачкина, но с другой стороны, если даже это одинаковый он перепутал, то есть там другие нейроны, которые излечил нос или хвостик или еще что-то, и их комбинация работает. В этом и сила, и особенность глубоких сетей. Ваши сети в этом плане, они сильно проще, они, конечно, сильно быстрее, в этом и главное достоинство. но они, как я понимаю, будут сильно ограничены в применениях. Например, если вы это используете действительно для голосовой колонки, которая должна распознавать 10 команд, и ее можно на одного человека достаточно быстро учить, на голос одного человека научите как первые телефоны работали она прекрасно будет я думаю что работать действительно 10 голосовых команд будет делать но я думаю что если вы возьмете там разные разными голосами потом будете говорить что я сомневаюсь что будет сильно это сильно работать к сожалению или к счастью там не знаю Ну и, конечно, я не могу не отметить, что мы переписывались с Юрием Бабуровым и с другими коллегами, что, конечно, на научной конференции, наверное, можно показывать то, что вы делаете в матлабе, но в профессиональном сообществе людей, которые занимаются сетями, матлабе это чисто я пишу то есть вот все примеры которые сделания сделаны по-другому вы не ну это бы это прям не стоит покажет потому что это но как бы знаете все сразу понимает что вы не в теме вопрос надо писать код на питоне. вы не можете написать на питоне, потому что вы не пользуетесь стандартными фреймворками, но тем не менее, если бы вы написали на питоне те же соображения, что и на MATLAB, то сравнить это было бы на порядок проще. и сравнивать это надо с моделями на питоне написанными, а конечно не с IBM SPSS. нет, MATLAB это чисто мое и все 

S03 [01:31:28]  : Выкладки сделаны как раз... Антон, можно маленькую вставочку сделать? 

S01 [01:31:33]  : Во-первых, если мне память не изменяет, хотя я тоже там, да, то сама вообще-то все это написано именно на питоне. То есть работа с нашей платформой идет как раз именно на питоне. Вот это раз. Второй момент. Вы так сказали, что это единственное какое-то преимущество, что типа 10 этих, и я-то с вами целиком не согласен, потому что, я не знаю, заметили вы, не заметили, да, то, предположим, если мы начнем любую сетку, там, из того, что есть глубокое обучение, глубокое обучение, притом по поводу глубокого обучения, кто вам мешает, предположим, из этой сетки сделать 2-3 слоя, там, сколько угодно слоев там, да? 

S09 [01:32:08]  : Вот этого я как раз не понял. Вот это был мой вопрос. 

S01 [01:32:11]  : Секундочку, секундочку. Я договорю, ладно? Договорю, договорю. Просто если вы обратили внимание, что скорость обучения пана, да, то есть она не падает никак. То есть она вот как шла по прямой, она так и будет идти по прямой. И при том любого объема данных, любого объема данных. Чем сложнее задача, тем быстрее пан будет обучаться. А у меня встречный вопрос. 

S09 [01:32:29]  : Тем точнее он будет работать. А на каких объемах данных вы реально тестировали и на каких объемах И на каких скоростях вы тестировали ее? 

S01 [01:32:39]  : Я еще раз повторюсь. Смотрите, я уже все написал в чат. Возможно, мы можем запросить, и это будет открываться по мере того, как наши будут проводить тесты или наши партнеры будут давать нам разрешение на раскрытие какой-то информации. Мы будем опубликовать эти тесты. На сайте Омеги это раз. Второй момент. Еще раз повторюсь. Вы можете открыть платформу и попробовать любую свою задачу решить на этом. Любую. Добавить объем данных и посмотреть, в чем разница между той машиной или той. Здесь IBM-то он такой непринципиальный. Просто, видимо, большинство экспериментов проводилось, если не ошибаюсь, еще вообще, по-моему, в 2013 году, когда было это открыто. 

S09 [01:33:19]  : Вы поймите, вы очень убежденные люди, конечно, в своей технологии. 

S01 [01:33:23]  : Нет, я в данном ситуации просто пытаюсь рассказывать то, что я знаю сам. 

S09 [01:33:29]  : Я студентов, когда со студентами работаю, и когда они показывают свои презентации, я, значит, за эти презентации их там критикую. я же говорю вы же выбрали картинки вы выбрали примеры то что показать на сайте не я это сделали вы то есть как бы вы этим какой-то месяц несли ну вот вы как бы как-то это донесли значит надо поработать над тем что вы представляете эти у нас физики принято такая шутка есть каждый экспонента это это сегмой да который не дожила до своего насыщения Но в этом смысле человек, который говорит, что мы тестировали это на любых объемах данных и не видно падение производительности, я вам честно скажу, вы не дожили до того объема данных, на которых падение производительности будет видно. Это просто очевидно. Я же как физик понимаю, что не бывает… Ничего линейного в этой жизни вообще. Всегда есть насыщение. Просто вы возьмите больше данных в 10 раз, а может быть в 1000, и вы увидите падение производительности. Поверьте мне, как бы бессмысленный спор. Нет, это однозначный факт. Ну, если хотите, я как физик-теоретик, квалифицированный, вам это утверждаю. Если вдруг она у вас не будет в насыщении, вот если вы сейчас ее в 10 тысяч раз данные увеличите, то с меня бутылка хорошего вина. Все остальные свидетели. Я запомнил. Обязательно запись передам. Давайте я вопрос задам. 

S04 [01:34:57]  : Давайте все-таки вопрос. 

S09 [01:35:02]  : Антон, у меня сложилось впечатление, глядя на вашу модель, что как раз двух слоев там быть не может. Вот я понимаю вашу математику, когда сеть однослойная и процесс обучения, а два слоя и больше там может быть разум? 

S03 [01:35:19]  : Оно может быть, да, но это еще не пробовалось. 

S09 [01:35:23]  : А как вы тогда будете корректировать свои? 

S03 [01:35:26]  : Надо думать. Сейчас я не готов на это ответить. Я понимаю этот вопрос. Меня тоже беспокоит. Я с этой сетью знаком буквально с марта месяца. Поэтому я глубоко просто не смог еще оперироваться. Но этот вопрос у меня возник в первый же день моего знакомства с этой сетью. Собственно, как и у вас. Я вас понимаю. 

S09 [01:35:47]  : Я думаю, что на самом деле здесь сильно можно не мучиться. Я на 95% уверен, что эта математика не будет работать для двух слоев и больше, потому что тогда ваша основная преимущество как раз там и потеряется. Но зато ее можно решать. Есть ведь известная история, что почти любую глубокую сеть можно смоделировать широкой. Вы в ширину, на самом деле, вам надо расти, и тогда ваше преимущество останется. 

S03 [01:36:14]  : Возможно, но есть мысли, я просто не пробовал. Надо пописать формулки и посмотреть, как это будет распространяться. Мне кажется, что можно сделать с той же математикой. 

S09 [01:36:27]  : Это пока открытый вопрос. Я бы сказал так. Отвечая всем, у нас там в чате целый холивар разгорелся. Я, к сожалению, не успеваю читать. Прекрасно, что вы не успеваете читать, потому что мы люди бессердечные и написали всякой гадости. Я к тому, что просто надо понять область применения вашей модели. Она не такая уж. У нее есть свой фокус и в этом фокусе она будет нормально существовать. Но просто не стоит делать сравнение и выходить за рамки. Те, где вы столкнетесь с очевидной критикой, которой вы не сможете ничего противопоставить. У меня такая рекомендация. 

S01 [01:37:21]  : Игорь, у меня просто сразу небольшая реманточка. Смотрите, как сам ПАН, то есть руководство компании прогресс, насчитало как минимум 30, по-моему, они там в легкую список составили, индустрии, в которых будет применяться эта сеть. И она будет именно там применяться. То есть у нас, грубо выражаясь, масштаб идеи такой, а Борис Злотин, ну если вы его немножко там кто такой вообще товарищ Борис Злотин, его команда, то вы поймете масштаб идеи какой. Масштаб такой, что тем самым просто PAN должен быть везде, где речь идет об искусственном интеллекте. То есть это просто другие скорости, абсолютно другие скорости всего. Обучение, переобучение, дообучение, забывание, там, я не знаю, сложности решения задач, точности решения задач и так далее, и так далее, и так далее. То есть я сейчас понимаю, то есть скептизм многих из вас там, да, но я вот просто, кстати, ну, Пузырёк мы уже поспорили, да, хоть я не пью, но я с удовольствием передам его Борису в Америку. Он ром хороший пьет на всяких пожарных, по 30 капель, но хороший ром. И я буду рад именно показывать сообществу, как мы продвигаемся, что мы достигаем. Поэтому сейчас мы как раз и создали платформу, которая нам позволяет именно создать то, о чем мы заявили, то есть суперкомпьютер. 

S04 [01:38:41]  : Хорошо. Удачи вам. Хорошо. Спасибо. Так, у нас есть еще было много комментариев от Юрия Бабурова. Юрий, у вас есть готовность обозначить вопросы, если у вас есть? 

S13 [01:38:56]  : А если вопросов у вас нет… Да, я, во-первых, да, чуть-чуть подкорректирую, да. Значит, я, в принципе, понял, как может быть многослойная сеть в подобной структуре. Ну вот вопрос был первый. Вы вроде как даже где-то показали, что два слоя дает 0.8 у вас. В одной из демок вначале показывали. 

S04 [01:39:21]  : Да, когда у вас были... Антон, когда у вас были связи с питоном, может быть вы к ним вернетесь, действительно там сначала было где 0.9 у вас на однослойной сети, а потом вы переключились на двуслойную, и на двуслойной вдруг стало хуже. Вы поняли вопрос? Вас не слышно, Антон. 

S03 [01:39:46]  : Антон, микрофон включите. Да, да, да. Я вопрос понял, но нет, двухслойной сети не было. Двухслойной сети ПАН не было. Была сеть, где МНИСТ, но там использовался просто входные слои сверточные и последний слой был Пинетовский и только после него сравнивалось. То есть многослойной сеть Пинетовской не было, нет. 

S13 [01:40:15]  : Вот это вот результат 0.8, а результат 0.8 на одном слое. Это тоже был сверточный слой потом или нет? 

S03 [01:40:24]  : Это сейчас в конце. Сейчас я попробую, подождите. Можно я включу, да, презентацию? 

S04 [01:40:29]  : Да, включайте, конечно. 

S03 [01:40:30]  : Может быть вот к Питоновским. 

S04 [01:40:36]  : Там внизу должно быть где-то. Вот оно. 

S03 [01:40:40]  : Да, да. вот об этом речь то идет я правильно понимаю да вот это вот и вот это слайда 0 8 до 1 слой вот смотрите то есть первые слои они были одинаковые исследовался именно последний слой то есть это как подготовка образов для Здесь полносвязный сравнивался со слоем пинет. То, что до этого, они отрабатывали одинаково и как бы были входными данными для полносвязного слоя, а здесь для пинета. И пинет здесь однослойный. 

S13 [01:41:25]  : Спасибо. А теперь могу я показать вам слайд? 

S04 [01:41:30]  : Да, Юрий, пожалуйста, давайте. 

S13 [01:41:41]  : Смотрите, вот типичная нейросеть, вот эта двуслойная нейросеть с какими-то параметрами, вот это типичный результат ее на месте. Вот я ноутбук отправил в чат и потом еще дам ссылку всем желающим. Запускается очень просто. Просто берете ячейку по порядку, она запустилась. checkop запустил. сможете выбирать параметры, какие-то условия подобавлять. а у картиночек миста поставить там... вот я поставил, если два слоя, добавляет второй слой, если там очень глубоко еще добавляется 40 флайов, не знаю сколько. с параметром layers 1 односвязанное, layers 2 двусвязанное. По умолчанию также включены штуки, чтобы нейросети были. Можно сравнить с этим. Вот. Эта нейросеть на одном слое выдаёт 0.9. Для неё типично. Вот это сейчас тоже вариант с двумя слоями, там с немножко другими параметрами. Можете её там с одним слоем запустить. Выдаст 0.9. Тренироваться будет две минуты на CPU, на 50 тысячах вот этих цифр, прогнанных за 10. можно поставить ленинрейт побольше, результат будет похуже, в зависимости от параметра, может быть совсем ужасно. да, действительно, вы правы, в классическом режиме ленинрейт равно 1, нейросети не тренируются потом и без батчей, потому что они дадут вам примерно 0.4-0.5. но вот ваша сеть, если бы она тренировалась тоже без всего, она бы тоже дала, я думаю, в диапазоне от 0.4 до 0.7. То есть это нормально для одного слоя и ровно таким образом. Никакого превосходства пока что нет. 

S03 [01:43:38]  : Я не понял, вот сравнение последнее. 

S13 [01:43:46]  : сравнение последнее с высоким лёнин рейтом мы очень быстро... у нас нет как такового шага, я так понимаю лёнин рейт это шаг, да? да, если у вас шаг один, точнее единицы деленные на количество нейронов. У вас получается как раз это аналог шагового обучения, потому что вы говорите, что вы одну тысячную меняете веса, потому что у вас тысяча нейронов. Вот у вас и получается, что ваш шаг обучения одна тысяча. 

S03 [01:44:27]  : Нет, это не корректно так сравнивать. Почему? Потому что там идет полная корректировка сразу на всю величину ошибки. Одна тысяча – это потому что тысяча нейронов. То есть мы сразу шагаем на эту единицу. Просто она распределяется на всех. Это некорректное сравнение этого шага. Learning rate здесь и шаг здесь – это разные вещи. 

S13 [01:44:57]  : Вот смотрите, в классических нейросетях ошибка не распределяется на количество нейронов. Она высчитывается для каждого веса. 

S09 [01:45:15]  : Юр, я прошу прощения, но какая нам разница? Ну что нам сравнивать с классической инерцией? Мы же понимаем разницу. Здесь скорее нужно смотреть с точки зрения компетенции. Дальше. Нужно смотреть просто на уже общие результаты того подхода или того и что он дает в тех или других случаях. 

S13 [01:45:34]  : Да, дальше смотрите. Вот последнюю штуку покажу. Вот где-то она здесь была. Смотрите, для двух слоев с правильно подобранными параметрами мы достигаем на нейросети, видите, тренировочный, геотестовый, данные которых нейросети не видела, 96-97%. Замечательно. вот значит это есть то чем нейросети лучше других технологий именно в том что они лучше обучаются вы показываете да вы вы вот по сути показывать что вот вот здесь вы уже вот здесь вот там вот на каком-то уровне вот хотя на первой эпохе уже показал 0.97, но показывает, что вы еще чуть раньше находитесь тоже на неплохом значении 0.8 или 0.7. Для этих задач тоже, да, есть применение. Теперь возвращаюсь к тому, что я хотел сказать. Для ваших вот этих задач тоже есть применение. Оно очень узкое, вот реально очень узкое, потому что практически всегда данных становится чуть больше из вашего режима, где ваша сеть показывает преимущество. превращаетесь в такой режим, где уже нейросетки могут работать от 30 примеров буквально. И нейросетки тогда начинают вас обгонять. Это вот я посчитал. 

S03 [01:47:10]  : на ходу надо ставить задачу да я не могу сравнить сейчас эти данные по моему да тоже по сравнению с нашим примером тот пример сделан достаточно давно Возможно там еще не использовалось пакетное обучение, там оптимизация сейчас тоже ускоряет немножко эти процессы. Мне кажется это некорректное сравнение. 

S13 [01:47:37]  : Смотрите, вот это сравнение. Тут эпоха, две, несколько минут. Я говорю немного о другом. Да, вот есть режим такой small data, когда у вас данных единиц буквально 10 или 20 или 50 или 100. У меня есть одно применение, один коммерческий проект, в котором я именно работаю в таком режиме, умею работать в таком режиме, обучаться. И да, я в таком режиме умею обычными нейросетками показывать хорошие результаты. Но как только мы выходим из этого режима, данных становится больше, например, в ходе накоплений, в процессе работы, то сразу мы переходим в другой режим, где ваша нейросетка уже неконкурентна. Вам на это нужно найти решение. Потому что вот этот вот кейс, когда данных очень мало, он очень-очень узкий. 

S05 [01:48:33]  : Вы говорите, что у вас есть направление, 

S13 [01:48:38]  : там вот в каждом из этих 30 направлений ваша ваша сетка будет не конкурентны как только вы выйдите за эти там 

S01 [01:48:48]  : Голословное такое утверждение. Откуда такая уверенность? Может еще на пузырек надо забить? 

S13 [01:48:57]  : Смотрите, я этим направлением занимаюсь достаточно давно. 

S01 [01:49:00]  : Так у нас тоже не первый год в искусственном интеллекте. 

S03 [01:49:04]  : Можно я прокомментирую, господа? Юрий, в самом начале я показывал, в самом начале презентации, Сравнение со статикой, где обучалось на 7000 образов, где статика уже подвисла, и был эксперимент на 100000 образов, который показал достаточно хороший результат. Это на одном из первых слайдов. то есть то, что я вам показал примерчики, да, они в итоге простенькие. вы все правильно говорите. и на простеньких примерах у нас будут сопоставимые результаты. потому что когда речь идет о полсекундах, шестьдесятых секундах и так далее. смотрите, еще раз. 

S13 [01:49:45]  : да, у вас есть еще один режим более быстрого обучения. согласен, это другой режим. то есть в этом режиме вы готовы поконкурировать с другими. сетками и так далее, не в качестве, а именно в скорости обучения. 

S01 [01:50:01]  : При такой постановке задачи... Скорость отклика какая будет у вас? Быстрее пана. Хорошо. Именно качество решения, именно качество волнует. Мы за что вообще? 

S13 [01:50:13]  : Мы говорим об классических, намного выше пана. Откуда вы знаете? Просто по построению. Знаю, потому что много раз подобные проекты, типа вашего, пробовал. 

S01 [01:50:25]  : Подобные это вот какие? Можно примеры скинуть сюда? 

S13 [01:50:28]  : Хокинсовый, например, htm, я вот в чат скидывал. с чем они схожи с паном там 

S01 [01:50:42]  : очень похожий первый слой как раз вот нет нет у нас принципиально новый другой нейрон формальный вот просто вы обратите внимание не слои там не вот это все нейрон другой это совсем все другое все другое точности нет смотрите у кого есть такой нейрон который мы показали 

S13 [01:51:03]  : Почитайте работу, на которую я ссылался. 

S01 [01:51:09]  : Вы же такой там нейрон, да, применяете? Или просто, как его называется? Я просто не специалист. С одним синапсом там, да? 

S13 [01:51:18]  : Я лично дебажил такие же нейроны, как у вас. 

S04 [01:51:22]  : Хорошо. Коллеги, у меня есть предложение. Давайте все-таки мы дадим возможность задать другие вопросы. У нас еще много спикеров. Юрий, вы можете же, вы свои замечания высказали. Вы можете отправить ноутбук автору. Он будет посмотреть. Авторам, точнее, они будут признательны. Да, я тщательно писал, что я как только доберусь, 

S03 [01:51:47]  : надо провести сравнение. 

S13 [01:51:49]  : чтобы подготовить ноутбук с их моделью, ну займет какое-то время. а вот это вот у меня вот есть вот это вот студентов я учу про нейросети рассказывал. 

S01 [01:52:01]  : там ноутбук тоже есть. 

S13 [01:52:02]  : про их режимы работы и все прочее. вот я ссылку на этот ноутбук выложил. Вы его можете сами попробовать позапускать. 

S03 [01:52:09]  : Хорошо, спасибо, Юрий. Я понял, да. То есть информация для работы есть, есть с чем сравнить. 

S13 [01:52:15]  : Вот есть несколько режимов работы. У вас есть преимущество для одного режима Small Data и второго режима быстрое обучение. Хорошо, с этим можно начинать работать. Не очень много мест, где вот такое надо. Ну вот можно работать с этим. Все правильно. 

S04 [01:52:36]  : Спасибо, Юрий. 

S13 [01:52:38]  : Просто я хочу сказать, что когда вы показываете результаты нейросетей, которые нейросеть у вас выдает 0,8 на одном слое керосин, хотя нейросеть без всяких сверточных в начале слоев просто на обычных слоев выдает 0,9, то это какая-то вот обман прям. То есть, сравнивать надо именно с нормальным результатом нейросети. 

S03 [01:53:02]  : Мы некорректно сейчас здесь сравниваем. Абсолютно некорректно. Надо одинаково брать источники. 

S13 [01:53:08]  : Вот будут у вас примеры, вы их возьмете и посравниваете. 

S04 [01:53:14]  : Хорошо, Юрий, спасибо. Владимир Смолин, пожалуйста. Меня слышно? Да, слышно. 

S06 [01:53:25]  : У меня просто весь сеть говорят о том, что низкая пропускная способность. Я, конечно, не все слышал и, может быть, из-за этого не все понял. Я не буду обсуждать вопрос локальности каких-то глубоких идей, которые перевернут весь мир. Там это такая очень вязкая и тяжелая тема. Это каждый может по желанию обсудить. У меня как бы вот не сложилось по этому поводу впечатление. Есть, значит, про сравнение. Тоже, конечно, значит, ничего как бы убедительного особо не было, поскольку все сравнения должны быть как бы более корректны. То есть общая рекомендация какая? Что есть как бы это самое SOTA, так называемый, State of the Art. соответственно, находите такую соту, которую вы можете превзойти. И это будет для всех значительно более убедительным, чем то, что вот мы что-то попробовали, а вот сделайте какую-то задачу, которая state of the art. Ну, это как бы методическое пожелание. Хотя, повторюсь, что, может быть, просто слезть у них не очень хорошее, и я, к сожалению, чего-то просто не услышал. Понять модель, как я понял, не только мне, но и большинству слушающих не удалось. Новый нейрон с массой связи, его тяжело сравнить. Допустим, при одинаковом числе нейронов, если у него в 10 и тем более в 100 раз больше связей, то понятно, что информационная возможность у него выше. Но что положительно, что мне всегда нравится, что редко встречается, что основным недостатком, из-за чего нейронные сети долго учатся, состоит в том, что у них распределенная память. Из-за того, что разные образы друг друга на этой распределенной памяти взаимодействуют, это сильно затягивает процесс обучения. Вторую запять, я подозреваю, что я не все правильно понял, но у меня сложилось такое впечатление, что здесь как раз некоторые уход от этой распределённости совершён. То есть на разных медиаторах на одном и том же синапсе разные образы запоминаются на разных медиаторах. И всякая локализация памяти, она, конечно, ускоряет обучение. Это большой плюс. Но у меня сложилось такое впечатление о том, что рассказали, повторюсь, может быть неправильное. Речь идёт о чём? Что мы имеем 20 или 100 корреляторов, И каждый коррелятор учен за один раз. Подали образ, и этот образ на этом корреляторе запомнили. Второй образ на втором, на третьем, на четвертом, на пятом. Конечно, будет линейный рост времени обучения от числа образов. Ну, очевидно. Если у нас образов больше, чем число корреляторов, то мы подаем n плюс первый образ, если у нас n корреляторов, и смотрим, на какой он лучше ответил. И усредняем то предыдущее, которое было с этим. И все равно будет линейный рост времени обучения. Вот это Игорь Пивоваров посылал в магазин. Он шел за бутылкой, что он говорит, что не бывает линейного роста обучения. Вот простой пример. Если эта модель работает иначе, может быть, она не выиграет. А вот если построить такие корреляторы, Они будут линейно учиться и никогда кривая не займется. Будет линейный рост времени обучения. Ну и когда подаем какой-то произвольный сигнал, смотрим, а какие корреляторы на него лучше ответили. И, значит, голосуем среди первого класса корреляторов, второго класса корреляторов и, соответственно, будем ходу бедно распознавать. я не думаю, что это будет лучше, чем нейронная сеть, но, соответственно, какое-то распознавание, конечно, оно даст, что надо, собственно, и продемонстрировать. повторюсь, может быть, я неправильно понял, потому что связь была плохая, и, может, и объясняли не очень хорошо, а может, и то, и другое. значит, то, что глубокую сеть всегда может заменить широкую, Я с этим тоже, мягко говоря, не согласен, что за счет глубокой сети удается много новых свойств сигнала выявить, вот эта именно аппроксимация становится более интересной. Но в целом ситуация вот такая, что Я не думаю, что нас стремились обмануть. У меня сложилось впечатление, что можно построить аналогичную модель, которая будет работать как рассказывали. Хотелось бы, чтобы авторы прокомментировали, насколько я неправильно понял то, что они рассказывали. Видимо, в этом мой вопрос. Чем их модель отличается от набора корреляторов, который каждый локально запоминает некоторые группы сигналов или их усреднения, и потом голосует о том, к какому классу относится входной образ. 

S03 [01:57:44]  : Так, да, можно я, да? Вы все правильно поняли, вы все правильно рассказали. Связь, видно, хорошая, значит, я плохо рассказывал. Значит, так и есть. Если мы обучаем вот эти там двумстам образом, да, получаем каких-то 20 классов, да, и потом берем какой-то новый образ и смотрим, к какому классу он относится. Вот. здесь я не рассказывал, но это тоже реализовано то есть когда вы берете всех кисок там много кисок и говорите, что это один класс не разные классы, не 50 классов кисок, а один класс и тогда сеть каким-то своим внутренним чутьем вылавливает у нас вот эти паттерны, которые относятся к кискам И вот комментарий Игоря Пивоварова о том, что она киску от собак не отличит. Она отличает кисок от собак. Об этом я просто не говорил, потому что тестов проделал на самом деле достаточно много. То есть я правильно понял ваш вопрос? Так, Владимир пропал, у него что-то со связи. 

S04 [01:58:58]  : Владимир, звук. Владимир, мы вас не слышим. 

S06 [01:59:02]  : Микрофон включите. Я звук выключил, прошу прощения. А сейчас слышно? Да, слышно, слышно. Я выключил звук, а сейчас забыл включить. Вот, ну, собственно, я бы хотел, чтобы вы мне рассказали, в чем я неправильно вас понял, а вы мне рассказали, что я правильно понял. Да, вы что правильно поняли, прекрасно. Вот, ну и, собственно, может быть, в другом я вас неправильно понял, что, конечно, когда у вас Значит, много корреляторов кошек и много корреляторов изображений собак, то, естественно, среднее голосование по кошкам будет выше, чем среднее голосование по корреляторам собак. И когда мы подали кошку, и наоборот. Я думаю, что какое-то различение, безусловно, будет. Вопрос выше или ниже, чем в нейронных сетях, я думаю, что ниже, ну, по крайней мере, на современных нейронных сетях. Можно, конечно, найти какую-то плохенькую нейронную сеть. Кстати, у Рашида, если почитать, строим модель нейронной сети, Значит, министр на одном слое распознается 96% и учится на 100 нейронах. как бы сказать, уровень-то большой. Поэтому с нейронными сетями то, что вы нам данные показываете, они в общем-то... Хотя, конечно, у Рашида там тоже есть некоторые ухищрения, там он учит вообще без нулевого, в смысле вот без порога все элементы, это тоже дает некоторый эффект его модели. Ну, в общем, какие-то уловки всегда есть, которые позволяют есть, и краткий рассказ о своей модели, естественно, не дает возможности рассказать Все тонкости, которые там были, они, скорее всего, были. Вот. Ну, собственно, вот мой комментарий. Вопрос как бы я задал, неправильно ли я понял. Если все-таки после того, что я сказал, вы все-таки хотите добавить, что я что-то неправильно понял, я с удовольствием послушаю. 

S01 [02:00:34]  : Можно я добавлю, Антон, коротко, со своей колокольни. Просто разница в чем, мы хотим показать, что, в принципе, если мы возьмем любую стандартную сетку и ПАН в сравнении, то ПАНу потребуется значительно меньшее количество вот этих итераций, образов, для того, чтобы научиться распознавать кошечек от собак, чем любой сети. Притом многократно, если не сказать тысячекратно. Вот в чем будет отличие пана здесь. И, соответственно, по количеству ошибок также. То есть пан будет превосходить. 

S06 [02:01:07]  : Игорь вам предложил спорную бутылку. Я уже Игоря отправил в магазин, что он вам бутылку должен. Почему? Потому что ваша сеть, как я ее понимаю, говорят, что это я правильно понимаю, соответственно, она линейно растет по объему запоминаемой информации, то есть никакой противодействия различных образов не идет, там и вот этого экспоненциального времени обучения его естественно 

S09 [02:01:32]  : Но речь была не про это, речь была про другое. Речь была про линейный рост от производительности. Как про другое? 

S06 [02:01:39]  : Именно про это я рассказал, что не бывает таких сетей, которые учатся линейно. Я тебе рассказал модель, которая учится линейно. 

S01 [02:01:47]  : Владимир, Владимир, мы с самого начала. Я прошу прощения, с самого начала презентации, да, еще в тот раз и в этот раз, да, заявили о том, что мы поломали то, что написал господин Минский. То есть он как раз вот то, о чем говорит Игорь Пивоваров, по-моему, и написал. Вот мы все это дело поломали. 

S06 [02:02:09]  : Юрий Бабурин вам правильно рассказал, что скорость обучения – это один из показателей, а второе – это результаты обучения. У глубочих нейросетей, хотя они долго учатся, у них результаты учатся. Но позитивную нотку я хочу внести, что я считаю, что и нейросети не обязательно должны быть с распределенной памятью. Есть сети с конкурентным обучением, и там осуществляется некоторая локализация памяти. Я скажу даже более, что весь прогресс нейросетей, который сейчас идет, это же не просто ведро нейронов, которые настраивают связи. Весь прогресс в нейросетях состоит в том, что увеличивается локализация памяти в тех моделях нейросетей, которые есть. То есть те модели, которые были начальные и которые сейчас, сейчас локализация памяти значительно более большая, чем в тех первых моделях, которые там были 10, 20, 30 лет назад. И процесс локализации, конечно, ускоряет. И в нейросети тоже процесс локализации нужно вводить. Насколько ваша модель близка к мировому тренду, мне, поскольку я не до конца все понял, трудно сказать. Но в целом сама идея. Мне нравится, что надо локализовать процесс обучения и работы интеллектуальных систем, а распределенная память сильно тормозит их работу. Но пока что то, что вы показали и то, что показывают нейронные сети, там результаты выше пока что нейронных сетей. С распределенной памятью. Хотя я не фанат распределенной памяти. И считаю, что без нее можно обойтись. 

S04 [02:03:44]  : Спасибо, Владимир. У нас есть вопрос от Ивана. Иван, там у нас очень много сообщений, поэтому если у вас есть ряд вопросов, то давайте все ваши вопросы и комментарии. 

S10 [02:04:03]  : Здравствуйте. Я хотел Антону задать вопрос. от входа зависит то, у входа есть интервал, и от входа зависит то, какой вес будет выбран. правильно я это понял? на выходе есть несколько нейронов, там скажем 10, то есть конкретное значение, заданное заранее. 

S03 [02:04:36]  : Это я тоже правильно понял? Есть обучающие множество. 

S10 [02:04:45]  : Нет, сейчас. Есть входной сигнал, вот эта картинка просто, 1024. Каждая из них – это 1 байт, то есть от 0 до 255. Ну, в этой задаче, да. мы пример рассмотрим, потому что мне кажется легче понять. теперь у нас есть интервалы. интервал это скажем 8. получается, что входных нейронов столько же, сколько и размер этой картинки. 

S03 [02:05:21]  : входных нейронов просто число входов. 

S10 [02:05:27]  : в конкретном случае 1024 получается. Получается, что к нам приходят эти сигналы, картинка какая-то, и у нас для каждого отдельного нейрона в зависимости от входного сигнала мы получаем какой-то интервал. В зависимости от интервала мы получаем какой будет активирован и обучаться на данном этапе вес. А теперь по какому алгоритму, на основе чего мы решаем этот вес, с каким связать выходным нейронам? 

S03 [02:06:15]  : Там веса связаны с конкретными нейронами. То есть на каждом входе есть количество дистрибьюторов, равное количеству нейронов. Мы подаем какую-нибудь картинку и подаем ему желаемый результат. То есть мы сами изначально решаем, это так нейронные сети обучаются, с помощью обучающих образов, обучающих пар. То есть подается пара, вход и желаемый выход. 

S10 [02:06:46]  : Это понятно. У нас желаемый выход – это 10 чисел условно. Скажем, что у нас 10 разных классов. Это у нас 10 чисел. 

S03 [02:06:58]  : Правильно? Нет. Это означает два числа – единичка и нолик. Единичка – если нейрон активирован. Какой нейрон активирован, то к тому классу и относится. Показал максимальную сумму. из всех 10. 

S10 [02:07:13]  : Так вот, какой вес относится к какому выходу? На основе чего это решается? Как это решается? Выбранный вес у нас зависит от интервала. Интервал у нас зависит от входного сигнала. 

S03 [02:07:36]  : давайте я покажу вот это один момент маленький непонятно абсолютно где это как это соединяется вот смотрите первый вход у него есть количество вот этих дистрибьюторов каждый из которых связан с конкретным нейроном вот например первый вход он активирует у нас сейчас я лучше другую картинку покажу вот это вот первый вход активирует вот этот интервал. вот этот вес связан с первым нейроном, второй со вторым, последний с последним. 

S10 [02:08:13]  : то есть они изначально задаются? 

S03 [02:08:18]  : вот эти связи они жесткие. 

S10 [02:08:20]  : то есть связь вот это N1 и 

S03 [02:08:27]  : и x 1 до n 1 x 1 получается каждый вход с каждым выходом соединен каждый с каждым да да конечно конечно каждый с каждым да да да это в общем-то как классики 

S10 [02:08:46]  : Хорошо, спасибо. Я просто думал, что тут еще какой-то момент есть. 

S03 [02:08:50]  : Нет, здесь все точно так же, как и там, только добавляется количество интервалов. Соответственно, у нас растет количество весов для этой сети. 

S10 [02:09:01]  : Хорошо, спасибо. 

S04 [02:09:04]  : Спасибо. Коллеги, у нас еще четыре вопроса есть и 20 минут. Поэтому я прошу вопрошающих постараться компактно и отвечающих соответственно. От Казаринова есть вопрос. Пожалуйста. Я сейчас не могу вернуться прочитать. Если, может быть, вы голосом быстрее спросите. Виктор Казаринов, вы с нами? Я читаю вопрос. Если нет представительной тест-сьюты с большим диапазоном примеров из разных областей по сравнению с другими решениями, то как можно создать суперкомпьютер? Нужно же вначале тщательно исследовать основные свойства сети. Вы же утверждаете, что вам не хочется возиться с детальным исследованием вашей сети. Вы можете представить фармацевтические компании, которые таким путем выпускали лекарства без длительных исследований? Нам были представлены слишком примитивные примеры. Их недостаточно для качественной оценки вашей сети среди других. Такой вот вопрос у нас есть. Будете комментировать? 

S03 [02:10:16]  : Это не вопрос, это просто констатация, по большому счёту. 

S01 [02:10:25]  : Я, кстати, не утверждал, что мы там не исследуем и приостановили и так далее. Я уже, в принципе, ответил письменно даже в чате на этот вопрос, но могу повторить. Команда, естественно, вкладывает и основу, в принципе, всей финансовой, там, нашей, всех финансовых возможностей вкладывается прежде всего в защиту самой технологии и в ее дальнейшее развитие именно в плане как технологию. Сейчас мы патентами закрываем все больше и больше областей, где это будет применяться. То есть это постоянные процессы. Соответственно, мы больше и больше поизучаем. Еще раз на всякий пожарный. Команду возглавляют крупнейшие специалисты в патентном законодательстве. Инженеры-тризовцы. которые сами себе придумывают новые технологии, где когда-то будет применяться эта система. И поэтому также уже как бы работаем, команда работает на опережение. Я не утверждал того, что мы не вкладываем деньги в развитие самой технологии. Это вы что-то, по-моему, услышали не так или что-то не то, не так поняли. 

S04 [02:11:28]  : – Олег, спасибо. Еще вопрос от Ильи был. Илья, вы с нами? 

S12 [02:11:42]  : Да, как слышно меня. Да, слышно. Ладно, я тут вопрос уже не буду задавать. И у меня другой вопрос, он более проще. Вы когда обучали эту нейросеть свою, вы когда ее полностью обучили, и она, как вы говорите, она быстро учится, на каких-то 8 итерациях, какие вы на вход после обучения давали картинки и те же картинки на которых и обучали или картинки другие разные те и другие естественно тех на которых она обучалась она и 

S03 [02:12:18]  : показывала практически 100% результаты, естественно, которые подаются тестовой картинке, уже другое. Если вы внимательно посмотрели те же самые видео, которые продемонстрировали, там видно процент с кожисти. Из 100% там, например, яблоко 60, все остальные какие-то там доли процента, ну или там может быть 10. Но, в общем, естественно, тестовые образы использовались. и второй вопрос. 

S12 [02:12:47]  : вот это, что у вас выделено желтым, оно как-то соединяется при входе вот, например, с первого состояния? у вас три идут ветки, получается, и они соединяются, что ли, когда идет вход на первую? 

S05 [02:13:05]  : на первый узел. 

S12 [02:13:06]  : там скрытый слой. это как скрытый слой работает? 

S03 [02:13:10]  : нет, нет, нет. вот смотрите, подается вот сюда вход, вот сюда x1, он соответствует вот этому интервалу, у нас участвует в расчете сети, в обучении вот этот вес. 

S12 [02:13:23]  : А вот это желтым выделено, вот это еще два веса рядом, они не соединены? 

S03 [02:13:28]  : Это просто показывает, что при подаче на первый вход интервал у них одинаковый, это же один вход, то есть он активирует вот этот, вот этот и вот этот. А теперь мы говорим, что вот этот вход, который мы подали, у нас относится к первому классу. То вот здесь мы им говорим, что здесь должна быть единичка, а везде нули. Теперь мы подаем второй образ. Он у нас активирует, предположим, следующий интервал. И вот здесь мы уже говорим, что второй образ у нас соответствует второму классу. И мы используем вот этот вес для обучения. 

S12 [02:14:07]  : а вы, получается, разделяете картинки? разрезаете? 

S03 [02:14:11]  : вот здесь нет связи никакой. внутри желтого связи нет, это просто показан интервал активированный. 

S12 [02:14:20]  : а картинка полноценная идет или она разделена на части? нет, полноценная картинка. если 32х32, то есть 1024 входа. Хорошо, спасибо. Тогда больше вопросов нет. 

S03 [02:14:38]  : Спасибо. 

S04 [02:14:39]  : Спасибо. Еще был вопрос от Дмитрия Салихова. Я сейчас увидел вопрос, почему у нас группа по сильному искусственному интеллекту или General Artificial Intelligence, а мы обсуждаем вопрос Supervised Learning. У меня есть на самом деле ответ, но, может быть, докладчики попробуют ответить. 

S03 [02:15:06]  : Это не очень понятен вопрос. 

S04 [02:15:11]  : Вообще у нас вот эта тусовка, в которую вы попали, это обсуждение технологии создания так называемого общего или сильного искусственного интеллекта, а не машинного обучения. 

S03 [02:15:24]  : А здесь как бы... Как часть такая достаточно немаленькая, я так полагаю. Поэтому вроде я как считаю, что по адресу. 

S04 [02:15:36]  : Моё видение здесь такое на самом деле, что я здесь увидел безусловную заявку на решение проблемы One-Shot Learning. Но насколько она состоятельна, мне кажется, нужно показывать на больших наборах и сравнивать с теми результатами, которые показываются на современных конференциях или на современных лидербордах именно современными системами, а не теми, которые были в обращении лет 15-20 назад. Это один момент. А второе – это пресловутое энергопотребление или физическая производительность. Потому что у нас в определении EGI есть такая важная вещь, потребление ресурсов. Если бы человек распознавал тигра, вылезающего из кустов полтора часа, а тренировался распознавать тигра в течение трех лет, то тогда бы ни одного живого человека на свете бы сейчас не было. Поэтому тренироваться нужно быстро и решение принимать нужно быстро. И если действительно, как говорят авторы, с помощью вот такой технологии, которая обучается в тысячи раз быстрее, а распознаёт в десятки и сотни раз быстрее, С сопоставимым качеством. Может быть с чуть более худшим качеством. Качество версус производительность. Это вообще отдельная история, про которую можно долго говорить. Но если за счет меньшей потери и небольшого качества достигается порядок выигрыша в производительности на обучении, на решении задачи, то это, в общем, как бы пресловутый кирпичик, из которого можно строить пресловутые нейросимвельные архитектуры с гораздо более высокой производительностью для систем как реального времени, так систем с низким потреблением, работающих в оффлайновом режиме. Вот мой ответ на этот вопрос Юрию. А дальше у нас... Извините, можно я добавлю, Антон? Прямо пять слов. Это Дмитрий Салихов спрашивал, да, пожалуйста. 

S01 [02:18:00]  : Вот то, что вы проговорили, это четко пропан. Экономия — это железо, электричество, скорость, память, свойство уметь не забывать информацию, умение дообучаться без перезагрузки всей сети и т.д. Возможность работы на самых минимальных мощностях, лаптопы и т.д. То есть сама ПАН, да, ее использование, она просто, скажем так, даже еще в 2014 году, то есть они поняли то, что именно сама сетка ПАН способна увеличивать возможность, то есть, грубо говоря, из лаптопа делать нормальный компьютер, из нормального компьютера хороший сервер, а из обычного сервера делать хороший там этот, ну как он называется-то там, когда сервер чуть побольше, то есть более мощный сервер. Сейчас мы, как я уже говорил об этом, да, мы будем делать именно суперкомпьютер, чтобы показать, это будет уже реально показано суперкомпьютером, на котором можно будет решать задачи той же сложности, которая решается на реальных суперкомпьютерах, которые занимают стадионы и так далее, которые стоят миллионы, миллиарды и так далее. 

S11 [02:19:10]  : Можно я все-таки маленький комментарий за AGI поясню? Конечно, Дим, пожалуйста. Мне кажется, так и не раскрытым остался этот вопрос, в чем, собственно, требования AGI. Они, собственно, не в том, что нам нужен какой-то one или few-shot learning. Тут дело даже не в количестве примеров, требуемых для обучения, а скорее в том, что… Он должен сам разбираться с теми вещами, которые он должен распознавать. В вашей сети вы все объекты, которые он должен распознавать, задаёте вручную. Вот эти категории, выходной слой задаётся вручную. Если вы отправите такую сеть в какую-то новую среду, Объекты, которые заранее неизвестны, она там ничего не распознает и ничему не обучится. В этом, собственно, и вопрос был. 

S01 [02:20:04]  : Можно добавлю просто, как раз в ответ на ваш вопрос, Дмитрий. Смотрите, сейчас мы, ну я вот как мыслю, да, эту тему. Легче вот EGI делать на каком-то количестве железа, которое немеренно стоит, да, и вот в тех вот сетках, там они разные там, то есть многослойные. трехслойный, и так далее, и так далее. Я, честно, не спец в этом деле. И который занимает море железа, и которым нужны вычислительные мощности. Чем дальше, тем выше, тем больше. То есть, либо вы делаете, действительно, принципиально новый синапс, то есть, ну, вот этот самый, то есть, сети, который позволяет просто, ну, сократить все эти расходы, то есть, ну, реально там в тысячи раз. На чём проще будет сделать этот интервью, ну, как вот EGI? 

S11 [02:20:52]  : – Ни на том, ни на другом. EGI нужно делать на совершенно другом принципе. У вас, ещё раз, у вас нейроны обучаются, когда видят какую-то эталонную метку. У нас в жизни эталонных меток нет. Мы учимся сами по себе, просто так. 

S03 [02:21:05]  : Дмитрий, можно я прокомментирую? Да, я прекрасно понимаю ваш вопрос. И говорить вы тоже совершенно правильно. Значит, во-первых, сеть позволяет масштабироваться. Это вопрос о том, что мы можем запоминать новые образы и вводить новые классы. А во-вторых, процесс классификации для этих сетей находится в изучении. Возможно, удастся решить эту задачу, и тогда она будет уметь сама классифицировать объекты и учиться без обучающих пар. 

S01 [02:21:38]  : Еще маленькую ремарку, господа. Я в общий час скинул, я не знаю, пропустили или не пропустили, ссылки на Яндекс.Диск, на некоторые документы. В одном из этих документов есть как раз то, что делали непосредственно главный ученый Борис Злотин и его ближайшее окружение. они попытались выписать, скажем так, области применения. Документ на английском языке, но я думаю, для вас это ни для кого там не преграда, как раз по областям применения именно PAN они рассматривали. То есть рекомендую ознакомиться, может быть, больше станет что-то понятно именно, куда мы целим и для чего вообще все это делалось. 

S04 [02:22:15]  : Хорошо. Спасибо. Следующий вопрос или ряд вопросов у Евгения Евгеньевича Витяева. Евгений Евгеньевич, мне найти или вы озвучите? Да. 

S08 [02:22:33]  : Мой вопрос состоял в том, как осуществляется патентование. Дело в том, что я тоже подал заявку на патент по формальной модели нейрона. Она, правда, совсем другая и, наоборот, не проще, а сложнее. Но мне интересен сам процесс. подачи таких заявок, потому что я, вообще говоря, несколько удивлен, что такая, так сказать, вот, ну, простая, но оригинальная, так сказать, модель, тем не менее, так вот хорошо продвигается. Мне как бы даже интересен сам опыт, вот, и я бы даже попросил немножко поделиться, как это вообще делается. 

S01 [02:23:15]  : Это вам, наверное, консультация нужна Борису Злотину непосредственно, его супруге Аллы Зусман, да, то есть вообще эксперт именно, супер там качество. С чем связано это ихнеискусство, да? Борис Злотин, Алла Зусман там и команда, то есть генеральный там Владимир Простянник, это тризовцы, да? То есть ребята уехали в Америку там в девяностые годы, там, ну, воле судеб. бежали из Молдавии, то есть от нехороших людишек, но не суть важна, да, и просто зарабатывали себе на хлеб тем, что решали, то есть нерешаемые задачи для там Future 500 компании там, да. То есть если взять там послужной список Злотина, там это порядка 30 тысяч задач. Практически каждый из них это на уровне патента, либо обхода патента. Они крутые в этом настолько, что я думаю, не знаю, кто круче. Но я, честно, еще не знаю специалистов круче. Я пытался тоже и поюзать, и найти, и поискать, кто может быть круче, и так далее. Но когда именно человек думает над тем, чтобы не просто решить задачу, а еще обойти какие-то патенты, то есть он всегда находит, то есть он решает как бы и эту задачу, и, соответственно, накапливает такой опыт. 30 тысяч задач, я считаю, что это колоссальный опыт. То есть за его карьеру для предприятия. 

S08 [02:24:27]  : Я знаю тризепс. Дело в том, что основная компания действительно в Штатах. 

S01 [02:24:34]  : Сейчас, скажем, его ученик Анатолий Гин, который тоже является членом команды, Он непосредственно мой учитель, с которым я знаком. Я учусь у Бориса Злотина лично. С Гином мы просто там сейчас друзья. Они как раз и возвращают, в принципе, тризис сюда. Я не знаю, вы знаете, может быть, как у Александра Кудрявцев, тоже сильнейший тризис. Работает на «Русал», если не ошибаюсь. Ой, по-моему, на «Русал». Хотя, может быть, ошибаюсь. Сибур, Сибур, по-моему, там он работает вот где-то, тоже для них решает задачи. Сейчас он все больше и больше собирает конференции там, да, в институте, в МИИСе. То есть я побывал на одной из последних из них. Это уже действительно, то есть, ну, как бы прирастает в геометрической прогрессии. И главное, интересно, что молодежь пошла уже в это дело. То есть, они обучают весь мир этому. Американцев, канадцев, китайцев, этих, как его, корейцев, Самсунг там, Сони, кто там еще, вот все, что у вас известно есть, ну, Ford, да, то есть, ну там. Проктор Гэмбелл и так далее. Все просто вылетают на название. Вот все, что вот есть знаменитое, помните? Да, вот все практически было связано с Борисом Глотиным, как бы там ни было. Они даже софт писали по решению задач. Как вы думаете, такие люди будут с какой-нибудь фигней заниматься? 

S08 [02:26:00]  : Ваш ответ, он проясняет, почему так вот все получилось, почему вам все это удалось. 

S01 [02:26:06]  : И вот мне понравилась, знаете, фраза в чем? Они сами, я с Борисовым разговаривал, говорит, мы прекрасно понимаем, что когда люди, нормальные ученые, начнут смотреть технологии, они поймут, блин, Это же так просто, да? То есть это действительно можно было решить, просто подумай иначе, да? А все же просто в Минске вот читали, на Минском учатся там, да, и так далее. То есть вот мы на этом сыграли. И сейчас поэтому мы защищаемся все вот это патентами и так далее, и тому подобное. Вперед и дальше, и так далее. То есть даже спустя, в 13-м году бы там, да, то есть началась созданная технология, мы не показывали эту технологию, никому не показывали вообще, то есть. Очень узкий круг именно инвесторов, которые самые ближайшие там, да, то есть как-то что-то имел в понятии то общее, чтобы не было возможности разгласить тайну. Защитились, теперь мы показываем. И вы говорите, что это действительно простые вещи. На самом деле простые. Здесь даже, мне кажется, математика стала вот в самом процессе какой-то элементарной, да, то есть вместо вот алгебры там, ну, высшей школы там, да, это просто обычно стала началкой какая-то. Вот. И поэтому, может быть, так легко все и работает. Но это же работает. Это действительно дает результаты, то есть, ну, это, как его, приближающиеся именно к биологическому мозгу. Вот какого? Нейронная сетка способна без проблем забывать информацию? Вряд ли. Нейронная сетка способна без проблем дообучаться. То есть, грубо выражаясь, вы сделали там видеоряд, ищите разбойников там в метро, да? То есть у вас вот тысячи фоток есть, вы обучили там свою нейронную сетку, вам надо добавить тысячи первого. Сколько времени уйдет на это? Сейчас я знаю, что вот эти многослойки как-то имеют, что достаточно там кусок переобучить. А в ПАНе это достаточно просто добавить. Ну как бы вкинуть. На, смотри, ещё одна будет. Всё. И процесс дальше идёт. Если надо выкинуть из этой сетки, так же просто выкинули, всё, забыли. Сетка забыла. Вот в чём процесс. То есть это намного ближе именно к биологическому мозгу. Вот я о чём. обучение есть очень похожие методы весовые матрицы я не нет я не против что мир создаешь что-то подобное честно я знаю что какой я почему и создаю в группе почему интересуюсь вообще сам всей этой темы хотя не ученый там да и не физик там да но уж простите меня там да но я действительно хочу быть в теме именно искусственного интеллекта да хорошо 

S04 [02:28:28]  : Олег, спасибо. У нас еще один последний вопрос и комментарий от Николая Робчевского. Николай, пожалуйста. 

S00 [02:28:40]  : Да, спасибо за доклад. Беляков очень толково рассказал о сути. Я хотел бы добавить, что если говорить для тех, кто не очень включен в тему, то вот этот коммутатор, который переключает по интервалам, означает на самом деле вот что. Что в отличие от обычных невральных сетей, функция преобразования входа в выход формируется как комбинация сигмоидов. Здесь она формируется явным образом вот этим коммутатором, потому что коммутатор – это ничего иное, как таблично заданная функция, которая мопирует вход нейрона на его выход. И таким образом вот то, о чём говорил Игорь Пивоваров, о том, что это как бы эквивалент просто такого количества слоёв, сколько сегментов на коммутаторе, это, в общем, совершенно правильно. Второй момент – это то, что Дмитрий Салихов абсолютно верно сказал, что это не имеет никакого отношения к EGI. Потому что, да, это система, которая может учиться только в режиме supervise learning. Поэтому ни она, ни ее конкуренты, которые требуют много ресурсов и медленно работают, они для ЭДЖА не годятся. Ни то, ни другое. Ну и третий момент. Олег Говоров очень много рассказывал о том, как хорошо защитить патентами все это. Дело в том, что саму суть этого решения патентам защитить нельзя, а все, что защищено, оно работает только в одном смысле. Никто не сможет запретить вам, авторам, использовать эту технологию. Никто не может претендовать на то, чтобы запретить вам использовать ее. А запретить использовать другим вы совершенно не можете ее. Если вы на это надеетесь, то это означает, что вы совершенно не понимаете ситуации в системе патентования в Штатах и в мире. Потому что, во-первых, Китаю совершенно начхать на то, что вы тут запатентовали. Во-вторых, способов обхода патентов настолько много, что патентные споры будут продолжаться гораздо дольше, чем требуется для того, чтобы перейти от этой технологии еще к чему-то другому. И у меня по этому поводу все. То есть на патенты это не то, что является серьезным или важным в этой затее. Спасибо за внимание. Хорошо. 

S04 [02:32:14]  : Николай, спасибо. Олег, будут короткие замечания? 

S01 [02:32:20]  : Нет, спасибо мне, кстати, за критику. Просто очень важно получить будет запись этой встречи, потому что реально серьезные вопросы, серьезный разговор. Мне это все интересно. Мне это интересно показать именно Борису Злотину. Я думаю, найдет время посмотреть и команде по одной простой причине, что будут понимать, какие ответы им искать вообще для себя и так далее. Я не знаю просто, выберут ли время они когда-нибудь навстречу здесь, не уверен. честно говорю, потому что реально закручены сейчас, особенно с этим, этот проект заканчивается, то есть платформа вот-вот, она уже есть там, да, то есть надо его двигать, там есть расписание, четкие дедлайны, не уверен, что они сюда смогут там выйти, но если вдруг когда-нибудь будет, они обязательно ответят на эти вопросы. Или, может быть, просто какие-нибудь ответы смогут там, я не знаю, в паузах написать, тогда я просто готов буду их там переслать в группу. 

S04 [02:33:16]  : Ну на самом деле было бы гораздо интереснее, чтобы все-таки у вас был повод получить от Игоря Пивоварова бутылку, а тем остальным продемонстрировать, как ваша архитектура работает в глубоких нейронных сетях с формированием высокоуровневых фич, которые потом можно идентифицировать как уши, лапы и хвосты. Хорошо. Большое спасибо. Было очень интересно. И особенно было оживленно в чате. Я даже не знаю, можно ли теперь этот чат выкладывать. 

S01 [02:33:49]  : Ну, кстати, если есть возможность. 

S04 [02:33:50]  : По-моему, как-то его можно куда-то... Да, ну вы, так сказать, спокойнее относитесь. 

S01 [02:33:57]  : — Абсолютно нет, я нормально, в адеквате просто. Вы, если что, меня простите, если вдруг… Я вроде резких там телодвижений не делал, то есть имею в виду, никого, надеюсь, не обидел там своими комментариями там и ответами. Я реально благодарен вам за то, что состоялся этот разговор, да. Я благодарен за помощь Антону Белякову, что он помог мне с этим, да, потому что он, ну, технический специалист. Я — нет. Просто говорю сразу. Поэтому спасибо большое реально за потраченное время. Я понимаю, что мы просто много времени потратили, реально два с половиной часа. Я просто с нетерпением буду ждать записи. Ну и, конечно, буду продолжать следить за группой обязательно. 

S04 [02:34:35]  : Хорошо, спасибо. До новых встреч, коллеги. Спасибо за вопрос. До свидания. Всего доброго. 

S01 [02:34:41]  : Всем спокойной ночи, спокойного времени суток. 









https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
