# 9 апреля 2025 - RAG. Примеры использования внешний памяти и источников данных для улучшения качества работы LLM - Павел Саловский (NotAIbook) — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/gvloZQaF_8w/hqdefault.jpg)](https://youtu.be/gvloZQaF_8w)
- [видео в RUTUBE](https://rutube.ru/video/f33cf2556b5eed3d58a870a86266276e/)
- [видео в ВК](https://vkvideo.ru/video-210968399_456239217)
- http://notaibook.site

# Суммаризация семинара:

## Основные тезисы выступления

### Введение в RAG (Retrieval Augmented Generation)
- Павел Соловский представлен как основатель компании NotAIbook, занимающейся разработками в сфере больших данных и их интеграции в RAG
- Демонстрация базового RAG: GPT не знает информации о Павле Соловском, но после загрузки резюме может отвечать на вопросы о нем
- RAG позволяет подкладывать данные в модель и извлекать информацию, используя модель как движок, без необходимости переобучения

### Механизм работы базового RAG
- Файлы разбиваются на чанки (кусочки)
- Каждый чанк преобразуется в токен с помощью embedding-модели
- Все токены складываются в векторную базу данных
- При запросе пользователя система ищет наиболее релевантные фрагменты с помощью similarity search
- Найденные фрагменты загружаются в контекст модели для формирования ответа

### Проблемы стандартного RAG и базы данных
- Модели забывают контекст при его переполнении
- Ограниченное контекстное окно даже у больших моделей
- Галлюцинации в ответах и недостаточная точность
- Важность проверки фактов (fact-checking)
- Возможность использования SQL для работы с базами данных через LLM
- Возможность использования изображений как источника информации

### Онтологии как решение проблем RAG
- При большом объеме данных (миллионы файлов) стандартный RAG теряет значительную часть ценной информации
- Онтологии сжимают текст примерно в 10 раз
- Из текста извлекаются триплеты (субъект, объект, предикат)
- Триплеты связываются в графовую модель с прямыми связями
- Используется формат RDF/OWL, для запросов - SPARQL
- Текст превращается в структурированные данные, с которыми можно работать как с базой данных

### Преимущества онтологий
- Сжатие данных примерно в 10 раз
- Возможность логических выводов по связям (если Алиса знает Боба, а Боб знает о Леонардо да Винчи, то Алиса может знать о Леонардо да Винчи)
- Возможность работы с гораздо большими объемами данных
- Более насыщенные фактами ответы
- Возможность видеть связи на большую глубину

### Построение онтологий с помощью LLM
- Традиционно онтологии строились вручную (годы человеческого труда)
- Команда Павла научилась строить онтологии автоматически с помощью LLM
- Демонстрация извлечения триплетов из текста испанских законов и построения онтологии
- Решение проблем: параллелизация, соединение частей онтологии, проверка непротиворечивости, удаление дубликатов
- Сейчас построение онтологии на "Преступление и наказание" занимает полчаса вместо суток

### Практическое применение
- Работа с секретными или внутренними базами знаний предприятия
- Возможность работы с маленькими локальными моделями без потери качества
- Работа с финансовыми или секретными данными без выхода в интернет
- Создание отчетов "на лету" и прямая коммуникация с данными
- Извлечение неочевидной информации из данных без сложных аналитических систем
- Персонализированная память для чат-ботов (сохранение и структурирование истории общения)
- Применение для образовательных ботов (отслеживание прогресса обучения)

### Философские аспекты и перспективы
- Связь онтологий с концепцией эйдетического искусственного интеллекта и эйдосов Платона
- Стратегическая цель - создание генеральной онтологии, позволяющей добавлять любые знания и непротиворечиво их использовать
- Возможность достижения инкрементального обучения без катастрофической потери памяти раньше прогнозируемых сроков

## Вопросы и ответы
- Обсуждение технической реализации (Python, LangChain, LangGraph)
- Возможность интеграции с цифровыми двойниками и имитационным моделированием
- Проблема несогласованных единиц измерения в разных источниках данных и возможные решения через расширение онтологии

Семинар представил обзор передовых подходов к интеграции языковых моделей с данными, с акцентом на онтологические методы, которые значительно повышают эффективность работы с большими массивами информации и открывают новые возможности для искусственного интеллекта.

# Расшифровка семинара:

S02 [00:00:04]  : Коллеги, всем добрый вечер. Сегодня у нас в гостях Павел Саловский, и мы поговорим о том, как можно синтегрировать большие языковые модели со своими данными, чтобы данные могли заговорить. А мы могли поговорить с ними. Павел, пожалуйста. 

S00 [00:00:21]  : Благодарю, Антон. Итак, говори со своими данными. Обещается новый уровень работы с данными. Меня зовут еще раз Павел Соловский. Я основал компанию NotAIbook, ноутбук с искусственным интеллектом. И в рамках этой кампании мы ведем разработки, в том числе связанные с большими данными, интеграции больших данных в RAG. И, естественно, первая задача, которую мы решили и решаем, это как заставить большие данные работать внутри RAG. Я начну, пожалуй, с эксперимент живого. Если у аудитории достаточно высокий уровень, прошу меня простить. Если для кого-то это будет банальным, то что я сейчас покажу, но тем не менее. Проведем живой эксперимент. Я прямо зайду в GPT-чат. Вот я был у чата. Я сейчас попытаюсь объяснить, что такое РАК простыми словами, чтобы мы с вами были в одной системе координат. Я спросил чат, GPT-чат, кто такой Павловский, и чат ничего мне не ответил, потому что он мне, к сожалению, ничего не знает. Но я не обиделся, поэтому я могу взять и попытаться загрузить с компьютера, например, какую-нибудь резюмешку свою, где описано, кто такой Павел Соловский. Итак, резюме загружено, и я повторю свой вопрос. Кто такой Павел Соловский? Вот, выясняется, что я квалифицированный специалист искусственного интеллекта. Очень приятно, спасибо, GPT-Chat теперь меня оценила. То есть, фактически, мы здесь продемонстрировали элементарный RAG, базовый. То есть, модель не обучена была, ничего не знала о персоналии, мы подложили файл, и теперь наша модель может говорить о персонале. Фактически, получилась магия, то есть мы без обучения модели добавили в ней какое-то свойство. Собственно, это является первичной и основной задачей RAG – Retrieval Augmented Generation – давать данные подкладывать данные в модель и извлекать эту информацию из данных, используя модель как движок. То есть LLM здесь работает как некий двигатель. Фактически у нас получается такой многослойный уровень использования LLM. То есть можно использовать GPT-Chat напрямую. Можно использовать GPT-чат как RAG, как модель с дополненными данными. А можно вставить RAG в различные бизнес-приложения, в зависимости от того, что мы хотим получить. Мы хотим получить ответы, мы хотим получить анализ. Текстов мы хотим получить генерацию отчетов на основе представленных шаблонов. Даем шаблон, даем входящую информацию, просим сгенерировать отчет на основе шаблона. И модель будет работать для нас. То есть получается у нас такая матрешка. Модель внутри рага, раг внутри приложения. В последнее время очень многие команды работают с SQL-коммуникациями, я бы так это сказал, то есть с SQL-запросами, с адаптацией SQL-запросов к работе искусственного интеллекта. Для чего это все нужно? Для чего нужны попытки работы с данными и с базами данных внутри RAG? Потому что пока что наши модели забывают контекст, если он переполняется. Если мы попытаемся подкладывать, вернее, в течение разговора контекст переполнится и модель начнет забывать начало разговора. Мы имеем проблемы и с контекстом в том числе. То есть, если мы даже будем использовать RAG и использовать контекст в моделях, у нее тоже контекстное окно ограничено. Даже огромные модели типа Google все равно имеют ограниченный контекст. И мы не можем бесконечно накидывать данные. Рано или поздно модель опять же их забудет. Также модели галлюцинируют, ответы их не очень точны и очень важным является проверка Факт-чекинг, то есть проверка фактов, насколько модель точно ответила и почему она приняла то или иное решение. И вот здесь ссылки на данные, то есть возможность так или иначе сосредоточиться на контекст, который мы использовали, она очень важна. И если вы выцарапать этот контекст из самой модели достаточно сложно, Сейчас делаются различные попытки и модели, которые могут объяснять себя, в том числе reasoning модели, которые объясняют по крайней мере цепочку рассуждений. То при работе с данными сослаться на данные достаточно легко. Немножечко еще пройдемся по типам рагов, которые бывают у нас. Ну, самый простой рак, он подкладывает, по сути, файлы в модель. То есть мы берем файлы, режем их на чанки, на кусочки. Каждый чанк с помощью embedding модели превращается в токен. Все это складывается в векторную базу данных. И дальше мы можем, когда пользователь задает какой-то вопрос, Наши проиндексированные чанки или кусочки файлов мы в них ищем по подобию, то есть используя similarity search. В принципе, механизмы очень похожие на то, как ищут поисковики типа Яндекса и Гугла, как они ищут сайты, проиндексированные сайты, ждать тот или иной ответ. И вот также у нас, представим, что у нас не один файл, с резюме Павла Соловского, а миллион файлов. Конечно мы их все не сможем сунуть в контекст модели, загрузить в контекст модели. Нам нужно выбрать наиболее релевантные файлы, поэтому с помощью вот таких вот простых механизмов индексации мы получаем В итоге релевантный файл или несколько релевантных файлов, которые загружаем в модель и уже по ним просим дать тот или иной ответ. Так работает простой Semantic Search RAGk. Но что если у нас вместо файлов база данных? И здесь мы тоже можем работать с базами данных. Собственно, это уже сейчас полешинель, то что LLM модели пишут код на лету, поэтому мы с помощью промта просим модель, вот мы здесь спросили, дай мне пожалуйста топ-5 invoice, мы просим трансформировать человеческий текст, человеческий запрос, SQL-запрос. И вот здесь она объясняет, то есть как раз это тот самый fact-checking или explainable model, она объясняет, как она построила запрос. Select invoices from и так далее. То есть, если мы не верим в ответ модели, мы можем пойти и проверить, зайти в базу данных, найти эти факты. В наших продуктах мы даже делаем ссылочку на базы, где ты можешь кликнуть и тебе прямо откроется нужная часть базы данных. Также враг можно использовать и картинки. Например, если мы здесь вот на примере мы подложим фотографию аварии и спросим, что случилось. То есть какие ты видишь повреждения. LLM нам с удовольствием описывает перечень повреждений. То есть вот это готовый кейс для страховых компаний. Они все равно сейчас большинство оценок делают по фотографии, поэтому в принципе это готовый кейс для автоматизации страховых случаев. И картинки тоже могут быть источником информации, потому что мы из картинки можем извлечь описание и дальше это описание использовать в различных бизнес-задачах. извиняюсь, у меня такая фиксация на бизнес-задачах. Это не совсем научная тема, но зато она больше такая инжиниринговая. Ну и третий вариант. Если у нас не база данных, А если у нас действительно миллион файлов или не миллион файлов, например, 10 миллионов файлов, что произойдет, если мы будем строить индексы на этих файлах и пытаться искать релевантный контекст для нашего LLM? Скорее всего, мы найдем релевантный контекст, но выпадет значительная часть ценной информации. Ну, например, просто потому что в индекс попадает только топ 10, 20, 30 файлов, а все остальные 10 миллионов файлов будут в нижней части индекса, и их мы не сможем использовать по причине ограниченности контекста. Поэтому вот если, например, мы спросим наш файловый рак, знает ли Алиса Леонардо да Винчи, Если нигде в текстах не сказано, что Алиса изучала, например, Леонардо да Винчи на курсах, или смотрела картины Леонардо да Винчи, или что Алиса любит Леонардо да Винчи, то ответ будет, у меня нет такой информации. Или, скорее всего, Алиса не знает Леонардо да Винчи. Но что будет, если мы построим антологию на наших данных? Что такое антология? В нашем случае есть десятки разных вариантов онтологии. Есть семантические графы. Так вот это не семантический граф, это антологический граф. Во-первых, мы сжимаем текст раз в 10 примерно. Мы берем текст исходный, извлекаем из него триплеты. Субъект, объект, предикат, подлежащее сказуемое определение. За счет изъятия фактов текст сжимается в десяток раз. И далее мы связываем все эти факты, все эти триплеты в графомую модель. То есть мы их связываем не с помощью токенов, не по близлежащему нахождению. То есть понятно, что папа и мама где-то будут на графе расположены рядом по значению. Здесь связь совершенно прямая. Папа женат на маме. И мы это соединяем буквально точно. Используется формат RDF. он же Oval, расширение Oval. А для тросов используется SPARQL. То есть, после того, как мы сжали данные десяток раз, превратили их в триплеты, а триплеты, в свою очередь, связали в граф по специальной методологии, мы получаем фактически базу знаний, которая построена на неструктурированных данных. То есть теперь все наши текстовые данные превращаются в структурированные данные. И теперь мы можем работать с этими текстами как с базой данных SQL. То есть SparkQL позволяет делать запросы к этой базе. Данный формат пришел к нам, и данная идея работы с антологиями пришла к нам пару десятков лет назад, когда, помните, были такие энтузиасты, они пытались весь интернет превратить в гигантскую базу знаний. Для этого, для описания страниц, применял мета-язык RDF. Попытка частично удалась, частично нет. Но до сих пор в мире создано гигантское количество антологий готовых. Часть из них open source, она открытая, часть закрытая. А часть антологий строят гигантские компании для себя, для своих нужд, вручную. Ну, например, для чего может служить антология? Возьмем какую-нибудь летную компанию, не знаю, Boeing, Airbus, может быть даже Aeroflot, я не в курсе, есть ли в Aeroflot и Антологе. Например, нам необходимо подчинить самолет. В самолете поломалось крыло. В этом крыле более тысячи запчастей, причем поломалось определенное крыло определенного года выпуска. Если мы вручную попытаемся составить смету и спецификацию на замену этого крыла, мы потратим Приличное количество времени. Мы должны будем перебрать все спецификации, зайти в ERP-систему, зайти в нормативно-сметную документацию, вытащить оттуда плоги, как-то сопоставить с текущим состоянием, понять, какого выпуска у нас крыло, проходило ли оно модификации, ну и так далее, и так далее. Вот задачи отличные решают промышленные онтологии. Когда у нас есть полный граф, который описывает все взаимосвязи, какая запчасть с какой запчастью связана, какие запчасти находятся в крыле самолета, какие в хвосте самолета, какие фюзеляжи. И мы, в принципе, нажав несколько клавиш, можем получить участок графа, который описывает вот эти все связи, и по ним уже составлять смету. Плохая новость состоит в том, что до появления LLM и до сегодняшнего дня почти все компании делают эти антологии вручную. То есть сидят специально обученные аналитики, используют программы, где вручную связывают субъект, объект, предикат. Такие вот триплята формируют, формируют домены. Значит, это у нас раздел, например, запчасти. Это у нас раздел, не знаю, масла. Это он сделал какое-нибудь техническое обслуживание. И вот ручным образом создают такие антологии. Это годы человеческого труда, годы усилий аналитиков. Хорошая новость состоит в том, что мы научились строить эти антологии автоматически. К сожалению, я не смогу показать код, просто потому что мы пока еще имеем коммерческое преимущество, и я не могу делиться этими знаниями в открытую, но я смогу примерно показать принцип как подойти к этим антологиям. И корица Sapientisat, то есть знающему достаточно. Сейчас я снова вернусь в LLM. Для того, чтобы построить антологию, я буду использовать файлик с испанскими законами. У меня есть специальный файлик, где есть небольшой кусочек испанского законодательства. И сейчас я задам вопрос. Извлеки, пожалуйста. Так, наверное, есть смысл посмотреть на этот файлик. Ну, в общем, там юридический текст. Извлеки, пожалуйста. Я не знаю, что видно на экране сейчас. Да, видно. Чат виден. Извлеки, пожалуйста, из данного текста триплеты. Субъект. Там ошибочка. Да, она спокойно поедает эти ошибки. 

S02 [00:18:44]  : Извините, у меня много вопросов, но они подождут. А вот сейчас у меня в моменте вопрос, позволю себе. Вы сейчас задаете вот этот вопрос. Какой-то промпт есть, который ей поможет вот это вот извлечение делать? Или это вы делаете без промпта? 

S00 [00:19:00]  : Нет, на лету. Вот весь промпт – это мой вопрос. Вот она поехала лопатить, перелопачивать наш документ. 

S02 [00:19:18]  : То есть вот то, что сейчас происходит, это не тот код, который вы говорите, который вы не можете показать. Это то, что часть GPT умеет из коробки пока, правильно? 

S00 [00:19:28]  : Она умеет это из коробки, да. В этом и прелесть. Не все еще поняли, чего она умеет. Оказывается, она умеет и такие вещи. А теперь я попрошу сделать следующее. А теперь построй онтологию. на этих триплетах. Фактически я формирую промпт MoOnline в триплетах. То есть создай связанный граф. 

S02 [00:20:11]  : Александр Балдачев прям почувствовал, про что речь придет, вовремя появился. 

S00 [00:20:17]  : Вот она спокойно выделила основные узлы. Вот она встроит онтологию так, как она ее поняла. То есть вы сами понимаете, что мы можем заставить ее действовать по определенной методологии, если захотим. И вот она поехала строить даже граф нам сама. Понятно, что тут сейчас куча ошибок, какие-то допущения, но сам факт Да, я хочу визуализировать. Визуализируй, пожалуйста, в виде связанного графа на одну страницу, чтобы видеть все связи одновременно. Она начала писать код, так как она поняла мою задачу. Она использует matplotlib из питона, ну, собственно, разумно. Собрала все триплеты. И сейчас нарисует нам картинку. Да? Вуаля. А дальше начинаются проблемы. То есть, в принципе, моя команда прошла этот этап где-то год назад, когда мы обнаружили, что с помощью LLM мы можем строить, оказывается, и графы, и антологии. Дальше начинаются проблемы. Мы загрузили книгу о преступлении наказания Достоевского. и попросили построить антологию. Ну, где-то за сутки она ее построит. То есть, если мы хотим точную антологию, мы должны задавать определенные правила, мы должны задавать определенные форматы. То есть, здесь идет не одиночное обращение к движку, вот не так, как я сейчас сделал, в три захода практически, а там больше, там десятки обращений к LLM по различным тематикам. В итоге тратится уйма времени, ну сутки. А если у нас мы идем к заказчику и вообще собираемся решать промышленные задачи, то у нас нет сутки на одну книжку. То есть мы годы. То, что делают люди вручную, они сделают быстрее. Поэтому мы пошли дальше и стали разбирать проблемы параллелизации. сделали парализацию, уперлись в то, что мы разрезали книгу на 100 страниц, построили онтологию по каждой странице, а теперь как соединить все эти отрывки. Как вы здесь видите, допустим, муниципальные выборы, иностранец, резидент, какая-то отдельная онтологическая кусочек, то есть частичка, которая выпала из общей онтологии. Как ее соединить потом со всей онтологией? Это первая проблема. То есть первая проблема парализации решили. Вторая проблема дополнения связности онтологии на больших данных решили. Третья проблема непротиворечивости, убирание дубликатов, ну то самое лук как растение и лук как поле и лук как взгляд, если еще мы имеем ошибки с иностранными словами, более-менее решили. И так далее, и так далее. Я сейчас не помню весь список. Более десятка проблем так или иначе мы разобрали. И сейчас в принципе мы можем строить эти онтологии достаточно быстро. То есть сейчас преступление-наказание это полчаса. 

S02 [00:25:20]  : Можно я здесь уточню? Когда вы говорите, что мы теперь можем, в результате этих ваших экспериментов у вас родилась какая-то методология, то есть это вот некоторый набор инструкций – делай раз, делай два, делай три, или это тот самый код, который вы не можете показать? 

S00 [00:25:40]  : Это код и внутри него инструкция. Делай раз, делай два, делай три. 

S02 [00:25:43]  : То есть это набор некоторых промптов, которые в определенной последовательности загоняются в LLM в процессе диалога для того, чтобы решить определенную задачу. И вы это можете автоматизировать в том числе, правильно? Верно. То есть у вас есть некоторый робот, который последовательно эти промпты загоняет. 

S00 [00:26:04]  : Верно. Более того, там помимо промтов, там же кодовая обвязка, логическая обвязка из Python. То есть, в принципе, это полноценный код. А LLM мы используем как движок, в который мы отправляем задачи. Сейчас мы решили следующие задачи. То есть, мы можем… Ну, во-первых, мы подложили под онтологию… С ней можно работать с листа? то есть она создает TTL файл, то есть в принципе она превращает текст вот в такую вот штуку. Здесь вот есть кусочек этого TTL файла, желтенький, и к нему можно уже задавать вопросы. А можно положить это в графовую базу, и тогда мы в принципе не ограничены объемом онтологии, то есть мы можем достраивать антологию. Например, положили пострепление Казани, а потом положили братья Карамазовы. И вполне себе можем изучать Достоевского во всех его проявлениях. А можем построить антологию крыла Боинга, а дополнительно антологию крыла Аэрбуса. И, например, сравнить по импортозамещению, что там замещается, а что не замещается. Теперь можно достраивать эти антилогии, комбинировать между собой. И, в общем, мы не ограничены производственными мощностями. Сейчас у нас уже есть несколько внедрений. Они сделаны на Западе. Здесь, в России, мы сейчас разворачиваем демостенд, и я думаю, что течение пару недель я надеюсь он будет готов если все все сложится ну максимум месяц чему это приводит ну вот я собрал несколько примеров того как антология улучшает качество ответов gpt-чат есть вообще ситуации патовые когда не может ответить ничего GPT-Chat, то есть у нее нет информации. А есть, когда она что-то отвечает. Ну вот, например, мы здесь задали вопрос. Что такое SAMSA? SAMSA – это фреймворк Apache Spark. Наверное, надо как-то увеличить. Слева у нас ответ у GPT-Chat. А справа у нас ответ от антологии. И мы видим, что, конечно, GPT-chat знает о SAMSA, и мы можем почитать, но с точки зрения насыщенности фактами, пожалуй, антология будет круче. То есть именно за счет того, что она оперирует с графом, она выдает массу-массу точных ответов. GPT-чат, вы можете просто сравнить структуру ответа, она начинает немножко расплываться. 

S02 [00:29:14]  : Можно вот здесь уточнить, Павел, а вы говорите, что антология так руче, но в антологии же эта информация про самзу, откуда-то она должна была взяться, то есть предполагается, что вы до этого, как вы задали этот вопрос через вот тот механизм, про который вы рассказали, какие-то документы пропустили через вот ваш pipeline, он в итоге загрузился в базу данных, после чего можно задавать вопросы. 

S00 [00:29:38]  : Спасибо. Да, верно. Дальше то же самое сравнение. Это Sentry, тоже один из фреймворков Hadoop. И опять Вот здесь ответ GPT-Chat сильно слабее, то есть здесь онтология, конечно, сильнее, потому что, да, мы сделали, во-первых, мы собрали там несколько мануалов, описаний, сделали, построили на них онтологию, естественно, наш ответ лучше. Для чистоты эксперимента, конечно, можно было бы поспрашивать на файлах. На файлах тоже делали эксперименты. Я вам скажу так. Примерно до 300 файлов, если загружать, если использовать RAG, построенный на семантическом анализе, то есть semantic search RAG, до 300 файлов разница между ответами от онтологии и RAG сильно не будет ощущаться. Но чем больше файлов, тем начинает проявляться эта проблема, когда часть источников просто выпадает из индексации, а мы не можем все 300 файлов подложить в модель, она не пропустит, у нее контекст не безразмерный. И вот здесь антология начинает иметь преимущество, потому что фактически мы подкладываем очень сжатый текст. Там нет лишнего. Там только факты. И причем эти факты связаны. Возвращаясь к нашей истории с Алисой и Бобом, если мы спросим у Semantic Search платформы про Алису знает ли она Леонардо да Винчи, то она либо не ответит, либо скажет, что у нее нет информации, либо будет галлюцинировать. В случае с антологией ответ будет примерно таким. Алиса не знает Боба. У меня нет информации, что Алиса знает Боба. Вернее, Алиса знает Леонардо да Винчи. Но Алиса знает Боба. А Боб в молодости интересовался картиной Монолиза. который, художником которой является Леонардо да Винчи. А поскольку Боб является другом Алисы, то, вероятно, Алиса могла слышать от Боба про Леонардо да Винчи. И, вероятно, Алиса знает. Да, то есть это нормальное логическое рассуждение, которое применил бы человек. Подобные графы используются в криминалистике, например, или, например, в анти-манней лондон для того, чтобы предотвращать мошенничество. вопрос какой-то? нет нет. здесь мы имеем кардинальное преимущество, поскольку мы видим связи на очень большую. то есть братья, сестры, друзья, соседи, подобие. если мы заложим сюда списки антология позволяет добавлять урлы, то мы будем в ответе получать, сейчас покажу, вот здесь на этой картиночке видно, что какие-то альтернативы SAMSA у нас, помимо того, что она перечислила альтернативы, Здесь виден кусочек запроса, вот этот как раз Sparkle, который мы отправили в онтологию. Очень похож на селект сиквела на самом деле. И ответ получился такой емкий, очень насыщенный фактами. Движемся дальше. Помимо многочисленных промышленных применений антологии, есть еще масса интересного, как это может быть использовано, например, в робототехнике. Потому что вот знаменитые ролики, когда перед роботом раскладывают на столе предметы типа яйца, сковородку. ложку вилку и говорят вот что ты можешь с этим сделать робот начинает разговаривать слух размышлять но очевидно если здесь яйца и если здесь есть сковородка то наверное я могу пожарить яичницу и понятно на бэкэнди у него скорее всего работает лм данный момент то есть скорее всего этот reasoning, то есть размышления, вытащены из LLM, потому что, конечно, рецепт яичницы в LLM есть и объяснение, зачем служит сковородка и яйца. А дальше роботу задают вопрос, а что бы ты сделал с этими предметами? Как бы ты пожарил яичницу? Он говорит, наверное, я бы взял сковородку, разбил бы яйца и поставил бы ее на плиту. То есть вы понимаете, что LLM могут использоваться в робототехнике как своего рода антологии знаний о мире. То есть мы можем вытаскивать, поскольку LLM обучена на куче данных, в принципе она очень многое знает о мире. И это может быть использовано для того, чтобы помогать робототехнике размышлять над задачами. Прелесть использования онтологии в том, что мы можем дообучать роботов буквально на лету. То есть в такой ситуации, когда доставили в робот, например, GPT-chat 4, задали ему задачу, которую он не смог решить в силу отсутствия знаний. Но не знает он, кто такой Павел Соловский и как с ним бороться. Или он не знает, как ему управлять станком, данным станком. Именно данной модели не обучался он на ней. Мы можем спокойненько подложить ему антологию или подложить ему файлы, и он их изучит. То есть, либо будет использовать как рак модели, либо построив антологию, он сможет объединить уже имеющуюся его базу знаний, которые мы ему даем. получается, что в принципе это такой ключик промежуточный к тому, что пока модели плохо обучаются на лету и три огромных ресурсов, я прочитал статью у Антона, он там описывает подробно эти проблемы, то как раз антология является таким мостиком сейчас, то есть можно их использовать для того, чтобы добавлять знания, нужные знания, без обучения модели. Еще я пока расскажу про Эйдесы. Года четыре назад я размышлял о том, как построить эйдетический искусственный интеллект на основе Эйдесной модели. 

S02 [00:37:05]  : У вас даже доклад, по-моему, у нас был на семинаре на эту тему. 

S00 [00:37:08]  : был такой доклад, и в конце вы мне задали вопрос, как бы можно представить модель AIDS, то есть на что это похоже. Тогда я сказал, что это похоже скорее на статью из Википедии, где есть Простейшее определение объекта, обозначены все связи, логическая схема. Он может быть включен в качестве базовой структуры в сложные конструкции, может сочетаться с другими ADS-ами и так далее. Так вот, сейчас, мне кажется, я гораздо ближе к пониманию, что такое ADS, в крайней мере относительно онтологии. И у меня есть стойкое подозрение, что Антологии являются по большому счету эйдосами и совокупностью эйдосов. То есть они как раз являются такой моделью. Это несколько философское отступление, но тем не менее, мне кажется, оно расширяет понимание глубины антологии и что такое мир идей Платона. Вот эти кусочки графов и графы и антологии, которые соединяются одна с другой. И естественно, рано или поздно они соединятся в общей антологии. То есть мы можем предположить, что существует антология всего. Ее называют картиной мира, философии. И как ни странно, тоже может помогать в том числе и роботам, потому что не имея общей антологии, исходной антологии, очень сложно оперировать с кусками антологии. То есть робот будет иметь какую-то делать функцию, но при переходе, при подключении другой задачи он будет теряться. И вот сейчас стратегическая цель, по крайней мере моей команды, это выход на генеральной антологии, когда мы сможем в принципе добавлять любые навыки знания и непротиворечиво их использовать для решения задач. Пару слов о том, где мы сейчас с точки зрения сильного искусственного интеллекта. Вот опять же в статье у Антона есть замечательная диаграммка. где он говорил, что тест-тюринг неформально, мы прошли в 23 году, и в 20 каком-то году LLM превзойдет галлюцинации, то есть научится бороться с галлюцинациями с помощью self-building ontologies. Вот этот self-building ontologies, то есть онтологии, которые LLM может строить, я вам показал, мне кажется, Это доказывает то, что self-building у Tab тоже уже пройдет. То есть в принципе мы можем строить антологии с помощью LLM. Моя команда это доказывает. И вы можете сейчас попробовать проводить эксперимент. Следующее на 30-е годы. у Антона отмечен как подход incremental learning mastered without catastrophic forgetting, то есть инкрементальное обучение. Как я уже говорил, моя команда сейчас, тактически эта задача поставлена. То есть, если мы сейчас хорошо наберем финансирование и бизнес проектов, которые мы сами себя финансируем, у нас нет инвесторов, то я думаю, мы это решим через несколько лет. То есть, гораздо раньше, чем в 30-е годы. По крайней мере, смею на это надеяться. Пожалуй, финальные слова о том, насколько это все может быть полезным, даже не для клиента, а в целом для человечества, для людей. В первую очередь, как я уже говорил, снимы искусственный интеллект. Это возможность работы с уникальными базами знаний, еще с секретными базами, например, или с информацией, которую мы не хотим отдавать наружу, например, в GPT-чат. Я не упомянул о том, что моя команда научилась, собственно, запускать LLM. Но это не Rocket Science. Многие пытаются запустить LLM on-prem, то есть без выхода в интернет. Проблема в том, что такие LLM они маленькие, то есть мы не можем продублировать для 5 чат всю ее мощь. И какая-нибудь Lama, даже вот сейчас вышла четверка, если не ошибаюсь, все равно она немножко уступает по качеству. И запуск таких моделей без интернета, он сопряжен необходимостью покупки серьезных ресурсов и не каждая компания это потянет. к тому же для скорости работы часто используется квантизация, то есть модели сжимаются, теряя качество, но использование RAGA то есть либо баз данных, либо антологии, либо даже обычных семантик RAG, значительно повышает качество ответов. То есть, опять же, использование данного подхода дает нам возможность работать с очень маленькими моделями, не теряя при этом значительно в качестве. а у заказчиков, которые могут себе позволить собственный законтур, мы можем поднимать модели, которые будут работать, которые работают, например, с финансовыми данными или с данными, которые имеют уровень секретности. И, опять же, никуда в интернет выходить не будет. То есть, это уже такой искусственный интеллект, который может работать внутри предприятий и вполне себе продуктивно. Обращение к данным напрямую, оно открывает массу преимуществ, начиная с того, что отчет буквально на лету, то есть мы можем вот так в режиме вопросов-ответов работать с отчетностью, то есть нам не нужны сложные системы, мы можем напрямую общаться с данными. Мы можем извлекать из данных различную информацию, которая не очень видна наружи, минуя, между прочим, ALAB кубы, минуя аналитические системы. Или наоборот, строя RAG над аналитическими системами, и тогда мы получаем тот самый explainable AI, когда мы можем объяснять аналитические данные, почему то или иное решение придется принять. Что нарисовано на графике? Почему, например, открытие филиала, склада на Урале будет более продуктивным для логистической компании, чем открытие этого склада, например, в Москве? И вот помимо математической фактуры, мы можем объяснять эту фактуру с помощью вот такого разговора с данными. ну и вишенка на торте это персонализированная память то есть сейчас опять же моя команда работает над проектом где мы пытаемся антологии приспособить для чат ботов что это нам дает проблема клиентов мы пообщались с заказчиком прошло какое-то время Заказчик опять нам приходит. Хорошо, если мы сохранили историю коммуникации с ним, а если не сохранили, а если нас история коммуникации с ним перевалила вот за те самые 300 страниц текста, то скорее всего мы будем забывать ключевую информацию о клиенте и о том, о чем мы разговаривали с заказчиком. А если у нас учебный бот, который должен обучать студентов. Он же должен помнить, что было на прошлом уроке, как-то держать в памяти программу, учитывать индивидуальные особенности студента. И опять же здесь антологии могут помочь, потому что мы можем сжимать данные уроков, сжимать процесс коммуникации со студентами и превращать это в в базовом варианте просто в базу знаний, а в идеальном в антологию. И тогда мы получаем практически неограниченную память таких коммуникаций. И мы можем очень сложные вещи коммуницировать с заказчиком, со студентом, с коллегами, при этом не забывая контекста, не теряя памяти того, о чем мы говорили. По большому счету у меня все, я готов ответить на ваши вопросы. 

S02 [00:46:36]  : Павел, большое спасибо. Очень интересно. Отдельное спасибо, что отнеслись к моим пожеланиям ответить на вопросы, которые были заданы в статье. Коллеги, если у кого-то есть вопросы, то прям давайте у нас народу немного на лету. Включайтесь, задавайте, но пока включайтесь, хочу уточнить следующее. Вот эта вот обвязка или фреймворк, с помощью которого вы автоматизируете то, что вначале вы показали вручную. Тот весь код, которым вы с нами не можете поделиться, он у вас написан на чем? Вы какой-то фреймворк используете, типа ланчейна или ему подобного, или это у вас все чисто свое? 

S00 [00:47:18]  : Нет, там есть LangChain, там есть Python, LangChain, LangGraph, то есть все открытые фреймворки. Ключевое здесь даже не фреймворк, ключевое здесь именно понимание, как извлекать из LLM, как LLM использовать и заставлять ее делать с текстом то, что мы хотим. То есть вот с кодом ее уже научились делать, она уже программирует и даже сайты пишет, по крайней мере, интерфейсы. Черт побери, что же мешает с текстом, заставляет его делать то же самое. Я вам показал, что это возможно. 

S02 [00:47:51]  : То есть, грубо говоря, ваше решение состоит из некоторой ЛЛМки в данном моменте. Это, как я понимаю, ЧАД-ЖПТ, но в принципе это может быть любая. 

S00 [00:48:03]  : На ЛАМе мы тоже делали такое. 

S02 [00:48:04]  : Дальше, в качестве фреймворка у вас лендчейн, лендграф, пайдай, падантик и все, что там вокруг, но основная ценность – это ваши набитые вами шишки и ноу-хау того, как это все дело соединить для того, чтобы работать с графовыми базовыми данными и на чтение, и на запись. 

S00 [00:48:24]  : Да, ну и по секрету скажу, у нас есть методолог, с которым мы консультируемся, который съел собаку на антологиях. И он понимает, что спросить. То есть, опять же, видите, человеческий фактор. То есть, тут даже не математика, тут грамотная постановка вопросов и задач. 

S02 [00:48:42]  : Вот так я бы сказал. Хорошо. У меня еще много вопросов на самом деле, но давайте дадим возможность задать коллегам. Алексей, пожалуйста. 

S01 [00:48:54]  : Добрый день. Алексей Незнонов. У меня два маленьких вопросика. Первый — это опробовали вы тем же самым образом работать не просто с кусочком, по сути, текстового представления, антология, да, как с предикатами на текстовом уровне, а с каким-то как раз цифровым двойником, который может обращаться к какому-нибудь имитационному моделированию. Ну, простейшим, например, имитационной модели в виде какого-нибудь набора дифференциальных уравнений. 

S00 [00:49:24]  : Сейчас у нас идет проект с логистикой компании, где мы будем решать такие задачи. Пока не решали, но я думаю это возможно. То есть вопрос в том, чтобы преобразовать данные или пояснить данные. Вот иногда мы приходим к заказчику, а там куча табличек с циферками. Конечно, мы из них никакого смысла извлечь не сможем. Мы просим составить описание, то есть аналитики делают описание. Данные циферки или данные таблички, значит вот это, а вот эти таблички, значит вот это. Можно комбинировать два подхода, то есть мы имеем какую-то математическую модель или мы имеем, допустим, SQL database. структурированную базу данных. И мы над ней уже строим онтологию. То есть у нас получается такой вот блин. В таблицах есть метаданные, есть какие-то комментарии, которые в принципе при поиске никак особенно не используются. Если ты не знаешь структуру своей базы данных, ты не сможешь толком с ней ничего поделать. Но если использовать эти метаданные плюс добавить хорошее описание, можно построить онтологию над табличными данными. И тогда у нас получается полная красота. Мы задали вопрос голосом, текстом. Можно голосом, speech-to-text используя. Например, какие у нас обороты за прошлый год? Покажи в разрезе, я не знаю, топ-5 клиентов. В принципе, в запрос можно прямой идти в SQL, а может идти к антологии, которая ответит, даст ссылки на таблицу. И мы в нашем ответе увидим человеческий ответ, увидим ссылочки на таблицы и можем зайти туда и проверить. Вот такая просто фактически копайлот аналитика, можно так сказать. Она облегчает, например, знакомство с базами данных для новых аналитиков в разы, потому что приходят люди, они не знают, как доставать данные, потому что надо изучать, как устроена база данных. Эта штука сильно облегчает работу с Data Lake. озером данных, когда у нас как попало, как попало в кучу свалено разной разносортной данной. Просто путем построения онтологии мы можем заставить эти данные, мы их формализируем, то есть мы получаем хоть какую-то структуру, с которой уже хоть как-то можно работать, а не просто кучу файлов. Я не знаю, ответил ли я на ваш вопрос. 

S01 [00:51:56]  : Да, более того, даже ко второму перешли, который еще не задал. А второй был как раз про то, что если мы имеем источники из разных кусков нашего знания корпоративного, то у нас могут быть файлы, например, с разными единицами измерения. Более того, если у них эти единицы измерения совсем разные и нет сходу в тексте формулы перевода, что самое страшное, да, то начинаются, как мы говорим, галлюцинации реального мира. То есть у нас начинается при, например, построении BI, а, кстати, сейчас модельки, BAM-ки, да, очень хорошо делают базовый BI, да, вот вы его как раз начали описывать. Но если как раз они используют разные таблички, в одной табличке у нас там метры квадратные, да, а в другой там дюймы квадратные, то начинается просто ужас. И мы на этом деле как раз очень капитально зависли. Почему? Потому что здесь либо надо тогда с нуля проходить весь онтологический цикл, из базовых мета-онтологий строить сбоку, имея онтологии нетеизмерения, выравнивание всего этого, например, в СИ. Либо как раз нужно что-то еще придумывать, но нас напрягает именно галлюцинация. Почему? Потому что они очень часто хотят придумывать несуществующие единицы измерения и тому подобные вещи. Или, например, какие-нибудь древние единицы измерения, локти, которые они где-то там у себя случайно ищут. Вот с этим и сталкивались. 

S00 [00:53:25]  : Да, при попытке опять же построить, выжать из данных что-либо, если данные необъяснимые, то нужно просто докладывать аналитическую, типа аналитической записки. То есть вот это значение значит то-то. Здесь, пожалуйста, вот используй такую таблицу координат или мер. Здесь локти переведи в ярды. 

S01 [00:53:50]  : Не помогало. Оно, поскольку не в антологии, он на этапе как раз обращения к антологии как раз и начинает дополнительно эволюционировать, когда переводят это в текст. А вот если это в антологии лежит, то все хорошо. Вопрос, как это положить в антологии в нужный момент. Вот вы же очень четко говорили, это мне очень понравилось, о том, что вы саму бямку, да, заставляете делать антологию из текста. Допустим, вы заставили делать из двух разных текстов одни измерения, а другие другие. Если вы теперь самой бянке скажете, считай, что там дюймы, а тут метры, то это как раз нормально не помогает. 

S00 [00:54:31]  : не сталкивался, но я думаю, что может помочь онтология. То есть ей можно задать что-то типа... вообще, конечно, у нас есть нормальные сантиметры и метры, но вот в данном случае применяем вот такой расчет. То есть что-то типа альтернативной онтологии. Чем прелесть онтологии? Ты, в принципе, можешь дописывать в нее дополнительное расширение понятий. То есть, например, сказать, что вот это, конечно, сантиметр, Но у нас используются также ярды. И вот тебе формула. А вот здесь вот URL, который, пожалуйста, имей в виду. Мы можем добавлять. А здесь сказать, что у нас вот эта онтология устарела, то есть она относится к 2020 году. Но появилось новое, которое уже к 2024 году. И мы будем знать, что есть и старое, где вот так считали, а есть и новое, где вот уже так считают. 

S01 [00:55:24]  : То есть вы тоже предлагаете это именно на уровне слияния онтологии все-таки делать? 

S00 [00:55:29]  : Ну, как первый подход к штанге, да. 

S01 [00:55:33]  : Попробовать. Большое спасибо. 

S00 [00:55:35]  : Коллеги, у меня одна минута буквально. Я сорри, у меня еще тут встреча. 

S02 [00:55:40]  : А, ну хорошо. Ну тогда жалко. На самом деле очень много вопросов у нас оставалось. Но, тем не менее, спасибо за доклад. Было очень интересно. Остальные вопросы… Я могу попытаться в чатике поотвечать или что-то… Да, давайте я тогда в чат набросаю еще вопросы, которые есть, а вы как сможете ответить. В общем, Павел, огромное спасибо за доклад. Было очень интересно. Приглашаю вас, если состоится у нас семинар в Рыкьявике, удаленно выступить, то же самое на английском языке рассказать, может быть, только чуть-чуть более компактно. Это когда будет? Я сейчас не помню. Давайте я вам напишу. Хорошо. Спасибо большое. Я напишу. Спасибо за внимание. До новых встреч, до связи, до свидания. 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
