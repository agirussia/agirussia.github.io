## 16 января 2025 - Системы управления базами знаний в контексте управления данными - Алексей Незнанов — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/5pxIDlKa46w/hqdefault.jpg)](https://youtu.be/5pxIDlKa46w)
- [слайды](https://github.com/agirussia/agirussia.github.io/blob/main/presentations/2025/Neznanov_KnowledgeManagement4DataManagement_2024.pdf)
- [видео в ВК](https://vkvideo.ru/video-210968399_456239209)
- [видео в RUTUBE](https://rutube.ru/video/9880d2576f8fb1f0ffcf1676d3df2823/)
- [расшифровка](https://github.com/agirussia/agirussia.github.io/blob/main/workshops/2025/16_January_2025_Knowledge_base_management_systems_in_the_context.md)
- http://cs.hse.ru/ai/issa
- http://postnauka.ru/author/a_neznanov
- https://yandex.ru/video/preview/803415799613384695

*ВНИМАНИЕ: СЛЕДУЮЩИЙ ТЕКСТ СГЕНЕРИРОВАН ИИ, РАЗРАБОТАННЫМ УЧАСТНИКАМИ НАШЕГО СООБЩЕСТВА!*
  
**Суммаризация семинара:**

Общая Тема и Контекст:
Семинар посвящен обсуждению систем управления базами знаний в контексте управления данными. Спикеры затронули вопросы антологизации, интеграции знаний в корпоративные системы и перспективы развития интеллектуальных систем.

**Основные Тематические Блоки:**

1. Антологизация и Теория Антологизации:
   - Обсуждение постулатов, расставляющих практику антологизации и теорию антологизации.
   - Выделение универсальности GbIn и GbOut, независимых от информационных систем и моделей реальности.
   - Упоминание важности мастер-данных и качества данных в контексте антологизации.

2. Системы Создания Работы:
   - Обзор систем управления базами знаний и их интеграции в корпоративные системы.
   - Критика закрытости и дороговизны инструментов, а также упоминание открытых систем.
   - Упоминание ISO 15926 и его стоимости в 2,5 млрд долларов.

3. Логико-Математические Инструменты:
   - Обсуждение логик и математических методов в контексте антологизации и управления знаниями.
   - Упоминание охраняемых и относительных моделей численности.
   - Критика векторизации как метода гибридизации и предложение альтернативных подходов.

4. Мультимодальные Понятия и Данные:
   - Обсуждение мультимодальных данных и их интеграции в контейнеры знаний.
   - Упоминание перехода от XML к JSON и JSON-LD.
   - Развитие запросов от Sparkle к GQL с расширениями.

5. Гибридный Искусственный Интеллект:
   - Обсуждение перспектив гибридного искусственного интеллекта, сочетающего символическое и машинное обучение.
   - Критика векторизации и предложение графовых и мультимодальных подходов.

6. Корпоративные Решения и Модели:
   - Обзор корпоративных решений, таких как Microsoft Common Data Model и их переделки базовой терминологии.
   - Упоминание важности самоописуемых знаний и метаданных для управления мастер-данными.

Вывод:
Семинар показал значительный интерес к системам управления базами знаний и их интеграции в корпоративные системы. Были обсуждены различные подходы к антологизации, включая логико-математические инструменты и мультимодальные данные. Спикеры подчеркнули важность гибридного искусственного интеллекта и самоописуемых знаний для управления данными и знаниями в корпоративной среде.







S07 [00:00:04] : Коллеги, всем добрый вечер. Сегодня у нас в гостях Алексей Незнанов, и он нам расскажет про систему управления базами знаний в контексте управления данными. Алексей, пожалуйста. 
S01 [00:00:21] : Добрый день, коллеги. Давайте быстренько начнем. У нас сегодня будет несколько основных постулатов, которые расставят из предыдущих обсуждений практики антологизации и теории антологизации. Этот слайд хорошо указывает на основания, которые послужили базовым входом. Почему? Потому что GbIn, GbOut это универсальная совершенная вещь и она не зависит от того, какими мы информационными системами в принципе занимаемся или даже какими моделями реальности занимаемся. Отмечу, что здесь специально выделено в середине управления мастер-данными... Слайдов не видно. Интересно. Давайте еще раз. А вот так? Что-то появилось. 
S05 [00:01:18] : Замечательно. Нет, появилось... О, а теперь видно. Отлично. Спасибо. 
S01 [00:01:22] : Замечательно. Прошу прощения. И поэтому мастер данных и качество данных это два очень интересных столпа. Почему? Потому что через них мы увязываем антологизацию как управление знанием, И как раз увязываем что? Более общую рамку управления данными. У нас есть очень красивый слайд, свежий. Почему? Потому что я его поменял на Revised Edition дама ДМБОКа. И в любом случае это действительно наша общая рамка, потому что ничего более подробно описывающего всю область DATOPS я лично не знаю. Что важно, здесь действительно в начале 2024 года у нас появился Revised Edition. Все говорили о третьем издании, но третье издание задерживается. Но в результате они перерисовали как основные красивые базовые слайды, так и поменяли, например, идеологию базовой терминологии. В результате чего появилось это желтенькое пятнышко на самой книжке. И сама книжка требует теперь уже и нового русского перевода. У предыдущего издания хороший был русский перевод, отмечу. И, в принципе, здесь есть все, что нужно знать обычному человеку про управление данными. Что важно? Важно, что мы будем отталкиваться как от базовой терминологии, так и от процессов от нее. Что еще важно, это то, что дальше мы сейчас кратко пройдемся по управлению данными, чтобы как раз понять контекст. Более того, сразу будет в нескольких местах понятно, а где там вытекают антологии. Почему? Потому что в целом, да, про информацию и данные мы не будем повторяться. Это мы многократно обсуждали. Отмечу, что для нас данные, да, это то, что формализовано и является чем? Физическим объектом на неком носителе. Ну а дальше как раз мы уже говорим про информационные артефакты, к которым сейчас дойдем. При этом понятно, что у нас есть информационные системы, которые занимаются обработкой данных. Тоже не будем останавливаться. Дальше мы говорим, что ОТО важно. Данные-то, оказывается, есть свои, в своей информационной системе, и чужие, в чужих информационных системах. Отсюда возникнет у нас интероперабельность. Более того, именно здесь мы вдруг понимаем, что данные у нас набраны метаданными, и метаданные-то тоже данные, и появляются как раз своих мастер-данные, да, оперативко-исторические данные, абсолютно по-разному управляемые. Ну, а, соответственно, чужие делятся на референсные, то есть справочники чужие, да, и гармонизированные данные, которые мы себе тянем. Ну или с ними же гармонизируемся. Дальше скажем, что акцентов можно расставить здесь огромное количество. Вот этот слайд, который у меня уже стал стандартным, показывает, какие основные из них. От физики до нормативки и как раз юриспруденции. И, что особенно важно, в центре здесь есть вещи про гармонизацию, актуализацию, версионирование и прочее. И это как раз то, о чем дальше пойдет разговор в смысле управления знаниями. Также мы сразу скажем, что метаданные это наш всё, и более того, онтологизация в первую очередь естественно работает с метаданными. Почему? Потому что описание данных это во многом и есть знание с точки зрения нормальной корпоративной работы. Что еще важно? Метаданные бесконечны, их уровней бесконечное количество, но на самом деле мы обычно доходим до описания на естественном языке и где-то там успокаиваемся. При этом сейчас обычно выделяют порядка 20 уровней стандартных метаданных при хранении и передаче данных средствами современной усилительной техники. Имеется ввиду от каналов связи до и отдельных битов памяти и до прикладного программного испечения. Это число уровней огромно и в результате уже просто для согласования уровней метаданных требуются очень-очень серьезные знания. При этом это обычно является областью архитектуры данных и моделирования данных и туда мы сегодня не полезем. Дальше мы говорим, что у нас про метаданные в уже 2010-х годах появились очень хорошие учебники. Более того, они есть как вытекающие из предыдущего поколения, так и совершенно новые. И также сразу говорим, что когда мы теперь хотим у себя в качестве данных иметь что-то хорошо описанное, понятное и интерпретируемое, мы сразу говорим, о, информационные артефакты появляются. И вот тут-то начинаются очень интересные разговоры о том, а в какой парадигме с ними работать, а как и так далее. Мы сейчас самую важную, опять-таки, для корпоратов рассмотрим, а именно, что у нас появляются документы, как базовые информационные артефакты. Документы автоматически метод-данными снабжаются, и у них появляются, например, такие стандартные метаданные, как реквизиты. Ну и наконец документы появляются первичные, то есть которые по сути содержат измерения. И вторичные это анализы и интерпретация. Дальше мы очевидно говорим, что на самом базовом уровне с точки зрения метаданных у нас появляется основное метаданное суть формат. Что такое формат? Это способ кодирования в некой знаковой системе, которая ложится либо на логическую, либо на физическую структуру данных. Что важно? Все, что касается форматов, это знания. Вряд ли тут кто-то поспорит. Особенно с учетом того, что сейчас опять-таки у нас те самые 20 с лишним уровней метаданных требуют чего? Начиная от атомарных значений в виде чисел, строк, времени и прочего прочего и вплоть до форматов сериализации и десериализации нам нужно все это согласовать и Тут как раз появляются три базовых уровня моделирования данных. Почему? Потому что как раз они автоматически вытекают из всего того, что мы говорили об уровнях метаданных. И дальше как раз появляется необходимость описания документов с точки зрения их метаданных. Как? Так, чтобы это согласование шло от самого нижнего уровня до самого высокого. Ну и понятно, что у нас, да, в России, да, есть ГОСТ 7097, бывшая 6.30, и там, например, да, четко списком вводятся 30 базовых реквизитов и документов. И если мы до этого знаем, с какими форматами данных с точки зрения хранения и передачи, да, в памяти и по сети, например, мы работаем, то дальше говорим, а давайте кусок из тех данных, которые хранятся или пересылаются, да, скажем, что это документы, у них есть реквизиты. И дальше возникает вопрос об его содержании. Понятно, что с точки зрения международки, да, Dublin Core там известно, есть много чего еще, туда тоже не будем сейчас лезть. Что важно, как только мы говорим, что у нас появляется кусок данных, мы очень любим называть его файлом. Отсюда файловые системы. И, что особенно важно, тут же появляются знания о том, как файловые системы у нас работают именно для организации информационных систем в самом общем смысле. Но, что такое содержание файла? Это как раз, в первую очередь, опять-таки тот же самый формат, только теперь формат конкретно файла. И есть обалденный справочник на библиотеке Конгресса, который называется Format Descriptions, где можно посмотреть самые популярные форматы, и более того, можно их там посравнивать, посмотреть историческое развитие и так далее. Обалденный источник. Что еще очень интересно, это то, что сейчас мы обычно всегда говорим при серьезных информационных системах не о файлах и не о файловых системах, а о базах данных. И в базах данных у нас что появляется? Некая единица хранения, зависящая от модели данных этой базы данных. Отсюда как раз сначала все развитие классическое, вплоть до альфа и омега SQL, а дальше нод only SQL. Для некоторых студентов вызывают очень интересные эмоции, когда они понимают, что нового это не нет. Ну и что интересно, здесь это действительно торжество о метаданнах. Почему? Потому что дальше мы поймем, что с точки зрения приложений у нас любая база знаний, Это часть базы данных. Что важно? У нас с точки зрения того, что уже говорилось про куски данных с позиции своей информационной системы и чужой информационной системы, есть очень хорошие варианты формализации чего? Как раз мастер данных и мета-данных. Мне очень нравятся фридманские варианты, когда мы эти данные разделили на то, что уже раньше сказали, но дальше поставили стрелочки. А что у нас первичное и вторичное? И оказывается, что справочные данные и схемы Первично мастер данные это все объединяет, а как раз к ним уже идут оперативка исторические. Ну и, наконец, когда мы говорим о хранилищах данных, то есть, на самом деле, о больших наборах баз данных в разных, возможно, парадигмах сделанных, у нас появляются те самые хранилищ, как Warehousing, и появляется знаменитый спор Инмана с Кимбалом, и витрины данных. Почему витрины данных важны? Потому что это тот взгляд на данные, который и требует антологизации с позиции конечного пользователя. Дальше появляются всевозможные варианты того, как мы с этим работаем. Появляются датапроводы, которые обеспечивают пересылку данных туда-сюда-обратно. Ну и самое главное, что у нас появляются типы датапроводов разнообразные. И появляются естественные TL и LT системы. То есть такие датапроводы, которые не просто присылают данные, но еще в процессе серьезно их трансформируют. где трансформируют и как трансформируют это очень важный вопрос и это еще одна точка стандартной антологизации вот так Что еще важно, как только мы это все вместе соединяем, у нас появляется семантизация процессов управления данных и датапроводов, когда у нас управление с точки зрения ДМБОК идет уже всем по уровням метаданных в больших хранилищах с разными парадигмами и архитектурами данных и возникает что? возникают большие данные. Туда тоже не лезем. Важно, что как только мы говорим о больших данных, то надо подметить, что они очень часто и бывают внезапными. То есть мы работаем, работаем с маленькими данными, потом хлоп, у нас появляются большие. Туда тоже сейчас не лезем, хотя и интересно. Ну и наконец последнее здесь это понятие самой интероперабельности и почему именно из него в основном вытекают требования к антологизации разных уровней. А почему? Потому что мы только что сказали про большие хранилища, дальше сказали про представление в виде всяких витрин данных для конкретных. целей конечных пользователей и вдруг выясняется что нам нужны специальные усилия для того чтобы обеспечить совместимость да причем совместимость на разных уровнях абстракции дальше положить это в сервисе конкретных решений на разных платформах И главное, потом уметь автоматически конвертировать форматы данных при общении на разных протоколах с разными интерфейсами тех отдельных информационных систем, которые входят в нашу большую информационную систему. Отсюда очень хорошее расширение сейчас OSI. И референс-модул OSI на семи уровнях уже активно развился в более интересные системы, но о них тоже не будем говорить. Что интересно, если, так сказать, надо войти в эту тему, то есть очень хороший проект прошлого десятилетия, европейский Interoperability Framework, и как раз у него есть очень красивая базовая картинка, которая говорит, о чем это вообще. Что нам здесь важно, это то, что у нас есть в том числе выделенная семантическая интероперабельность, И вот она как раз и ведет нас к представлению о базах знаний с точки зрения корпоративного рынка. Что теперь? Теперь чуть-чуть совсем о качестве, поскольку я говорил, что это второй столб. И здесь у нас получается, что стандартные есть вещи, и мы дальше можем их раскрывать вплоть до тех знаменитых 55 стандартных характеристик качества данных, которые сейчас в последних учебниках приведены. Но, да, полнота, корректность и актуальность это столбы. Дальше мы говорим, что у нас они все время раскрываются, раскрываются, раскрываются. И появляется, что возможность от атомарных данных и до как раз полностью некой информационной системы работать с чем? С продуктами данных. Продукт данных оказался исключительно важной концепцией и как раз не только восьмитысячная серия стандартов про само качество данных, но и в первую очередь Square, двадцатипятитысячная группа стандартов обеспечила, что нормальное выравнивание работ по всему IT-информатике как таковой, дальше инженерия программной и инженерии данных. и как раз продукт данных здесь вылез по сути сам собой и мы говорим о качество продукта данных это не качество программной системы в целом и не качество программной системы отдельно от того какие они данные обрабатывают и не качество алгоритмов да это нечто свое и как раз очень хорошие здесь справа есть классические термины Их здесь не 55, чуть меньше, но зато да, в них можно в каждую углубиться, прочитать отдельных несколько лекций по каждому из этих слов и потом вернуться обратно к требованиям той самой антологизации. Почему? Потому что именно требования того, чтобы данные были качественными, во многом вызывают на практике ту самую антологизацию, которая становится драйвером качества данных. Ну и наконец, что сейчас с этим происходит? Происходит очень интересная вещь. Мы в виде развития искусственного интеллекта в целом тут же дорабатываем сами механизмы управления данными и появляется знаменитый естественно 8183 группа стандартов Data Life Cycle Framework. Почему? Потому что жизнь-цикл издеважен. Это тоже можно долго раскрывать. А дальше еще более интересные вещи. Для машинного обучения свой POD-фреймворк. 23.053. Отмечу, что это, видите, 23-тысячная группа, что очень интересно, да? И она очень интересно стыкуется с 22-тысячной и 25-тысячной. Ну и, наконец, знаменитый 52.59 уже сейчас, да? Который говорит, что у нас появляется именно Data Quality в решениях на основе машинного обучения. При этом тут еще есть слово аналитика, но в принципе это все про анализ интеллектуальный, или как говорят в России, интеллектуальный анализ данных с учетом методов машинного обучения. Шикарнейшая вещь. Почему? Потому что она уже разделилась на два стандарта в 2023. Отдельно стала базовая вещь, а отдельно теперь тоже фреймворк. Отсюда у нас теперь есть общий фреймворк, да, дальше общий фреймворк машинного обучения с точки зрения как раз того, а что такое искусственный интеллект в этом смысле, интеллектуальная система, и, наконец, а то, как качество данных трактуется с точки зрения решения наношения машинного обучения. Там же и пребиасы, там же и как раз про сертификацию, да, и критические системы, и все остальное. Отмечу, что сейчас у нас стандарт, который вышел как по большим данным, так и по машинному обучению, да, они серьезно отстают. Они остались на уровне где-то 2010-х годов. Ну и как раз теперь про знания. Если мы понимаем, что у нас есть дата Ops, то теперь у нас хлоп, и авиаопса, и МЛОпс, и мы такие, чтобы что-то с этим делать, надо каким-то образом всю инфраструктуру обработки данных положить на алтарь искусственного интеллекта. И вот ее по-разному кладут. Я же заявляю, что в любом случае нам нужно явное знание, поскольку только оно реально измеримо именно как знание формальное, и им в результате этого измерения можно управлять. Дальше мы говорим, что социтное знание тоже жутко важное Но именно его мы туда и обратно, между явным и неявным уровнем, каким-то образом разгребаем. И нам важно его превратить в явное, в конкретной информационной системе. Дальше. В прошлый раз и потом еще летом говорили много о том, как вообще антологизируются объектно-признаковые данные на основе заземления реального мира. И как раз у нас там в начале возникало измерение контекста измерения. Антологизировались по разным парадигмам про объектно-признаковые данные, пространственно-временные данные, и выникало как раз понятие шкалы единицы измерения, и появлялись базовые антологии, дальше появлялись общие антологии и, наконец, антологии высокого уровня, а потом уже все это сводилось к преклонным антологиям. Туда не лезем в базу, просто быстренько проходим. Первое. У нас есть знаменитая 80-тысячная группа стандартов про то, что такое вообще измерение с точки зрения количеств. Она так и называется, группа Quantities and Units. Шикарнейшая вещь. Я к ней периодически обращаюсь. Дальше. Мы как раз говорим, что с 1993 года, когда появилась эксплицитная спецификация концептуализации, много чего произошло. На самом деле давно уже мы в основном ссылаемся как минимум на студеры. про формальную и точную спецификации которые да еще и общие и еще кстати одновременно до человека машина читаем и все остальное Дальше говорим, что вообще говоря, сами онтологии как таковые, даже если рассматриваем стандартные три способа их определения, то есть онтологический про бытие, алгебраический про формальные системы и инженерный про базы знаний, это ни о чем с точки зрения того, что нам-то нужно, чтобы в конкретной системе, где есть работа с данными и ввод-вывод появилась антологизация с точки зрения чего как раз анализа интерпретации и возможности получения вторичных документов из первичных и хорошо бы чтобы потом возникла автоматическая интероперабельность и о ней не надо было серьезно отдельно думать вот такой вот отдельный аспект ну и как раз дальше что получается есть требования к антологиям туда тоже это все и так знают появляются базовые схемы формализации, начиная от классических логик и преснопамятного пролога до всевозможных понятийных структур, мультиграфов, гиперграфов и тому подобное. И, наконец, что очень важно, у нас логика разделяется как бы на три очень больших куска. Классическую, да, дальше то, что поверх многосортной логики предикатов первого порядка и, естественно, common logic, да, и как раз все, что дальше, сейчас называем их не классическими, не стандартными и потом подобными логиками. Нам особенно важна значимость логики, но это не будем сейчас даже рассматривать. И, наконец, то, что минимально необходимо конкретно классическим корпоративным рационным системам для того, чтобы работать с мастер-данными, как минимум, дескриптивка. Почему? Потому что без боксинга, боксинга-боксинга и прочих вещах, связанных с классификациями и возможности выведения многосортных переменных, нам не жить. все что выше, да, это уже хорошо бы, но не обязательно в конкретном приложении. Имеется ввиду и активные логики, и логики как раз пространства временных всевозможных связей, да, с обязательным предикатом предшествования, и так далее, так далее, так далее, вплоть до логик причинно-следственной связи, смотри за буков Y и все остальное. При этом для даже только классических декоративных логик число резинеров, которые умеют с ними работать и с различными видами подлогик, огромное количество. Выделяют там обычно около 15 классов, из них примерно 5 легко разрешимых, остальные 10 сложно разрешимых, и понеслось. Снизу здесь примеры некоторых списков резонеров и как раз понимание того, что это огромная отдельная и очень важная область, без которой, в принципе, невозможна эта логизация. Она есть. Мне очень нравятся вот эти два учебника базовых. То есть первый 2007 года, второй через 10 лет. Первый про Server Implementation Applications, Description Logic. А второй про Introduction to Description Logic. Причем обе отмечу от Кембриджа университет пресс. Есть еще очень интересные MIT учебники, но они мне меньше намного нравятся. Здесь уже на вкус на цвет. Что еще интересно? Понятно, что у нас есть много отдельных миров, в которых мы занимаемся сначала организацией логик и решателей логических, а затем как раз всевозможных на всех уровнях метаданных вариантах антологизации. Большинство из них, к сожалению, закрытые. И это очень печально. И поэтому под Индией приходится про это рассказывать. Но, к счастью, есть один открытый, который поддерживается World Wide Web Consortium на сайте v3.org, и он пока что еще с точки зрения базовой стандартизации основан на XML, хотя уже есть все варианты на JSON и прочем, и потихоньку очень активно преобразуется. Я когда выступал весной с докладом, да, 20 лет спустя, как раз оказалось, что вот прошло 20 лет, наконец-то все начало меняться. При этом эти 20 лет отсчитываются обычно с 2004 года, когда как раз стандартизовался первый ООЛ. Извиняюсь, как он произносится, но он не сова, а язык описания антологий. Отмечу, что на практике он сам по себе используется как бы с одной стороны полностью с другой стороны без добавок очень редко это то же самое что с искуэлем четыре тысячи страниц до а4 да никто либо не знает либо они все сразу не нужны нужны что то еще что интересно да тоже почти На границе 2010 года вышло несколько учебников. Я здесь прикопал парочку тех, которые лично тоже мне нравятся. Первый очень веселый и как раз Леманговский «Чем хорош?» он показывает базовые потребности построения semantic web идеологии и этим исторически ценен. а второй как раз объясняет всякие интересности с точки зрения разработки именно сайтов и тоже этим интересен понятно что сейчас тоже все перерабатывается о чем далее что важно мы говорим что с точки зрения развития у нас возник стандартный от версионирования всех этих стандартов и соответственно многие старые решения оказались сейчас чисто легоси часто бывает куда деваться Более того, прямо в данный момент мы работаем над вариантом перехода от Legacy решения к не Legacy решению. И все это сталкивается с тем, что когда у нас IT-инфраструктура обработки данных одна и антологизация одна, обработка данных становится другой и антологизация становится другой. Почему? Потому что ничего кроме нормальной обработки данных с точки зрения инфраструктуры компьютерной у нас для вас нет. Что еще важно? Важно, что как только мы говорим о базовой формализации, то понятно, что можно посмотреть, чем народ балуется из открытого. И дальше мы увидим, что основной СУБЗ, поскольку является Neo4j, то на ней же есть замечательнейшие туториалы и описания того, как мы работаем с чем-то, с семантическим представлением в виде семантических сетей либо графознаний. Отмечу, что сам по себе Knowledge Graph стал настолько обширным термином, что в него входит все. От добавок для ллм и лбм, до как раз всевозможных формальных систем алгебраических. И никто не знает, что в конкретный момент обозначает Knowledge Graph. Надо всегда уточнять. Мы говорим, что он может обозначать что угодно с точки зрения формализации, интерпретации, изоляции и хранения знаний. С точки зрения графового подхода. Понятно, что у нас, как только мы говорим о семантических сетях, возникают онтологии разных уровней. И предметные онтологии обычно надо разбирать в конкретной области. Сейчас несколько примеров приведу. Как раз на верхнем уровне появляются онтологии верхнего уровня. Они же на верхнем уровне, значит верхние уровни. Здесь примеры знаменитой БФО. Понятно, что их много. И дальше пример Так, это сама БФО. Все, наверное, знают прекрасно, что это такое, да? А дальше как раз знаменитый обзор 2020 года от рабочей группы по АМФ. Мы к нему еще раз вернемся скоро. И там как раз у нас есть классификации. Антология уровня по принципам построения, варианты выравнивания. И в принципе там у нас по-моему 38 штук антологии, среди которых там коллеги выбрали 8 основных, на которых можно основывать что-то серьезное с точки зрения корпоративного строительства. Что важно. Дальше мы в любом случае, когда получаем антологию верхнего уровня, берем некоторые общие антологии. Почему? Потому что, как я говорил, все упирается в заземление и как раз измерения, и нам нужны как минимум антологии и измерения. Их базовых примерно 5, QSD, MO, HV, VO и так далее. И на самом деле пример OEM от товарищей из Нидерландов один из самых удобных. Почему? Потому что он как раз ориентирован на сущности, а не на индивидуалы. И тем самым удобней с точки зрения объяснений. И плюс к нему есть очень хорошее выравнивание для почти всех приложений. Ну реально популярная штука. Дальше пример классический, да, просто привязки к контексту, где мы говорим, что измеряем. Дальше привязка к реальному контексту измерения, когда мы говорим, что у нас есть 4D. И появляются, да, точки, время и как раз то, что реально меряем, в данном случае температуру, да. Измерили температуру, получили время, получили точку и получили некую геопривязку, например. Понятно, что подобные схемы могут расширяться и расширяться и расширяться. Они, собственно, и расширяются. И эти контексты во всех серьезных системах есть. Про них мало говорят, потому что они типа и так есть, что о них говорить. Но зато, как выяснились обсуждения последних нескольких лет с людьми, которые начинают работать в области Они туда боятся лезть, потому что сложно. И это как раз несколько формальных систем разного уровня. Хорошо, что не все 20 уровней метаданных надо изучать. И потом это вызывает очень плохие терминологические проблемы. Проблемы понимания того, что такое шкала измерения. И проблемы хранения данных, которые завязаны на антологию с точки зрения как минимум интероперабельности. Но у нас, к счастью, есть базовый вариант посмотреть классические антологии почти во всех форматах. Почему? Потому что есть проект Stanford APRTG. Там как раз у нас есть одна из трех базовых терминологий, где у нас Entity Class, Entity Individuals и как раз нормальная базовая прорисовка. в виде как раз эматической IT и атрибутированного мультиграфа, который имеет несколько очень интересных свойств. Понятно, что во многих областях предпочитают работать с фреймами, во многих областях предпочитают работать с автоматическим выравниванием небольших антологий и, соответственно, там главное это автоматическое сразу задействование некого движка логического вывода А можно все это загнать в одну, выровнять изначально и даже про движок логического вывода не говорить, поскольку всегда можно сделать запрос на почти человеческом языке сейчас, а логику оставить где-то за кадром. И то и другое тоже вызывает очень серьезное обсуждение. Почему? Потому что база знаний сейчас, как я говорил, в любом случае это часть некой базы данных, и в результате интерфейсы многообразные. Некоторые приземляют на SQL, некоторые приземляют как раз на SPARQL, некоторые приземляют на какие-то специфические языки, и в результате это растет как снежный ком. Например, у Palantir есть набор обзоров, не буду я сейчас на них тут ссылаться, они, к сожалению, недоступны нормально в открытую, где очень интересно прослеживаются требования, задач расследования, investigation, с точки зрения как раз онтологических привязок разного уровня. Обалденная вещь. Мы же с вами вернемся к тому, что заявлено в начале доклада. А как у нас, да, в несчастном кровавом интерпрайзе устроены базы знаний. И поскольку про некоторые из устройств все равно под индей, мы мазками и примерами. Итак, первое. С точки зрения контекста измерений, он есть всегда, без него ничего не бывает. И Кровавый Энтерпрайз скромно заинтересован в том, чтобы не просто он был, а все как раз с точки зрения контекста измерений было выверено, автоматически поддерживая интероперабельность и прочее. Здесь очень красивый пример. из знаменитого цикла Cross-Industry Semantic Interoperability с акцентом на IOT-платформы и картинки из того же самого примера, из части 4D, где указано, что да, любой объект это на самом деле не просто объект, а объектопризнаковые данные, где признаками являются шкалы и значит измерения имеют большой-большой контекст, который как раз у нас справа изображен. При этом, видите, автоматически этот контекст расширяется вплоть до топ-левел антологии и бизнес антологии. Ведь LOA и BOA обязательно присутствуют. Возможно, не явно. То есть, если мы прокрутим назад до такой вот картинки, и здесь увидим значение минус 24,11, то на самом деле оно автоматически нас всегда выводит на BOA и LOA. Возможно, на несколько BOA. Ну и дальше как раз возвращаясь обратно, мы говорим, что с точки зрения онтологии, да, есть на все ссылки, но само измерение это что такое? Это обязательно датчик и соответственно контекст датчика. Дальше как раз некие события, которые предшествовали и посуществовали измерению самому. Да, обязательно контроллер, который преобразует это в некий цифровой двойник измеряемого. понятия почему потому что в любом случае датчик увязывал с неким понятием дальше возможно дополнительная цепочка контроллеров агрегирующая причем и по времени и по месту и по как раз набору понятий и наконец это все куда то уходит с точки зрения хранения данных Почему именно хранение данных? Потому что в классическом варианте описания антологии это хранить не получается. И это невозможно в принципе. Попробуйте большой временной ряд, многомерный, сохранить в виде части какого-нибудь знаниевого графа. Ну и наконец, теперь уже совсем важные вещи. Что мы видим? Сейчас мы потихоньку переходим от хранилищ данных, озером данных. Некоторые, правда, говорят про болото данных, да. И здесь, естественно, только антологизация. У Microsoft Command Data Model, да, у Oracle, да, он, например, называется Oracle Data Model базовый, да, и так далее, так далее. У всех есть. У Amazon есть, у, значит, Google есть. И без этого не жить. В этом смысле у Microsoft это самые хорошо описанные в открытых источниках начиная с ссылки на из же Common Data Model. И я его люблю показывать, поскольку он является основой чего? Знаменитого справочника архитектурных решений для Data Lake. Шикарнейшая вещь. Дальше. Если мы говорим про антологизацию, в первую очередь мастер данных для того, чтобы управлять качеством данных, то у нас у того же Фридмана есть набор очень крутых статей про то, что Давайте передумаем о мастер-данных и референс-данных как-нибудь. И окажется, что после этого передумывания у нас получается, что и референс-дата, и на самом деле мастер-дата это антология. То есть это хорошо было понятно еще на самом деле в начале 2000-х, когда начали заниматься как раз интероперабельностью объектных брокеров, но полноценно формализовалось только на границе 2020-х годов, то есть где-то лет пять назад. Ну и дальше мы говорим, что все бизнес-правила в классических IT-системах, а также все, что над ними уже находится с точки зрения реальных сценариев работы, это все тоже должно быть антологизировано, чтобы как минимум не запутываться в том, что является бизнес-правилами. Классическая ситуация на не антологизированных или недостаточно антологизированных IT-системах в чем заключается? В том, что вы пишете бизнес-правила, потом они начинают противоречить друг другу, В них все допутываются и начинают активно забивать. Чтобы этого не происходило, единственный вариант это нормальная онтологизация. Дальше. Это конечно же у тех же людей, о которых я ссылался как на обзор онтологий, есть обзор методов интероперабельности и как раз моделей данных для этой интероперабельности. Понятно, что все это сейчас ссылается на ISO 1033. Да, 10000 группа. И он постоянно обновляется, причем хорошо обновляется. Последнее обновление было в прошлом году. И да, у нас действительно есть Product Data Representation and Exchange. И, что важно, У нас есть вот такие вот картинки. Они многообразны. Это непосредственно из начала этого обзора. Но на самом деле их рисуют все, начиная от ISO до NIST, заканчивая GOSTом, в разных учреждениях. Почему? Нужно на всех указанных ранее уровнях метаданных и вариантах, и парадигмах обработки данных и хранения данных, что сделать? Привязать это к нужным антологиям, чтобы сохранилась интероперабельность. На самом деле, изумительно страшный вопрос. Как только туда погружаешься, волосы седеют, и потом приходится их, наверное, красить, да? Что здесь уже видно? Здесь видно, сколько можно предложить стандартных реализаций, стандартизированных для того, чтобы разбить на кусочки. Начиная там от Digital Twin'a, да, и заканчивая, например, вещами, связанными просто со GeospatialData, да, где есть там, вон, CityGeographyML и прочее-прочее. И это еще маленькие картинки, они на самом деле намного больше, просто показывать на слайде неудобно. Ну и есть у нас классический IFC, да, это фреймворк как раз интероперабельности, который говорит, что если мы выравниваем здесь 303 стандартом, то нам надо что обеспечить? Во-первых, да, халархия и таксономии, а во-вторых, возможность коинцензоризации по любым отношениям. Как-то недавно тоже это обсуждали. Я удивился, что народ в принципе не знает базовых международных стандартов, которые развиваются уже почти 30 лет, с 90-х годов, и обеспечивает сейчас реальную интероперабельность в полностью автоматическом режиме огромных объемов данных с очень сложной структурой. Да, это в большинстве случаев серьезные и дорогие решения, но для них есть в том числе и некоторые открытые вещи. Например, на основе онтостипа. Стип здесь это как раз сокращение от Standard for Exchange of Products. Кстати, раньше было очень крутое сокращение YIB, по-русски красиво звучит. От него как раз потихоньку отошли сюда. И, например, от коллег, которые в НИСТе об этом писали, да, причем писали уже 15 лет назад, да, есть свеженькая реализация. Они в то время сделали в 2010 году, а теперь в 2022 году повторили. Это уже на новой технологической базе, да, у них называется New Implementation. Замечательно. Любое количество уровней, шкалирование, масштабирование и возможность как раз Четко связывать между собой выровненные по ISO 1033 системы. Шикарно, шикарно. При этом как именно хранить в конкретной СУБД, да в общем всем на самом деле плевать. Почему? Потому что вот это все прекрасно раскладывается на стандартную революционку, если есть нормальное отображение. Другое дело, что в каждой из конкретных систем обычно это сделано как всегда не идеальным образом, потому что исторически складывалось и потихоньку именно с точки зрения драйверов искусственного интеллекта сейчас активно перерабатывается. Это как раз то, почему на новой схеме. Во-первых, потому что как раз у них теперь совершенно упрощенная система с точки зрения базовых языков и технологий. А во-вторых, очень круто сделано то, что да, есть теперь стандартный вывод ВОЛ. Более того, есть и экспрессовый вариант, и оловый вариант, и возможность как раз канализации на тоже совершенно простых вещах, на основе RESTful сервисов, ресурсно ориентированных, а не так, как раньше, через VSDL-ку, с swap-ом и так далее. В этом смысле я, как ужас, вспоминаю классические объектные брокеры конца 90-х типа Корба и так далее. И возможность там работать с антологиями с огромными усилиями и лишними словами. И на каждые 10 слов одно реально нужное. Сейчас с этим все намного проще. Поэтому, с точки зрения примеров областей, приведу только две. Первая — медицина. В медицине у нас давно все онтологизируется. Почему? Очень легкая вероятность того, что при какой-то ошибке кому-то очень сильно прилетит. И цена ошибки — это человеческая жизнь обычно. Сейчас напомню, у нас примерно на каждые 10 врачебных вмешательств одна врачебная ошибка, и примерно из каждых 10 врачебных ошибок одна приводит к существенному вреду для здоровья. Причем из всех этих врачебных ошибок больше 60% легко как раз недопускаемы, если следовать стандартам. Отсюда как раз и все мили и миллиарды долларов. от SnowMEDа и UMLC до всевозможных предметных антологий на любую тему. Более того, в рамках HL7 есть Assert and Syntax, а в FHIR все это перетекло в более простом режиме на основе вествал сервисов, джейсончиков и всего остального. И все это активно сейчас используют с точки зрения интероперабельности. Почему? Потому что базовый HL7, а также все его расширения с точки зрения и упрощения с точки зрения FIRE, они реально работают. Вся область медицины сейчас на этом стоит. Дальше область нефтегаза. Знаменитый 2,5 миллиарда долларовый ISO 15926 который оказался настолько крут, что им тяжело пользоваться, и все пользовались кусочками. Но, тем не менее, до сих пор именно его базовая терминология и стандартные маленькие кусочки интероперабельности вовсю используются. В том числе есть его представление с точки зрения базовой антологии на основе тезавруса на сайте 1926.org. Вот пример запроса, что такое Data Management. И как раз это напрямую перетекает во все следующие стандарты. Что такое следующий стандарт? Это OpenSAPS OSData Universe, в котором у нас есть OSD антология. Поддерживается сейчас акцентурка и неплохо поддерживает. И как раз там все строится на основе сначала верхнего уровня антологии, самой области, потом как раз общая онтология единиц измерения, там своя, но выравниванная с QED-C. А дальше как раз у нас есть архитектура базовая, которая в том числе учитывает онтологию на уровне описания сущностей и выравнивания. То есть здесь естественно мы видим в первую очередь управление метаданными и вопрос А у нас как с точки зрения, например, человека, который хочет на открытых решениях что-то сделать, туда зайти? Понятно, что есть система управления самыми семантическими сетями типа Neo4j. К ним есть, например, Neo Semantics как биндинг для несчастного Спаркля, а также классический туториал о том, как с самых простых задачек дотянуться, в кавычках, до звезд. Можно смотреть на это с точки зрения метаданных. И есть открытые, вполне приличные системы создания, выравнивания, выдачи и всего остального. И в них во всех, когда они развиваются, появляются. Как здесь, например, Entity Reference, как ссылки на онтологию. И ссылки на активы других типов с точки зрения выравнивания и синонимов. Business Slurry и все остальное. То есть сейчас в открытых решениях управления метаданными есть возможность нормально стыковаться с антологиями в виде семантических сетей, а если даже это не сильно нужно, то на уровне бизнес-глассариев делать себе фреймовые антологии. Ну и наконец, с точки зрения основных игроков. Понятно, что стандартом де-факто многие считают полпати. Да, вот семантика компании австрийской. Почему? Потому что они давно это делают, выровнялись со всеми стандартами, да, включая I-25, I-964, да, I-1033 и все остальное. И как раз у них действительно есть все, что нужно корпоративному заказчику, вот и до. Другое дело, что дорого сложно и, так сказать, внедрять особую радость, да, но работает. Симпатично. Одно удовольствие, например, работать там как раз с Тезаурусом, тут же находя примеры использования во всех документах организации, да, и, например, там делая работу с амонимией. Как скучаю, да. Дальше очень интересный пример Anzo. Почему? Потому что у него есть Open Anzo. Может кто-то видел, а может даже кто-то работает. Cambridge Semantics, они другой подход проповедуют. У них другая модель. Игра в Data Module у них другой. И в этом смысле они с Pool Party постоянно в контрах. На тот же уровень вышли и действительно конкурируют. Мне еще очень нравится Stardog. Почему? Потому что у Stardog вообще намного интереснее работа с графовыми представлениями, потому что они свой язык запросов, собственный, достаточно оптимизировали, чтобы автоматом делать на его основе красивые визуализации, без дополнительных преобразований и витрин данных. Это дорогого стоит, это очень круто. Поэтому в них, конечно, шикарнейшая вещь. И более того, они сейчас разрабатывают очень интересные решения для как раз использования в качестве input-output бемок. Ну и как раз импортоза. Я всего два примера назову. Тринидату и унидату. Тринидаты интереснее тем, что у них последовательно описались в горшковом книжке именно про онтологизацию мастер-данных, а потом управление метаданными. И как раз их система очиграф в этом смысле вполне себе работоспособна. Она, конечно, поменьше и попроще, чем предыдущие, но действительно рабочая. Уни-дата мне нравится само по себе меньше, но они зато более чарные и маркетинговые. И там Куницов с коллегами написали книжку «Ценность ваших данных». То есть не просто справочник того, как с антологиями работать, а скорее для менеджеров. В итоге с разных сторон я знаю эти фирмы, в том числе изнутри, и, пожалуй, остановлюсь на этом. Дальше понятно, что можно быстренько привести выводы с точки зрения того, что все называется по-разному, и птичьи языки надо знать. Я обычно студентам загоняю четыре основных термологических системы, из которых три постоянно используемые и четвертая историческая. И главное, что корпораты очень любят эти системы использовать в разнобой. И это печально. Дальше на самом деле все информационные системы серьезно антологизированы в той или другой мере. в той или иной мере. Почему? Потому что без этого в принципе невозможно добиться ни качества данных, ни управления мастер-данными. Но как раз эволюционно все эту потихоньку переписывают, иногда революционно. И например Microsoft с тем же Common Data Model очень интересно себя повела, когда им даже базовую терминологию пришлось переделывать. Но это отдельно надо рассказывать долго и красиво, но зато это ржачно. Дальше это очень-очень дорого. Тот же самый ISO 15926, 2,5 млрд долларов базовая оценка. У кого есть такие деньги, если захотим свою предметную область даантологизировать с достаточной мерой формализации, чтобы автоматом работала интероперабельность. В итоге получаются не пересекающиеся во многом миры как программирования, так и как раз архитектуры знаний, архитектуры данных. И действительно их все по сути объединяет некий академический мир, с которым все заходят в университеты. В этом смысле Stanford молодцы! Дальше. Гибридизация модели I, которая сейчас активно происходит, она это все очень хорошо перетешнула. Почему? Ну, в первую очередь, вход-выход наконец-то стал на более-менее нормальном языке. Во-вторых, мультимодалка пошла в жизнь. И в-третьих, это очень важно, наконец-то мы начали быстрее и полнее антологии получать и проверять на основе неструктурированных данных, то есть наборов текстов, картинок, диаграмм и тому подобных вещей. Там огромное количество проблем и главное все равно все это завершается нормальной формализацией с валидацией на уровне бизнес-правил, но действительно это хотя бы нормально заработало. Если в 2000-х годах об этом мечтали, в 2010-х делали долго-дорого, то сейчас это делают быстро, но с разным пока качеством и все предполагают, что где-то через пару-тройку лет это войдет в основные системы управления знаниями корпоративной. Дальше, технически, мы перешли по форматам данных от знаменитого мира XML к миру JSON. Можно отдельно рассказать про все подстандарты. Дальше от ALL и соответствующих кусков перешли к JSON-LD плюс Shackle в основном. И как раз на этом дескриптивка наконец-то пошла в широкие массы. И наконец, с точки зрения запросов, на самом деле все больше уходим от Sparkle классического и приходим к GQL с расширениями. Я уже видел десятки расширений. Но то что в SQL вошел, это конечно выдающееся достижение, считаю, позапрошлого года. И теперь мы действительно это интегрировав, позволяем развивать нормально интегрированные базы данных, базы знаний с совершенно новым поколением интерфейсами, как API, так и пользовательскими. И это круто. Ну и наконец, да, сам по себе вот вывод становится интеллектуальным и бянки и лэнки, они нас теперь поджидают на каждом углу и с ними в корпорациях активно борются с точки зрения всевозможных вещей, связанных с не просто как раз тем, что они что-то делают не так, эволюционируют, а с тем, что они должны как раз каким-то образом нормально в себя подтягивать контекст не только классический, например, раговский, да, но и графовый. Соответственно граф-рек разных типов на марше. Граф с точки зрения того, как представляют собой контексты тоже на марше, с цепочками объяснений, с графами объяснений и так далее, и так далее. Ну и, конечно, все это кончается моей любимой темой интерпрет... интерпера... Тьфу, все уже. Простите, коллеги. Интерпретируемостью методов анализа данных и хорошо интерпретированной методами машинного обучения искусственного интеллекта. Отмечу, что там в любом случае у нас есть как минимум общие антологии как раз про контекст измерений. Ну а дальше уже от преднатой области очень сильно зависит. Например, в медицине, да, очень интересно то, что есть антология и как раз базовые все вещи на, например, да, то, как производятся измерения медицинские. И для этого есть лоинг. Вот он. Если не ужмякнуть, она откроется на другом экране, да? Да, его не видно будет. Бедненький несчастненький Лоинг. Ну ладно, сейчас тяжело вывести будет. Я к тому, что в любом случае сначала антологизируется измерение их контекста, потом создаются некие этизаврусы на то, что мы меряем в конкретной области и как это просто связать с элементами данных, в том числе большими временными рядами и прочим, а уже затем все остальное. Без этого субстрата ничего красивого на уровне каких-то сложнейших понятий не вырастет. Ну и дальше здесь большой-большой кусок про уже то, как мы используем данные именно в машинном обучении, но я, пожалуй, на этом останавливаюсь. Как раз, в общем, быстренько уложился, можно было даже чуть помедленнее говорить, но зато можно обсудить хорошо. Спасибо. 
S07 [00:48:47] : Да, спасибо. Спасибо, Алексей. Очень быстро, очень емко. Графрак прямо то самое. Я что-то пропустил эту новость. 
S01 [00:48:55] : Майкрософт совсем недавно очень круто обновился. Как раз именно граф и вариант. 
S07 [00:49:02] : А можно сразу тогда то самое. А кто сейчас этот графрак поддерживает? Фу, графрак. Майкрософт. В SQL сервере? Нет. Почему? Отдельный проект. Какие графовые базы данных поддерживают 
GQL? S01 [00:49:19] : GQL, насколько я знаю, полностью еще не одна, потому что даже Mimer еще не заявляли о поддержке, а Mimer заявляют же всегда 100% поддержки SQL. нашу главную СУБД, 100% поддерживающую SQL. Поэтому не одна. Но Microsoft очень быстро написали в этом году транслятор в GQL из своего графового варианта в SQL-сервере. У них же там хороший граф-вариант, да? Оно вполне рабочее. Я думаю, что в 2016-й версии все будет. Да, 15 у них вроде не будет. И то же самое, кстати, происходит у несчастных рокловцев. Они чуть-чуть отпаздывают, но зато тоже неплохо, так сказать, над Легаси настраивают. А вот как раз все остальное, честно говоря, не знаю, потому что сейчас просто не делал обзор именно того, какие родмэпы у кого. Потому что подгребцы сейчас не этим озабочены, распределенку будут делать и шардирование. Коллеги из кусков именно графовых, у них основная сейчас проблема это оптимизация OpenCypher. для какой и прочем, но они почти все демки как минимум выкатили, они вполне работоспособные. Когда все это скажется на итоговом продакшене, посмотрим, но я думаю, что буквально за год, ну вот за 2015-25, да. И самое веселое, что у QL на самом деле много гитек. Почему? Потому что он совершенно не запрещает никакое хранение и как раз никакие расширения. И более того, к нему уже предложено как минимум несколько десятков расширений. Народ там в комитетах активно работает. И посмотрим, чем все это закончится. Я думаю, это хорошо бы к лету как раз будет обсудить, потому что будут понятны как раз уже роудмэпы. И вообще хорошо бы, чтобы кто-то, кто именно этим занимался, рассказал о текущем состоянии родмапов. Мне просто сейчас было некогда это рассматривать. 
S07 [00:51:24] : Окей. Вопросы у нас от участников. Так, есть от Алекса Шкотина. Алекса Шкотина. Три вопроса. Во-первых, что такое фреймовые онтологии? Какие антологии? Фреймовые антологии. 
S01 [00:51:40] : А, тогда мы говорим, что основной моделью данных для хранения знаний является фрейм. 
S07 [00:51:50] : Ну а антология какая? Фреймы же могут быть разные, и мы соответственно фреймы описываем с помощью тех же самых антологий, правильно? 
S01 [00:52:06] : Да нет, совокупность фреймов это и есть антология То есть даже есть соответствующая формальная система Антология на основе фреймов Но я скорее спрашивал о языках, на которых записана эта антология О, вот это сходу я наверное не вспомнил, потому что это было достаточно давно Я последний раз серьезно работал году в 2008 с этим 
S05 [00:52:28] : Пока об этом не речь, то это моё дилетантское мнение, это старая терминология. 
S01 [00:52:35] : Дело в том, что коллеги, например, у меня сейчас работают с этим и, значит, совершенно не плачут, а наоборот радуются и посматриваются высока. на меня, который сейчас вынужден нагрузить 2.2.3.0.0 файлы. Почему? Потому что у них, если есть хорошая инструментальная поддержка, все замечательно. Потому что там же тоже классические декоративные логики, боксинг на основе как раз типов фреймов. Там же у нас есть трафрейм-темплейты и реализация фреймов. И значит как раз матчинг на основе как раз заполненных слотов этих фреймов. Очень хорошая штука. 
S05 [00:53:11] : Так а что это за система? 
S01 [00:53:14] : Что это за система? Ну в данном случае, которую я упоминаю, да, это банковская система большая, которая много где работает по миру. Называть не могу. 
S05 [00:53:25] : Спасибо. 
S01 [00:53:27] : Следующий вопрос. Да, давайте. 
S07 [00:53:31] : Следующий вопрос тоже от Алекса. Можно ли сказать, что T-Box это схема? 
S01 [00:53:42] : Ну, в принципе, да. Потому что с точки зрения лингвистики ничего этому не противоречит. Но я бы так не говорил. 
S05 [00:53:51] : Ну, вроде бы это ее логическая, скажем так, функция по отношению к абоксу. Мы просто терминологии дескриптивных логик. 
S01 [00:54:02] : Просто схема уже занята, поэтому если мы посмотрим, например, любой учебник про ООЛу и дескриптивную логику, там так не говорят. Но можно, я думаю. 
S05 [00:54:14] : Нет, но мы же не про то, как говорят, а про суть дела. Терминология везде разная. Так же, как с Фреймом. Можно покопаться, что за ним стоит структурно-математически. То есть я имел ввиду чисто логический, не терминологический вопрос. Безусловно, Т-бокса не называют схемой, а его логическую функцию по отношению к А-боксу. 
S01 [00:54:39] : — Так, я поэтому сначала сказал, что ничего лингвистически этого не противоречит. Можно назвать... То есть я не вижу проблем, чтобы это было схемой. 
S05 [00:54:47] : не лингвистику, а судьё. 
S01 [00:54:50] : Спасибо. А суть терминологически просто сейчас это слишком схема загруженная слово с точки зрения терминологии, поэтому в дескриптифике так не называют. А смысл я бы как раз Короче, если кто-то скажет, что в моей терминологии это назовем там схемой, да, и соответственно будем каким-то образом по этим схемам кто-то строить, как бы именно по схемам в классическом понимании, да, в общем, то я не против. 
S07 [00:55:16] : Да, я понял, спасибо. Так, еще вопрос Алекса Шкотина. И на каких языках идет антрологизация? И потом рука Андрея Воронко была. Дмитрий Иванович, вы после Андрея тогда. Значит, Алекс, от вас вопрос. На каких языках идет антрологизация? 
S01 [00:55:34] : Идет, на самом деле, на любых языках, начиная от естественного епитона и заканчивая, соответственно, Спарки или чему угодно. Почему? Потому что у всех волнует интероперабельность каких-то описаний и как раз возможность запросить из них что-то. И это потихоньку плывет очень сильно. То есть я не зря же отметил, что если посмотреть обзор, тот самый, да, на всякий случай, давайте я на него еще раз переведусь, он реально клевый. Где он тут был? Бам-бам-бам-бам. то здесь можно увидеть, что рассматривается там огромный список из десятков различных систем и у всех используются в принципе у себя какие-то разные языки, но все говорят, что у нас есть Вот такая вот штука для выравнивания. И на границах у нас есть интерфейсы. Для интерфейсов есть соответствующий протокол. И в начале 2000-х это был XML, но до этого был вообще, допустим, как раз KIF. А дальше это сейчас JSON-основанные симпатичные представления, которые являются контейнерами для примерно таких вот штук. Поэтому сам по себе язык, да, описание конкретной предметной онтологии, да, на самом деле в индустрии вообще никого не волнует. От слова совсем. Это очень крутое заявление. Ну, с чем сталкиваюсь? Понятно, что это мой опыт, да. По моему опыту никто из... каких-то руководителей проекта или направлений, никогда не запрещал рассматривать никакие из языков протоколов, форматов API для работ с антологиями. Поскольку большинство с Rostec закрыты, кроме Semantic Web и еще парочки тестовых, то это в любом случае выбор слишком многокритериальный. Пространство парэта просто огромное. 
S07 [00:57:41] : Окей. Андрей Воронко, пожалуйста. 
S06 [00:57:45] : Здравствуйте. У меня вопрос по… Когда вы рассказывали про IoT, вы сказали, что нельзя хранить данные… Если я правильно понял, нельзя хранить данные от датчиков и такую информацию в виде графа или знаний. У меня два вопроса. Первый, у меня есть версия ответа, но я хотел бы вашу услышать. Первый вопрос, почему? И второй, если это не хранится в виде графа или знаний, то получается, что под каждый формат датчика, под каждый протокол нужно делать какие-то специальные Ну, утилиты, преобразования или что-то, и это все делается вручную каждый раз, чтобы система общим работала. 
S01 [00:58:32] : Спасибо. Первый кусок – да, второй – нет, если кратко. Почему? Первое, потому что, когда мы пытаемся хранить большие объемы данных, они у нас достаточно однородные, и бессмысленно каждый акт измерений снабжать полным контекстом измерения. Там используется система иерархически вложенных контекстов измерений, которые агрегируют пространство, время, объекты, холорхи, системы и тому подобное. То есть мы, например, говорим, что у нас есть временной ряд датчика, а рядом еще один временной ряд датчика. Эти датчики на устройстве. Соответственно, сначала агрегируем по времени, потом агрегируем по устройству, потом агрегируем по какому-то активу, ассету и так выстраиваем иерархию механизмов хранения и на каждом уровне у нас свои ссылки на элементы онтологии. И естественно мы говорим, что у нас есть как раз описание так называемых продленных контекстов. То есть, например, временной ряд, да, это с такой-то там дату, да, или с такой-то секунды и прочее. Там время, извиняюсь, пространство, да, в таком-то куске пространства Земли, да, или космоса. Оборудование, да, там такие-то устройства или такой-то актив. И как только мы начинаем это агрегировать, у нас на агрегатах возникают свои ссылки на как раз те самые продленные контексты. То есть мы говорим, что для одного измерения у нас просто отметка времени, а для всего временного ряда отрезок времени. И понятно, что автоматом встают задачи поиска как раз контекст-дрифта, дата-дрифта и всего остального. Это очень важная задача. И автоматом встают задачи автоматического продления контекстов. Почему? Мы такие, а можно ли продлить, например, контекст через точку смены зимнего времени на летнее? А не забудем ли мы это, да? А можно ли продлить, например, в пространстве, да, все больше и больше расширять контекст? Почему? Потому что Земля же там, допустим, у нас все-таки геоид с очень сложной формой, да? А мы, например, сначала думали, что у нас просто координаты x, y, да? Мы такие, ой, да, надо, значит, теперь переходить в геокоординаты и так далее. И вообще задача расширения контекста измерения и продления его, да, исключительно сложная задача, но зато очень важная. Только она позволяет делать что? Первое, да, снизить пространство хранения. Второе, повысить скорость отдачи и накопления. То есть и LTP и OLAP программы будут работать более-менее быстро. Если все это попытаться охранить в одном, еще круче, в одноуровневом варианте графа, то и не скорости, и огромные объемы памяти. Плюс все равно придется строить очень сложную систему иерархии контекстов, чтобы как минимум делать логические утверждения о том, кто в кого вкладывается. 
S06 [01:01:25] : А есть стандарт на это? 
S01 [01:01:27] : Да, на практике это все делают заранее. И как раз потом действительно приходится все время сталкиваться с преобразованием форматов, Да, с преобразованием единиц измерений, шкал и тому подобных вещей. Ну а куда деваться? Я еще ни разу не сталкивался с системой, в которой бы это все сделано было на одном принципе каком-то. 
S06 [01:01:51] : Понял, спасибо. 
S01 [01:01:56] : Упоился вопрос. Как оцениваете перспективы автоматизации формирования антологии в ближайшие годы? Какие основные точки соприкосновения используют антология языковых моделей? Ну, я этим и закончил. Я очень хорошо оцениваю, потому что мы действительно пришли к нормальной скорости хотя бы чернового производства из неструктурированных данных антологии и антологически сконтролируемого поиска в неструктурированных данных. И главное, к нормальной скорости поиска аномалий и всевозможных гадостей в антологиях. И это действительно будет активно расширяться. И языковые модели здесь очевидны зачем? Затем чтобы как раз в сырых и кривых данных как раз хоть что-то находить и нечеткое. В этом смысле товарищ Задеб очень порадовался. 
S07 [01:02:43] : Спасибо. Еще 10 вопросов были выше. Дмитрий Иванович Свериденко. Можно ли подробнее о логике математических инструментах создания и работы с антологиями? 
S01 [01:02:54] : Это надо отдельный доклад дать, я увидел. Дело в том, что здесь был кусочек только по тому, почему антологизация с одной стороны кажется скрытой, а с другой стороны везде есть, и почему ее как раз мало кто знает. А вот как раз не просто о базах знаний, которые всегда интегрированы куда-то, а о самих системах создания работы это надо отдельно. Я сначала хотел привести нескольких примеров, но не смог их вывести из под идеи в принципе, поэтому о них надо на долгом горизонте попробовать сделать примеры, переговорив со всеми возможными, так сказать, участниками. Это все серьезные инструменты, которые я знаю, да, они все закрыты или очень дорогие, а открытые, и так вы все знаете, да, и протяжей, и джену там, и прочее. 
S07 [01:03:44] : Спасибо. Дмитрий Иванович, здесь у Вас еще вопрос есть про студента. Доступны ли Ваши... Несколько раз были упомянуто что-то про студентов. Доступны ли Ваши лекции о данных, информации и знаниях? Поясните Ваш вопрос, Дмитрий Иванович. Так, если Дмитрий Иванович не может пояснить, здесь вот вопрос, доступны ли ваши лекции? Видимо вы говорили про то, что вы студентам лекции читаете. 
S01 [01:04:17] : Ну да, это же четыре смежных дисциплины. Дисциплина по как раз управлению данных в целом, по моделированию данных, по хранению и передаче данных и как раз управлению знаниями. знания в интеллектуальных системах, как она сейчас называется, классические дисциплины. Они, поскольку исходно сейчас все актуализированы корпоративными дисциплинами, автоматом не выкладываются. Старые версии даже выложены в интернет. Такие куски есть для двух актуальных дисциплин на вышке высшей школы экономики. А как раз в целом можно заказать как обучение корпоративное. 
S04 [01:05:00] : На какой сайте? 
S01 [01:05:02] : Презентацию, естественно, скину так же, как и предыдущую. Предыдущая оказалась изумительно хороша, потому что в ней почти все закрыты вопросы по тому, как мы переходим от самих понятий базовых измерений к объектно-признаковым данных, их обработке вплоть до модели данных. И я теперь просто на нее везде ссылаюсь. Так же попробуем сделать, может что-нибудь потом добавлю. 
S07 [01:05:26] : Я так понял, что народу будет интересно, если будут какие-то ссылки. Ссылки все кликабельные. И если вдруг у вас возникнет желание и возможность рассказать про отдельный доклад, как вы сказали, про логико-математические инструменты по созданию онтологий. а также по рефакторингу антологии и ревизии антологии, то это, я думаю, тоже будет очень интересно публике. 
S01 [01:05:55] : Да, тут появилось сразу несколько вопросов. Давайте с конца. Скину, естественно, коллегам. Антон просто положит на наш любимый сайт, так же, как и предыдущие две презентации. Что закладывают стандарты в онтологии хроморитмышления тех, кто далее этим пользуется? Какой процент стандартов понятия онтологии, что вы описали, были выработаны на Западе? Как вы оцениваете перспективы русского интеллектуального универитета? Ой, какой сложный вопрос. Ну, процент, наверное, около 99. и то все русские кто там участвовал да они участвовали совместно с кем-то из зарубежных коллег на самом деле это нельзя говорить там о западе потому что очень было много восточных коллег например все все основания до работы с понятиями до это и россии и запад и восток вместе начиная до отгон вели заканчивать шоу дженом И с точки зрения телефонного суверенитета, я позволю себе не отвечать на вопрос, поскольку это может привести к плохим последствиям. 
S07 [01:07:02] : Бурной дискуссии, чрезмерно бурной дискуссии. Хорошо. Коллеги, есть ли еще вопросы? Уважаемому докладчику. 
S01 [01:07:12] : Если письменных вопросов нету, можно... Да, если что, вместе с Гантером Грюли работал наш руководитель Кузнецов. один из главных, так сказать, людей УСИИ. Или как раз те же Чоу Джун, Джан, как его правильно, интересно. Значит, они работали вместе с индийцами, да, Агравали. Агравали, все, наверное, знают, да? Вот. А они, в свою очередь, работали с 
MIT. S07 [01:07:33] : Так, здесь меня просят отправить ссылку на сайт. Сейчас ссылку на сайт отправлю. Так, коллеги, еще есть ли вопросы какие-то уважаемому Алексею? Да, можно дискуссию. 
S01 [01:07:50] : Потому что это в принципе же продолжение предыдущей встречи. Предыдущая встреча она как раз была же на слайде с знаменитым Антоном с архитектурой. 
S07 [01:08:01] : Ну, я не знаю, значит, как мы можем устроить дискуссию, если на самый интересный вопрос вы сказали, что это заслуживает отдельного доклада. Вот. То есть, может быть, меня бы интересовала, во-первых, все-таки спойлер на тему того, как вы создаете, для начала автоматически создаете онтологии. Вот. 
S01 [01:08:28] : Я занимаюсь архитектурой данных на основе онтологий. Я готов отвечать за управление мастер-данными и соответствующие онтологии мастер-данных. Общими онтологиями начинает единица измерения, Я думаю, что я знаю все антологии, которые связаны с единицами измерения, контекстами измерения и так далее. Дальше я могу говорить о качестве данных, говорить о governance в целом, то есть о прослеживаемости, версионировании и прочем. Причем и для антологии, и для моделей данных. Ну и наконец, о как раз механизмах хранения, преобразования, повышения эффективности. А вот как раз о выявлении знаний из неструктурированных данных. Я давно не занимался, я в последний раз сам серьезно занимался этим в 2012, когда мы с коллегами о домашнем насилии где на первом канале потом нас очень хорошо и как раз после этого момента я больше этим не занимался а вот сейчас же коллеги занимаются и очень интересно это делают на основе как раз одновременно граф рагов да и например биомаг разных типов особенно инстрактовых очень-очень интересно 
S07 [01:09:43] : Понятно. Но на самом деле, если вдруг у вас есть коллеги, которые могли бы это рассказать, я думаю, не только я был бы признателен за приглашение на наш семинар. А у меня тогда другой еще вопрос. Коллеги, если какие-то есть еще вопросы по докладу, вы встревайте, а я буду пользоваться паузой, чтобы задавать докладчику вопросы, которые меня интересуют. Смотрите, тогда за inference. У меня сейчас сегодняшний кейс. Я долго-долго держался, у меня не было практических поводов заниматься LLM. А сейчас я на одной задачке тестирую наши нейросимвельные модельки по сравнению с LLM. Я наблюдаю следующий феномен. Сегодня целый день этим занимаюсь. Я беру некоторые данные публичные, тестовые из интернета, где нужно решать задачу классификации. И беру нашу опять-таки с полки публичную символьную модель, которая решает задачу классификации. Классификация текстов, все банально. И наша моделька решает эту задачу классификации на двух корпусах данных разных совершенно, с какой-то точностью, с какой-то скоростью. И двумя ЛЛМ-ками, Олямой и Гвеном вторым, я тоже решаю ту же самую задачу классификации. И дальше я сравниваю две вещи. Я сравниваю точность, я сравниваю производительность. Без какого-либо тюнинга, без какого-либо дообучения, без какого-либо контекста наша моделька из коробки дает на 2% лучше, больше, чем любая из этих ЛЛМ-ок. При этом LLM примерно работают медленнее в 2000 раз. Соответственно, отсюда я делаю вывод, что если у меня одним из составляющих является производительность и стоимость инференса пресловутого, то символьные методы на моем сегодняшнем кейсе примерно в 2000 раз дешевле, чем Понятно, да? Вот. И, соответственно, возникает вопрос, да? А вообще, насколько, значит, на сегодняшний день, вот с учетом того, все, что вы рассказали и не рассказали, мы можем говорить о том, что все-таки в долгосроке мы должны будем учиться строить символьные модели? делать инференс на символьных моделях методами той или иной вероятностной или невероятностной логики. А трансформеры, в том виде, как они сейчас есть, останутся как промежуточный этап. 
S01 [01:12:39] : Ну, мне кажется, что они в любом случае останутся как промежуточный этап, как минимум потому, что сейчас уже видны по свежим публикациям новые классы моделей, в том числе наконец-то с как раз дообучением настоящим, да, и как раз, например, с автоконструктированием и прочим, которые, скорее всего, будут дальше гибридизироваться не просто на уровне решателей и тулов, как, помните, вы в тот раз красиво говорили, да? А на уровне как раз того, что перестроенная архитектура сама может стать неким тулом, выделиться и стать агентом. В этом смысле сейчас же свежие многоагентные представления. которые говорят как раз что у нас на входе-выходе есть нормальный язык описания про который в том числе я говорил например там 1033 стандарты сочные вполне могут обмениваться данными формально и потом валидировать это какой-то логикой и иметь tool, машинка вывода, причем любого там, что дескриптивка, что логика возможностей, что еще что-то И дальше как раз спокойно работать. Мне кажется, что текущие бямки как таковые, да, это реально промежуточный результат, который, с моей точки зрения, выхлоп оставят в виде генеративок мономультимодальных. А сами по себе они, конечно, очень серьезно преобразуются. В чем проблема с точки зрения классических вещей, связанных с анализом текста, не на LLM? С тем, что они дорогие в разработке. То есть, как я уже говорил, там разное количество миллиардов долларов стоят сами антологии, да? А хорошо доделанный движок логического вывода, плюс обвязка, плюс нормальные коннекторы к данным, плюс как раз конвертеры, плюс все остальное, это тоже очень дорого. Почему? Потому что там у нас нет возможности сказать, мы в 99% там будем правильно работать. Нет, там нужно 100% ровно. И в результате стоимость очень большая. И в этом смысле мы по сути размениваем стоимость ручной доводки, даже если программист использует LLM в качестве копайлота, на попытку сделать очень быстрый прогресс самих систем вплоть до AGI, где AGI это все равно, по моему мнению, будет система, которая будет оркестрировать огромное количество всевозможных решателей. Потому что я, как, например, АГИ уже сделанный, я все равно же использую и решатели всевозможные, и как раз калькуляторы и прочее. И поэтому я не очень понимаю все эти разговоры о том, что видите ли, трансформер плохой, это АМАГИ. Но это мое пока мнение, да. 
S07 [01:15:14] : Ну то есть в терминах современных трендов вы за мультиагентность? 
S01 [01:15:20] : В любом случае. То есть я считаю это абсолютно неизбежным. Но это мультиагентность на основе хорошо проработанных форматов аналога OSI, то есть интерфейсов до открытых систем. И соответственно с возможностью нормального аудита и некоторых решателей, которые обеспечивают объяснение. ну я в основном сейчас занимаюсь попутно как раз интерпретируемость и объяснением да и вижу что здесь как раз самый крутой прорыв произошел за последние пару лет а можете поподробнее в чем в чем вот именно прорывность точки зрения объяснимости 
S07 [01:16:01] : интересно у меня здесь старая или новая я сюда добавлял сейчас секунду потому что здесь у нас есть коллеги которые включая меня которые как раз занимаются объяснимостью в том числе 
S01 [01:16:18] : После того, как было предыдущее поколение, здесь оно есть как раз, да? То есть X-Plane и LAI от Google, Azure Machine Learning от Microsoft, соответственно, LimeShape и прочее от коллег. наконец-то до обучалка с учетом там констрастирования все прочее у нас появилось что во первых вот эта штука эта штука здесь в 2022 но они же упросьт статью 2024 уже выпустили она шикарная почему потому что они сказали что давайте мы объединим интерпретабилити значит эксплейнабилити дьюрабилити и много чего еще в релиабилити что в общем логично и дальше такие а давайте с разных сторон зайдем и они развивают плекс дальше очень круто развивают я тащусь откровенно и вот у них сначала было описано три варианта да это как раз калибровка неопределенности значит устойчивое обобщение и как раз автоматическая адаптация причем адаптация с акцентом на вот последнее зеро счет перформанс То есть мы показали одну картинку Антона и теперь модель сразу научится определять Антона. И самое веселое, что они выстроили иерархию моделей и как раз многоагентную по сути систему на основе этой иерархии, которая могла отменять распоряжение нижних уровней и в том числе в целом отказывать от классификации. Помните, в тот раз мы об этом серьезно беседовали про отказ от ответа? Вот, шикарнейший вариант. Вот в прошлом году был еще следующий прорыв, следующий уровень, где они туда добавили ллм-ки, инстракты, и получили просто невообразимое качество работы с контекстом естественного языка. Дальше у нас есть... А-а-а, здесь еще не было. Да, дальше, естественно, grefreg. Я ссылку кинул в чат. Видели, да? Он очень активно развивается в течение полутора лет уже. И что интересно, у них дошло все до того, что появились разные схемы и плагины промпт-тюнинга. Промпт-тюнинг в том числе обеспечил себе некоторые эксплейны, объяснения. И как раз появились обертки, которые теперь на основе этого делают аналоги Change of Thoughts и варианты всевозможных цепочек рассуждения, которые можно запускать параллельно, по-разному стахастически обрабатывать и ставить на выходе мажоритарный декодер. Оказалось, что этого достаточно для того, чтобы избежать почти 99% аномалий в ссылке на неправильные данные. Я не ожидал. Это оказался очень интересный прорыв, который совершенно был неожиданный для меня. кто бы мог подумать. Ну и, наконец, с точки зрения мультимодалки, я думал, что еще несколько лет народ потратит на то, чтобы сделать устойчивый перенос стилей по временной оси, а они хлобысик лету прошлого года сделали, то есть буквально за год. Но то, что было в качестве показов первых генераторов видео 2023 года, и то, что было выведено летом в продакшен прошлого года, то есть через год, это просто небо и земля. И сейчас коллеги, например, игроделы активно уже все это начинают использовать и очень хорошо настроены, что уже в следующем году появятся новые методы. Я просто, к сожалению, сам не отслеживал, какие. нормальной прогрузки по временной оси сразу многих объектов которые образуют сцену с нарративом на этой сцене это я считаю будет следующий супер прорыв а можно чуть подробнее прояснить про мажоритарный декодер это не про то как мы верифицируем релевантность ответов и отсутствие галлюцинации Нет, потому что мы это делаем теперь на следующем уровне. Мы делаем цепочки рассуждений и как раз промтюнинг, потом цепочка рассуждений уже на основе него, а потом уже, когда параллельно запускаются несколько цепочек рассуждений, стоим на выходе мажоритарку и говорим, что если у нас из 10 цепочек 7 выдали похожий ответ, то есть там нужна мера сходства естественно, то вот его и возьмем. А как мы меру сходства будем считать? Вот это сейчас главная проблема у коллег. Они пробуют очень разнообразные. Я им тут присылал, кстати, знаменитый же обзор 33 мер сходства векторов и прочее. И мы с ними там развлекались. Почему? Потому что оказывается очень легко почти любыми мерами сходства потерять что-то важное при рассмотрении достаточно больших кусков текста, где каждый термин может стать решающим. И сейчас, насколько я понимаю, коллеги там очень хотят сделать мажитарный декодер на основе как раз многочисленных проверок на сходство. Это будет примерно так же, во что уперлись системы рекомендательные. Когда у систем рекомендательных стало очень много атрибутов у объектов, И у них стало очень большое пространство парета в рекомендации, да? Они же не могут тебе выдать там... Посмотри, типа, вот эти 10 тысяч фильмов, да? Им нужно там один выбрать или пять. Вот. И там как раз стали делать много разных методов анализа сходства, да? Чтобы выбрать что-то из этого огромного пространства парета. Я думаю, здесь будет то же самое. А вот как они дальше это оптимизировать будут, я пока не знаю. Я просто не вижу какого-то механизма универсального, который бы это хорошо оптимизировал и сказал, что теперь быстрее все будет. 
S04 [01:22:04] : Окей. 
S05 [01:22:08] : Можно маленький вопрос? Конечно. Про пример. Если мы широко трактуем онтологизацию, а тут говорилось и про питон, и даже про естественный язык, то вот, допустим, у меня есть информационная система на предприятии, да, и кто-то вот, специалист приходит, говорит, слушайте, ребята, да у вас тут онтологизации нет, как вы вообще живете? Ну, информационная система, конечно, взаимодействует с другими. И у него есть какое-то обоснование, что, посмотрев, он ее не обнаружил. Тогда они говорят, ну, а как нам ее сделать? Да, это просто. А вот можно понять это как-то более детально? Что онтологизации нет, и потом, раз она появилась, даже неважно, какими средствами. Я вот что хочу сказать. Если вы говорите про повышение интероперабельности, Ну, это, наверное, всё-таки отдельный тип. Вопрос там. 
S01 [01:23:16] : Да нет, это основной тип. Почему? Потому что главные два свойства интероперабельной системы, которая антологизирована, это то, что всегда можно выгрузить кусок данных из этой системы, которые можно трактовать как знания. И второе, это то, что если мы из неё выгрузили Какой-то кусок данных чем-то дообогатили в другой системе, то всегда можно вставить обратно. И система поймет, чем дообогатили, почему дообогатили, как с этим разобраться с точки зрения хранения, пересылки, показа и так далее. Это два главных свойства, с моей точки зрения. Если мы показываем, что в той системе, о которой вы говорите, есть возможность выгрузить в контейнер внешние какие-то данные и отдать, и они самоописываемы внутри контейнера, только допустим, со ссылками на общую антологию и топ-левел антологи, то замечательно. А если еще мы этот контейнер можем потом изменить во внешней системе и обратно вставить, то вообще замечательно. 
S05 [01:24:22] : Ну так это все-таки интерфейс на уровне знаний. 
S01 [01:24:30] : То есть мы должны иметь самоописуемые знания, причем на чем большем количестве уровней метаданных, тем лучше. 
S05 [01:24:37] : Да, понятно. Спасибо. 
S04 [01:24:42] : Вопрос можно задать? Конечно. Скажите, пожалуйста, вот вы в конце своего доклада много посвятили тому, что нас ждет гибридный искусственный интеллект, сочетающий достоинства символьного и машинного обучения. Но сейчас основной метод это векторизация. А вот вы считаете ли вы, что для того, чтобы построить настоящий гибридный искусственный комплект, нужна своя модель вычисленности? 
S01 [01:25:07] : Обязательно. Я вообще не считаю векторизацию нормальным вариантом гибридизации. Это я считаю просто такими тестиками от бедности. Почему? Потому что на уровне векторизации мы сразу теряем то, что у нас и считается базовой семантикой. Поэтому, например, Microsoft GraphRack уже намного круче, потому что он не просто кусочками текста или векторами вставляет, а все-таки графами. А еще более свежие реализации, они еще более интересны в этом плане, поскольку они как раз мультимодальные. Мы говорим, допустим, что мы имеем некое измерение, К нему прикреплена картинка, к нему прикреплен таймстэмп, к нему прикреплено голосовое сообщение, к нему прикреплено видео. И значит все это вместе в контейнере, который обеспечивает разбор этого как куска знания. Вот это я считаю нормальным уже рагом. 
S04 [01:26:04] : Спасибо большое, бальзам на сердце. А вот вы как относитесь, скажем, есть такая модель высшего численности, называется относительный или охраняемый? 
S01 [01:26:15] : Я, к сожалению, не видел ни одного полноценного варианта, где бы это именно применялось для более-менее сложной задачи, а не было бы просто демкой. Если покажете, я просто буду счастлив исключительно. 
S04 [01:26:27] : С удовольствием. 
S01 [01:26:28] : О, круто, ловлю на слове. Просто я видел несколько демок, причем разных, в основном в университетах тоже разных, но вот как раз работающий не видел, а очень жаль. 
S04 [01:26:41] : Это с системой работы работающих с телекоммуникацией. 
S01 [01:26:45] : Ого, замечательно. Надо смотреть и потом обсуждать. 
S07 [01:26:50] : Дмитрий Иванович, а вы не хотите на эту тему доклад сделать? 
S04 [01:26:56] : Я задал вопрос по поводу логик и математических методов. Давайте послушаем, а после этого можно будет отреагировать. 
S07 [01:27:07] : Вопрос в смысле, когда будет доклад после доклада про логик и математические методы? Хорошо, коллеги, еще есть вопросы? 
S04 [01:27:20] : У меня есть вопрос по модальности. 
S02 [01:27:24] : Насколько я понимаю, две дополнительные модальности, которые вкручиваются, это визуальные и аудиальные, потому что с них легче всего данные собирать. Я бы хотел получше, чтобы вы раскрыли вопрос вот этих вот модальностей, которые добавляются. У нас есть, я предполагаю, какая-то модальность, в которой работает LLM-ка сама по себе. Далее мы хотим туда добавить визуальные, аудиальные данные, делать мультимодальные понятия за каждым словом. наполненные сенсорным опытом. А какие еще модальности могут быть? Не случится ли так, что мы будем добавлять все больше и больше вот этих вот сенсорных модальностей, но их качество будет посредственным? Это вот возвращаясь к вопросу философскому об идеальном сенсоре для восприятия реальности, да? 
S01 [01:28:30] : Ну, пока сейчас я знаю несколько очень хороших примеров робототехники, свежей прям вот, декабрьской, да, и кинестетика, плюс хаптика, плюс как раз стереозрение, плюс Лидар помогают очень хорошо. Я вообще за как раз прямое но заземление давайте скажем по-русски да и соответственно когда у нас есть граундинг то там автоматом возникает кроме видео и значит текста да естественно кинестетика хаптика и как раз допустим там лидар или что-то для 3d окружения и это обалденно то есть это сразу переводит систему совершенно другой класс другое дело что там как раз становится очень Как бы это сказать, длинные вектора. И поэтому сейчас активно народ работает над оптимизацией чисто вычислительной. Кстати, здесь многие ругали, например, NVIDIA за новые карточки. А я, например, приветствую то, что она намного быстрее работает с короткими числами. Почему? Потому что как раз возможно нужно именно так работать, но с большим количеством модальностей и с более длинными векторами. 
S02 [01:29:48] : У меня вот еще пару моментов важных, которые я хочу вскрыть здесь. Считаете ли вы, что у робототехники уже появляется искусственное сознание с этими всеми мультимодальными штуками и Второй вопрос. Вот я стал думать о том, что есть слепые от рождения люди, у них визуальной модальности в принципе нет, но нельзя сказать, что у них нет сознания, оно просто работает на других модальностях. Тогда встрет вопрос, а может ли сознание работать только на одной модальности? Если это одна модальность уже есть у ЛЛМ, то есть ли у них сознание? 
S01 [01:30:32] : Ну, у человека, даже если нет видео, есть как минимум не только, допустим, слух, но и кинестетика. А кинестетика дает огромное количество сведений о мире с точки зрения реальной деятельности. И в этом смысле я просто не люблю слово «сознание», в принципе. Я люблю всевозможные, допустим, когнитивные характеристики. то сейчас робототехника вовсю демонстрирует замечательный рост базовых когнитивных характеристик. Ну, я к списку стандартных 2004, например, да? И, что важно, демонстрирует взрывной рост. То есть как раз вот после того, как научились работать более-менее серьезно с мультимодалкой и заземлили ее на реальных роботов, понеслось что-то совсем новое. Как я еще раз, на всякий случай, повторю, да, про сознание я не люблю говорить, а вот с конективными характеристиками все очень хорошо. И каких можно будет, например, лишиться, но при этом остаться в кавычках в сознании, да, это надо будет смотреть. Тут я не сильный психолог. Спасибо. 
S07 [01:31:44] : Коллеги, еще вопросы? Так, вот вопрос от Алекса Шкотина. В чем прелесть ТЛО? Они скорее создают иллюзию интероперабельности. 
S01 [01:31:54] : Ну что значит создают? Нам же нужно на что-то как раз привязывать те куски знаний и данных, которыми мы обмениваемся. Если ТЛО нет, получается, что ни на что вообще привязывать. Поэтому я не согласен про иллюзию интероперабельности. Она либо есть в некоем уровне, либо нет. 
S05 [01:32:13] : Дело в том, что если у нас описана какая-то предметная область, и мы, например, грубо говоря, знаем ее аксиоматику, и после этого мы передаем данные этой предметной области в другую систему, нам нужно не ТО привязывать, нам нужно передать эти аксиомы. Не зная аксиомы, они не смогут работать с нашими данными. 
S01 [01:32:37] : Пока никто не говорит, что нам нужно только ТЛО, нам обязательно нужны еще общие предметные антологии, то есть Common Anthologies и Business Anthologies, кроме Top-Level Anthologies. Просто Top-Level задает нам как бы некий базовый язык описания разных сущностей, индивидуалов, работы с пространством-временем и тому подобными вещами. Моя любимая картинка из БФО, она же про это. Они же специально имеют четко кусок, ниже которого не лезут. Большая презентация стала. Где был кусок? Где-то вот тут. То есть это вот почти все же основные понятия БФО, например. Но мы же видим, что функциональность есть. время пространства есть, различия в соответствующих индивидуальных сущностях есть, материал есть. Все, значит, базовый язык у нас какой-то есть. А дальше, естественно, нужна бизнес-онтология. Почему? Потому что если мы задали какие-то на уровне бизнес-онтологии серьезные правила, а тем более, если мы туда включили как раз имитационные модели, я считаю, что любая нормальная онтология, бизнесовая или предметная, обязана включать в себя имитационные модели. напрямую, физически, ну или там химически и прочее. И понятно, что без них не жить. Но они-то нужны только когда мы в этой области что-то должны дорассчитать. А сам по себе кусок данных, контейнер, да, который ссылается на эту бензонатологию плюс, допустим, на БФО с точки зрения базового языка, чтобы его распарсить, да, это совершенно достаточно. Потому что если мы что-то уже из физики взяли и передали там, не знаю, в химию, то в химии просто так с ним же ничего не сделаешь. Зато если туда в химию придет еще один кусок физики, то эти два куска физики можно будет обработать, что-то там понять и использовать. 
S05 [01:34:30] : Ну тут скорее речь идет о том, что нам нужна онтология физики, а не вот эти вот оккурант, континуант и прочие Это вещи, которые могут быть приняты кем-то, но могут быть и не приняты, потому что есть другие схемы. 
S01 [01:34:49] : Тогда нужно выравнивание верхней уровня хонтологии. Очень большая проблема в интероперабельности. Почему? Потому что если кто-то в какой-то принятной области использует вообще другую парадигму Тейлошки, а Тейлошек у нас же разных много именно с точки зрения схем. А где у меня было здесь? Вот. То есть, например, даже базовая классификация, да, про как раз онтологические и дженерик варианты с категорическими данными, да, и основанные на Foundation и National Language Ontology разных уровней, да, они уже вот сдают разные рамки. И эти рамки нам нужно соблюдать. Как только мы их не соблюдаем, все, никакой интероперабельности. Какие бы у нас там крутые бизнес-онтологии не были. 
S05 [01:35:30] : И скорее речь идет о том, что нам нужна формализация логики и физики. Всего на всего. 
S01 [01:35:38] : Не обязательно. Дело в том, что в разных предных областях у нас вовсе не обязательно иметь это все сразу. И можно не иметь физики, можно иметь химии. Нужно иметь что-то, что в этой области нужно. Плюс нужно уметь всегда хранить измерения в контексте, потому что они как раз универсальны. И как раз иметь язык описания этого, то есть верхнюю уровню антологию. А вот как раз, что мы, например, в физике делаем, у нас там есть тематика, да? Отдельно есть совершенно независимая онтология химических реакций. Это совершенно нормально, так оно везде и работает. Но мы зато сохраняем, если привязку к онтологии физики, когда взяли кусок контейнера и поместили его в... Извиняюсь. Секунду. Значит, если мы поместили этот контейнер в антологию к химии, то мы там ничего не сможем сделать в плане физики, но и не надо. Зато еще один контейнер, когда придет из физической антологии, мы сможем два этих контейнера каким-то образом совместить и получить новое знание. Они все равно содержатся. 
S05 [01:36:47] : Нет, смотрите, если у нас есть антология физики, Конечно, нам не всегда нужна ее подробность для описания тех или иных процессов. Но современные ТО выглядят как некоторые, ну, грубо говоря, ну, специфические, что ли, или надуманные конструкции. Вот почему еще? Потому что если мы возьмем какой-нибудь философский трактат, из него тоже можно сделать антологию верхнего уровня. И сколько ее будет? Какой смысл? Какой смысл формализовывать, а здесь мы не имеем даже трактата, то есть мы должны сами где-то или получить описание того, что такое континуант, что такое депендент и так далее. То есть мы получаем тем самым, ну я про Биэфо говорю, мы получаем тем самым еще один трактат философский. Смиту можно было спросить, какой философской доктрины он придерживается или у него свои? 
S01 [01:38:02] : Мне, например, это вообще не важно, потому что как только мы говорим, что у нас верхневой антология должна быть такая, чтобы можно было в ней сохранять именно с точки зрения контейнера знания, которые не противоречат современным модельным представлениям о мире и позволяют заземлить измерения, то мне от него больше ничего не надо. Да, они могут быть разные. Но при этом они вполне могут быть, во-первых, выровнены, а во-вторых, как раз нормально координировать знания из разных преданных областей. А вот как раз у философов обычно, ну кроме некоторых, которые конкретно про это писали, да, с заземлением и как раз интерпирабельностью печалька. 
S05 [01:38:45] : Димаш, хитрый вопрос. 
S01 [01:38:48] : Ну, естественно, хитрый. Дело в том, что у нас вообще у философии, как я неоднократно говорил, да, но это не я первый сказал, да, главная проблема, то, что философы почему-то думают, что какую-то в кавычках правду о мире, да, можно выразить средствами естественного языка, причем конкретного там, английского, французского, немецкого, русского, да. Я, например, видел разные переводы немецких философов на русский язык, и они, например, были по смыслу разные. 
S05 [01:39:15] : Ну, тут есть знаменитое утверждение, что философские литературы непереводимы. 
S01 [01:39:20] : Эта литература очень специфическая. Поэтому я как раз и говорю, что в этом смысле инженерные онтологии, они у философии очень далеки. Они как раз близки к тем теориям актуального знания, которые обеспечивают заземление. 
S05 [01:39:35] : Очень как-то хочется без ТИО обойтись. 
S01 [01:39:39] : Пока не получается. У меня ни в одном проекте и не получилось, потому что даже если без ТЛО, свое ТЛО возникает автоматом, как только тебе надо контейнеры с знанием гонять между компонентами системы. 
S05 [01:39:54] : И что она похожа на какую-то из стандартов? 
S01 [01:39:59] : Да, например, на УЛО у меня один раз получилось похоже. УЛОшники же по-другому, совсем не как БФОшники сделали. Один раз у меня получилась антология, поскольку это была область, где заземление проходило через всегда, через мнение человека. И там уже на втором уровне возникли как раз базовые модальности, в смысле как раз возможности, уверенности и прочего. не те модальности, которые мы до этого обсуждали. И как раз прямо на втором уровне они обеспечивали очень удобную укладку в контейнер, где ты сразу мог сказать это контейнер, это как раз там мнение, там факт такой-то доли уверенности или прочее. 
S03 [01:40:42] : А можно я сейчас вот немножко стряну, прошу прощения. Вот в вопросе Алица на мой взгляд, прозвучала такая идея, что, мол, если ТЛО не обеспечивает интероперабельность, то зачем мне нужно вообще? А когда ты начал обещать, я сейчас пытаюсь выяснить, по-моему, интероперабельность главная, заключается в том, что тебе надо гонять Низкоуровневые вещи хорошо переиспользуются везде, типа quantities и так далее, которые входят где-то в набор. То есть они shared, очень хорошо разделяемые. Входят в такой must-have-набор для интеропирабельности. А вот все эти штуки с высокоабстрактными currency-рутами и thing-ами, Они уже начинают мешать, потому что если они там разные, то сводить их друг к другу как раз-таки здесь очень сложно. 
S01 [01:41:38] : Это так? Нет, я не согласен, потому что я-то как раз здесь не про некоторые из вариантов, которые нужны для некоторых мыслительных конструкций, а именно про те, которые нужны в работе. А в работе нужно, сейчас вот тут недалеко, вот, в работе нужно вот эту. И если эту есть, то дальше к нему можно прибавить что угодно. И в любой нормальной ТЛО это есть. То есть, если ТЛО для меня не обеспечивает переносимый интероперабельный контекст измерения, то все, для меня это как раз не общая ТЛО, это какая-то частная ТЛО. Такая высокоуровневая, но в конкретном домене. 
S03 [01:42:17] : Вот это преймворк для низкоуровневого передачи. Когда мы говорим про высокообстрактные штуки, возможно, насколько вот эти востребованы, самая там верхушка, точнее корень ТЛО. 
S01 [01:42:37] : Стоп. Егор, гляди. У нас же с одной стороны есть верхушка с точки зрения базовой как раз холорхии тех же доминов, как преданных областей. И здесь как раз я на твоей стороне. Они могут быть очень специфичны и отражать эпистемологические фреймворки в этом домине. А вот как раз с точки зрения того, что в каждой этой ложке должен быть некий синк или объект или кто-то еще, это однозначно потому, что нам нужен корень, за который можно зацепиться при парсинге объекта знаний. Для меня это перпендикулярные вещи. 
S05 [01:43:11] : Но все-таки корень физический тут никуда не бежит. 
S01 [01:43:15] : Не обязательно. Это корень может быть очень далеко от физического, например, объекта. Ну, в БФО, например, там на четвертом уровне. У меня там в основных онтологиях, которыми я пользуюсь на практике, он второй-третий уровень. Но я могу считать, что он еще где-то дальше от корня. Главное, что этот корень есть. Почему? Потому что если у него нет за что цепляться, то это с точки зрения ссылок на эту бизнес-онтологию, которая привязана к этой ТЛО. То есть мы же бизнес-онтологию должны прицепить к каким-то понятиям ТЛО. 
S03 [01:43:44] : Вот, слушай, здесь классный момент, по-моему. Собственно, я и там в разработке программе, да хотя бы как просто что это объект этого домена. а вот часть bfo, там succurrent, там какой-нибудь event, я не знаю, что там, и так далее. вот эти высокоуровневые штуки, там скажем так минус один уровень, да? Насколько они востребованы? 
S01 [01:44:20] : Очень востребованы. Почему? Здесь, например, есть замечательная штука. У Quality есть Relation to Quality. Вот мы тут недавно осенью обсуждали в чате как раз про отношения. Вот оно как раз есть. Вот оно Relational Quality. Шикарнейшая вещь, которая хорошо описана, хорошо характеризована и прочее. Есть Function отдельно, очень удобный тоже, к нему можно хорошо привязываться. Есть как раз пространство-время. Те же самые, например, Special Region полноценный. Почему? Потому что как раз оказалось, что вот эта вот штука, особенно зелененькая и синенькая, она очень хорошо отражает особенности тех же стандартов на передачу бизнес-объектов. И партридж, например, тот самый, который до этого писал знаменитую книгу про реинжиниринг и реюз, он абсолютно также распределял объектно-ориентированные модели, чтобы у них как раз была возможность нормально стыковаться. В этом смысле это достаточно удобно. Да, мне, например, самому не очень нравятся названия континуента Accuriant, их все время приходится студентам долго объяснять. Но куда деваться? Я бы их как раз поменял. Но сами уровни, они очень удобные. Почему? Потому что то, что появляются те самые, например, временной регион сразу от Occurrent, а у континента как раз Special Region ниже, потому что есть Inmaterial и Material, это реально важно. но это как раз надо это можно уже это самое в чате так сказать позадавать вопрос друг другу с точки зрения того на каких уровнях мы к чему привязываемся да или вообще отдельно действительно антона попросить сделать кусок с точки зрения например выравнивание тло и бо на разных уровнях Поэтому как раз так все написано. И в этом смысле я чисто из практики иду, из архитектуры данных. Почему? Потому что в архитектуре данных мне важно одновременно, чтобы мастер-мал занимал, эффективно было и при этом хорошо описывалось универсально. А это противоречивой конструкции. и противоречивые требования. Куда деваться, да? Вот. В результате, в этом смысле, если чётко на твой вопрос отвечать, вот этот, например, кусочек про материальность и нематериальность очень нужны. Почему? Потому что Material Entity, да, который непонятно что такое, но у него есть как раз континент FIAT и Special Region, да, это клёво. Оно реально очень полезно много где. И я ж не зря ссылаюсь на ту самую книжку... Нет, это уже эти самые... Вот, да, Арпа и Смита, да? Ты же её имел в виду? Пробил динатологию из БФО. Там же это очень круто в примерах подписывается. А, она шикарнейшая книжка. Я ее всем рекомендую всегда. Студенты именно после ее прочтения начинают врубаться, зачем вообще БФО нужно. Потому что просто по чтению гайда или как раз сайта там вообще непонятно студенту, например, зачем это БФО было сделано. А вот эта книжка, она же от частного к общему и с примерами. Прямо построение реальных антологий. Шикарнейшая книжка. 
S05 [01:47:55] : Такой совсем простой вопрос. А что такое Spatial? Что-то там для Immaterial Entity. Просто пример Immaterial Entity и пример ее 
Spatial Rich. S01 [01:48:09] : Это не стилище. Проще книжку посмотреть, чем сейчас мы будем эти самые примеры все приводить. Но на самом деле у нас, видите, есть... Как бы, например, хотим мы математические объекты писать. Мы говорим, Material Entity, ура, ура, у него там появляется, да, Tesson Boundary. И мы такие, вот, для двухмерных, одномерных Zero-Dimensional уже введены прям специальные классификаторы. Обычно сюда дальше продолжают и говорят, что например у нас есть топологический сайт. Видите, у нас есть сайт. Мы говорим, у нас topological site. Вот topological, да, появляются как раз в какой теории, с какими свойствами топологический объект и прочее. Я прям такую антологию видел, очень клевое. 
S00 [01:48:51] : Лёш, а там же ещё Spatial Region, это же просто привязка к пространственному... пространственная привязка объекта. Да, это нормальная. Это как раз, когда у тебя слева Continuum Flat, Fiat boundary, это придуманные твои границы, а это когда ты привязался к какому-то конкретному пространству, которое ты задал где-то раньше, ну, в другом месте видишь. 
S01 [01:49:15] : И более того, именно Special Region позволяет автоматом применить логику как раз пространственную с точки зрения стандартного Геоэда и пересечению там мест. А FIAT позволяет автоматом здесь же применить топологическую логику и топологические расстояния. И, например, сделать геоинформационную систему, которая показывает на одном слое расстояние реальное, геометрическое, а на другом слое, например, расстояние за сколько минут можно дойти до метро. 
S05 [01:49:49] : Очень интересная тема. 
S02 [01:49:51] : А я хотел бы спросить такой общий вопрос. С вашей точки зрения, Что нам еще не хватает, чтобы создать этот полноценный 
AGI? S01 [01:50:03] : Очень много всего не хватает. Во-первых, нам не хватает как раз тех самых решателей, о которых Антон очень любит рассказывать. Почему? Потому что он много видов их ручками трогал. Нам не хватает нормальных имитационных моделей, привязанных к антологиям. Мы, например, занимались, когда антологией для генерации задач по физике, химии и прочим оказалось, что это уже требует дополнительных изысканий, потому что привязки классических имитационных моделей, например, кинематики, к, например, антологии школьной физики, нет. Мы сами сделали, и прочее. А это нужно, это же в базе должно быть. И, например, для всех стандартных вещей, связанных с реальными измерениями в основных актуальных знаниях, это все нужно доделать. То есть в некоторых областях, в той же медицине, когда делали довинчи, кое-что сделали, но оно частное получилось, его надо универсализировать. И очень много такого надо универсализировать. А как раз оттуда много где надо убить лишнее. А дальше как раз с другой стороны, надо естественно нормально работать с логическими решателями, так чтобы они тоже универсализовались. К сожалению, тут на 10 лет был с моей точки зрения вообще провал, да? И многие даже старые проекты хорошо бы сейчас поднять наоборот и продолжить. Особенно про французов со всякими нечёткими логиками, да? Вот. Ну и вообще задай бы сейчас, да? 
S02 [01:51:26] : А у вас не возникает такого ощущения, что мы с неправильной стороны идем? У человека язык как надстройка сформировался позже, над сенсорными всеми этими... модальность именно с сознанием. А тут мы пытаемся, получается, обратную сделать. У нас вот есть наш объем большой текстовых каких-то данных, и потом мы к ним что-то там начинаем уже прикручивать вроде... А я про текстовые данные вообще ничего не говорил? А, ну ладно, значит я что-то не так помню. 
S01 [01:52:00] : Да, я про тексты данных специально ничего не говорил. Я как раз всегда говорю про заземление и не забываю лишний раз напомнить, потому что очень часто уходит из фокуса. И говорю как раз про те же самые кинестические каналы, в смысле модальностей, как видео-аудио. И самое весёлое при этом, что у нас же сейчас нет задачи сделать искусственного ребёнка, который вырастет искусственного хулигана и так далее. задача задачи решать извиняюсь за тавтологию а чтоб задачи решать надо чтобы они решались с некоторым уровнем качества и поэтому я например и сам себя считаю не просто отдельным там индивидом а индивидом с экзокортиксом это мне очень нравится и вообще когда кстати Левенчук например подхватил эту идею и понес ее в массы я ему был очень благодарен и даже специально это ему сказал при встрече 
S02 [01:52:54] : А ваш экзокортекс, он как выглядит? Можете пояснить? 
S01 [01:52:59] : Ну как, это как раз системы некоторых формальных моделей, которые обычно в соответствующем ПО сделаны. Вот плохо, что этого ПО очень много разного, да? У меня сейчас по разным областям, да, несколько там троимитационных моделей, да, они все в разных программах, их десятки. Это ужасно. Дальше, естественно, есть обобщающие для записок вещи. Мне, например, до сих пор OneNote больше всего нравится, хотя я графовые тоже уважаю. Дальше как раз есть, с точки зрения современных языков программирования, системы для самых формальных систем. Мне очень нравятся Rust и Julia, да? И, соответственно, склейка Python. Ну и так далее, так далее. Много всего. Главное, что это должно как раз тоже интерпретироваться, и в этом смысле мне В этом году, например, жутко понравилось, что на основе Python именно как языка склейки и Rust как языка создания реальных модулей просто обалденные прорывы произошли. Я даже как раз летом говорил, что Rust лучшее, что случилось с Python за последнее время. И, например, системы пакетирования стали совершенно нового поколения. Те же UV-шки, да? Они для меня очень важная часть экзокортекса. Почему? Потому что пакетирование – это же не только контейнер данных, который мы в другую антологию передаем. Это еще и контейнер алгоритмов, контейнер имитационной модели и прочие виды контейнеров. И мне здесь очень нравится, что Microsoft сделал как раз на основе офиса систему, где можно обмениваться маленькими контейнерами с артефактами из разных приложений. Понятно, что раньше давно уже был OLE, Object Embedding, а теперь вот у них на новом уровне это, и мне очень нравится. 
S02 [01:54:44] : Спасибо. Я просто изначально про интерфейсы мозг-компьютера подумал, а потом понял, что вы через обычные взаимодействия. 
S01 [01:54:53] : Да, мне очень понравилось, что на телефоне после очередного апгрейда андроида резко улучшилось распознавание. 
S07 [01:55:03] : Алексей, а вопрос такой. У нас цикл семинаров, который сейчас идет, родился 1 января. Мурная дискуссия после Нового года случайно случилась. И там, в частности, говорили и о редакторах онтологий, и о DSL. Вот, и на следующем семинаре через неделю, надеюсь, мы на эту тему поговорим. Но еще там возник вопрос про как раз пресловутый решатель, или про систему нечеткого, вероятно, соднования аксиматического вывода. Вот у вас в качестве наброса Есть какие-то соображения по поводу того, что есть на этом свете? Потому что масштабность, широта и глубина ваших знаний впечатляет. Я всегда говорю, что есть три системы. На вероятностной логике Бена Герцеля, PLN – probabilistic logic networks, есть анеоксиматическая логика Пиванга, NARS. На самом деле PLN вырос из этого же NARS. И есть и то, и другое, это с открытым кодом. Есть вероятностная логика. Евгений Евгеньевич Витяева, но это как раз Дмитрий Иванович из его команды, но это, к сожалению, закрытый код. Вот у вас есть что-нибудь продолжить этот список на предмет не просто исчисления предикатов, а именно того, что можно было бы использовать для будущих решателей на основе онтологий с возможностью как раз вероятностных или нечетких выводов? 
S01 [01:56:42] : Ну, у меня есть как раз у коллег прям, по-моему, он даже открытый. Его немцы и французы совместно с нами делали. Движок нечеткой логики на основе всей как раз. Очень интересный, да. Дальше есть прям, который использует движок темпоральной логики. симпатичный, с учетом как раз не только предшествования и кусков непрерывного времени, но еще и так называемых перекрывающихся действий. Я не зря тогда Егору напрессал про конусы причинности. И наконец есть, но он закрытый, к сожалению, это движок причинно-следственных связей. вот я не зря же очень люблю упоминать за букву y ту же самую к сожалению хорошего открытого движка причинно-следственного я не знаю все даже которые делались в академии они в какой-то момент вдруг исчезали хотя люди которые их делали не исчезали они наоборот становились еще более крутыми и переходили куда-то работать секрет или ну то есть очевидно да Потому что я знаю прямо человека, например, в Канаде, который очень хорошо себя чувствует и занимается движком причинно-следственной связи, но больше ничего не публикует и так далее. Но они очень клевые были те, которые я видел. И что особенно важно, есть же промежуточный кусок по логикам действий, action logics, и они тоже есть нечеткие. Вот и там тоже есть же открытые всякие убогие движки, вот. А как только народ хотел что-то очень крутое сделать, он делал, а потом куда-то тоже пропадал из видимости академической. Вот это бы интересно посмотреть. 
S07 [01:58:28] : Понятно. Коллеги, ну нам, наверное, нужно уже закругляться. Напоследок вопрос последний от Алексея Муравьева, видимо, в контексте того, что, когда говорят об онтологиях, часто еще говорят о графах знаний или об инженерии знаний. И Алексей задает вопрос, что такое знание, какое определение вы используете? 
S01 [01:58:53] : А я не использую никакого определения, как обычно. Я поэтому даже ни в одну презентацию нормально это не включаю. Сразу говорю про то, что мы разделяем явное и неявное, аптоцитное и прочее, и начинаю про классификацию. Потому что мне просто неинтересно туда лезть, где уже очень хорошо сказали философы. В этом смысле мне очень нравится ссылаться на последние, имеется ввиду, с конца 90-х годов книги по социологии научного знания. Почему? Потому что они как раз начали совмещать базовые вопросы самого знания как конструкта с процессами накопления знания. И, соответственно, мне в этом смысле после Поппера, Куна и Локатыша очень нравится как раз следующее поколение, где пошли всякие конструктивные определения и вплоть до Дойче и компании, которые констрактор теории сделали и прочее. И самое главное, что их же много. они все разные в основном частные и все сходятся в одном да знания это не данные вот так же как информация да это не материя энергия да так знания это не данные это что-то большее мне этого достаточно Хорошо. По-хорошему можно в следующий раз рассказать быстренько про вот как раз последние потуги социологии и науки. Кстати, я на какой-то момент даже, помню, в открытую выкладывал. Сейчас, секунду. Я, наверное, даже сходу найду. One second, please. Это, наверное... О, точно. Оно у меня, наверное, сохранилось давно, и я туда не лазил. Ой, какая прелесть. Так, сейчас я ссылку дам. Но это как раз без самых последних. О, пригодилось. Пымс. Сейчас я на экран вытащу. Она вытащится? А, вытащится. О, даже на экран вытащится. Я в свое время давно уже сделал, надо обновить. Я у себя-то обновлял, сюда не выкладывал. И здесь есть вот такая классика, начиная сначала, вообще, социология науки как таковой. Он как раз через Поппера, Локатыша и всех прочих, через Хакинга. Вот у Хакинга очень интересно как раз, кстати, про знания. У нее же про произведение интервью. Шикарнейшая книжка сама по себе, да? И потом у Хакинга была следующая, клевая, вот она, Historical Anthology. Если не смотрели, обязательно посмотрите. Шикарнейшая вещь. У нее есть русский перевод, по-моему. Ну а дальше как раз уже в 2010-х годах было несколько очень хороших книг про как раз связь оснований знания и актуального знания, чем социологическое знание отличается от политической и прочее. И как раз тем, какой кусок научного метода за что ответственен с точки зрения как раз эпистемиологических конструкций. Кстати, Егора можно спросить. Он, наверное, последние тоже всякие штудировал. 
S03 [02:02:11] : К сожалению, нет времени слишком много читать пока. 
S01 [02:02:15] : Да, это у всех стандартная проблема. Особенно у студентов с первоисточниками. 
S07 [02:02:22] : Хорошо. Алексей, коллеги, большое спасибо за участие. Алексей, спасибо за доклад. Участникам спасибо за внимание и за вопрос. Алексей, будет очень здорово, если вы сможете что-то, продолжение рассказать или рекомендуете коллег, кто мог бы рассказать именно про создание антологии. 
S01 [02:02:43] : Я как раз про ЛЛМки и создание антологии спрошу как раз коллег. 
S07 [02:02:48] : А у нас в ближайшие два четверга будет тоже разговор про онтологию. В следующий раз я буду рассказывать про свои проектики и, может быть, используем это как повод для дискуссии про редакторы онтологии и про DSL. А через две недели у нас будет докладка про дифференциальные и динамические онтологии. Всем спасибо и до новых встреч. Спасибо. До свидания. 
S02 [02:03:14] : До свидания. Спасибо. 







https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
