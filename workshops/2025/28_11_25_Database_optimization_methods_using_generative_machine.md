## 28 ноября 2025 - Методы оптимизации БД с использованием генеративных подходов МО - Василенко Никита - семинар AGI
[![Watch the video](https://img.youtube.com/vi/ZotclrCBpCc/hqdefault.jpg)](https://youtu.be/ZotclrCBpCc)


### Основная тема и контекст
Доклад посвящен направлению **LLM for DB** (использование языковых моделей для улучшения работы баз данных), конкретно — задаче оптимизации планов выполнения SQL-запросов. Актуальность обусловлена тем, что эффективность работы БД напрямую влияет на скорость бизнес-приложений и потребление вычислительных ресурсов.

### Часть 1: Проблематика оптимизации в реляционных БД
*   **Декларативность SQL:** Язык описывает «что» нужно получить, но не «как».
*   **Планирование запросов:** База данных строит дерево выполнения (план), выбирая порядок соединения таблиц и физические алгоритмы (например, *HashJoin* vs *Nested Loop*, *Seq Scan* vs *Index Scan*).
*   **Cost-Based Optimization (CBO):** Традиционные оптимизаторы оценивают «стоимость» плана на основе статистики (кардинальности).
*   **Сложность:** Количество возможных планов растет экспоненциально (для 12 таблиц — сотни миллионов вариантов). Перебрать все невозможно, используются эвристики, которые часто дают неоптимальный результат. Применение классического ML ограничено жесткими требованиями к скорости (Latency).

### Часть 2: Обзор применения LLM в базах данных (существующие подходы)
Спикер выделил несколько популярных направлений и их недостатки:
1.  **Text-to-SQL (Интерфейсы):** Генерация SQL из естественного языка.
    *   *Проблема:* Даже при высокой точности (91%+) оставшийся процент ошибок критичен для бизнеса (риск положить продакшн).
2.  **Query Rewriting (Переписывание запросов):** Использование LLM для перефразирования SQL-запроса.
    *   *Проблема:* Трудно гарантировать семантическую эквивалентность и корректность результата.
3.  **LLM как Администратор (DBA Agent):** Настройка параметров БД (knob tuning), диагностика сбоев, выбор индексов.
    *   *Проблема:* LLM часто галлюцинируют, работают медленно и дорого, требуют проприетарных данных для обучения, которых мало в открытом доступе.

### Часть 3: Предлагаемое решение (Методология авторов)
Авторы предлагают гибридный подход для ускорения запросов с помощью **хинтов** (подсказок оптимизатору, какие операторы использовать или игнорировать).

**Архитектура решения состоит из двух контуров:**

1.  **Быстрый алгоритм (Онлайн-оптимизация):**
    *   План запроса рассматривается как **текст**.
    *   С помощью LLM получается эмбеддинг (векторное представление) плана.
    *   Ищутся похожие планы в базе знаний (поиск ближайших соседей).
    *   Применяется набор хинтов, который был эффективен для похожих планов.
    *   *Особенность:* Работает быстро, не требует генерации текста моделью в реальном времени.

2.  **Оффлайн-оптимизация (LLM-агент):**
    *   Используется, если быстрый алгоритм не сработал.
    *   LLM выступает в роли агента-администратора, который ведет «диалог» с базой данных.
    *   Агент предлагает хинты, запускает запрос, смотрит на реальное время выполнения и корректирует стратегию.
    *   Лучшие найденные решения сохраняются в базу знаний для использования быстрым алгоритмом.

### Результаты
*   **Быстрый алгоритм (поиск по эмбеддингам):** Ускорение бенчмарка на **~19%**. Это подходит для работы в реальном времени.
*   **LLM-агент (диалоговый режим):** Нашел почти оптимальные планы, ускорив выполнение на **~54%** (при теоретическом максимуме 62.5%).

### Ключевые моменты из обсуждения (Q&A)
*   **Безопасность продакшена:** Агентную оптимизацию нельзя запускать на «живых» запросах пользователей, так как перебор вариантов может временно замедлить систему. Оптимизация должна проводиться в фоне или на тестовой среде (оффлайн), наполняя кэш решений.
*   **Бюджет:** Работа агента ограничивается бюджетом (количеством попыток или временем), так как каждый тестовый запуск стоит денег/ресурсов.
*   **Индексы:** Наличие индексов учитывается в тексте плана, который подается модели. В реальности (в отличие от академических работ) подход к индексам проще: «если тормозит — добавь индекс».

**Вывод:** LLM пока слишком медленны для планирования запросов в реальном времени, но отлично подходят для оффлайн-анализа и генерации знаний, которые затем используются быстрыми алгоритмами.



**Расшифровка доклада:**




S01 [00:00:01] : Коллеги, всем добрый вечер. Начинаем очередной семинар русскоязычного сообщества разработчиков общего сигнала искусственного интеллекта. Доклад сегодня, с моей точки зрения, интересен тем, что, во-первых, это работа, которая имеет, с моей точки зрения, большую практическую значимость, потому что практически любое бизнес-приложение и приложение использует ту или иную базу данных, и вопрос эффективности использования базы данных в бизнесе является очень важным. Есть целая компания, которая специализируется на повышении производительности баз данных. Это их основной бизнес. Это с одной стороны. А второй интерес, почему меня вызвала эта работа, которую мне довелось рецензировать, связана с тем, что В последних двух моих работах, одна из них, которая докладывалась на конференции нейроинформатика и форуме сознания, большое внимание уделяется не просто принятию решений, энергетической эффективности процессов принятия решений. То есть не только энергетической и экономической эффективности решений, но и самой эффективности и энергетической, в том числе, эффективности самих процессов принятия решений. Вот. И здесь это все... Значит, эти статьи, они продолжают на самом деле идеи Бивонга, высказанные еще более 30 лет назад. о том, что для пресловутого инференса важен не только сам инференс и его результаты, а и то, с какой скоростью этот инференс выполняется, какие ресурсы он потребляет, и что сама оптимизация учрительных ресурсов является направляющей в ходе этого самого инференса. И хотя сегодняшняя работа будет не про инференс системы вероятностной логики, а про выполнение запросов к корреляционным базам данных, на самом деле смысл там примерно один и тот же. То есть мы работаем с некоторыми информационными объектами, либо с SQL-таблицами, либо с... графовыми базами данных и совершаем над ними какие-то операции и хотим убедиться в том, что мы эти операции выполняем достаточно быстро, чтобы запросы укладывались в отведенное для их выполнения время, а с другой стороны, чтобы время на эти запросы и потребление этих ресурсов было минимальным. Поэтому, мне кажется, будет интересный доклад вот с этих двух Никита Василенко, институт системы информатики СУРА, нам про это сегодня расскажет. Никита, пожалуйста. 

S00 [00:02:44] : Спасибо. По вступительным условиям. Сначала небольшие организационные моменты. У меня доклад разделен на три части. как из названия стало понятно, там есть база данных, есть языковые модели, и вот они как-то женятся, и из этого что-то получается. Поэтому, так как у нас сегодня аудитория небольшая, первую часть доклада — это про введение базы данных. Вдруг кто-то что-то не знает или не слышал, вот я по верхам немножко пройдусь про базы данных, Все запросы в реляционных баз данных планируются в большинстве типов баз данных. Вторая часть уже будет про то, как языковые модели непосредственно используются для баз данных. И третьим небольшим блоком будет то, что мы уже как раз сделали. Справа иллюстрация примерно. тех объемов работ, которые есть по базам данных, это огромное количество, небольшая часть из них посвящена языковым моделям для баз данных, ну и наша, скажем так, совсем небольшая работа, которая чуть-чуть область знаний нашего расширяется. Доклад будет примерно минут на 25-30. Если есть какие-то вопросы, опять же аудитория у нас небольшая, поэтому давайте сразу по ходу что-то будем обсуждать. Да, как я уже сказал, для языковых моделей используется большое количество баз данных, в том числе векторных баз данных, и в целом это очень такое популярное, и научно популярное, и популярное в целом направление, то есть DB4AI или DB for LLM. А вот LLM for DB – несколько меньше направление, и оно чуть менее изучено, хотя на данный момент большое количество работ уже публикуется на эту тему. Ну и дальше мы посмотрим примеры, как языковые модели применяются в различных проявлениях для паст данных. Итак, для начала, на основу, ничего, что я на полный экран не буду делать? Подходит? Окей. Да, нормально. 

S01 [00:05:22] : Можно и так. Лучше бы, наверное, конечно, было на полный экран. 

S00 [00:05:26] : Давайте, мне нетрудно. 

S01 [00:05:29] : Да, давайте тогда лучше. Ага, да, вот так вот, привычнее. 

S00 [00:05:33] : комфортно. Итак, как все мы знаем SQL язык декларативный, то есть в отличие, например, от Python, где мы в явном виде задаем, как нам нужно сделать там двойной цикл или еще что-то, SQL язык он декларативный, то есть мы На один наш запрос можем получить несколько путей его выполнения. То есть самая базовая вещь — это соединение. Соединить таблицу 1 с таблицей 2. Как это сделать? Мы к таблице 1 будем присоединять таблицу 2 или к таблице 2 будем присоединять таблицу 1? То есть, казалось бы, результат запроса останется один и тот же, а вот время затраченное будет несколько раз другим. Вот у нас есть самый базовый пример с четырьмя таблицами, а справа это древовидный план его выполнения. Снизу у нас это обращение к таблицам. Например, в левом плане мы должны считать таблицу A, считать таблицу B с диска и их соединить. Далее мы должны к результату A и B присоединить C и так далее. Наверху мы уже получаем результат в виде какого-то количества строчек. Возникает вопрос. У нас есть дерево 1 слева и дерево 2 справа. Результатом они ничем не отличаются, но при этом по времени исполнения они могут быть разные. Также они разные по количеству обработанных строчек. Дальше я про это еще скажу. И вот нам нужно выбрать тот план, который будет наилучшим в смысле времени его исполнения. В большинстве реляционных баз данных workflow таким образом состоит. У нас есть текст запроса, он определенным образом парсится, переписывается в терминах реляционного алгебры. Я этого касаться не буду, но математический инструмент есть. И потом составляется логический план. Как раз что из себя логический план представляет. Вот мы берем таблицу A и B, мы ее должны соединить, потом соединяем таблицу C и D. Порядок не важен. Главное, что мы соединяем и, собственно, это все. А дальше мы составляем уже физический план. Про него чуть подробнее и попозже. Физический план — это уже не просто порядок соединения, например, таблицы А и таблицы Б, а конкретный алгоритм. Для типов соединения у нас есть несколько алгоритмов и для типов обращений к таблицам. Например, для обращения к таблицам Два самых базовых алгоритма – это последовательное сканирование всей таблицы и сканирование по индексу. То есть, если у нас на колонке построен индекс, что нам выгоднее в конкретном случае – сканировать всю таблицу или сделать обращение по индексу? То же самое для соединений. У нас есть несколько типов. Про head join и nest loop я чуть попозже расскажу. хэш-джойн, когда мы хэшируем правую и левую колонки, прогоняем через какую-то хэш-функцию. А nested loop это, по сути, двойной цикл, просто-напросто, по левой и по правой колонке. Чуть позже это будет. Соответственно, вопрос. Вот у нас есть четыре обращения с четырем таблицам, у нас есть соединение. нам нужно получается кроме их последовательности, что первым соединять, нужно еще и выбрать какой физический оператор для этого применить. То есть вы видите, что у нас пространство возможностей еще расширяется. Вот для сравнения, как работают HashJoin и MethodLoop. HashJoin, что он делает? берет левую колонку и ее полностью хэширует, а дальше по правой колонке переходит и поэлементно сверяет, есть ли похожие уже в захэшированных и, соответственно, таким образом выдает результат. Когда это у нас хорошо работает? Когда у нас обе колонки очень большие. То есть у нас не получится не считерить, нам нужно честно отхешировать одну и пойти по второй и постепенно ее хешировать и таким образом находить совпадение. Другой момент это двойной цикл, где есть внешняя таблица и внутренняя. Соответственно мы идем по каждому элементу из внешней, проходим по каждому элементу изнутри. Таким образом, мы получаем результат join. Соответственно, левый лучше всего работает, когда обе колонки очень большие, а правый, когда одна из колонок очень маленькая, по которой мы join. Далее. Как мы это можем оценить? Как мы можем сравнить планы? Вот левый и правый план. Но на самом деле ввели такую некоторую оценку стоимости. Назвали ее Custom. Справа мы видим простой план. Select из двух таблиц и некоторые условия на одну из таблиц и условия на Join этих двух таблиц. Здесь видно... Здесь есть некоторые оценки. Оценки того, какое количество строчек получится после работы того или иного физического оператора. Например, при индексном сканировании таблицы 2 мы получим, скорее всего, одну строчку. Это такая оценка. При сканировании с помощью Bitmap Index Scanner таблицы 1 мы получим примерно 10 строчек. И вот давайте мы создаем такую модель, которая будет брать на вход эти оценки строчек, называется это оценками кардинальности, и выдавать какую-то стоимость, конкретно здесь. и выдавать некоторый кост. Кост — это стоимость в условных единицах. Она на самом деле ничего не означает. Изначально эта единица была привязана к стоимости сканирования одной таблицы, но на самом деле сейчас это уже просто условные попугаи. И вот давайте мы каждому плану плану-кандидату, который мы хотим выбрать, мы посчитаем все оценки кардинальности, то есть количество срочек, которые получатся здесь, здесь и здесь, например, и присвоим некоторый кост. Посчитаем просто через формулу. И таким образом мы сможем каждый из этих запросов сравнить, каждый из этих планов сравнить и выбрать тот, у которого стоимость будет наименьшая. Вот это workflow подавляющего большинства элиционных баз данных, Postgres, Oracle, они все работают на вот этой стоимостной модели. Cost Based Optimization называется. Так в чем сложность? А сложность вот в чем. у нас, например, рассматривается только LabDip3, то есть вот такой формы, у нас при, например, 12 таблицах количество перестановок 500 миллионов, а для BUSH3, то есть у тех, у которых могут быть несколько ветвей, количество, ну вот, 2.8, 10.13, то есть Если мы даже будем перебирать, оценивать с помощью нашей стандартной модели 100 миллионов планов в секунду, перебрать все планы заняло бы больше трех дней. По сути, невозможно. Учитывая, что в секунду любая база данных перебирает гораздо меньше планов, и в секунду ей гораздо больше запросов нужно выполнить. Поэтому рассматривается только LeftDeep3, Опять же, в подавляющем большинстве кто-то пытается исследовать, как оптимизировать Bushi 3, как искать наилучшие варианты, но на самом деле ничего из этого пока в продакшене, по-моему, не существует. На этом, на самом деле, все по введению. Как вы поняли, оптимизация самих запросов – это дело сложное, потому что очень много вариантов. Основные подходы, как сейчас это делают, любыми способами пытаются обрезать количество вариантов. Понятное дело, никто не рассматривает все варианты перестановок, различные эвристики, то есть какие-то экспертные оценки и так далее. Уже достаточно долгое время различные email-подходы пытаются использовать, пытаются уточнить оценки кардинальности, то есть оценки результата сканирования таблиц и джоинов. чтобы это были как можно более точные оценки. Очень часто пытаются увеличить корреляцию вот этой стоимости в виде какого-то коста, в виде вот этих условных единиц со временем исполнения, пытаются предсказывать планы, время исполнения планов и много-много другое. В целом, любую другую область представить, там ML в тот же рост используется. Но при этом на самом деле в продакшн-системах использование ML очень ограничено. По одной простой причине, что вам нужно все это делать очень-очень быстро, А какая-нибудь нейросеть, она работает несравнимо дольше. И поэтому... Поэтому вот эта тема по оптимизации запросов, она остается актуальной и до сих пор после уже 50 лет ресерча вот этих планов, они появились уже очень-очень давно. Вот эта оценка стоимости, стоимостьная модель, это все буквально 50 лет назад и, по-моему, даже чуть больше появилось. Итак, теперь про то, Собственно, языковые модели используются в базах данных. Тут стоит понимать, что рассказ будет не только про планы, но и в целом, как языковые модели пытаются интегрировать в базы данных в различных задачах, связанных с базами данных. И самое базовое, понятное и популярное приложение – это языковая модель как интерфейс взаимодействия. То есть когда мы пытаемся из языка создать SQL-запрос. Самая популярная задача – Один из самых популярных бенчмарков – это Spyder, первая версия, который выпущен был в 2018 году. Сейчас я посмотрел, вот уже топовые модели достигли 91%. То есть Spyder – это большой корпус текста, который переведен в SQL-запросы. И, соответственно, огромное количество моделей пытаются перевести текст в SQL-запросы. Spider 1, по сути, уже близок к стопроцентной точности, 91%, но создатели опубликовали вторую версию, и тут уже топ-1 занимает только 70% точности. То есть работы еще на самом деле очень много, разнообразие языка, примерно такое же, как разнообразие SQL-запросов, хотя, казалось бы, у вас есть схема данных, то есть как таблицы друг с дружкой взаимодействуют, и что трудного написать какой-то несложный запрос. Но вот даже при точности 91 или даже 99.9 все равно существует вероятность, что вот этот 0.1% он станет критическим. И поэтому, да, некоторые, например, обучают языковые модели на конкретно свой домен, на конкретно свои схемы данных, но универсального решения нет. Почему? Потому что Ну, как я уже сказал, да, что вот этот 0.1%, даже если он будет, никто не возьмет на себя ответственность за этот плохой SQL. Вот недавно Cloudflare отключился на какое-то количество времени, что-то типа дня. Это самый большой провайдер, наверное, всего интернета. После расследования выяснилось, что проблема была в одном искривизапросе, где просто не было необходимой фильтрации одной. То есть да, это может человек допустить, и тогда у человека просто performance review будет, наверное, не очень хорошее у того, кто написал этот SQL-запрос. А вот что будет, если целая команда разработала текстовый SQL-языковую модель, а эта языковая модель потом то же самое, положила половину сети из-за одного SQL-запроса? Так, ну это по интерфейсам в целом все. Тут на самом деле никакой экзотики нету. Просто пытаемся перевести человеческий язык в SQL различными способами, добавлением различных данных типа схемы и так далее. Вот слева довольно такая generic схемка, как все может работать. Семантический парсинг, припроцессинг, маппинг слов человека, например, таблица такая-то и этой таблицы в базе данных, и потом генерация SQL. Тут еще всякие оптимизации, execution, но это скорее что-то из фантастики. Итак, языковой запрос в языковой модели для переписывания запросов. Вот это уже несколько, чуть более, скажем так, экзотический подход. То есть это когда у нас уже есть SQL-запрос какой-то, и мы его хотим так или иначе переписать именно текст запроса. То есть убрать какие-то ненужные фильтры, например, или переписать немножко его. Здесь самый генерик вариант, как это все работает. Вот есть некоторое хранилище правил для переписывания. Они там, например, как здесь, семантические, могут быть эквивалентные между собой, просто по-разному написаны. Могут быть правила, которые уже существуют, уже заранее написаны экспертно. После этого на вход подается SQL-запрос, это правило применяется, запрос переписывается, ну и, соответственно, оценивается, и такой небольшой фидбэк-луп по извлечению правил работает. Это такая тоже достаточно базовая, как языковые модели для переписывания запросов применяют, но, опять же, Здесь самая большая проблема — это эквивалентность, то есть понять, что у тебя получился точно такой же запрос, и понять, что этот запрос выполняет именно то, что ты хочешь, а не он просто убрал, например, какой-нибудь там фильтр, и смысл запроса потерялся. А справа — это просто недавно пост, у одного айтишника вышел про то, как они все-таки переписали с помощью языковой модели один запрос, который отрабатывал 100 минут, а стал отрабатывать 14. Есть и позитивные варианты, но в целом здесь такой же вопрос, а кто будет нести ответственность за применение SQL-запроса, который будет написан неверно, например. Другой подход – это различные проверки на эквивалентность. Тут авторы даже какие-то солверы свои придумали. и также статистические проверки, то есть просто чуть-чуть исполнить запрос и посмотреть, насколько он те же самые строки выдает. Я надеюсь, все понимают, что чуть-чуть исполнить запрос – это чуть-чуть денег, и, соответственно, каждый раз, чуть-чуть исполняя его, ты теряешь чуть-чуть денег. что тоже не очень устойчиво. Другой подход – это китайские коллеги сделали. Здесь даже нет какого-то обучения языковой модели, ей просто подается запрос и ищутся очень похожие запросы, которые оптимизировались с помощью каких-то конкретных правил. а правила взяты уже из существующих, которые просто эксперты сделали. Соответственно, потом языковая модель просто выбирает конкретное правило, при этом запрос автоматически переписывается, а не с помощью языковой модели. Это что касается переписывания запросов. Теперь переходим к... более для меня более близкой теме это непосредственно оптимизация планов и подсказок про подсказки чуть позже будет то есть как я уже говорил большое количество планов нам нужно либо перебора как-то упростить либо уточнить чтобы он был поточнее Один из примеров, как люди это делают, это... Ну вот он мне не очень нравится, но он очень показательный. То есть они скармливают языковые модели, понятно, некоторые инструкционный промпт, запрос, статистики, количество уникальных значений, все, что есть в базе данных, они скармливают. И языковая модель просто по шагам как бы собирает этот план. Ну, здесь, конечно же, возникает вопрос, насколько она быстро будет работать по сравнению с планировщиком, который написан на плюсах. Ну, ответ очевиден, и в целом вот работа слева — это олицетворение того хайпа, я бы сказал, который есть у сообщества относительно языковых моделей, потому что пытаются применить их как бы в любой постановке, везде и везде. Вот это не хорошо, не плохо, это вот, ну скажем так, ландшафт некоторый, который сейчас есть. А справа Немножко другая работа, уже ближе конкретно к моей тематике. Здесь поступили очень просто. На вход подается запрос, текст запроса, и он прогоняется через языковую модель, и получается его embedding этого запроса. А потом говорится, давайте обучим простой классификатор, который предскажет, Какой план нам выбрать? Базовый или другой, как бы измененный? Ну и тут довольно так странно, они находят сначала лучший план, Потом сравнивают его с дефолтным и говорят, вот, мы сравнили этот дефолтный план хуже, чем лучший, и вот давайте обучим модели находить по тексту запроса, какой план лучше, а какой план хуже. Ну, как бы очень странно, у вас текст запроса один и тот же, а планы разные, при этом вы по запросу пытаетесь это определить. Ну, в общем... Это такая preliminary research, как они сами назвали, но в целом я согласен с ними. Опять же, суперпопулярная тема – это языковые модели, похожие на database администраторов, то есть на администраторов баз данных, которые, например, занимаются непосредственно администрированием базы данных или настройкой различных параметров базы данных. То есть здесь очень натурально вписываются языковые модели в том плане, что вот у нас есть какие-то настройки, вот у нас есть некоторый перформанс, и давайте мы просто языковую модель попытаемся научить работать как DBA. То есть она смотрит на performance, изменяет какие-то параметры, снова смотрит на performance. По идее, должно быть все очень круто. Но тоже есть некоторые сложности. Например, Одна из работ посвящена настройке параметров базы данных, их большое количество, два из них тут пример представлен слева. И при этом, как мы видим, что языковая модель использует и некоторые гайдбуки в виде гайденса, и подсказки от DBA-администраторов. и при этом выдает не точное значение какое-то, которое нужно выставить, а некоторый рейндж, просто он чуть лучше, чем предыдущий. И администратор баз данных уже должен выбирать между этим рейнджем. Здесь видно, что языковая модель не очень хорошо подходит в виде конечного инструмента для настройки. А вот как помощник, Здесь гораздо больше успеха можно найти. Далее вот этот средний рисунок. Эта работа посвящена выбору индексов. Я призываю вас его особо не рассматривать, потому что я его сюда добавил как пример нагромождения всего и вся в один архитектурный рисунок. Отсюда можно сделать единственный вывод, что языковая модель, как вещь в себе, ответом для выбора индексов не является, потому что здесь кроме языковой модели, даже вот в двух местах, еще огромное количество Если кому-то интересно, как автоматически выбираются индексы с помощью языковых моделей, предлагаю всем перейти по этой ссылке и досконально изучить эту работу. Еще одно направление – это диагностика проблем с производительностью или просто диагностика каких-то неэффективности с помощью уже такого агентного подхода, когда присылается какая-то аномалия или, например, trader detection или какое-то нестандартное поведение. Ну и некоторые агенты, люди обучают, которые делают наблюдения, находят в своих знаниях алярок некоторую информацию. 

S01 [00:31:57] : Никита, мы вас не слышим. Раз, раз, раз. Так, Никита. Мы вас не слышим. Так, у кого звук пропал? Коллеги, меня слышно? Слышно, все хорошо. Никиту не слышно. Да, Никита, Никита, вас не слышно. Никита, мы вас не слышим. Так, Никита говорит сейчас. Никита, мы вас не слышим. Так. Сейчас попытаемся. Никита, вас не слышно. Почему-то пропал ваш звук. Может быть попробуйте переподсоединиться? Вы нас слышите? Так, у вас микрофона не видно. 

S00 [00:33:47] : Так, а теперь? 

S01 [00:33:48] : Во, пошел звук. 

S00 [00:33:51] : Прошу прощения. 

S01 [00:33:53] : Давайте отмотаем чуть-чуть, на буквально 2 минуты. 

S00 [00:34:04] : Так, где-то здесь, наверное, звук пропал. 

S01 [00:34:07] : Да, вот где-то в конце этого слайда звук пропал. 

S00 [00:34:12] : Так, ну про второе. 

S01 [00:34:14] : Это да, рассказали, что нагромождение. Раз уж сразу, значит, мы здесь остановились, скажите, а вот нагромождение здесь реально тачат ЖПТ? Он какую-то роль в этой статье играет? Или он здесь просто для того, чтобы не обвиняли, что не используется современная технология нарисован? 

S00 [00:34:37] : Это сложный вопрос, потому что, ну, вот если зайти на сайт WillDB, это топовая конференция по базам данных, найти статью, которая бы не использовала ML или чаще всего уже языковые модели, там очень сложно. То есть, как бы отделить успех того или иного решения от успеха языковых моделей становится все труднее. Это лично мое мнение. 

S01 [00:35:12] : Понятно. Хорошо. Спасибо. Давайте дальше. 

S00 [00:35:17] : Так, про третий. Это уже такая история больше про агентный подход, когда у нас есть какая-то аномалия или какая-то ошибка, фейлер, который нужно объяснить для DBA. Вот здесь видно, что Видно, что приходит CPU user очень большой, то есть агент используется различными тулзами. какие-то наблюдения делает, потом собирает а-ля рак-подход, собирает некоторую информацию и, соответственно, объясняет, что произошло, конкретно что где произошло. Но в целом я не могу как-то прокомментировать, насколько вот конкретно это решение хорошо работает. То есть здесь такой... Такой момент, что вот, например, все знания, которые такие агенты используются, то есть они либо есть в открытом доступе, либо они проприетарные все. То есть, например, какие-то банки, другие компании, которые активно пользуются базами данных, там, с хайлоудом, никто не будет делиться конкретно своей какой-то информацией и фейлер детекшенами и так далее. поэтому лично по моим ощущениям chat gpt знает столько же сколько и обычно такие агенты если какой-то дополнительной информации не дается так теперь что дальше вот все что я перечислил как я уже сказал Все вот эти решения, они делят между собой примерно одни и те же проблемы. Например, что языковые модели продолжают галлюцинировать. Это очень всем понятный, очень понятный кейс. Здесь же, например, в тексту SQL модели не обращаются к таблицам или не к тем таблицам обращаются, или агенты в частности рассказывают о решениях, которые не существуют для каких-то ошибок или объясняют эти ошибки не так, или пытаются оптимизировать, но при этом тыкают пальцем в небо. другой момент это стоимость вот это больше касается тех языковых моделей которые здесь я имею ввиду и в деньгах и во времени Это больше касается моделей, которые конкретно для оптимизации запросов сделаны и для ускорения работы онлайн, скажем так, оптимизации. Опять же, чтобы запустить какую-то очень большую модель, там 70 миллиард, ну это несколько, обычно, десятков тысяч долларов. то это могут сделать только большие компании. Но тут сразу вопрос, а почему бы им не поднять просто три дупликата базы данных за те же самые деньги и пропускную способность понадеять, что они еще больше ускорят. Или самое простое, что языковая модель работает просто дольше, чем планирует сам планировщик. И здесь такие яркие противоречия. Также многие модели, например, тренируют под одни и те же конкретные схемы данных и распределения данных. То есть они просто заучивают их, и на этом все. И это на самом деле большая проблема, потому что бенчмарков, не так много в базах данных открытых бенчмарков. И соответственно, ну а мы не можем сделать огромное количество распределений данных, различное большое количество схем и так далее, так далее. Это что касается выучивания под конкретные схемы. Также тренировка в целом модели, например, затруднена для агентов. Как я уже сказал, например, архивы ошибок и различных поломок, фейлеров в базах данных. Потому что это просто рабочая живая среда и open source бенчмарков таких, по крайней мере, я особо популярных не слышал. Все это проприетарно. То есть каждая компания, она старается, кто занимается языковыми моделями, именно по крайней мере базовых данных, все равно эти базы данных делаются под себя и языковые модели учатся под себя но как бы насколько это генерализуемо не совсем понятно ну и также самое такое базовое то есть языковую модель делали для делали для разнообразия, для diversity, чтобы она там, например, генерировала различные тексты. А насколько это нам поможет в оптимизации запросов. То есть у нас-то цель найти самый лучший, единственный план. языковая модель например будет генерировать различные понятное дело что там есть температура и все такое но сама архитектура нам для чего тогда с языковыми моделями то есть это все нам нужно обходить каким-то образом либо решать вот эти проблемы Ну и немножко про возможности. Вот тут в статье мне эта цитата понравилась, что языковые модели предстают перед нами как дышки общего назначения для Data Management. То есть это не просто база данных, это не просто агент по управлению по управлению база данных или по оптимизации запросов. Это уже некоторая большая среда, которая, наверное, возможно, в себя вберет базу данных и база данных станет частью языковой модели, а не наоборот. Но, тем не менее. По возможностям агентные подходы на самом деле очень популярны сейчас. Во-первых, Те агенты, которые работают с данными, находят какие-то бизнес инсайты и так далее. Таких агентов еще, по сути, нет, и вот эта работа ведется буквально сейчас. Если попытаться загуглить статьи, то в большинстве статей это будет 25-й год, скорее всего, какая-нибудь вышла ровно вчера. с таким уже не расталкивались. И второе – это агенты а-ля DBA, то есть DBA-администраторы, которые непосредственно занимаются диагностикой оптимизации и базы данных, и запросов. Также, упоминающаяся тема того, что Вот раньше у нас, например, когда только ML зарождался в базах данных, предлагалось, давайте мы все, что мы понапридумывали в базе данных, вот мы это все там в планировщике, мы это все выкинем и вставим ML-модель, которая нам лучше всего все покажет. Но от этого отошли. и пришли к более такому обобщающему подходу, симбиозу, что давайте вот у нас есть наработана экспертиза в планировщике, большая куча ивристик, мы там понаписали вот эти вот космодели, которые оценивают там операторы, планы и так далее, и так далее. Она все-таки неплохо работает. Давайте мы ML-модель будем объединять с тем, что мы уже наработали. И в симбиозе в таком же это все будет вместе работать. Еще одно направление, это на самом деле маленькие языковые модели. Мне кажется, они гораздо лучше подойдут, просто потому что они могут быть быстрее, они быстрее обучаются, и они могут обучаться в том числе и у клиента. То есть мы разрабатываем базу данных, и встает вопрос, например, мы что-то ускорим с помощью языковой модели, но клиенту это нужно будет тоже как-то передать. Обучить General Purpose, языковую модель, это довольно большая крупная задача и явно не для оптимизации баз данных. А вот маленькие языковые модели, которые могут быстро собрать небольшой датасет и обучиться, такие как мини-трансформеры, то почему нет? Ну и, как я уже сказал, отсутствие бенчмарков и отсутствие больших корпусов конкретно для баз данных сейчас является большим ограничением в развитии агентов. И data agents, которые как бы ищут бизнес-инсайты различные, и агенты для диагностики и оптимизации. Ну и много-много другое. Отсылаю вас вот к этой статье. Она буквально опять же в 2025 году вышла. Ну там очень большая статья на 50 страниц. Но в целом довольно интересно почитать. Так, перейдем теперь непосредственно к нашим баранам. Тем, что я занимаюсь и вот тот маленький кружок, который мы с коллегами немножко расширили. Ранее я уже упоминал, что у нас есть некоторые подсказки для оптимизатора. Эти подсказки говорят, что тот или иной физический оператор не нужен. Сверху мы видим стандартный план. то есть здесь количество срочек, которое запланировано, то есть предсказано, скажем так, моделью, а справа то количество срочек, которое на самом деле получилось. И вот мы видим здесь, что здесь есть последовательное сканирование на определенную таблицу и с помощью подсказки мы можем отключить конкретный отключить конкретный оператор, чтобы планировщик перепланировал тот же самый запрос, но уже не используя определенный физический оператор. Зачем это нужно? При некоторой комбинации вот этих подсказок, которые слева указаны, вот мы использовали семь из них, запросы могут быть ускорены, причем значительно. даже на порядке в некоторые моменты времени. Опять же, то же самое делается с помощью и администраторов баз данных, то есть администратор садится, может посмотреть, как перформит запрос, где у него какие-то недостатки, и сам написать вот эти подсказки. отключить какой-то конкретный оператор, как в данном случае. Есть подсказки для порядка джойнов, то есть что с чем сначала джойнить. Но перебирать впрямую руками все эти подсказки – это очень накладно. для семи бинарных подсказок, которые могут быть только вонов, количество комбинаций 128, что уже подразумевает, что мы один и тот же запрос должны исполнить 128 раз, чтобы получить полную картину, насколько мы можем этот запрос ускорить с помощью этих подсказок. Есть некоторые нейросетевые подходы, которые тоже эти подсказки либо предсказывают, либо предсказывают время исполнения перепланированных планов, но мы там не можем понять, почему конкретно нейросети решила, что вот этот план быстрее или медленнее. Но при этом Еще такой один момент, что очень часто в ML решениях вот этот древовидный план его кодируют вручную. Например, каждый физический оператор это вектор, который там ноликами и единичками заполняется в зависимости от типа оператора. рисует еще кардинальности и много много другое в общем это все делается руками и под каждую задачу приходится переделывать это все поэтому Поэтому у нас возникла такая идея. Давайте попытаемся предсказывать или советовать эти подсказки, вообще не опираясь на смысловую нагрузку всего этого плана, а давайте его просто рассмотрим в виде текста. Он у нас есть в виде текста, не в виде дерева. Давайте просто его в виде текста и попробуем Если два плана близкие по какой-то метрике в виде текста, то они, скорее всего, разделяют за одни и те же подсказки. Сейчас я про это еще расскажу. И при этом эта система должна очень быстро работать относительно других различных ML решений. У нас будет этап онлайн оптимизации. И при этом у нас должен быть этап offline оптимизации, куда мы можем выдать некоторый бюджет и сказать, ну вот с этим бюджетом, пожалуйста, оптимизируйте мне конкретный запрос. Это вот я так медленно подвожу к тому, что в онлайн-оптимизации LLM как бы не очень хорошо вставляется, потому что нам нужно это все-таки сделать достаточно быстро. А вот в оффлайн-оптимизации, когда у нас есть некоторый бюджет, LLM вставляется как будто бы по смыслу гораздо лучше. И вот здесь небольшая схемка. У нас есть какой-то входящий SQL-запрос. Мы используем алгоритм сопоставления планов, который LLM не использует. Он использует только имбеддинги. Про оба алгоритма я сейчас расскажу. И если этот алгоритм отработал, он нашел нужную подсказку для запроса, мы его просто исполняем. Тут вопросов нет. А если какие-то проверки не пройдены или подсказка просто не найдена, мы отдаем его Этот запрос уже в оффлайн-оптимизацию, сейчас мы исполняем его как бы в полной постановке, но при этом отдаем на оффлайн-оптимизацию, ну, скажем так, агенту нашему, и этот агент уже находит находит оптимизацию, находит список подсказок и их складывает в хранилище. Ну и соответственно, дальше это хранилище уже в быстром алгоритме используется. Так вот, что по быстрому алгоритму? То есть, вот как я сказал, давайте план использовать не в виде дерева или еще чего-то, а просто текста. Мы этот текст прогоним через языковую модель, возьмем из нее эмбидинг. И тогда у нас каждый план получит какую-то точку в пространстве планов. И теперь у нас есть некоторое количество планов, которые были оптимизированы с помощью подсказками. Вот, например, план P, он перешел в этом же пространстве планов в другой план H от P, при этом это один и тот же запрос. но уже H от P был оптимизирован, то есть он быстрее отработал, чем план P. А теперь представим, что у нас приходит новый запрос с новым планом P0. Тогда мы опять же возьмем его план, как текст, прогоним через Embedding, возьмем его векторное представление и найдем в окрестности N найдем в окрестности N какое-то количество похожих на него планов. А у каждого из этих планов есть свой оптимальный хинцет, то есть набор подсказок. В этой области N мы тогда найдем самый популярный хинцет и применим его на P0. И он отразится куда-то в другую пространство планов HP0. Он может быть быстрее, может быть нет. Мы этого не знаем. Мы просто запланировали запрос, он еще не исполнился. Но при этом мы предполагаем, что вот... новая область, она в среднем должна быть быстрее по времени исполнения, чем вот эти планы в области N. То есть область K в среднем быстрее, чем область N. И вот это такое очень простое и понятное предсказание по аналогу. Оно, на самом деле, достаточно действенно оказывается. То есть, похожие планы действительно разделяют между собой какие-то похожие наборы подсказок. И теперь, что касается второй части, то есть, оффлайн-оптимизации. Там мы не использовали языковую модель в качестве генератора чего-либо. Мы только использовали имбединги. А здесь же мы хотим построить некоторый диалог. где языковая модель будет агентом а-ля DBA, то есть а-ля администратора баз данных, который смотрит на запрос и предлагает различные подсказки, потом смотрит на результат этих подсказок, то есть на время их исполнения, и предлагает новые версии, если это нужно. То есть вот у нас, например, нода S0, строится диалоговое дерево, и изначальный нод — это нода S0. То есть мы даем языковой модели Prompt, мы даем запрос и говорим, Предскажи мне, какой хинт лучше всего сейчас использовать? Вот один из них, например, предлагает S1, выключай оператор A. Мы его выключаем, мы тестируем в базе данных прям онлайн результат запроса по времени исполнения. Показываем ей, например, что он ускорился или не ускорился. Языковая модель должна понять, что при выключении оператора A у нас время замедлилось, тогда другое, например там D или A не будем, будем B выключать. И таким образом приходить к какому-то ускорению. Мы попытались обучить модель на такой диалоговой основе. Вот у нас в итоге собралось примерно 70 тысяч таких диалоговых ветвей. Почему? Потому что ноди, которые покажут ускорение, на самом деле несколько путей ведут. И вот мы все эти пути дисковой модели показали и дообучили ее таким образом. Вот пример, как это все работает. Базовый системный промпт, потом сам запрос и ответ модели, что вот давайте индекс only scan отключим. Потом мы видим, что у нас, например, ничего не получилось, а давайте мы последовательное сканирование отключим. Вот тут какие-то результаты получились. Первая модель, которая быстро предсказание осуществляет, у нас получилось ускорить на 19% бенчмарк. Ну, потому что это онлайн-оптимизация из 60%, то есть далеко, но еще очень большой запас. И при этом вот этот лм-агент, который мы обучили вести диалог, скажем так, с базой данных, он... почти оптимальное решение нашел для каждого из запросов. То есть на 54% ускорил по сравнению с максимумом 62,5%. На этом у меня все. Давайте что-нибудь обсудим. 

S01 [00:57:21] : Да, Никита, спасибо. Вот у меня по последнему вопросу. Вот то, что ускорение произошло по каждому из запросов, имеете в виду, что каждый из запросов повторялся несколько раз? И при каждом исполнении запроса агент пробовал применять разные вот эти хинты до тех пор, пока он не нашел на те хинты, которые максимально увеличат время выполнения данного запроса, правильно? 

S00 [00:57:51] : Да, да, то есть у нас может быть несколько диалогов таких вот для одного и того же запроса. И агент предлагает различные варианты в зависимости от нашего бюджета, по сути. То есть мы можем в любой момент вот эту диалоговую схему как бы остановить. Мы останавливали, по-моему, на трех приложениях в тестировании. То ли трех, то ли четырех приложениях. То есть за 3-4 подхода к запросу на самом деле львиная доля оптимизации была найдена. 

S01 [00:58:27] : То есть, грубо говоря, если с третьего-четвертого раза агент не подбирал ключики к запросу, то вы останавливались на том, что есть, правильно? 

S00 [00:58:38] : Да, брали лучшее из того, что получилось в диалоге. Если ничего не получилось или получилась только деградация, берем дефолт, значит, у этого запроса нет возможности ускориться. 

S01 [00:58:50] : И ограничения были связаны с тем, что просто агенты сами ели вычислительные ресурсы, которые тоже стоили денег. Правильно? 

S00 [00:59:00] : Да. Мы конкретно тестировали это в базе данных, то есть тут какой-то трейд-офф. Либо агент долго работает и много попыток делает, и мы много загружаем базу данных, либо он делает мало и может не весь перформанс тогда раскрыть для себя. 

S01 [00:59:23] : Извините, там какой-то момент звук пропал, когда вы отвечали. Это стоимость самого агента или еще и проблема в том, что это тормозит весь процесс? 

S00 [00:59:35] : Нет, это конкретно стоимость запуска запроса в базе данных. То есть каждая нода в этом диалоге — это новый запуск одного и того же запроса с разными просто подсказками. 

S01 [00:59:49] : А, то есть он одновременно один и тот же запрос запускает? 

S00 [00:59:54] : Да, то есть вот, например, ну, здесь несколько вариантов, но агент поочередно выбирает один. То есть поэтому это и оффлайн-оптимизация. Не, это понятно. 

S01 [01:00:07] : Угу. Я-то просто, значит, у меня-то родилась гипотеза, я-то тогда, значит, неправильно понял. Так, а нет такой истории, что вот у нас есть база данных, да, где время от времени к ней запускаются разные запросы, да? Мы сажаем агента, который сидит и смотрит. Ага, вот, значит, такой запрос, вот снова такой запрос, вот снова такой запрос. И каждый раз, когда новый запрос приходит, Он специально его не запускает, он просто каждый новый запрос запускает с разными параметрами. И естественным путем. То есть мы специально эти запросы не запускаем, просто ждем, когда они появятся. Ну и на лету как бы это все оптимизируем. 

S00 [01:00:53] : Некоторую сетку параметров подкладывает под запросы, которые действительно нужно исполнить. 

S01 [01:00:58] : Да, да, да. Совмещая приятное с полезным, что называется. 

S00 [01:01:03] : Вот здесь у нас обучено на бенчмарке Job. Это очень популярный бенчмарк, топ-1, наверное, во всем мире для баз данных, и его сделали там группа авторов порядка 10 лет назад, что ли. И, по-моему, то ли в этом году, то ли в прошлом году они опубликовали новую статью, что-то типа ревизитинг, наш бенчмарк и так далее, где они писали, что вот джоп уже устарел, нам нужен новый бенчмарк. И один из абзацев был посвящен тому, что Для любых пользователей баз данных ускорение в среднем на 5% ничего не значит, если хотя бы один запрос замедлился на 50% и они это заметили. Вот что-то примерно так. Если запрос постоянно исполняется и исполняется, а агент подкладывает какую-то оптимизационную сетку под этот запрос и время его исполнения замедляется, то это 100% вызовет гораздо больше нервов и недовольства клиента по сравнению с 5% ускорением в среднем всего ворклоуда. В продакшене, в боевом использовании BD, было бы гораздо легче все заходящие запросы каким-либо образом тестировать. Ту же BIOS оптимизацию применять на кучу параметров и находить лучший. 

S01 [01:02:41] : К сожалению, человеческая натура не позволит. То есть, вместо того, чтобы смотреть на пользователей как на кроликов, лучше ночью запускать это, гоняя холостые запросы в ограниченном количестве, для того, чтобы заранее это соптимизировать. Как-то так, да? 

S00 [01:02:58] : Ну да, да, а то клиенты уже на нас будут как на кролика смотреть. 

S01 [01:03:01] : Да, ни один клиент при оптимизации запросов не пострадал. Окей, а еще у меня вопрос. Я, по-моему, когда рецензию делал, спрашивал. Вот мы с самого начала, если вернуться, я не посуществую, перематывать не надо. Вот мы когда оптимизируем план SQL запроса, Вот, у нас же там не только кардинальности влияют, значит, сама и структура таблиц, у нас же еще наличие или отсутствие индексов влияет на тех или иных таблицах и колонках, то есть как бы физическая, например, структура самой базы данных, вот это оно в каком месте и как оно учитывается? 

S00 [01:03:47] : Вообще, вот сейчас я открою план какой-нибудь. А вот здесь даже написано, что если в nested loop, ну вот в join, внутренняя таблица имеет индекс, и тогда по ней не нужно проходить полностью всю таблицу, а нужно только по индексу пройти до какой-то конкретной строки, то это как бы лучше, потому что это просто быстрее, чем перебирать всю таблицу. И, соответственно, то же самое заложено и в стеймосную модель. Вот при оценке кардинальности, вот здесь, например, почему он использован? Потому что очень малое количество срочек во внешней таблице и во внутренней, то есть 10 и 1. Если бы здесь было на одной из колонок, в одной из таблиц очень много строчек и был индекс, применили бы индекс scan. Здесь, конечно, его тоже применили, но просто потому что он тут есть. Это все равно быстрее было бы. То есть, да, как бы наличие или отсутствие индекса, вот конкретно в COS-модели, оно влияет при расчете вот этого стоимости потому что стоимость она поднимается от операторов от самых нижних операторов и вверху просто как бы вот суммируется каждый с каждым Ответил на вопрос? 

S01 [01:05:31] : Да, я уточню вопрос. Да, ответили, да, но я уточню. Вот смотрите, я просто сейчас, когда гляжу на запрос и план, я не вижу того, где индексы есть, а где нету. То есть, нам же для того, чтобы для этого запроса построить план, нужно же еще и знать, где индексы на каких таблицах есть. То есть, там DDL нужен? 

S00 [01:05:52] : Ну вот в плане видно, что тут есть индекс. Вот он тут. Индекс HAN using TANK-USER2. 

S01 [01:05:58] : Мы же этот план построили со знанием индекса? Да, да. Я что хочу сказать, что если мы пытаемся тренировать модель на построение планов, то мы должны скамливать не только SQL, но и SQL вместе с DDL. 

S00 [01:06:19] : Ну, по идее, да. Тут такой момент, что чем больше, тем лучше, по сути-то. Вот, например, в этой работе, здесь, так, сейчас я увеличу, вот здесь, например, Казалось бы, как языковая модель адекватно воспримет число с семью знаками после запятой. Но вероятность того, что языковая модель от этого что-то поймет, очень мала. Поэтому я бы даже добавил, чем больше, тем лучше, но все должно быть очень качественно. 

S01 [01:07:07] : Понятно. Ну да, но здесь как бы вопрос тоже галлюцинации. Если мы, так сказать, берем, тренируемся, значит, на SQL-запросах с одними индексами, а потом выясняется, что в продакте, так сказать, в целевой базе данных этих индексов нету или они другие, то результаты предсказания будут, мягко говоря, сомнительными. 

S00 [01:07:30] : Ну да, и еще такой момент, что По моему опыту, к индексам подход в академии и в реальности, он все-таки немножко другой. Пока читаешь обсуждения каких-нибудь администраторов баз данных, которые конкретно продакшеном занимаются, у них отношение к индексам очень простое. Если таблица часто не обновляется и данные упорядочены, то строй индекс. А если нет, то нет. И все на этом. А в Академии там как бы, ну, я вот честно даже глубоко очень не погружался, потому что там огромное количество статей про то, как индексы строить, какие конкретно индексы строить, разные типы там, B3, не B3, и много-много-много-много другое. При этом в продакшене, по моим ощущениям, всё гораздо проще и жёстче. Либо работаем, либо нет. 

S01 [01:08:44] : Непонятно. Вопрос в продакшене от бедности или от того, что... не могу ответить на вопрос. Понятно, работают не трожь. Ладно, спасибо. Коллеги, есть какие-нибудь еще вопросы к Никите? Вопросов нет. Ну тогда, если вопросов нет, Никит, большое вам спасибо и за общеобразовательную часть и за содержательную, где используется, где не используется, где надо, где не надо, ну и что вы делаете в этой области. Было очень интересно, спасибо вам большое. 

S00 [01:09:32] : Спасибо, надеюсь, тоски не нагнал. 

S01 [01:09:35] : Да нет, почему, было как раз очень интересно. Все, спасибо, всем до свидания, всего доброго.







https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
