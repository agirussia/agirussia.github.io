## 9 октября 2025 - Новые приключения Марти (Марти 5.2 - математическая модель "себя в мире" как первая ступень самосознания) - Игорь Пивоваров - семинар AGI
[![Watch the video](https://img.youtube.com/vi/XnqxJSIKGnE/hqdefault.jpg)](https://youtu.be/XnqxJSIKGnE)
- [видео в ВК](https://vkvideo.ru/video-210968399_456239231)
- [видео в RUTUBE](https://rutube.ru/video/746e51a64dea8a664deaf6f55b5ccb46/)
- [видео в Telegram](https://t.me/agirussianews/2206?comment=111748)

### Краткое содержание

Игорь Пивоваров представил обновленную версию своей модели искусственного интеллекта "Марти" (MARTI - Modular Artificial Intelligence) на заседании сообщества русскоязычных разработчиков общего и сильного искусственного интеллекта. Модель, изначально основанная на идеях Сергея Шумского, развивается Игорем самостоятельно и в последних версиях значительно отошла от первоначальных подходов. Доклад является расширенной версией выступления на конференции "Сознание 2025".

#### Ключевые концепции и архитектура

**Доязыковой интеллект:** Пивоваров определяет интеллект как способность к долгосрочному планированию, постановке и достижению целей в условиях неопределенности. Он подчеркивает, что интеллект является доязыковым понятием, появившимся в природе задолго до языка.

**Основные принципы модели:**
*   **Упрощение:** Интеллект должен упрощать сложный внешний мир, создавая абстрактные модели и объекты.
*   **Иерархичность:** Модель должна уметь вкладывать абстракции друг в друга, как в понятийном, так и во временном смысле, позволяя планировать от общих концепций к конкретным действиям.
*   **Предсказание:** Мозг рассматривается как "машина для предсказаний", которая прокручивает различные варианты развития событий, чтобы выбрать оптимальную стратегию.

**Архитектура, вдохновленная мозгом:** "Марти" — это модульная гибридная цифро-символьная модель, архитектура которой вдохновлена строением мозга:
*   **Неокортекс:** Моделируется ансамблем "колонок" или микро-агентов, которые параллельно строят модели мира и предсказывают варианты действий.
*   **Базальные ганглии:** Выступают в роли управляющего центра, который выбирает оптимальный вариант из предложенных "колонками".
*   **Мозжечок:** Автоматизирует удачные, уже отработанные действия, исполняя их "на автопилоте".
*   **Таламус:** Является распределительным узлом, оркестрирующим работу всей системы.

#### Модель "себя" и разделение на "Что" и "Где"

Центральной идеей "Марти 5.2" является динамическое формирование "модели себя". Модель анализирует входящий поток данных и идентифицирует те его части, которые являются наиболее предсказуемыми и управляемыми — это и становится ее "моделью себя". Это похоже на то, как ребенок со временем понимает, что его рука является частью его самого, так как ее движения предсказуемы в отличие от остального мира.

Для реализации этого в "Марти" существуют два типа "колонок":
*   **Колонки "Что" (What):** Анализируют общую ситуацию, строят траектории событий и предсказывают наиболее желательное следующее состояние.
*   **Колонки "Где" (Where):** Специализируются на координатах "себя". Они определяют, какие действия нужно совершить агенту, чтобы переместить себя в положение, необходимое для достижения желаемой ситуации, предсказанной колонками "Что".

Изначально модель не знает, какие компоненты входного вектора относятся к "себе". Она создает множество прото-колонок "Где" для разных компонентов и оценивает их предсказательную силу с помощью коэффициента PMI (Pointwise Mutual Information). Те колонки, которые показывают высокую корреляцию между действиями агента и изменением соответствующего компонента, идентифицируются как часть "модели себя" и начинают работать в паре с колонками "Что".

#### Тестирование и результаты

Модель тестируется на классических играх из набора OpenAI Gym, в частности, на играх Atari, таких как Breakout и Pong. Эти среды выбраны, так как они стандартны, разнообразны и используют обучение с подкреплением (Reinforcement Learning) с редкими вознаграждениями (sparse rewards), что усложняет задачу и приближает ее к реальным условиям.

Результаты "Марти" показывают значительное превосходство над случайными действиями и базовыми алгоритмами, но пока уступают передовым моделям, таким как DeepMind DQN. Например, в игре Pong средняя разница забитых и пропущенных мячей у "Марти" составляла -11, в то время как у DQN — +20. Однако Пивоваров отметил, что недавние технические улучшения, связанные с корректной настройкой параметров среды тестирования, позволили на порядок улучшить результаты, и модель продолжает развиваться. Важным преимуществом "Марти" является его эффективность: модель работает на одном CPU, в отличие от ресурсоемких нейросетевых архитектур.

#### Выводы и "Теорема о самосознании"

Игорь Пивоваров утверждает, что "Марти", в отличие от больших языковых моделей, делает первый шаг к самосознанию, так как умеет выделять себя из общего потока информации. На основе этого была сформулирована "теорема": **отделение себя от окружающего мира является необходимым, но недостаточным условием появления самосознания.** Следовательно, модели, неспособные на такое разделение, не могут обладать самосознанием.

### Расшифровка доклада:

S03 [00:00:01] : Коллеги, всем добрый вечер. Начинаем очередное заседание сообщества русскоязычного разработчиков общего и сильного искусственного интеллекта. И сейчас у нас в гостях после длительного перерыва Игорь Пивоваров. Он нам расскажет о новых приключениях модели Марти. которую они начинали разрабатывать вместе с Сергеем Шумским и представляли, по-моему, на целых двух семинарах прошлые годы. И сейчас Игорь, как я понимаю, ее самостоятельно дальше развивает и расскажет о достижениях марта 5.2. Игорь, пожалуйста. 

S04 [00:00:36] : Добрый вечер, коллеги. Я сразу скорректирую, я с самого начала, модель Марти это исключительно моя модель, но она во многом изначально базировалась на идеях Сергея Шумского, и мы с ним обсуждаем регулярно какие-то продвижения. Она просто последние несколько лет сильно ушла от довольно прилично ушла от некоторых его подходов, которые он сейчас развивает. Он основным текстом занимается. Но базовые идеи, конечно, остаются самые основные. Но сегодня я буду рассказывать не столько про с неизбежностью расскажу про саму модель, но довольно контурно. А буду рассказывать про один особо интересный аспект, который, собственно, я рассказывал на конференции «Сознание 2025» в этом году, где мы с Антоном встречались, и он меня подговорил сделать доклад здесь, на семинаре. И я хочу вообще сказать большое спасибо Антону за то, что он все эти годы бессменно ведет этот семинар, тратит кучу времени на поддержание этих чатов и выкладывание записей на эту комьюнити. Это большое дело, Антон. Спасибо, что меня сюда пригласил. Я думаю, что если бы здесь были возможности аплодисментов, то все бы похлопали, но не придумали еще этих аплодисментов. этих приложений. Я сегодня расскажу более расширенную версию доклада, который я делал на этой конференции «Сознание». Ну и приятно, что Владимир Сергеевич тут тоже есть, который тогда был вынужден убежать с середины, даже нет, наверное, с первых десяти минут доклада. У него там был параллельный доклад в другой секции. А сегодня это будет более детально. Ну и вот Красновидово. Это был доклад, сделан в Красновидово. Я оставил этот слайд оттуда. Это прекрасный слайд, чтобы показать, что такое это Красновидово. Как замечательно устроен русский язык. Название деревни очень ясно читается. Для тех, кто меня не знает, пару слов скажу по себе. Я физик по образованию, физик-теоретик. Занимался квантовой теорией поля и биофизикой. Программист всю жизнь. С юности зарабатывал программированием самым разным. С нейросетями столкнулся еще в 96-м году, но тогда туда не пошел. Занимаюсь в искусственном интеллекте в основном всякими организационными и аналитическими задачами, ну и вот делаю свою собственную модель машинного обучения, про которую сегодня буду рассказывать. Большинство людей меня знает по моей конференции OpenTalks.ai, которую мы делаем с 2018 года. Что я в принципе сегодня порассказываю? Какой я вижу связь сознания и интеллекта? Так как у нас была конференция о сознании, доклад был привязан к ней, то нужно рассказать про эту взаимосвязь. Какой должна быть модель интеллекта с моей точки зрения? Какие у нее должны быть основные принципы, на которых она базируется? Как ее тестировать и как мы ее тестируем? Расскажу про Марти, что он из себя представляет, как устроена эта модель и что там является моделью себя. Покажу результаты, демонстрацию и сделаю некую теорему в конце. И специально для этого семинара я еще вставил несколько слайдов. Во-первых, там будет несколько слайдов с техническими замечаниями для людей, которые Так же, как я занимаюсь тестированием, это будет полезно, мне кажется, для некоторых. Сразу сделаю свой любимый дисклеймер, что слова не точны, и если есть какие-то понятия, которые, как правило, нечёткие, то у одного человека они попадают как бы в одни слова, в рамки одних слов, а у другого человека это как бы в рамки других слов, поэтому очень сложно иногда биться за какие-то определения, поэтому я, собственно, никогда не бьюсь за определения. и не собираюсь, если вам покажется, что определена не совсем корректно, ну окей. Мне кажется, важны не определения, а важна работающая модель. Если она работает, значит результат есть. Если она не работает, то какие бы определения мы не согласовывали, толку никакого. Дам свое понимание нескольких слов, которые я дальше буду использовать. Сознание принято сегодня в научном мире Наибольшее число исследователей сходится к тому, что под сознанием определяется весь субъективный феноменологический мир агента, субъекта. меня или вас, который воспринимает внешний мир, и в этом комплексе, внутри этого сознания, производится весь комплекс ощущений, переживаемых процессов, состояний, наша активность, в том числе наша когнитивная активность, то есть мышление как часть всего внутреннего мира. И также оно может включать в себя самосознание. На мой взгляд, возможно, сознание без самосознания. Я про это еще потом скажу. Обучение – это способность накапливать информацию о результатах своего поведения и менять это поведение. Ну, в первую очередь, с целью выживания. Это, конечно, определения, которые связаны, в первую очередь, с живыми организмами. И для меня обучение и сознание могут быть разделены. Я тоже это потом покажу. Интеллект я для себя определяю как способность к долгосрочному планированию действий, умению ставить цели и достигать их, при этом в условиях там неопределенности сильные, иногда добавляются граничными ресурсами. Ну да, это тоже может быть граничными ресурсами. Есть много разных определений естественного интеллекта, но я для себя вот это определяю так, и в частности… Так, а здесь какой-то слайд у меня был пропущен, ну ладно. А, он будет потом, ладно, окей. И вот если расставить все это на некой шкале эволюционной, как это возникало, это такой заведомо провокационный слайд, дискуссионный, и поэтому вопросы по нему, обо всем, конечно, в конце мы поговорим. То, на мой взгляд, в процессе эволюции жизни, жизнь сама возникла очень давно, как говорят очевидцы, примерно 3 миллиарда лет назад, но сознание возникло, в моем понимании, в прото-сознании, в очень упрощенной форме, задолго до появления других понятий, о которых мы сейчас говорим. И примитивное обучение возникло, безусловно, до интеллекта. И, на мой взгляд, примитивное такое протосознание возникло до самосознания. И важный момент, который я хочу на этом слайде подчеркнуть, что для меня интеллект – это доязыковое понятие. То есть интеллект не обязательно должен работать с языком. Интеллект в природе появился сильно раньше, чем появился язык. Безусловно, в человеческом сообществе появление языка и возможности коммуникации, записи знаний, их передачи очень сильно развило интеллект, без сомнений. И то, что сегодняшний интеллект человека на 90%, а может быть на 99% связан с языком, это факт. Но для меня является таким же очевидным фактом, что интеллект сам по себе имеет доязыковую природу, поэтому нужно его на самом деле исследовать в отрыве от языка. А это далеко, ну и это сейчас не мейнстрим, как вы все знаете, как тут сегодня было в чатике написано, мы живем в эру ЛЛМ. Боже мой, эра ЛЛМ. Кто бы мог подумать. В общем, условная такая шкала, я ее так для себя рисую. Я занимаюсь интеллектом доязыковым. Здесь интересная связка с самосознанием, потому что я считаю, что на самом деле интеллект это некий инструмент для кого-то, которым некто пользуется. Поэтому должно той идее сперва появиться некое самосознание, чтобы появился некоторый субъект. Мы говорим не о компьютерных моделях, а о жизни. Не важно, про это мы еще дальше поговорим. Какой должна быть модель интеллекта с моей точки зрения? Что она должна… базовые черты, которые в ней должны быть обязательно, как мне кажется. Во-первых, интеллект – это способность к упрощению. Мы смотрим на сложный окружающий внешний мир и очень сильно его упрощаем. Я, например, сейчас смотрю на вот там. экран перед собой, у которого на 2000 пикселей, но я вижу не пиксели, а я вижу слайды, я вижу картинки, я вижу задумавшегося Антона, задумавшегося Владимира Смолина, то есть я сразу вижу некоторые цельные объекты. Это предельное упрощение вне сложной окружающей богатой действительности до простых абстрактных объектов, с которыми мы дальше оперируем. И это очень важно. Интеллект должен уметь упрощать. По-другому это можно сказать создавать модели или абстракции. много разных может быть определений. Вторая история про интеллект, он должен быть иерархичен, он должен уметь эти абстракции, эти категории вкладывать друг в друга как матрешки и в понятийном смысле, с точки зрения антологии, как это говорят, что одно понятие входит в другое, и во временном смысле, что важно для моего сегодняшнего Хотя, в принципе, сегодня про иерархичность Маркера рассказывать не буду, но, в принципе, он иерархичен. И когда я говорю про иерархию, я имею в виду следующее, что, например, когда я говорю про конференцию... А, кстати, а вид на мышку на экране, на расширенном, да? Когда я говорю про иерархию, то я имею в виду, что вот была там концепция конференции, она была примерно там. в июле было известно, я знал, что в сентябре я буду делать доклад. Это был очень верхнеуровневый план. Я решил, что окей, я буду участвовать и буду делать доклад. Потом где-то в августе появились первые тезисы, в конце августа появились первые слайды, потом появлялись картинки, детали. Ну и наконец, когда я сейчас вам рассказываю конкретно этот слайд, Это уже непосредственно реализация длинного иерархического плана, который сперва был очень крупными мазками сделан. Сделать доклад на конференции. И в этом смысле интеллект он иерархичен. Мы делаем сперва крупные мазки. потом их детализируем, детализируем и переводим в непосредственные действия. Было бы очень сложно жить, если бы мы всю свою жизнь планировали, ну там, как это, сороконожка, какую ногу сейчас поднять и куда поставить. Это было бы крайне сложно. И в конечном итоге, что делает интеллект? Интеллект – это мозг. Наш мозг – это такая машина для предсказаний. Мы занимаемся тем, что пытаемся предсказать, что будет дальше, Исходя из наших вариантов видения мира, дальше мы выбираем для себя оптимальный план действий. Собственно, оптимальный вариант. И вот эта возможность человека прокручивать в голове, скажем, интеллекта, интеллектуальных существ, прокручивать у себя в голове какое-то количество разных вариантов и выбирать для себя лучший, не проворачивая их в физическом мире, в реальности, а в голове, позволяет нам выбирать для себя оптимальную стратегию действия. Вот это про интеллект. Допустим, мы делаем такую модель интеллекта. Как ее тестировать, если я сделал модель? Тесты – это большая история сегодня, потому что есть много моделей, и люди пытаются понять, это оно, это не оно, как с этим. И большинство моделей сегодня, естественно, большинство тестов сегодня языковые. Делаются всякие, вы знаете, всякие тесты на рассуждение и на прочее. Но моя модель, которой я занимаюсь, она доязыковая. Исходя из определения, которое я для себя сформировал, вот такой интеллект, это способность адаптироваться в любых средах, ставить цели, достигать их в условиях неопределенности с ограниченными ресурсами. Соответственно, тестирование должно моделировать разные среды, должно моделировать возможность агента ставить эти цели и достигать их в условиях неопределенности. Понятно, что 100% такого теста нет, но есть вещи, которые довольно близко к этому подходят. Ну и дальше просто вопрос, что выбрать. Я конкретно выбрал для себя довольно давно, вернее так, мое тестирование было из трех этапов. Первым делом я написал для себя некоторую свою собственную Нечто, некий полигончик такой. И мне он казался очень понятным. Какая-то букашка ползала, значит, управляемой моделью и что-то делала в лабиринте. Но при первом же в моих контактах с другими профессионалами стало понятно, что никто не понимает, как устроена моя тестовая среда и не хочет, главное, в этом разбираться. Никто не хочет на это время тратить. И что если ты хочешь, чтобы тебя понимали, нужно использовать стандартные среды, которые используются во всем мире. Так, чтобы их люди уже знали, что они из себя представляют. И поэтому из этих сред, соответственно, я перешел на OpenAI Gym, а потом внутри него перешел на игры Atari. Игры Atari – это следующее. Был такой компьютер Atari, в котором было довольно много игр, по-моему, я забываю, 120 с чем-то, что ли. В общем, довольно много игр, и прелесть этого… Значит, это библиотека, которую вы ставите себе на компьютер, и она воспроизводит эту игру. Прелесть ее в том, что там довольно простой интерфейс на Питоне и примерно 10 строчек на Питоне. Марти у меня написан на Яве, но это не суть важно. Есть кусочек, который работает с OpenGM на Питоне, потом он передает данные Марти, который работает на Яве, они как бы в паре работают, не важно. Но важно, что здесь есть строчек, ну там даже меньше 10 на самом деле, строчек на Питоне, очень удобный интерфейс для того, чтобы с ним работать. И прелесть в том, что там много разных игр. Фундаментальная вещь, на которой работают все вот эти игры, она называется Reinforcement Learning, это обучение с подкреплением. То есть агент учится в этой виртуальной среде, получая состояние этой виртуальной среды и получая вознаграждение за правильные действия, из-за неправильных действий. Примерно так, как мы учим, скажем, собаку. Ты говоришь, принеси мячик, она побежала, принесла мячик, говоришь, молодец. Если она там сделала что-то плохое, мы говорим, ты сделала это плохо. Специально, сделав пару новых слайдов, подчеркну такую сложную вещь. В этих играх Atari используются не просто реворды, а так называемые sparse rewards, то есть редкие вознаграждения. Агент получает вознаграждение, ну штраф получает, когда он пропускает мяч минус один, или когда он забивает мяч плюс один. И вот обратите внимание, все вот эти вот, допустим, это игра Breakout, ракеточка двигается, вот вознаграждение, допустим, вот только здесь получил реворд, или только когда он отбил, в смысле, когда вот выбил кирпичик. Тогда он получил плюс один. Если ракетка пропустила мячик, то он получает минус один. Во всех остальных случаях он вознаграждение не получает. Чтобы это еще больше драматизировать, я вам другую игру покажу. Понг, тоже на которой Марти работает. Вот смотрите. Правой ракеточкой управляет Марти, а левой ракеточкой управляет компьютер. Вот пока вы смотрите, что происходит, модель ни разу реворта не получает никакого. Все время ноль. Несмотря на то, что она много раз все сделала правильно, и вот только здесь, где модель пропустила мяч, она получила минус один. А всю остальную игру, давайте я еще раз покажу, чтобы это было… Это довольно непростая история, но вообще вся жизнь так устроена. Грубо говоря, когда партию в шахматы вы играете, только в конце станет понятно, вы выиграли или проиграли. А все ходы посередине, их нужно оценивать как-то по-другому. Это сильно усложняет жизнь разработчику, когда у тебя такая система реворда, но вот так устроена жизнь, и это ближе к жизни. Игры Atari до определенной степени воспроизводят идеальный тест, с моей точки зрения. Оно моделирует разные среды, у агента разный вход, он устроен по-разному. Единственное, что он не моделирует, он не ставит, агент не ставит цель. Эта цель, она как бы следует из этой игры. Агент должен просто найти оптимальную траекторию. Что сюда не входит? Не входит никакое оперирование абстрактными понятиями, язык, никаким образом здесь языка нету, никакого здравого смысла, о котором сейчас любят говорить и так далее. Это все где-то... за гранью этого теста. Ну вот, значит, такое тестирование. Теперь, собственно, перехожу к основным идеям Марти. И сперва еще до математики про то, на каких идеях он базируется. Марти базируется на том, что мы знаем про мозг крупноблочно. Крупноблочные некоторые идеи, которые мы знаем про мозг, и дальше из этого сделаны некие модели математические. Что мы знаем про мозг? У нас есть неокортекс, кара новая, которая осуществляет рациональные мышления, но все наше поведение определяется не неокортексом, все поведение определяется древним ящерным мозгом, который внутри находится, лимбический мозг так называемый, базальные ганглии, амигдала и так далее. Это поведение определяется там. И есть большая часть мозга, которая называется мозжечок. Когда мы говорим, что в мозге 80 миллиардов нейронов, из них 60 или 70 – это мозжечок как раз. А мозжечок занят тем, что он просто автоматизирует действия, про которые уже не нужно думать. Когда кора выработала планы и уже известно, что делать, мозжечок это позволяет делать, не думая дальше об этом. Что мы знаем про неокортекс? Все вы знаете, что такое нейронные сети, и сегодня их все моделируют, это довольно известно, я не буду рассказывать про нейронные сети, но еще мы знаем про неокортекс такой интересный факт, что он организован такими группами, колонками, и что нейроны там не разбросаны вообще там, хаотично, а есть такие как бы модули, и вот эта фотография, ну одна из, там, миллионов фотографий, которые на этой тему есть. Эти колонки, значит, мини-колонка состоит где-то из 100 нейронов, условно, а колонка состоит где-то из 50-100 таких микроколоночек. И мозг в общей сложности содержит, ну, около миллиарда этих, значит, мини-колонок. Каждый из них что-то делает. Дальше есть таламус, который является распределительным узлом, из которого все эти связи выходят, и он оркестрирует всю эту Таламус, гипоталамус оркестрируют всю работу мозга и считается, что эти рекуррентные таламо-критикальные связи, которые из таламуса идут в кору и обратно, работают для усиления движения внимания, для интеллекта. Центром лимбической системы являются базальные ганглии. Это командный пункт, ящерный мозг, который управляет нашим мышлением во многом. Это самая сильная тормозная система мозга. Она тормозит 90 или 95% всего неокортекса. По каким-то образом она выбирает, какие колонки тормозить, а какие нет. Сегодняшнее представление об этом, которые я разделяю, на которые Мартин основан и, соответственно, про которые я сегодня рассказываю, состоит в том, что все вот эти микроколонки, они занимаются, ну, так сказать, Такие микроагенты, которые занимаются предсказаниями, такой ансамбль этих колонок. А базальные ганглии управляют этим ансамблем, которые фактически вырабатывают разные варианты действий или варианты, допустим, следующего движения. Что делать? Бежать, нападать, остановиться, замереть или так далее. Животные какие-то действия. И выбирают из этого ансамбля действий то, которое нужно сейчас, в моменте. Вот это базальные ганглии, интереснейшая вещь, это то, о чем сейчас Ян Лекун последние годы говорит, что нужно моделировать базальные ганглии. Интересная вещь, о которой я только в прошлом году обратил внимание, которая стала центром того, о чем я сегодня рассказываю, это то, что в мозгу есть области, отвечающие за что и за где. What and where. И еще интересно, что они работают фактически одновременно. То есть если раньше входная информация, сенсорная, она как бы проходит... То есть другими словами, есть, допустим, зрительная карьера, есть... Области, которые отвечают за идентификацию объекта, за то, что это из себя представляет. А есть области, которые отвечают за его положение по отношению ко мне, рассчитывают это каким-то образом. И это уже довольно давно известно. Последняя статья, одна из последних статей 2018 года, но вообще этот факт уже известен лет, наверное, как 15. Но только никто пока не до конца не понимает, что с этим делать. Потому что эти области вроде как работают одновременно каким-то образом. И нейрофизиологи пытаются понять, каким образом, что и где, как они взаимодействуют, что с ними происходит. Ну вот, собственно, базовые идеи, которые лежат в марте, объединяя все вот эти отдельные кусочки информации, они выглядят так. Наш неокортекс занимается построением моделей мира и предсказанием вариантов действий. Причем не просто он занимается этим предсказанием, а на самом деле это делают гиперколонки, которых там очень много, и они делают это ансамблем параллельно. И там не один уровень, там много разных уровней. Нижние уровни, допустим, работают с сенсорной информацией и предсказывают сенсорные уровни. Потом следующие уровни работают с первым уровнем и предсказывают его движение. Следующие уровни со вторым уровнем и так далее. Отчасти это похоже на то, как сейчас есть такая идеология, когда делают ансамбль деревьев решающих, когда у вас CatBoost, например, у вас есть ансамбль деревьев, а над ним строится следующий ансамбль деревьев, который учится на ошибках предыдущего и пытается минимизировать ошибки предыдущего слоя, а последующий учится еще на ошибках предыдущего. Но отличие вот этой модели еще состоит в том, что здесь как бы два потока информации. Есть один поток снизу вверх. А есть еще поток сверху вниз предсказательный. Это вот то, что называется предиктивное кодирование. Я сегодня на этом останавливаться не буду, чтобы просто не перегружать, но упомяну, что вот эта модель, она реализует предиктивное кодирование. Соответственно, среди этих колонок есть колонки, которые предсказывают что, то есть которые специализируются на вопросе что, а есть колонки, которые предсказывают где. Базальный ганглий – это управленец всего ансамбля, который пытается выбрать из всего этого спектра предлагаемых вариантов оптимальный, и дальше он реализуется. А удачные варианты, которые найдены мозгом, они отбираются и потом просто автоматизируются. Есть мозжечок, который их исполняет на автопилоте. Вот, собственно, такая архитектура общая, базовая у Марти. С точки зрения специального слайда, уровень моделирования выше, чем сегодняшняя нейронная сеть. В смысле не лучше, а просто другой. уровень моделирования на один порядок выше. Минимальной единицей модели в марте является нейронный ансамбль, не сам нейрон. Модули, которые я буду показывать, представьте себе, что у каждой из этих модулей это примерно 10 тысяч нейронов, и тогда будет дальше все. То есть, в принципе, наверное, каждый из этих модулей можно реализовать нейронной сетью, даже почти наверняка. Мы потеряем сразу в понимании, что там происходит. А в маркете очень прозрачная модель, там все понятно, что там делается. Дальше не буду здесь останавливаться. В общем, тоже не буду. Потому что время, мне кажется, двигается... Антон, мы нормально по времени? Нормально, да? Отлично. Я хочу хотя бы в час уложиться, чтобы дальше я предчувствую массу дискуссий. Подробнее расскажу про MARTI. Я не буду приводить код или формулы, но идеи все основные расскажу. Во-первых, MARTI – это МОДУЛ АРТИФИЩИЛ ИНТЕЛЛИЖЕНС. Модульный искусственный интеллект или микро искусственный интеллект. Это гибридный цифровой символьный модель, потому что она принимает на входе цифру, но на самом деле все операции внутри идут символьные. Он иерархический, потому что там много слоев, и эти слои друг с другом общаются, и там есть временная иерархия. Модульный и асинхронный в том смысле, что там довольно много модулей. Сейчас рабочие модели, которые Это примерно 50 колонок в слой, и причем все эти 50 колонок выполняются одновременно, независимо от параллельных потоков. Искусственный интеллект предназначен для моделирования целенаправленного поведения, то есть он не для языка, он именно для целенаправленного поведения. Еще можно про марки сказать так, что это многоагентная модель. Каждая колоночка является микроагентом, который видит либо полный мир очень упрощенный, если так можно выразиться, либо какую-то часть этого мира, но все равно ее очень сильно упрощает, строит по ней свою картину мира и дальше исследует причинно-следственную связь событий и пытается предсказать наилучший исход со своей точки зрения. а решение принимается ансамблем. Это близко к сегодняшней то, что вот сегодня делают архитектуры, так называемые mixture of experts. Я-то это придумал еще сильно до этого, но просто Мы не публиковали это. Хотя нет, не только я. Во многом это базируется на идеях Джеффа Хокинса, а до него еще Марвин Минский писал про этот Society of Minds. Я придумал это сильно, скажем так. Многие умные люди придумали еще сильно до этого. Марк тоже так работает. Считайте, что там много маленьких экспертов, Собственно, и у людей так бывает. Чтобы решить задачу, нужно собрать много экспертов. Вопрос весь всегда в том, кого слушать и как слушать. Слушать самого громкого или самого старого, ну и так далее. В человеческих сообществах это всегда челлендж. Ну вот тут, значит, тоже есть некая логика принятия решений. Я покажу там несколько картиночек. Марки там одна из особенностей, просто одна из фишек реализации. Я там какое-то время написал с визуализацией, но он там как-то визуализируется автоматически с помощью библиотеки D3 и выглядит он Примерно так. Есть таламус, который принимает на входе информацию. Он как бы через слой устроен. Коннекторы, парсеры, коннекторы, парсеры, коннекторы, парсеры. Коннекторы – это некоторая сущность, которая принимает на вход цифровой вектор и переводит его в символ следующего слоя. а парсеры, собственно, колонки, которые работают с этими символьными последовательностями и строят по ним причинно-следственные связи и делают по ним какие-то выводы. И информация передается снизу вверх, это слева направо с точки зрения этой схемы, ну и обратно. И вот здесь нарисована фактически двухуровневая минимальная модель. колонки. Я сейчас про них расскажу, кто из них, потому что на модели не очень понятно. Значит, маркер получает на вход цифровой вектор любого размера. И теоретически любой природы, но фактически эта модель подразумевает, что в этом векторе где-то есть какие-то составляющие, которые саму модель характеризуют. Вернее, которые характеризуют самого агента. где они неизвестны, но вот какой-то вектор на входе есть, но где-то в этом векторе какие-то числа, одно число или там сколько таких чисел каким-то образом характеризуют самого агента. И это как бы базовое. Если мы делаем робота, изначально Марти проектировался, чтобы быть мозгами для робота, то когда робота мы делаем, то заранее всегда известно, где его координаты, каких его частей, где он в пространстве. То есть это вот эти вещи, которые известны. То есть можно их не вычислять, а маркер занимается их вычислением. Потому что про игру Atari мы заранее этого не знаем. А выход это тоже вектор следующего действия. На входе у нас вектор состояния среды, который как-то устроен. Мы не знаем как. Вот какой-то вектор входной. Дальше его кусочек – это вектор о действии, которое было сделано, и подкрепление, в данном случае 0. Большинство подкреплений равно нулю, но иногда оно равно единичке или минус единичке. На выходе, соответственно, Марти выдает в ответ тоже какой-то вектор действия. Он набирает какую-то статистику векторов. В самых простых играх Atari этот вектор действия – это одно число, просто вектор из одного числа. Значит, что делает дальше Таламус? Сперва Марти набирает какое-то время случайным образом, просто какое-то время совершает случайные действия. Он просто набирает информацию об окружающей мире. Ну, как ребенок, который родился и просто как-то шевелит ножками, ручками, все двигается. Это просто какая-то рандомная деятельность. набрав довольно большую статистику. Реально, по-моему, сейчас 5000 шагов набирается для того, чтобы следующий шаг сделать. Входной вектор разделяется на несколько групп. Во-первых, он есть целиком, то есть это полная картина окружающего мира, но из него выделяются отдельные кусочки, которые могут претендовать на то, что это потенциальный зародыш и модели себя. Ну, грубо говоря, сразу скажу, как устроена эта модель себя, чтобы это было более понятно на этом слайде. Когда ребенок наблюдает за нейром, то мир вокруг как-то устроен, но в конечном итоге мы обращаем внимание, что какие-то куски этого мира, какие-то куски матери окружающей более предсказуемы, чем все остальное. Вот моя рука, она для меня намного более предсказуема и управляема, чем весь остальной мир. И в какое-то время ребенок начинает понимать, что вот это относится к нему. И вот эти части мира, они относятся к нему, они являются его частью, а не просто элементами окружающего мира. То есть то, что является для нас наиболее предсказуемым. Я, кстати, хотел отдельно вставить, но забыл. Надо будет вставить ссылку найти. Это идея, которая очень давно еще философски сформулирована, еще философ. была уже как-то обоснована. И вот Марк из этого цифрового вектора выделяет такие компоненты, которые могут потенциально претендовать на то, чтобы быть такими вот честными себя. Ну и отдельно выделяется вектор движений. Что дальше делается? Каждый из этих коннекторов делает связку цифра-символ. И у него на входе какие-то цифры, а на выходе у него символ. Что он делает? Он набирает довольно много этих векторов и потом их кластеризует. Кластеризация – это когда у нас есть много-много точек в многомерном пространстве и просто выбираются области, в которых эти точки близки друг другу. Причем эту кластеризацию можно сделать не бесконечным, но количество способов, наверное, четко надо посчитать. В общем, оно как-то связано с количеством точек. Но количество кластеров может быть потенциально... Способов разбиения на кластерах может быть очень много. И поэтому Марки делает параллельно сколько-то этих кластеризаций, но сейчас в моменте модели, которые я показываю, результаты, которые я показываю, 50 кластеризаций. И 50 способов разбиения пространства. Коннектор получил, допустим, 10 тысяч векторов, и он делает 50 кластеризаций, допустим, по 1000 кластеров, группируя эти векторы в отдельные кластеры. Каждый номер кластера, можно использовать номер, а можно считать его буквой. Если это кластер 0, то это буква А, если это кластер 1, то буква Б и так далее. Считайте, у вас появилось 1000 букв. Грубо говоря, мы разбили непрерывное входящее пространство векторное на дискретное количество символов. И каждый символ что-то обозначает, наверное. Это зависит от того, что этот коннектор получает на вход. А дальше парсеры, которые сверху работают с этими символами. Для каждой кластеризации, соответственно, создается колонка. Колонка – это такой вычислительный поток, который работает с этими символами. И этих колонок есть два типа. Одна колонка Одни колонки это колонки типа что, парсируют ситуации, ну или там парсируют действия, не важно. Это колонки типа что, которые работают как бы с ситуациями. А есть колонки типа где. Это колонки, которые работают с положениями. По сути они получают, вот если вернуться на пару слайдов назад, они получают... Это коннекторы, которые получают какую-то одну координату из входящего вектора, которая потенциально является такой координатой себя. Модель упрощенная, поэтому одна координата. Соответственно, эти парсеры получают вход из кластеров, построенных вот на этой упрощенной модели. И дальше про эти парсеры, что они из себя представляют. Колонка типа «что». Колонка типа «что» получает на вход вот эту букву текущей ситуации. У нее на входе, как бы, она получает кластер из полного вектора. Соответственно, у нее на входе, ну, некоторые вот там текущие ситуации в форме буквы. Она строит из них слова и ведет корреляцию этих слов. И вот эти вот слова, как бы последовательность букв, это, по сути, ну, некоторые траектории в изначальном пространстве. Одну минутку. 

S03 [00:42:32] : Секунду. Да, коллеги, пока Игорь ходит, я сделаю анонс, что... Игорь, пока ты ходил, я тут делаю анонс, что на следующей неделе у нас семинара не будет. А через неделю тоже семинара не будет, потому что будет проходить конференция нейроинформатика в Москве. И в понедельник планируется оффлайн заседание нашего сообщества на полях конференции нейроинформатика. Будет совместно с конференцией проводиться круглый стол по поводу того, что такое AGI. и какие шаги предстоят сделать, насколько мы к нему приблизились, и что можно сделать по этому поводу силами российского сообщества и общества. Ну и помимо этого, для тех, кто не попадет на конференцию нейроинформатика, можно будет договориться провести вечерком где-нибудь офлайн заседание московского отделения нашего сообщества. Вот уближу к делу, информация будет в группе. Игорь, пожалуйста. 

S04 [00:43:31] : Я нормально слышно? Да, да, хорошо. Вот эта колонка типа что? Они строят эти слова, которые фактически являются такими траекториями во внешнем пространстве. И, во-первых, ведут таблицу корреляций, то бишь, какое слово за каким следует, какие ситуации следуют за какими. Во-вторых, ведет таблицу ревордов, оценивая каждое слово, оценивая подкрепление. Тут есть два подхода. Есть подход к кулинингу, когда можно каждое просто каждое слово определять его вес. Ну а сейчас в марте конкретно реализуется другой подход, подкрепление, которое идет от слова к слову. Это сильно больше информации хранится, но это позволяет работать с немарковскими цепями. А игры Atari, к сожалению, похоже, что не марковские цепи, что сильно усложняет жизнь для нас, для людей, которые с ними работают. Но и сама жизнь, на самом деле, не марковские цепи. Для тех, кто пояснил на всякий случай, марковские цепи – это когда следующее состояние целиком определяется предыдущим. Если вам не нужно знать историю, что было раньше, вам нужно знать только текущее состояние, чтобы знать, какие у него могут быть следующие. Но вообще в жизни не так. Обычно там предыдущая траектория довольно сильно определяет, и в играх от Арии, к сожалению, тоже не марковские цифры. Ну, не суть. Не суть, просто реворды устроены здесь так, что это реворды от слова к слову, а не как бы оценка ценностей каждого слова. Причем, если будет вопрос на эту тему, я подробнее расскажу, лично не хочу на эту информацию. Дальше. Это колонки типа «Что?». Дальше, соответственно, колонки типа «Где?». Это односимбольные парсеры, они не строят слов. они оперируют одной буквой и только корреляции смотрят, то есть какие буквы из какой могут следовать. Единственное, что они используют в качестве… Каждый символ этого парсера, он состоит на самом деле из двух буковок. Одна буковка – это текущая его ситуация, а другая буковка – это предыдущая траектория в какой-то длинной. Другими словами, какое действие привело в эту ситуацию? В самом простом варианте это один предыдущий экшен. Мы в ситуацию А пришли с экшеном вправо. Но реальные игры Atari устроены так, что там вот эта траектория значима. Поэтому вот этот символ траектории, который здесь используется, это не одно движение, а несколько. Ну там мы настраиваем параметры, но в моменте у меня там шесть… Вот модель, которую я показываю, это шесть как бы предыдущих движений. А вообще, в одной статье я видел, что, похоже, девять движений влияют. То есть, чтобы понять, что будет на следующем движении, нужно вообще девять предыдущих знать. Возможно. Ну, у меня сейчас шесть пока, и оно вроде неплохо работает. И он тоже строит корреляции между этими символами. То есть, другими словами, вот эта колоночка знает, с какой вероятностью после, например, вот текущей ситуации при действии, например, 0, произойдет какая-то другая ситуация, или при действии вправо произойдет какая-то другая ситуация. И может вот в этих терминах предсказывать что-то. Интересный момент вот этого Marty 5.2 в том, что эти колонки с самого начала Модель не знает, где во входящем векторе вот эти самые… где эта модель себя. Ее надо идентифицировать. И для этого такие протоколонки, где создаются для всех активных компонентов… Реально модель устроена так, что она берет и все активные координаты, которые имеют активность выше, чем X, ну или точнее есть, допустим, самые активные, те, которые активно не менее чем, не знаю, одна десятая от самой активной. В общем, каким-то образом выбирается верхний топ N координат, и по ним строятся потенциальные колонки типа где. И они все начинают работать. И все смотрят эти корреляции, и все, что они делают, они пытаются оценить коэффициент PMI, так называемый. Коэффициент PMI — это Point Wise Mutual Information, Они пытаются оценить условную вероятность того, насколько знание предыдущей траектории позволяет лучше предсказать следующую ситуацию. По-простому это можно сказать так. Если, допустим, условно двигается ракеточка, у нее там вправо-влево она двигается, и, например, вот мы сделали Модель сделала движение влево, и ракетка поехала влево. И эти действия коррелированы. То, что ракетка поехала влево, коррелировано с тем, что было до этого действие влево. И вот этот конкретный парсер, который занимается координатой ракеточки, он видит, что знание движения влево позволяет сильно точнее предсказать следующую ситуацию. А для него ситуация – это положение только ракетки. То есть он не знает, что это положение ракетки, а это самые координаты, некое число, некий символ в его понимании. А, например, если мы смотрим на положение мячика, то оно никак не связано с движениями ракетки. И там корреляция не будет никакой. То есть другими словами, вот эта вот условная вероятность, она не будет выше. Кому интересно, посмотрите, как устроен коэффициент PMI. Реально в марте устроен так, что все колонки считают PMI, и если PMI больше единицы, больше какого-то порога, больше единицы, то он считается, что это колоночка, что это идентифицированная колоночка типа где. А если он там падает меньше, по-моему, 0.5, то как бы это, значит, нет. Реально, у реальной работающей колонки этот имя где-то в районе двух. То есть другими словами, переводя на русский, знание предыдущей траектории позволяет в два раза точнее предсказывать положение, чем это было бы случайно. И эти колонки работают в паре. Колонки типа что и колонки типа где. Как это происходит? Колонка типа что предсказывает следующую желательную ситуацию. Вот исходя из своей статистики, из корреляции, из ревордов, которые были, Эта колоночка говорит, вот мы в такой ситуации, я считаю, что лучшее продолжение здесь это вот такое, DEF. А с ней в паре работает колоночка где, которая смотрит на это предсказание и говорит, ага, окей, но вот чтобы в DEF перейти, значит, она смотрит, какие ее положения связаны с этими, и говорит, ну, с наибольшей вероятностью туда приведет действие 0. То есть здесь в чем, как бы... Интересные нюансы. Представьте себе, что я играю в теннис как человек. Я должен побежать направо или налево. Мячик летит направо. Я понимаю, что для того, чтобы его отбить, я должен оказаться там. Правильная ситуация отбития мяча состоит в том, что я нахожусь в том углу с ракеткой, которая его отбивает. И тогда мячик будет отбит. Но я не могу воздействовать на весь мир. Я могу воздействовать только на себя, только на свое положение. Поэтому что нужно сделать, чтобы прийти в ту ситуацию? Я должен свое положение поменять. Я должен, грубо говоря, оказаться там. и для этого мне нужно совершить какие-то действия, которые помогут мне оказаться там. Вот, собственно, колоночка типа «Где?», она говорит, что вот для этого конкретного кусочка агента нужно совершить вот такое, с ее точки зрения, такое действие, чтобы он оказался, чтобы мы сдвинулись в сторону нужной ситуации. Ну, а, соответственно, внизу стриатом, то есть базальные ганглии, проводят голосование и выбирают нужное движение. в данной ситуации смотрит большинство, движение, за которое проголосовало большинство экспертов. Когда я говорю, что эта модель выстраивается постепенно и эти колоночки появляются постепенно, это не то что видео, это последовательность кадров, которая была примерно в течение Сейчас я несколько кадров просто пощелкал. Здесь видно, как эта модель разворачивается. Она разворачивается с нуля. Сперва есть только входной вектор, это таламус. Он создал первые коннекторы. Вот какие-то коннекторы кластеризовались. Здесь для простоты всего пять кластеризаций у каждого коннектора. Создались протоколодочки, их довольно много. Коннектор первый создал. парсеры действий, потом какие-то из этих колоночек идентифицировались и стали колоночками действия, колоночками типа «где» и связались со своими колоночками типа «что», а остальные просто разрушились. Смарти изначально создает много этих протоколовок, но потом разрушает те, которые плохо работают, которые как бы не полезны. Ну, собственно, как вы знаете, мозги так примерно все и происходит. Разрушается куча разных связей. Еще раз возвращаясь к этой основной логике. Это многоагентная модель. Каждая колоночка – это отдельный агент, который видит свою часть мира в виде своего входа, который к ней приходит, строит по ней свою картину мира, смотрит на эту причинно-следственную связь, предсказывает наилучший исход, а ансамбль принимает решение. Ну, раз уж этот слайд есть, я про это сегодня говорить не буду, но, в принципе, Маркетинг – это иерархичная модель, там есть верхние слои, которые работают на большем временном горизонте и для которых каждое слово нижних является буквой верхнего. то есть если для нижнего это целое слово, то для верхнего это буква, а слово верхнего является несколькими словами нижнего соответственно. То есть верхние предсказывают ситуацию на одно слово, а для нижних это как бы последовательность действий, программа действий. Но я сегодня про это говорить подробно рассказывать не буду. Перехожу к результатам. Это уже простые понятные картинки, их сильно проще воспринимать. Брейкаут – это игра, где нужно просто отбивать ракеткой мячик, и каждое отбитие Если модель хоть как-то отбила, то это будет награждено ревортом, потому что любое отбитие приведет к тому, что блок наверху будет выбит, какой-то блок, неважно какой, но все равно модель получит реворт. Если она мячик теряет, то она получает минус очков. Breakout это самая простая, пожалуй, игра, но там она, правда, как выяснилось, там код сам с ошибками написан. Народ там в форумах это обсуждает. В общем, она далеко не всегда делает то, что нужно, и иногда, в общем, работает немножко криво. Ну вот вы видите здесь слева, это просто случайная модель, которая… блин, так, случайные движения. которые иногда отбивают, случайно двигаясь можно иногда и отбивать, а справа обученная модель. на 20 тысячах игр, но у меня не влезла вся игра с ней, потому что там я использовал бесплатный конвертер в GIF, движущийся, там было только тысяча кадров, но здесь тысяча кадров влезла. Ну вот здесь вы видите, что слева очки, которые модель выбила, Другими словами, 26-3 обозначает, что она 3 раза потеряла мячик, а 23 очка забила. Вот эта разница пропущенных мячей, 23-5-18, это будет разница пропущенных мячей. Это важный показатель, на который можно ориентироваться. Это, грубо говоря, сколько раз мы отбили, сколько раз мы пропустили. Это другая игра, Понг. То же самое. Слева случайные движения. Извините, по-моему, с разной скоростью немножко. На самом деле это все в замедленном виде, потому что реально Marty учится на сервере без картинки. Можно учиться с картинками, но картинка-то для человека нужна. Он учится там сильно быстрее. Слева... Слева случайные движения, а справа обученная модель на 3000 игр и видно, что она вполне себе играет прилично. Глазом видно, что она вполне себе научилась. Но если перейти к сухим цифрам, то почему эта модель до сих пор не опубликована на какой-то крутой конференции, вот на этой печальной табличке окражено. Здесь приведены результаты, в частности, разницы забитых и пропущенных мячей у разных моделей. Ну вот, допустим, человек. У Pong'а в среднем обычно выигрывает… А, ну нет, сейчас. При случайных действиях модель всегда проигрывает. И обычно где-то 21-1, например, проигрывает. Лучший результат, который в принципе можно достигнуть, это 21.0, когда модель выигрывает 21.0 у компьютера. Известный алгоритм SARS, который куленинговый алгоритм, который еще до нейросетевой, такой табличный Q-Learning, он в Pong'е проигрывает примерно минус 19. То есть он 2-3 очка забивает, но в среднем проигрывает. А значит модель DeepMind, Deep Q-Network, которая была представлена в 2013 году, она выигрывает в среднем 20-0. То есть средняя разница забитых мячей, да, мерится так, берется 5 тысяч игр, Модели участвуют, ну как они меряли, на этих 5000 игр, а потом как бы еще 500 игр берется и смоется среднее значение. И вот такое оно на 20. У Марти, как вы видите, оно минус 11. Ну, по крайней мере, это сейчас я сделаю еще пару уточнений в конце, но по состоянию на весну этого года был минус 11. С брейкаутом ситуация немножко лучше. Челка выбивает примерно 30 очков в рейкауте, но в среднем, не очень подготовленно. А DQN в среднем выбивает 168, то есть блоки на верхних уровнях больше стоят, чем блоки на нижних уровнях. Но лучшую игру Ду Куэн выбивает 225. Марки сейчас уже выбивает лучшую игру на 420, 390. Но среднее у него было 29. Сейчас я сделаю поправку, уже 60 на самом деле. Но все равно еще хуже, чем пока хуже, даже нет, последний я верю на 5108, но все равно еще хуже пока, чем у документов. Это на момент тезисов, когда я подавал на конференцию, тезисы были вот там 29, это был июль. июля текущего года. Другими словами, результаты марта сегодня сильно лучше, чем Но хуже, чем лучшая модель мира, которая играет в эту игру Атари. А сегодняшняя наука устроена как спорт высоких достижений. Какой бы вы концептуальный не были, но если вы не прыгаете Лучше, чем текущий чемпион. Ну и кому это надо? Никому. Поэтому мы пока не публикуемся на каких-то больших конференциях. Те отвечают, что ну вы еще маленькие, подрастите. Ну мы растем. График типичный выглядит обучение, с сыгранием, количество этих, разница запитых и пропущенных, она растет, доходит до какого-то предела. Сегодняшний Марти — это прототип, написанный на Яве. В отличие от всех нейронных сетей, больших, он работает на одном CPU, то есть это просто процесс, это компьютер с одним CPU, но правда многопоточный, потому что оно однопоточное и все-таки это уже прошлый век и очень долго все. То есть он работает 30 потоков. Модель, которую я сейчас показываю, она примерно 20 гигабайт памяти занимает. Явы устроены так, что там плавает немножко размер. И она примерно 2 миллисекунды шаг у этой конкретной модели. Когда иерархиум устроен, там немножко все. чуть увеличивается, но вот у этой модели примерно такие параметры. И вот что важно, я хочу сделать замечание, важно, я не знаю, Антон, для тебя точно она будет полезна, я думаю, что она для всех может быть полезна. Это мое неприятнейшее открытие этого года, вернее, я сперва это открыл года два назад, для себя матерился и думал, что я это пофиксил. Но потом оказалось, что не пофиксил, а оказалось, что все довольно плохо. И пофиксил в результате только сейчас, этой осенью. Значит, знаете, какая история? Этот гимназиум, которые эти игры «Атари», там есть разные версии этих игр «Атари». И там есть два таких странных параметра, которые сделаны, ну, я не знаю, ну, вернее так, я догадываюсь, с какими целями. Типа, чтобы там агент не переобучался, ну, в общем, для нейронных сетей. Значит, frameskip и repetition. Это как бы, это я сейчас просто техническое замечание для тех, кто работает в журнале, это будет полезно. Значит, repetition — это параметр, который определяет, значит, Модель отправляет в Environment действия, которые нужно сделать. Так вот Environment не всегда делает действия, которые модель ему отправила. А с вероятностью Repeat Action он может сделать предыдущие действия. Это приятнейший сюрприз. Я его для себя обнаружил два года назад, ужасно мотивился, потому что ты работаешь с моделью, а она делает на самом деле не то, что ты ей сказал, а он делает что-то свое. И это можно как-то изменить, задав этот параметр. Кому интересно, я потом напишу. Но есть еще один интересный параметр, называется FrameSkip. А FrameSkip говорит следующее. Вот если вы дали Environment действие и он его делает, то сколько кадров он его будет делать? Оказывается, что Environment делает по умолчанию не один кадр, как вы бы предполагали. Вот вы дали действие, он один кадр его сделал. А, например, по умолчанию вот в версии самого начала, вот этот вот 2.5 обозначает, что он от двух до пяти раз его повторяет. То есть вы, например, можете послать в Environment, модель посылает, допустим, действия такие, например, вправо-вправо, ничего не делать влево. А на самом деле выполняется пять раз вправо. И это просто… ну дальше у меня только мат начинается на тему вот этого. А документация написана так, что без пол-литра не разберешься. Я вот честно разобрался окончательно только этой осенью. И вот я выделил жирненьким, как называется environment для брейкаута конкретно. в котором фреймскит равен одному и репетекшн равен нулю. Вот только в этой версии брейкаута, играя в Atari, вы можете быть уверены, и то на самом деле происходит, когда логи смотришь внимательно, есть ситуации, когда он все равно повторяет действия, но по крайней мере это минимизируется. Потрясающе ухудшает обучение модели, когда этого не знаешь. А когда ты это знаешь, то ситуация меняется кардинально. В этом смысле у меня Марти стал на порядок лучше играть. И вот если на момент, когда я показывал, среднее количество мячей было 29, забитых, пропущенных, то сейчас, модель у меня играет, около 100 тысяч игр сыграло, там 108. То есть вы почувствуете разницу, что называется. Ну, вот так. Так что просто имейте в виду. Те, кто будут с этим работать, изучайте внимательно, как это сделать, как работать с этими параметрами. Ну и переходя к финалу, Несколько таких замечаний сделаю дополнительных. Большое отличие марки от нейросети состоит в том, что это модель, которая выстраивается с нуля. Обычный нейросет, для тех, кто знает, мы сразу инициализируем гигантское пространство параметров размером 7 миллиардов, не знаю, сколько там, миллионов. И дальше эти параметры гоняем, подкручиваем. Маркет, в отличие от этого, он начинает расти с нуля, постепенно вырастая в соответствии со сложностью среды. Чем более сложная среда, тем больше колонок, больше кластеров, следующие слои будут появляться и так далее. У Марти нет этапа обучения и выполнения. Это такой life learning модель, то есть она учится все время. У нее нет как бы отличательной сети. И вот у текущей модели Марти есть модель мира, а есть модель себя в этом мире, которую он используют для лучшего прогнозирования. Конечно, в названии этого доклада есть небольшой первый шажок в своем сознании. Конечно, Марк не обладает феноменологическим сознанием в том виде, в котором мы это понимаем. Хотя у модели, как и у нейросети, есть какой-то свой сенсорный опыт. То есть у нее какое-то восприятие, древнее восприятие, она получает какие-то данные. Есть какой-то, условно, когнитивный опыт, она над ними размышляет. Есть какой-то агентский опыт. То есть некоторые элементы феноменологического сознания с точки зрения определения, если просто словами, Играть то они есть, но, конечно, понятно, что там нет никакого физического сознания. Но вот интересная вещь, в отличие от других моделей, что Марти умеет выделить себя в этом входном потоке информации, выделить то, что его характеризует агента. В данной ситуации это просто одномерный агент, если хотите, это просто одна координата, но тем не менее. И дальше он это эффективно использует для На базе этого мы выдвинули теорему, что отделение себя от окружающего мира является необходимым, но недостаточным условием появления самосознания. То есть это маленький шажочек к появлению самосознания. Понятно, что там еще нет самосознания. Но отсюда есть интересное следствие, что если модель не может отделить себя от мира, то она не может обладать самосознанием. И в этом плане я абсолютно уверен, что все ЛЛМ, конечно же, нет никакого самосознания, но 100%. Я могу про это отдельно много порассказывать. Ну вот, собственно, все. На этом доклад окончен. Еще сделаю отдельный дисклеймер, что данная работа была выполнена авторами исключительно в свободное время и без поддержки каких-либо организаций. То есть это не в рамках какого-то организации, это именно свободная, свободополетная история. У меня еще есть в приложении еще более подробная работа по шагам, но я думаю, что пока нет. Приложение, просто обозначьте, что оно есть. Ну вот, собственно, на этом все. Спасибо за внимание. 

S03 [01:11:33] : Да, Игорь, спасибо. Коллеги, давайте начнем задавать вопросы. У меня сразу вопрос, у меня на самом деле кончилась бумажка, где я записывал все свои вопросы, поэтому один вопрос, пока помню последний незаписанный. Подскажи, пожалуйста, она все-таки осознает себя или она осознает ракетку, которая движется на экране? 

S04 [01:11:59] : Смотри, модель ничего не знает про экран, ничего не знает про ракетку. Она даже не знает, она не представляет себе, как там выглядит что-то. Это мы видим ракетку, мы ее воспринимаем. Мы видим, что есть ракетка, есть мячик. Это как мозг, который, есть такая аналогия, что мозг это нечто сидящее в темном черном ящике, пучок проводов туда входит, поэтому к пучку что-то приходит, а из него тоже пучок проводов выходит, и туда тоже что-то выходит. Это ровно такая же история. Он во входном потоке информации находят какие-то, ну, допустим, если бы мы, допустим, сказали так, что, например, каждая там, что все входные, все входные там Данные – это какой-то нейрончик. Каждый элемент входных данных – это какой-то нейрончик. Грубо говоря, он определил какие-то нейрончики, которые идентифицируют его, агента самого, каким-то образом. Он не знает, как. То есть он не видит пока эту картину, как ее видим мы, но он понимает, Он понимает, что когда проявились вот эти колонки положения, колонки типа «где», которые работают с колонками типа «что», он добивается целей намного эффективнее. Там нет пока никакого самоосознания. Там есть просто выделение себя. Вот оно есть. Он выделил себя из потока. 

S03 [01:13:57] : Себя, то есть вот собой. Что является собой? Собой является то, что мы видим на экране, как ракетка? Или собой является формула, которая связывает действия ракетки входным потоком? 

S04 [01:14:14] : В данном случае, я думаю, когда мы говорим «модель себя», это такая… Я когда рассказывал на семинарии у Константиновича Анохина, там прям была целая буря эмоций, что я такое говорю, как модель себя. В данной ситуации у нас модель – это одномерный объект, то есть он движется просто по одной прямой. И вся его модель себя, грубо говоря, это координата, где он находится. Это не то, чтобы ракетка, это скорее какая-то координата этой ракетки. Я даже не знаю, это координата чего там, середины или там краешка, наверное, краешка, чего-то. 

S03 [01:14:53] : Окей, ну то есть, если я правильно понял, что если бы была такая игра Atari, где нужно отбивать два мячика двумя ракетками, то модель себя, это было бы две координаты. 

S04 [01:15:04] : А вообще в среднем обычно модель себя — это не одна координата. Это странно от Арии устроено. Например, в Понге, где эта ракетка движется, там, по-моему, три части этого вектора, три каких-то координата становятся моделью себя. Моделью себя становится то, что лучше прогнозируется, чем остальное. Как там устроен этот Виктор, просто фиг его знает, но как-то устроен. 

S03 [01:15:36] : Окей, спасибо. Следующий блок вопросов от Виктора Казаринова и от меня. Первый вопрос, ты упомянул про количество слоев, соответственно вопрос, количество слоев, хардкодится оно? или, так сказать, определяется динамически, или это вот просто некоторый гиперпараметр, который ты задаешь, так сказать, на старте эксперимента, и, значит, вот, грубо говоря, с тремя слоями не обучается плохо, а если задаем 10 слоев, обучаются лучше. И то же самое количество вот этих вот последних движений, которые нужно запоминать, то ли 6, то ли 9, это что, значит, некоторый гиперпараметр, который надо подбирать, или тоже что-то динамическое. И вопрос от Виктора. 

S04 [01:16:21] : Слишком много вопросов. Про траекторию проще сказать. Траектория это задаваемый параметр сейчас пока, хотя он тоже может быть вычисляем точно так же как и просто колонка типа где. То есть можно смотреть на на какой длине траектории точность предсказания становится выше, чем какой-то порог, или неточность меньше, чем какой-то порог. Но сейчас в моменте она просто задается как параметр. Количество слоев сейчас тоже задается как параметр, но Скорее, ограничения на него сдаются как параметр. Просто слои растут по достижении какого-то следующего этапа. Сейчас модель устроена так, что она А давайте я пока, я, наверное, могу пока остановить этот шаринг. Пока, наверное, не нужен. Так, я могу вот так вот, наверное, делать. Да, про слои. Сейчас ситуация движется так, что как бы слой кластеризуется, когда У него есть входной поток информации, и там какое-то время информация очень новая для него. И он смотрит, и каждая колонка получает какое-то количество нового. Он смотрит просто, когда перестают, количество новой информации постепенно падает. То есть, грубо говоря, там становятся все меньше и меньше ситуаций, которых он еще не видел. И вот когда она падает до какого-то уровня, то этот слой начинает кластеризоваться. Причем он кластеризуется тоже неравномерно, кто из них дорос, модель такая синхронная. И если она кластеризовалась, то может быть следующий слой построен. А следующий слой, например, если он будет очень простым, очень примитивным, то он просто дальше кластеризоваться не будет, скорее всего. Но сейчас пока это ограничение задается руками. Как и в нейросетях, кстати, оно руками задается. 

S03 [01:18:50] : Понятно, спасибо. То есть количество слоев в модели растет динамически до тех пор, пока не перестанет растать? Теоретически, да. 

S04 [01:18:59] : Практически сейчас оно минимальное, потому что и так работает, и так показывает. А сколько минимальное? Я сейчас работаю на самом деле иерархией с двух слоев. Это сейчас достаточно. 

S03 [01:19:12] : Ты это задаешь просто? 

S04 [01:19:14] : Да, я ограничиваю искусственно. Важно здесь все выловить. 

S03 [01:19:21] : Понятно, спасибо. На вопрос Виктора про количество слоев уже ответили. Теперь еще один вопрос от Виктора. Ваша система при продвижении информации слева направо через слои, это вот видимо картинка, где вот эти вот жгутики были, выполняет логический, статистический, прямой вывод. Можно ли выполнять обратный вывод от цели к предпосылкам? 

S04 [01:19:44] : Там выполняются оба вывода. Модель устроена так. Сперва идет поток вверх. Он получил входной вектор. Вектор превратился в символы. Его отработали колонки одного слоя. Потом они превращаются в символы следующего слоя. Их отрабатывает следующий слой. Это в одну сторону. Потом идет цикл предсказания, который не на каждом шагу… Да, я забыл сказать, что в марте используется модель, как это называется, не помню по-умному, но есть какой-то термин. Ну, грубо говоря, что мы действия повторяем несколько раз. Не на каждом шагу предсказываем, а там. Можно их повторять несколько раз и это тоже работает. У DeepMind модель раз в три шага предсказывала. У меня Март сейчас тоже раз в три шага предсказывает, этого достаточно. То есть он как бы предсказал и три раза делает одно и то же действие. И, соответственно, вот такой цикл предсказаний идет, наоборот, сверху вниз. Верхние слои, говорят, дают предсказание свое, спускаются вниз, значит, нижние пытаются понять, как соответствовать вот этому верхнему предсказанию, и что они должны сделать, чтобы ему максимально соответствовать, тоже передают вниз, и так идет до конца. А вот это предиктивность, так сказать, предиктивное кодирование состоит в том, что на следующих шагах там проверяется, то есть у тебя снизу идет текущее состояние, а сверху предсказанное состояние. И если оно соответствует, то как бы продолжает это все выполняться. А если оно не соответствует, то текущий цикл предсказания заканчивается, и нужно будет делать новый. Вот такая логика. То есть там два потока, снизу вверх, сверху вниз. 

S03 [01:21:39] : Спасибо. Следующий вопрос от Ивана. Если все пространство возможных входов разбивается на ограниченное количество классов и каждый класс переводится в символ, то получается система не может воспринимать никакие промежуточные выходы, то есть состояние или А или Б, а нечто среднее вообще никак не может быть воспринято. Так ли это? 

S04 [01:22:00] : Нет, конечно, потому что теоретически это было бы правильно, если было бы только одно разбиение. но разбиений-то много, их там 50, и они друг с другом пересекаются. Одно разбиение, условно, это событие отнесло к классу А, а другое разбиение отнесло его к классу Б, а третье к классу С. И так как, грубо говоря, Каждая колонка оперирует сильно огрубленной моделью того, что происходит как бы в реальности, снаружи, но их ансамбль Получается, что пересечения работают другими словами. То есть, когда у вас пересечение кластеров, они позволяют намного более точно это сказать. То есть, каждая по отдельности, она действительно, для нее очень дискретные состояния. Но ансамбль, нет, для него все промежуточные тоже работают. Ну, просто как ансамбль. Я не знаю, насколько я... Пересечение кластеров — это... В одном случае кластер у двух соседних пересекается A и B, в другом случае A и C, в третьем случае A и D, и это разные ситуации. Хотя у левого это все еще A. Спасибо. 

S03 [01:23:29] : Дальше у нас есть пара коварных вопросов. Один коварный вопрос у Виктора Артюхова. Допустим, я наблюдаю падающее яблоко. Яблоко падает всегда вниз. Предсказывается отлично. Следовательно, я – это яблоко? 

S04 [01:23:49] : Нет. Потому что предсказание, потому что этот коэффициент PMI, пытается найти корреляцию не со временем, а с действиями, которые были сделаны. Если агент все время делает действия, если у него только одно действие, и оно только вниз, яблоко падает вниз, то да, он себя будет ассоциировать с яблоком. Но так как действий сильно больше, действия с точки зрения игры это там вверх-вниз, вправо-влево условно, то он смотрит корреляцию с действиями. А яблоко всегда падает вниз, вне зависимости от того, какое действие, поэтому нет, не будет 

S03 [01:24:33] : Спасибо. Следующий вопрос от нас с Владимиром Смолиным, общий, по поводу использования аллюзий к колонкам, к таламусу, к стреатуму. обязательно нужно к ним, с практической точки зрения, обязательно нужно к ним отсылаться, или на самом деле в этой архитектуре просто можно писать, что вот модуль, отвечающий там за что, за где, модуль, который ассоциирован с действиями, модуль, который ассоциируем с предсказаниями, то есть есть какая-то ценность в том, что мы делаем вот отсылки к голове, или это, так сказать, для маркетинга. 

S04 [01:25:14] : Слушай, ну, и да, и нет. С одной стороны, можно было бы назвать как угодно. А с другой стороны, забавный вопрос. Базовые идеи о том, как это работает, они именно взяты с обратного реинженерирования мозга. То есть смотрели на мозги и пытались понять, как это работает. И вот есть идея, что это работает так, давайте это сделаем. Поэтому названо как бы теми блоками в мозге, которые вроде как так работают, по моему мнению. Назвать можно было как угодно, естественно, это же программирование. 

S03 [01:25:59] : – Насколько я помню, эти названия, они еще с работ Сергея Шумского или ваших совместных? – Да. – Хорошо. Еще вопрос от Владимира. Как в процессе обучения происходит разделение колонок? На колонке где и что? 

S04 [01:26:19] : Смотрите, они создаются по построению, они разные. Они создаются разными путями. Колонки типа «Что» изначально создаются из… Грубо говоря, у нас на входе пучок проводов. Мы одни колонки делаем, используя информацию из всего пучка, а другие колонки делаем, только вот на этом проводе, а еще какие-то колонки делаем только на этом проводе. И вот те, которые из всего пучка по построению, это колонки типа «Что?». А те колонки, которые только на одном проводе, это колонки типа «Где?». И они в этом смысле… Да, и колонок типа «Где?» создаются заведомо больше, потому что Марк не знает, где там именно вот это… потенциальные, где. Он их создаёт заведомо много. 

S05 [01:27:15] : Вопрос, мы колонки определяем заранее, вот эти будут где, а эти будут что? И количество их тоже задаёт? 

S04 [01:27:24] : Да, параметры сейчас задаётся, да. Ну так слушайте, в мозге же тоже у нас есть вот некая программа дифференциации, когда эти нейронные связи создаются от определённой области. Другой вопрос, как они потом работают? 

S05 [01:27:36] : Это вопрос без подвоха, это просто для понимания. 

S04 [01:27:39] : Да-да, но они по построению как бы создаются по-разному, просто ну вот так. 

S03 [01:27:46] : Игорь, а поясни тогда, у меня когнитивный диссонанс, ты в какой-то момент сказал, что у нас не как нейронная сеть, потому что у нас динамически растет набор параметров. Но я так понимаю, из того, что ты сказал, это касается только последующего, второго слоя, потому что первым слоем, я так понимаю, у нас мы заранее создаем нужное число колонок, правильно? 

S04 [01:28:08] : Нет, смотри. Количество колонок сейчас задается просто, ну, как параметр входной. Там очень много, естественно, как у любой модели, всегда очень много параметров. Количество задается просто, ну, как бы там, руками. Грубо говоря, 25 колонок, ансамбль из 25 колонок работает менее точно, чем из 50. А 100 колонок занимает больше вычислительного ресурса, намного, чем 50. 50 – это некое оптимальное число. Но с точки зрения, когда я говорю, что модель растет с нуля, вот представьте себе реальный живой организм, который растет с нуля. Когда он условно растет с нуля, на самом деле там есть некоторая генетическая программа, которая его развивает, то есть она заложена. Процессы дифференцировки и построения постепенного ткани заранее определены, они известны. Организм будет разворачиваться вот так, а не как иначе. И в этом смысле логика развития марки предопределена, она кодом предопределена. Он всегда будет разворачиваться, всегда будет развернут в эти 50 колонок. Что у этих колонок будет на входе и как они друг с другом будут соединены – это элемент случайности. Каждый раз по-разному происходит. Так же как и в мозгу. Какие нейроны с какими будут соединены – фиг его знает. Важно, что в конечном итоге вся система работает. И поэтому правила заданы. Модель растет с нуля, в том смысле, что в ней нет никаких данных, никаких параметров. Потом она набрала данных, смотрит и, окей, появился первый слой. В зависимости от богатства этих данных, от количества этих векторов, можно делать больше или меньше количества кластеров, у вас будет больше или меньше количества слов. У вас, соответственно, появится больше или меньше детальности в этих колонках, ну и так далее. То есть там огромное количество своего вариативности. 

S03 [01:30:19] : Игорь, я все-таки тебя хочу домучить. Вот смотри, у меня возникло ощущение, когда ты сказал, что количество колонок или чего-то, что ты сказал, Растет не как у нейросетей зафиксировано, а растет в зависимости от сложности среды. Мне показалось, что это похоже на то, что нам рассказывал, по-моему, лет пять назад Вадим Филиппов, что у него реально нейроны создаются по мере необходимости. Ну то есть нужны новые нейроны? Они создаются. Не нужны? Система работает на тех, что есть. Так вот, все-таки у тебя мы руками создаем нужное количество в зависимости от ожидаемой сложности среды и вычислительных ресурсов и подбираем этот параметр или все-таки по ходу тела они сами плодятся до какого-то максимума? 

S04 [01:31:06] : Есть теоретическая вещь, о которой я понимаю, как она может быть реализована, и что это не является обвиняющим моделям, а есть некоторая практическая реализация. Сейчас в практической реализации задается просто жесткое количество колонок. не очень хочется сейчас заниматься тем, чтобы обдумывать, чтобы модель решала и искала на этот компьютер большинство ресурсов. Сейчас просто четкое количество, но в принципе подход устроен так, что это можно делать, то есть постепенно я многие вещи оттуда как параметры отпускаю, то есть они там перестают быть параметрами, они становятся Перестают быть гиперпараметрами и становятся параметрами? Нет, становятся характеристикой среды. Например, количество кластеров. Количество кластеров сначала было параметром. Потом я пришел к тому, что Сейчас вообще, нет, это еще не было опубликовано. Давайте так, я скажу пока, после публикации потом расскажу. Ну, короче, количество кластеров перестало быть параметром. Оно стало осмысленным образом выбираться, неким образом. И стало сильно лучше работать, потому что из головы я думал там будет условно 30 кластеров, а оказалось, что нет, нифига. Все не так. Но просто я не хочу говорить вещи, которые там будут потом увлекованы, чтобы не испортить публикацию. 

S03 [01:33:00] : Окей, спасибо. Вопрос еще от Виктора Казаринова. Правильно ли он понимает, что каждый уровень вашей модели ориентировочно соответствует уровню абстракции? Или это что-то другое? 

S04 [01:33:11] : Да, да, абсолютно правильно. Абстракции причем в двух как бы сразу типах. И во временном, и в ситуационном. То есть в данной ситуации... в данном типе environment, что ли, это они связаны. Потому что, грубо говоря, потому что там все время события развиваются. И поэтому как бы ситуация и время, они связаны вместе. Я не знаю, как это точно... Ну, другими словами, если бы там были моменты, где была бы пауза, и ничего бы не происходило, то это были бы разные вещи, а так это растущий уровень абстракции. 

S03 [01:34:02] : Хорошо, спасибо. Еще каверзный вопрос от Владимира Смолина, на который я по-моему знаю ответ. Будет ли считаться действием себя, если стоять и ждать, когда упадет яблоко или мячик прилетит в подставленную ракетку? 

S04 [01:34:17] : Такое, кстати, тоже бывает. Это, кстати, очень хорошая стратегия, она реализуется регулярно. То есть Марти просто смотрит, что при некоторых розыгрышах оптимальная стратегия просто стоит на месте. Он стоит на месте, и мячик в него прилетает. 

S03 [01:34:32] : Спасибо. Вопрос от Дениса Назипова. У живых организмов, как мне кажется, проблема с выделением себя решается гораздо проще, если сигналы внешние от среды есть и есть внутренние от организма. И задача выделения себя гораздо проще решается, она, похоже, довольно грубо, о чем свидетельствует известная иллюзия резиновой руки. Насколько оправдана такая сложная постановка? 

S04 [01:34:58] : Ну вот как раз все было бы хорошо, Денис, если бы вы не назвали эту резиновую руку. Нет, конечно, потому что с первым тезисом я почти бы согласился. Есть сигналы изнутри, безусловно, а есть сигналы снаружи. Но, например, изображение моей руки, вот я смотрю на свою руку, Изображение моей руки, как она находится в пространстве – это не сигнал изнутри, это сигнал снаружи. Это сигнал, который я получаю через зрительную кору, распознаётся этот образ, и я понимаю, какой мой палец где находится. И дальше вот это распознавание внешнего образа входит в тонкую интимную связь с внутренними ощущениями. И поэтому, когда мы смотрим на резиновую руку и видим как бы эту руку из вне, а сигнала-то внутри нету, но вот за счет этой связи, которые выстроены за многие годы, человек начинает как бы чувствовать то, что там по ней ударили молотком. Но вот смысл в том, что это то, что мы видим снаружи, себя, мы видим себя проявление себя видим во внешних сигналах. Вот это и есть то, что модель себя. А для Марти там нет разделения. Вы же понимаете, там нет разделения на внешний, внутренний. Там есть просто сигнал. Он приходит, и он не знает, кто из них кто. Когда это будет робот, там будет пучок проводов, который приходит от камеры. А пучок проводов, который исходит, приходит изнутри, тоже можно будет это сделать. Можно будет задать просто жестко, что вот такая-то, такая-то, такие-то части этого вектора, это вот модель, это условие, это я. То есть не я, это он, это робот. Тогда он с ним будет соответственно заперировать. 

S03 [01:37:05] : окей так еще я себя вопрос ставлю вот где-то в начальной части ты сказал что поначалу когда игра начинается значит марте наблюдают за происходящим вот а потом в какой-то момент уже переходит действия Вопрос, это естественный переход или это есть два режима, где ты закладываешь, что 20 партий наблюдаем за происходящим, а с 21 начинаем пытаться играть? Или этот переход естественный? 

S04 [01:37:37] : Вот как устроен этот переход. Он набирает какое-то количество входящих векторов. Ну, сейчас это конкретно… А, нет, даже нет, сейчас не параметры. Значит так, два этапа даже там начальных. Первый этап. Он набирает какое-то количество векторов, чтобы дальше посчитать потенциальных кандидатов на вот эту, условно, элементы модели себя. Дальше он создает этот слой коннекторов первый, и коннекторы начинают получать свои вектора. Все это время Marty отвечает, если технически посмотреть на это, он отвечает ОК. Текущая модель устроена так. Это ябловский демон на Явле, у которого открыты порты. Ты к нему подсоединяешься по сокету, он получает данные по сокету и отдает их по сокету. И там общение идет текстовое. Он отвечает просто окей. Окей в смысле, что он получил данные, но сказать ничего не может. Первый этап – это просто набор формирования этих коннекторов. На втором этапе коннекторы набирают свою статистику, и когда они набирают достаточно данных, когда новые данные падают до определенного уровня, и они могут кластеризоваться, они кластеризуются, над ними создаются первые колонки. И вот в момент, когда создаются первые колонки, как только создаются первые колонки, Марти перестает отвечать ОК, а колонки начинают, ну, там, как умеют, коряво какие-то свои давать предсказания, и эти предсказания, значит, уже Стреатом обрабатывает, и он начинает выдавать какие-то движения. Они, на самом деле, поначалу случайные, но это он их уже выдает. 

S03 [01:39:43] : То есть за хардкоженое правило, что как только возникло какое-то N пороговая колонок, после этого мы начинаем делать движение? 

S04 [01:39:53] : Да нет, ну как за хардкоженое, дружище, ну вот как у нас развивается Имбрион. Есть клеточка, потом две, потом там сколько-то три, четыре, потом они делятся, появляются какие-то органы. Пока у тебя не сформировались какие-то органы, Вот у этого эмбриона. Он вообще ничего не делает. Они просто формируются. Начиная с какого-то момента у этого эмбриона начинает биться сердце. Вот оно появилось, оно начинает биться. Ты можешь назвать это захардкожено, а можно назвать это некоторой эволюцией. 

S03 [01:40:26] : Это естественно происходит. 

S04 [01:40:27] : Это какой-то этап. Модель проходит какой-то этап развития. Она постепенно развивается. Она сформировала какие-то свои структуры, и начиная с какого-то момента сформировавшись структур, она начинает вести себя по-другому. 

S03 [01:40:43] : Как, собственно, любая нормальная... То есть нет вот этого разделения на фазы? Оно естественное? 

S04 [01:40:51] : Оно... Да. Ну да. Просто модель должна дорасти до момента, когда она начнет выдавать в мир какие-то движения. 

S03 [01:41:00] : Окей. Сразу еще вопрос. Скажи, пожалуйста, ты же работаешь с памятью, то есть ты работаешь не с растром, ты работаешь с памятью. 

S04 [01:41:10] : Текущая модель, текущая марка работает с образом памяти. В этом смысле наше понимание интеллекта в том, что интеллект оперирует уже с предобработанными сущностями, поэтому не нужно тратить время на распознавание, допустим, зрительных образов, это делает зрительная кора, а она уже в неокортекс дает уже предобработанную информацию. В смысле марки – это некоторый образ памяти, который как-то сформировался. 

S03 [01:41:59] : Нет, я имею ввиду в Джиме же можно… Да-да-да. Слушай, а правильно я понимаю, что с памятью работать проще, чем сразу? 

S04 [01:42:08] : Наоборот. Наоборот, по отзывам всех, кто работал, на памяти учатся хуже, чем на… Ну, то есть, получается, все-таки это не преобработанное? 

S03 [01:42:17] : То есть, получается, память это более сырая, да? 

S04 [01:42:19] : Нет, это просто другой тип информации. Смотрите, память более сжатая. Нейросети хуже учатся на памяти, это факт. Мне никак руки не доходят, честно. Я даже написал-то декодер. Там надо написать энкодер-декодер. Просто я написал этот энкодер-декодер, прогнал, но качество картинки... То есть, в принципе, все то же самое будет сделать, но я не доделал еще. Все то же самое будет сделать, если взять, как бы, набор картинок из Atari, прогнать через энкодер-декодер и потом взять вот эту энкодерную часть, которая, ну, маленькая. То есть, как бы, энкодер-декодер даже мы... как бы сперва сваливаем в узкое представление, а потом как бы из него разворачиваем и учим на этом. То есть, опять-таки, автоматически обучаемая система без всякого супервайзинга, а просто вот сама на своих картинках, но потом у нас появляются сжатые представления. И вот на этом сжатом представлении Марк точно так же будет учиться, по моим ощущениям. 

S03 [01:43:24] : То есть я правильно понял, что тебе на памяти учиться сложнее? 

S04 [01:43:29] : Нет, не мне. Всем остальным миру учиться сложнее. Исходя из тех форумов, которые я видел, из тех статей... То есть нейросетям проще учиться на РАСТе? Да, потому что, да это же понятно, нейросети ведь, у тебя же размер нейросети не ограничен, ты можешь там... что угодно миллионов сделать, у тебя каждая точечка, это условно говоря, то есть они просто учатся на картинке, и на картинке учиться. Сегодняшние нейросети ведь по сути взяты со зрительной коры, я это там почти через доклад рассказываю. Конечно, в зрительной коре проще со зрительной информацией, ну сто процентов, но там же не про интеллект они, они про другое. 

S03 [01:44:12] : Хорошо, спасибо. Следующий вопрос от Владимира Смолина. А вот еще от Виктора Казарина есть вопрос. Значит, куда у нас все-таки лошадь запрягают? 

S04 [01:44:23] : Ты раньше выпускал народ в эфир, а сейчас уже перестал выпускать. 

S03 [01:44:26] : А я не знаю. Нет, просто есть вопросы. Коллеги, если кто-то хочет выпуститься в эфир, поднимайте руку. 

S04 [01:44:34] : У нас, кстати, еще есть вот тут Сергей Шумский, до сих пор с нами, что меня очень радует, так что можно ему какой-нибудь вопрос адресовать. 

S03 [01:44:41] : Хорошо, сейчас дадим один вопрос от Виктора Казариного, а потом дадим возможность высказаться в эфире. Виктор спрашивает, все-таки, что у вас является в качестве базовых элементов в этих слоях? Это какой-то вид искусственных нейронов или что-то другое? 

S04 [01:44:59] : Да нет, это не нейрон. Смотрите, еще раз. Искусственные нейроны – это модель обычного нейрона. Вот у нас в мозге как бы есть естественный нейрон. Мы сделали модель искусственный нейрон, который просто сумматор. И считаем, что это модель нейрона. Это моделирование на уровне нейронов. нейрон-нейрон, и мы строим как бы свою математическую модель из миллиона и миллиарда вот таких вот отдельных нейрончиков. Мы говорим со своей стороны, что суть не в нейронах. Каждый нейрон сам по себе, он выполняет некоторую функцию, но он не является тем уровнем, на котором нужно моделировать. Моделировать надо уровнем выше. А колонка из тысячи нейронов является функциональным элементом. И мы моделируем этот функциональный элемент. И мы его моделируем не нейроном. Знаете, как в физике, когда ядро моделирует жидкая капля. вы моделируете ядро атома как жидкая капля, исследуется сила поверхностного натяжения и прочее. И при всей абсурдности этой ситуации все знают, что ядро – это не жидкая капля, но тем не менее эта модель дает описание некоторых… у нее есть своя область применимости. Так же и здесь. Мы моделируем вот этот модуль, вот этот набор нейронов, забыли, что там внутри нейроны, вот он как-то устроен. Этот модуль это просто парсер символ, у которого на входе символы и на выходе символы, и он работает просто как бы как парсер. Символ на входе, символ на выходе и все. Как-то он там устроен внутри, неважно. Его можно сделать нейронами, но у нас не нейроны. 

S03 [01:47:01] : Еще есть вопрос от Ивана по существу. Предсказание происходит за счет таблицы вероятности переходов? Да. Спасибо. Сергей, добрый вечер. У вас есть что сказать? 

S04 [01:47:16] : Сергей Александрович, буду счастлив тебя увидеть. 

S00 [01:47:21] : Привет всем. Главное, что я должен сказать, что все результаты принадлежат Игорю. Моя фамилия там совершенно незаслуженная. Мне очень понравилось, как он всё это рассказал, мне просто очень приятно. Очень приятно, что как раз в последний месяц возник прогресс, и то, что там некие затыки были связаны просто с техническим непониманием системы. И как только... Игорь с этим разобрался, там, конечно, произошел просто взрыв в предсказательной способности. Мне очень нравится вот эта, значит, сама идея себя как наиболее предсказуемой части внешнего мира, потому что вот то, что Игорь сказал, что мозг — это вот некая сущность, куда ведутся провода и откуда ведут провода да но это очень близко к действительности и мы то как возникает а тем не менее модель себя там у каждого из нас есть этот должность да вот и вот это вот принцип то что я это то что я могу предсказывать практически наверняка да Это фактически, по-моему, очень правильная идея, и как-то она недооцененная, по-моему. И то, что введение вот этой вот сущности, она реально повысила результативность игры Марти, показывает, что действительно модель себя. Это есть, она не случайно возникла, она возникает в эволюции, но просто как некая необходимость. Здесь мы сыграем абсолютно единомышленники. 

S03 [01:49:26] : Спасибо. Так, вот еще вопрос, рука у Егора Чурилова. Егор, пожалуйста. 

S02 [01:49:40] : Да, здравствуйте. Мне вот эта тема, конечно, с моделью себя чрезвычайно интересна. Я полгода назад делал доклад по когнитивной архитектуре и тому подобному. И там концепция «Я» вводилась уже эмержентно, то есть не как самостоятельная, а как технологическая, как что-то наблюдаемое. вот там я определял как наиболее там вероятно предсказуемая лучше всего и так далее как наиболее лучше управляемая часть мира который ты или там выполняется некоторая норма, близость, управляемость. 

S04 [01:50:28] : Да-да, это примерно о том же. 

S02 [01:50:32] : Здесь, наверное, очень близко. Но, что я заметил, вот в чем, наверное, может быть, потеря, вот сейчас я так заметил, может быть, не вполне понял, но вот на этом примере с резиновой рукой и с тем, что мы видим, вот эти структуры Я, они как бы имеют несколько эшелонов рефлексии и присутствуют на разных эшелонах рефлексии и с разным, скажем так, динамикой возникновения. То есть, на мой взгляд, когда мы говорим про наблюдение уже некоторого объекта глазами, сенсорами, это как раз-таки новый эшелон рефлексии в ряду, и их стыковка с вот этими нейрональными реактивными контурами – это отдельная задача. которая вот так просто, скажем так, не решается, на мой взгляд. Здесь нужна какая-то уже другая архитектура, другой взгляд, который бы их не то чтобы разводил, да. Вот эти различные схемы, то что у Грациана есть схемы внимания, сознание как схема внимания и так далее, вот этот подход. Это уже метапараметрические такие кластера, которые над реактивными контурами, может быть, в нескольких этажах существуют. И вот эти области Я, у них, так сказать, вертикальная связанность, они друг друга управляют. 

S04 [01:52:02] : Игорь, я вас понял, и вопрос понял, и комментарий. Смотрите, мы физики, мы, Сергей Александрович, физики, Мы все предельно упрощаем, у нас такая работа. Чтобы понять сложный мир, мы должны выбросить из нашей модели максимум, чтобы оставить там только самые важные части, но глядя на них, что-то понять. То, что вы говорите, я думаю, правильно. Но Марти – сильно более примитивная модель. Это, как бы, прям, можно сказать, первый шажок на этом пути. Он показывает, как вот можно сделать этот ансамбль, как в этом ансамбле могут быть какие-то части, которые ответственны за себя, и сейчас это пока просто одномерный агент, который движется в игре, очень простой environment, условно 128 чисел, и никакого внутреннего мира богатого и так далее. Но когда я, например, смотрю на нейрофизиологов, которые пытаются понять, как вот эти вот области что, взаимодействуют с областями где, и строят такие все сложные контуры, вот это как раз всего усложняет. А мы сделали вот эту маленькую простую модель, которая очень примитивная, но показывает, как это может работать. А дальше вы правы, что по-хорошему надо все это сильно увеличивать, усложнять, там появятся какие-то другие слои, потому что модель себя, она, конечно, сильно сложнее, чем просто какая-то маленькая одномерная козявочка. Но преимущество этой модели в том, что она очень простая, довольно примитивная и позволяет понять что-то, пусть даже в очень примитивной форме, но тем не менее. 

S03 [01:53:52] : Спасибо. Так, вот есть рука у Дениса Назипова, но прежде есть еще вопросы от Владимира Смолина. 

S04 [01:54:00] : У меня только осталось, честно, минут 10 еще. 

S03 [01:54:03] : Значит, за 10 минут мы закругляемся. 

S04 [01:54:06] : Владимир Смолин там куча была. 

S03 [01:54:08] : Давайте, значит, три вопроса тогда. 

S05 [01:54:13] : Владимир, пожалуйста, не перебивайте. 

S03 [01:54:19] : Владимир, можно я задам ваш вопрос, который я начал задавать? Есть ли в марте обучение по разности между прогнозом и реализованным в реальности событием? 

S04 [01:54:30] : И да, и нет. Смотреть, что понимать по разности. В нейросетях разность подразумевается как тут один уровень, тут другой. Соответственно, обучение на разности обозначает, что мы что-то увеличиваем, что-то уменьшаем, чтобы они ближе сравнялись. У Марти есть прогноз сверху, есть прогноз снизу. Они либо совпадают, либо не совпадают. Вернее, этот прогноз отвечает прогнозу сверху или нет. Вернее, результат отвечает прогнозу сверху или нет. Если они не совпадают, то просто прогноз считается несостоявшимся, и происходят новые перепогнозирования. а предикт идет уровнем ниже. В этом смысле как бы разницы в том смысле, чтобы как-то они друг к другу ближе совмещали, как бы каким-то образом притягивались, что-то подкручивалось, вот этого нету. И я не знаю, как это могло бы, честно, в реальном мозге быть реализовано. Скорее, задача нижнего слоя максимально воплощать прогноз верхних так, чтобы результат был максимально близок к тому результату, который они ожидают. В этом смысле там есть обучение. Просто слово «разница» здесь, как вы понимаете, что вы вкладываете в это слово «разница»? 

S03 [01:56:10] : Спасибо. Сейчас, Денис, пожалуйста, ваш вопрос, потом Владимир еще один вопрос. Денис, пожалуйста. 

S01 [01:56:16] : Да, вот у меня, я просто вот то, что написал, во-первых, я хочу пояснить вот это вот про резиновую руку, то что на самом деле человек же резиновую руку когда веет, у него столько иллюзий возникает, даже несмотря на то, что он понимает, чисто формально, что вот это не его рука, но из-за того, что синхронизация происходит на уровне чувств, у него все-таки есть на более базовом уровне то, что он не может даже отменить своим умом и то, что, соответственно, и вот это вот, то, что мы опираемся на то, что вот это вот знание о том, что я есть, о том, что я конкретно являюсь, оно опирается именно на сенсорную привычку. То есть у нас нет такой задачи, которую вы ставите перед мартией. И вообще у живых организаций нет такой задачи. И как бы все остальные дальнейшие действия, ума, соответственно, они уже опираются на это. А у вас получается решается такая сложная задача уже как будто по другим способам, не как у живых организмов сделать. Мне кажется, вот это может быть препятствием. И то, что, например, мы же можем управлять, когда несколько агентов, Я хотел спросить, у вас же не сможете решить задачу, если там будет стратегия, где много юнитов? 

S04 [01:57:27] : Она не решит ее? Как раз идеально. На последнее проще ответить. Конечно, идеально она будет решаться, просто у вас, грубо говоря, к каждому юниту соответствуют какие-то отдельные колонки. Маркет – это же многоагентная модель. Считайте, что у вас к каждому людям соответствует не одна колонка, а 50 этих колонок. И на рой агентов строится колонок, если в рое 10 штук, не 50, а 500. Каждый из них, каждая 50 относится к каждому из них. То есть просто весь рой управляется вместе. Но это отдельная история, я этого еще не делал, я просто не знаю, как это сделать. Руки не дошли. А возвращаясь к вашему первому вопросу, когда вы говорите слово «мы», вы сразу, а потом перескакиваете на «живые существа». Это же очень разные вещи. Когда мы говорим «мы», подразумевая себя как человека, то нам, конечно, сложно себя ассоциировать с таким одномерным, простым объектом. Поэтому не нужно себя переносить сразу на их слова. 

S01 [01:58:33] : Я думаю, у Амёба в любом случае есть какие-то внутренние и внешние разницы, что у него нет такой проблемы выделить себя. 

S04 [01:58:40] : Ну, подождите. У Амёба, наверное, нет такой проблемы. Я думаю, что и модели себя-то у него нет никакой. В моем представлении, модель себя, в принципе, возникает только Какое-то обучение и какое-то протосознание и, соответственно, какая-то модель себя может возникать даже не просто у позвоночных, а у таких уже довольно развитых существ, у которых все-таки есть нервная система, и она уже не очень примитивная, выходит за рефлексы. Но, тем не менее, просто не нужно сразу переносить это на человека. Конечно, какую бы модель мы ни сделали, если ее сопоставляешь с человеком, все будет казаться смешным, неработающим и как бы там, это не то. Но нужно понимать, что надо поискать какие-то организмы, надо подумать, какие могут быть организмы, которые очень примитивно устроены, но тем не менее у них такие задачи могут возникать. А потом, знаете, тезис по поводу того, что какие-то сигналы изнутри, а какие-то снаружи. На самом деле, я думаю, что это довольно нетривиальный тезис. Если у вас этот мозг сидит, как некоторая система нейронов в черном ящике, то пойди разбери, какие сигналы снаружи, а какие внутри. 

S01 [02:00:23] : Это привилегировано выделяется. 

S04 [02:00:26] : Как? Что такое привилегировано выделяется? Как? Я лягушек, конечно, не препарировал лично. Но все мое познание анатомии говорит о том, что там нет никакой специальной преференции. Нейроны все одинаковые, сигнал к ним приходит условно. То есть это выделение идет на некотором абстрактном, на некотором уровне. Это не… Хардкодно, конечно, они какие-то есть, но, в общем, Это отдельная интересная история. 

S03 [02:00:59] : Спасибо. Коллеги, у нас буквально два вопроса осталось. Владимир, от вас короткий еще вопрос будет? 

S04 [02:01:06] : Давай слово дадим Владимиру Сергеевичу. 

S03 [02:01:10] : И у меня еще один будет вопрос. Я не знаю. 

S05 [02:01:12] : Давайте я буду выступать в декабре, а то сейчас надолго будет. 

S03 [02:01:18] : Хорошо. Тогда, Игорь, у меня вопрос чисто практический по поводу спорта больших достижений. Смотри, насколько я помню, в каком-то то ли докладе, то ли статье из Сергея, который ушел, к сожалению, там была история про то, что они показали какие-то результаты значимые, по-моему, Это про тексты было, но используя совсем не те ресурсы, на которых эти результаты приняты получать. Соответственно, вот вопрос, если, грубо говоря, ты начинаешь дышать в спину ДКН, который тренировался на кластере ЖПУ, и это получается на обычном ноутбуке, то это не является основанием для статьи на А*? 

S04 [02:02:01] : Является, но не для звездочки. То есть не будет? Но со звездочкой на это все наплевать. Когда ты говоришь, что бегаешь всего на 2 секунды медленнее чем чемпион мира, но на меня не работает целая конюшня стоимостью 10 млн долларов, все говорят, что надо начхать. Подумаешь, найди конюшню 10 млн долларов, это же твоя проблема. Это конкретно на со звездочкой не тянет. А вот когда она достигнет соты, а это похоже уже недолго осталось, как мне кажется, то вот тогда то, что разница в ресурсах просто неспоставима. Марки сейчас работает на одном ноутбуке, если уж строго говорить, сервер выделенный, потому что мне так удобнее. И это процессор типа AMD довольно продвинутый, 32-ядерный. Хотя и помощнее тоже есть. Тогда это будет аргументом, а сейчас пока, к сожалению, это не аргумент. Пока не для А*. 

S03 [02:03:15] : Но мы же про А*. Да, я про это спрашиваю. Хорошо, коллеги, всем спасибо, Игорь, спасибо огромное. Спасибо за доклад, было очень интересно и, главное, более содержательно, чем за 15 минут на сознании. Извиняюсь, что не все вопросы и все комментарии были озвучены, но не последний раз встречаемся и до новых встреч. Игорь, спасибо большое, удачи. 

S04 [02:03:41] : Антон, спасибо. Всем пока, до свидания. 

S05 [02:05:35] : Пока. Удачи! 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html


