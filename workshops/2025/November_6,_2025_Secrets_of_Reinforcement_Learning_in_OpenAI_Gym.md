# 06 ноября 2025 - Секретные тайны обучения с подкреплением в OpenAI Gym, Игорь Пивоваров - семинар AGI
[![Watch the video](https://img.youtube.com/vi/QpBSIsMP1RY/hqdefault.jpg)](https://youtu.be/QpBSIsMP1RY)
- [видео в ВК](https://vkvideo.ru/video-210968399_456239235)
- [видео в RUTUBE](https://rutube.ru/video/51d24bdfb671305659978b688970d7da/)
- [видео в Telegram](https://t.me/agitopics/53344/56853)
- [расшифровка](https://github.com/agirussia/agirussia.github.io/blob/main/workshops/2025/November_6%2C_2025_Secrets_of_Reinforcement_Learning_in_OpenAI_Gym.md)
- https://t.me/agitopics/55366/55367

## Обзор доклада по практическому использованию OpenAI Gym для тестирования AGI

На семинаре обсуждались практические аспекты демонстрации и верификации моделей искусственного интеллекта (AGI) с использованием общепринятых тестов, в частности, фреймворка OpenAI Gym (теперь известного как Gymnasium). Основным докладчиком выступил Игорь Пивоваров, поделившийся своим опытом работы с этой платформой.

### Основные тезисы доклада:

*   **Цель использования OpenAI Gym:** Необходимость верификации AGI на публичных и стандартных тестах, чтобы результаты были сопоставимы с достижениями мирового сообщества, а не на самостоятельно разработанных задачах.

*   **Gymnasium (ранее OpenAI Gym):** Это популярный фреймворк, предоставляющий набор разнообразных сред для обучения с подкреплением. Он был создан в 2013 году, поддерживался OpenAI, а сейчас развивается фондом Farama Foundation.

*   **Классические задачи:** Докладчик начал с примеров классических задач, таких как "Mountain Car" (машинка, которая должна раскачаться, чтобы заехать на гору) и "CartPole" (удержание стержня в вертикальном положении). Эти задачи иллюстрируют базовые проблемы обучения с подкреплением, в частности, проблему "редких вознаграждений" (sparse rewards), когда агент получает подкрепление не на каждом шаге, а только по достижении цели.

*   **Проблема редких вознаграждений и внутреннее подкрепление:** Для решения проблемы редких вознаграждений, как в игре Pong, где очки начисляются только за забитый или пропущенный мяч, была предложена методика. Агент сам себе дает небольшое отрицательное внутреннее вознаграждение на каждом шаге (например, -0.1). Это мотивирует его выбирать действия, которые продолжают игру (с маленьким минусом), а не те, которые ведут к проигрышу (с большим минусом).

*   **Дилемма "Исследование против Использования" (Exploration vs. Exploitation):** Для решения сложных задач, таких как "Mountain Car", агент не должен зацикливаться на уже известных, но неэффективных действиях ("эксплуатация"). Ему необходимо постоянно исследовать новые состояния и траектории, чтобы найти оптимальное решение, даже если это требует временных отклонений от цели.

*   **Игры Atari как бенчмарк:** Основное внимание в докладе было уделено играм Atari, таким как Breakout и Pong, которые являются сложным и признанным бенчмарком для тестирования AGI.

*   **Технические аспекты работы с Gym:**
    *   **Взаимодействие с окружением:** Весь процесс строится на цикле: агент получает из среды наблюдение (`observation`), совершает действие (`action`), а в ответ получает новое наблюдение, вознаграждение (`reward`) и информацию об окончании игры.
    *   **Важность версий игр:** Подчеркивается критическая важность выбора правильной версии игры (например, `BreakoutNoFrameskip-v4`). Разные версии могут иметь параметры (`frame_skip`, `repeat_action_probability`), которые вносят случайность в действия агента и усложняют обучение.
    *   **Типы наблюдений:** Агент может получать информацию в виде снимка памяти (RAM) — вектора из 128 байт, или в виде картинки с экрана. Работа с RAM вычислительно менее затратна.
    *   **Отладка и визуализация:** Для отладки на удаленных серверах, где нет графического интерфейса, можно сохранять кадры игры в виде изображений, что позволяет анализировать поведение модели без значительного замедления процесса.

*   **Особенности игрового процесса:**
    *   В некоторых играх, например Breakout, для запуска нового мяча после проигрыша необходимо совершить определенное действие ("fire"), которое иногда приходится "хардкодить", то есть прописывать в коде принудительно.
    *   Пространство действий в играх может быть "зашумлено" дублирующимися или бессмысленными командами, что усложняет обучение.

*   **Дискуссия о подкреплениях:** В ходе обсуждения был поднят вопрос об эффективности отрицательных подкреплений. Антон отметил, что в его экспериментах отказ от отрицательных подкреплений (использование "только пряника") часто повышал обучаемость. Игорь, ссылаясь на классические работы по обучению с подкреплением, возразил, что небольшое отрицательное подкрепление за каждый шаг (цена "жизни" или "энергии") помогает модели избегать бессмысленных циклов и быстрее находить оптимальные решения в задачах с редкими вознаграждениями.

## Расшифровка доклада:

S01 [00:00:02] : Коллеги, добрый вечер. Сегодня у нас немножко необычный семинар. Мы хотим поговорить по практическим вопросам того, как мы будем демонстрировать движение KGI. По мнению Большого Вячеслава Некоторое число участников нашего сообщества мало говорит о правильном AGI, нужно еще его верифицировать, и некоторые части этой части также считают, что верифицировать его нужно не на тех тестах, которые сам придумал, сам из пальца вытесал, и кроме тебя никто их не проходит, а нужно брать публичные тесты, на которых свое мастерство оттачивают и демонстрируют все представители мирового сообщества. Вот. И один из наиболее известных фреймворков для обучения с подкреплением является фреймворк OpenAI Gym, вот, где есть много сред и есть такая точка зрения, что веганность твоего решения заключается в том, насколько хорошо твое решение проходит и решает задачки во всех этих средах. Ну и у нас, поскольку пионер в нашем сообществе и основной чемпион в данном движении – это Игорь Пивоваров с марта. Ему он сам любезно предложил поделиться своим опытом, чтобы можно было узнать, чего он достиг в больших подробностях, задать какие-то вопросы и может быть кто-то еще, начиная с меня, тоже поделится своим большим опытом. Можно будет дальше провести практическую дискуссию того, как лучше и правильнее пользоваться OpenAI Gym, чтобы это было сопоставимо с чужими результатами. Игорь, пожалуйста, давай, начинай. 

S02 [00:01:46] : Да, Антон, спасибо. Коллеги, всем добрый вечер. Да, у нас сегодня такое, что называется, практическое занятие, редкое в наших семинарах. Обычно мы рассказываем про какие-то свои результаты или свои идеи. А сегодня скорее такой семинар для тех, кому интересно понять, как тестировать на существующих этих моделях и в частности этот бенчмарк, который называется Open AI Gym. Давайте я покажу для начала пару слайдов из своего последнего доклада, который был, он был про MARTI. Я, собственно, там рассказывал про то, как я тестирую MARTI и что тесты проходят на этой библиотеке OpenEdge. Эта библиотека Сейчас я еще по-другому ее покажу. Эта библиотека была сделана довольно давно, еще в 2013 году. И была статья, которая про это выпущена. Давайте я сейчас ее вот так покажу. Вот такая библиотека, называется она Gymnasium. Изначально это был проект какого-то университета в 2013 году, потом его взял на поддержку OpenAI, известной компанией, которая делает GPT. Мы его все знаем в основном как OpenAI Gym, они его сократили до OpenAI Gym. А потом, сейчас это перешло на поддержку фонд Форама Фоундейшн, поэтому вот с текущей адресами вы видите такое, gymnasiumforama.org. Я думаю, что здесь видны ссылки, мне кажется, да? Значит, эта токуная джем из себя представляет следующее. Тут довольно много разных Когда я сделал первый раз модель Марти, в 2018 году был написан первый код летом и мы его там в одном швейцарском университете показывали. Ну и я для него написал свой маленький тестик, как вот Антон говорит, ну не то, что высосанный из пальца, ну в общем, как мне казалось, ну как бы, который показывает, конечно, очевидно, что оно работает. Но этот профессор посмотрел на это и сказал, что мне показывать, я вообще не понимаю, что этот тест из себя представляет. Вы покажите мне на стандартном тесте из OpenAI Gym на одном из стандартных классических примеров. И в частности тогда речь шла в первую очередь вот про что. так называемых классических задач для реинфорсмент лининга, которые являются Смысл в том, что если вы хотите показать, что ваша модель чему-то учится, то желательно это показать на готовые задачи. Вот здесь есть несколько таких готовых задач, из которых я в свое время решил вот такую задачку любопытную. Она называется Mountain Car. Мы сейчас еще дойдем до Atari, я начну с более классических примеров. Mountain Car – забавная задача. Здесь тележка должна доехать до флажка. за 200 шагов. Каждый шаг дается минус 0.1 подкрепление и соответственно на 200 шагах игра заканчивается. Фишка в том, что даже на полной скорости, если она включит мотор на полной скорости, она не может доехать до этого флажка напрямую. И чтобы ей туда доехать, ей надо раскачаться. И ваш агент должен догадаться, что для того, чтобы достигнуть цели, нужно сперва пойти в другую сторону. Нужно сперва поехать налево, потом направо, потом опять налево, потом направо. И вот там, по-моему, два или три таких надо сделать раскачивания. И она в результате тогда действительно достигает вот этого флажка. 

S01 [00:06:43] : Игорь, а можно перебивать? Да, конечно. Скажи, пожалуйста, очень хорошо, а вот вопрос, ты подкрепление получаешь только когда флажка достигаешь? 

S02 [00:06:52] : Нет, в этом сетапе ты подкрепление получаешь на каждом шагу, минус 0,1. Это отрицательное? Отрицательное. А положительное когда получаешь? А положительное только в конце, только когда флешка достигаешь. Это довольно непростая задача. То есть, грубо говоря, много проб и ошибок делается, естественно, чтобы это достигнуть. Но, в общем, это одна из таких классических задач IRL. Мне просто про нее надо сказать, если мы говорим про OpenAI Gym, чтобы не говорить только про игру Atari. Другая классическая задача, вторая классическая, их две, две самых известных, это так называемый карт-пол, стержень. Тут задача агента состоит в том, что вы управляете тележечкой, которая на тросе ездит и должна поддерживать в вертикальном положении стержень, а он все время падает. ну то есть нужно двигать тележку так, чтобы постоянно этот стержень был вертикально. Ну и в идеале агент должен сделать так, чтобы он просто стоял там не падал, не шевелился. Вот эти две задачки считаются довольно классическими для reinforcement learning. С них дальше все начиналось. Помимо них, то что я использую в этом меню, здесь очень много разных этих сред, инвайверментов. Я использую игры Atari. 

S01 [00:08:33] : Игорь, прежде чем к Atari идем, вот смотри, то есть ты задачу с Сережкой решил, правильно? Да. Слушай, а каким образом она решается, если там не дается подкрепление до сих пор, пока ты не решишь конечную задачу? То есть там как бы за счет чего? То есть ты должен, получается, каким-то образом понимать физику? То есть, как ты можешь целенаправленно достичь вот этой вот конечной цели, если ты не знаешь физику? 

S02 [00:09:01] : Нет, там физику знать не нужно. Значит, давай мы так сделаем. Я на этот вопрос, это хороший вопрос, я на него отвечу чуть позже, когда мы ПОНГ будем обсуждать. Окей, окей, я тогда конечу. Потому что эта мысль очень правильная, я до нее долго доходил. И в этом смысле имеет смысл поделиться этим. Но на Pong это более наглядно будет. Дальше мы переходим до Atari New Page. Игры Atari. Игры Atari это подраздел OpenAI GYM. Джим это в принципе, да, гимназиум это типа фитнес, гимнастика, это в целом большой набор всяких сред для вашего агента. И прелесть этого всего джима в том, что там плюс-минус единый интерфейс. Ну вот сейчас мы до интерфейса, до этого и дойдем. Атарик конкретно здесь по моим подсчетам около 100 игр, там 106. Совершенно разные игрушки и среди них те, которые мы с Антоном используем это Breakout в первую очередь и Pong. В принципе не суть важно какие игры использовать, но вот мы посмотрим сейчас на те, которые здесь Я, собственно, использую игрушку Breakout. Возвращаясь к слайдам от Jim, вся эта механика OpenAI Jim заточена на обучение с подкреплением, про которое мы с Антоном начали говорить. Но надо еще раз правильно сказать, что агент получает из среды информацию о том, как устроена среда, информацию о том, как его действие повлияло на среду и реворт. либо положительный, либо отрицательный. То есть всегда ему говорит, что он что-то сделал хорошо, либо что-то сделал плохо. В случае тележки, значит, реворд устроен вот так. Минус 0.1 на каждый шаг и единичка, если он достигает флажка. А в случае игр отари, например, это просто очки за игру. Ты проиграл, минус очко. У тебя минус очко реворда. Плюс очко, значит, плюс очко реворда. Особенность этих игр довольно сложная, состоит в том, что у многих этот реворд агент получает часто не на каждом шагу, о чем Антон говорил, а только по достижении некоторых событий. реворт агент получает только в моменты, когда он либо пропустил мяч, либо выбил кубик. И в особенности сложно это именно для Понга выглядит, потому что в случае Понга Это игра с обученным агентом, вы сейчас увидите, как он играет. Проблема в том, что все это время он никакого подкрепления не получает. Подкрепление равно нулю. Хотя он делает правильные действия, отбивает мячик, казалось бы, да. И только здесь, когда он мяч пропустил, он получил минус один. А если бы он мяч забил, если бы его противник пропустил, то он бы получил плюс один. В этом месте можно сказать, как с этим оперировать. Такая история со спортсмеводзом – это классическая проблема. Смотрите, как со Спарс Ревортом надо работать. Когда среда вам дает подкрепление только на редкие события, то самая действенная методика, которую я знаю, она такая – модель внутри себя дает сама себе внутренний реворд, скажем, минус 0.1 на каждый шаг. Считайте, как это энергийное движение. Каждое движение условно минус 0.1. ну какой это, минус 0, 0, 1, минус 1, это неважно, но оно должно быть прилично меньше, чем реворд, который вы получаете в конце, если все хорошо. И вот здесь смотрите, какая происходит история, вот в этом, например, месте. Допустим, да, зеленая ракеточка, правая – это ваш агент, в данном случае Марти, который управляет А левая ракетка – это компьютер, который играет против него. У вашего агента есть опции. Он либо делает правильное действие и мячик отбивается, либо неправильное действие и мячик пропускает. Если он мячик пропустил, Понятно, что в каждом агенте, как бы этот агент ни был устроен, неважно, это нейронная сеть или это конечные автоматы, все что угодно, понятно, что ваше каждое состояние этой среды как-то описывается внутри агента. Агент строит свою картину мира этой игры. И в данном случае эта ситуация как-то описывается. Каким-то символом или состоянием это нам сейчас не важно. Какое-то состояние картины мира. Условно А. И в данной ситуации, если он этот мячик пропускает, то, допустим, дальше следует состояние B, где он мяч пропустил, где, допустим, мячик слева от него, а состояние B будет мячик справа от него, и за этим последует отрицательный реворд. И тогда агент для себя в своей картине мира записывает, что из состояния A он перешел в состояние B, и там он получил большой минус. И он знает, что переход из А в Б это минус один. А вот если он мячик этот отбил и мячик полетел дальше, то он и допустим мячик полетел обратно, налево. Например это состояние С. то за состояние C он получит реворд минус 0.1, просто реворд за шаг. И получается, что в месте, где он отбивает мячик, у него на одной стороне есть событие с большим минусом, Но есть продолжение с маленьким минусом. Обычная политика здесь такая – агент выбирает состояние максимальное, то есть с минимальным минусом, условно равным минусу 0,1. Минус 0,1 фактически обозначает, что он продолжает играть дальше. по сравнению с минус 1, которое он пропустил мяч. То есть тем самым, рано или поздно, пропустив какое-то количество мячей или отбив случайно количество мячей в этой точке, у него появляется состояние, ведущее к минус 1 и состояние, ведущее к минус 0,8, может быть, любые цифры. И может быть не минус 0.1, а минус 0.2, но тем не менее, у него появляется состояние сильно отрицательное и менее отрицательное. И вот здесь логика идти к менее отрицательному состоянию. Та же самая история и с Тедишко будет работать. Вот это логика, как работать с очень спарс ревордами, с очень редкими ревордами. 

S01 [00:18:00] : Игорь, можно здесь сразу вопрос? Да, вот здесь спасибо, но вот все-таки я вернусь к вопросу про тележку. Гляди, вот здесь для того, чтобы мне получить хотя бы первое положительное подкрепление, мне нужно случайно отбить мячик. То есть в результате случайного отбивания мячика я получаю положительный реворд. Но тележку-то я на горку случайно же не могу закатить? Или вот просто реально вот до тех пор пока случайно все звезды не сложатся что в результате случайных движений вправо-лево я раскачаю ее до того, что она улетит наверх, тут я получу положительное подкрепление и по мере проигрывания сотен тысяч партий в конце концов она будет залетать наверх всегда. Правильно? 

S02 [00:18:50] : Ну нет. Сейчас я... Давай мы вернемся к тележке. Смотри какая логика с тележкой. Там логика немного другая, но для любого агента она будет примерно такая же. Тележка в результате случайных действий крайне маловероятна, чтобы она попала наверх к флажку. Это очень маловероятно. И тут работает другой принцип. Рисовать бы, конечно, надо. А мы можем, интересно, здесь есть рисунки? Слушай, я не знаю. Слушай, здесь вот у меня какие-то рисунки. 

S01 [00:19:37] : Вот дробь какая-то есть. Вот я рисую. Видишь мой рисунок? Вот теперь ты рисуешь. Да. Тебе надо как-то это стереть. А вот у меня ластик есть. Сейчас я свое смотрю. 

S02 [00:19:49] : У меня тоже. Вот прикольно и интересно. Можно ли тут будет рисовать? Рисуй. Ну допустим. Рисуй. Вот мы как здесь как бы как устроено, значит есть известный, но у меня слайда на эту тему нету, но я сейчас, а кстати что у нас здесь текст наверное есть, есть текст. Я могу здесь не писать? Ну попробуй. 

S03 [00:20:14] : Exploration with Exploitation. 

S02 [00:20:25] : Существует ключевая дилемма в Reinforcement Learning. Она называется Exploration vs Exploitation. Эксплуатация – это исследование новых состояний, а эксплуатация – это использование старых. Про это есть известная история, про человека в казино, где куча однороких бандитов, автоматов, и есть гипотеза, что какой-то из этих автоматов или какие-то из этих автоматов могут оказаться более выигрышными, чем другие. Как человек должен вести себя, чтобы заработать максимум денег? Есть так называемая жадная стратегия, когда он находит хоть какой-то автомат, который приносит ему в среднем больше денег, и он постоянно сидит на нем и дергает его ручку. Это называется эксплуатейшн, использование найденного состояния. А есть эксплуарейшн, когда ты должен все-таки сперва обойти все автоматы, все подергать. И даже если ты нашел хорошее, ты все равно время от времени дергаешь какие-то еще другие ручки, оставляя этот эксплуатейшн. И вот в R&D, в Reinforcement Learning, в модели, которая учится только на этих подкреплениях, Очень важно не сваливаться в сильно жадную стратегию, то есть, другими словами, не делать один только эксплуатейшинг того состояния, которое есть, а нужно делать эксплуарейшинг. Как это в данной ситуации выглядит и как решается задача «стелешка». Условно мы начинаем из нижнего состояния. Я сейчас обозначу его состояние, буду фиолетовым обозначать. Вот мы начинаем из нижнего состояния и из нижнего состояния у нас есть два действия, вправо и влево. И вот у нас действие вправо приведет к движению сюда, действие влево приведет к движению сюда. Мы сделали один раз действие вправо, один раз действие влево и видим, что оно... мы нашли для себя какие-то новые состояния. Вот это состояние. Действие вправо привело сюда, действие влево привело сюда. Правильно? В каждом из этих состояний у нас снова есть два действия вправо-влево. Фишка в том, что агент должен исследовать эту сведу по какому-то алгоритму, открывая для себя все новые состояния и пытаясь найти новую траекторию в этих состояниях. То есть, например, отсюда, если он сделает направо, он попадет сюда, в следующее состояние. Но если он отсюда сделает направо, то он на самом деле... попадет сюда же, потому что у него не хватит мотора дальше ехать, он окажется здесь же. То есть в этом смысле эта ветка в данной ситуации окажется тупиковая. Но если он поедет налево и окажется здесь, а потом дальше будет двигаться направо и будет раскачиваться, то он сюда попадет с другой скоростью и это позволит ему. И в данной ситуации вся логика в том, что скорость должна стать одной из составляющих картины мира этого агента. Но здесь смысл в чем? Задача агента – обследовать все состояния и попробовать в них все движения. И когда он находит новое состояние, то считайте, что, грубо говоря, новые движения из него приводят к новым состояниям и новым возможным движениям. обследовать возможное состояние этого пространства и этих движений. Любая РЛ задача этому посвящена. 

S01 [00:25:00] : Я правильно понимаю, что ты, скажем так, либо хардкодишь это поведение, либо даешь некоторое внутреннее подкрепление за исследовательскую активность, за открытие новых состояний? 

S02 [00:25:13] : Там могут быть разные истории. Я, честно, вот сейчас, знаешь, я не был готов сейчас к разговору про тележку, я уже забыл, как я это решал. По-моему, у меня там еще было... Если я не ошибаюсь, у меня там еще было внутреннее кинематическое подкрепление. Я оценивал смену действий. Одно действие или разные. Я, честно, не помню. Это было уже 6 лет назад. Или 5. 

S01 [00:25:51] : В общем, ты как-то мотивировал исследовательскую активность, правильно? Да, да. 

S02 [00:25:56] : Но, видишь, решение задачи с тележкой, оно не сильно... Оно показывает, что твоя модель работает, но это не тот бенчмарк, который позволит реально... Как это сказать? на конференцию со звездой попасть. Потому что это считается понятной задачей, ее давно умеют решать, и она простая, наглядная, но не то чтобы против-сайенс. Поэтому я занимаюсь этими играми Atari. Мне кажется, что они куда более там сложные, интересные и по ним как раз benchmark. По крайней мере у того же DeepMind именно игры Atari, вот в 2013 году их работа, где их QNetwork выиграла, выиграла Atari, привела их к большим этим достижениям. Знаешь, у меня предложение. Давай, может, мы дойдем до интерфейса, какие-то вещи покажем, а то мы сейчас... Давай, хорошо. Так, а у меня он продолжает рисовать. А, мне надо... А если я нажму стоп, то будет что? 

S01 [00:27:10] : Не, погоди, надо стереть. А кто? Надо просто стереть, сейчас я соберу. 

S02 [00:27:14] : Не, ну а зачем? Я думаю, что если я нажму стоп, то что? А, экран перестал шариться, да? 

S01 [00:27:21] : Да, экран перестал шариться. 

S02 [00:27:23] : А, ну его можно заново расшарить, в принципе. 

S03 [00:27:31] : Это эпично. Так. Да? Брюки появляются? 

S02 [00:27:35] : Так, только как это теперь сделать? А, вот. Значит, сам... В общем, мы сейчас поговорили про тележку и про маунтинг-кар. А теперь мы вернемся к играм Атари. И к тому, как они работают. Тут общая логика игры Atari следующая. Возьмем Breakout на его примере. Все это библиотека на питоне, в которой у вас есть некий объект, с которым вы работаете. который делается вот примерно такой командой сейчас мы сейчас код посмотрим там конкретно в качестве примера я покажу ну там просто такой код самый базовый как это работает значит и или даже можно начать с этого кода давайте я даже не знаю с чего, как отправляюсь. Код сперва покажем. 

S03 [00:29:05] : Сейчас я здесь остановлю туда шаринг, а здесь например наоборот. Открою. И здесь сейчас секунду. В другом этом файле, где нет замка. Ага. Нет. И мы сейчас здесь расширим экранчик. Так. 

S02 [00:29:39] : А, вот хорошо, что у меня много экранов, я на них все вижу. Так, я не знаю, могу ли я сделать побольше? Возможно, не могу. Видно? Видно. Сейчас я могу сделать покрупнее. Не уверен. Ну ладно. Вот эта простая программа на питоне. Мы сейчас посмотрим, как она работает. Я прямо последовательно расскажу про нее. Вы установили библиотеку Jim. Кому будет интересно, я могу просто сбросить несколько команд, как она устанавливается. Это работает на Linux. По крайней мере, я работаю на Linux и большинство людей работают на Linux. Хотя, по-моему, есть какая-то версия для Windows, но я не уверен. В общем, на любой линуксовой машине это нормально работает. Импорт джимназиума с джимом. Мы, в принципе, как говорим Питону, мы будем работать дальше с джимназиумом. Дальше мы говорим ему, команду, какая нас игра интересует, сделай игру. Мы говорим ему, сделать игру, в данном случае это Pong-Ram и некие его разновидности, я сейчас про разновидности скажу. Тип наблюдения, который мы хотим – RAM, то есть наблюдение за памятью. И render human – это обозначает, что мы хотим, чтобы она показывалась на экране. Ее можно рендерить, а можно не рендерить. Этой командой мы создали Environment, он дальше обозначен как переменный Environment. И дальше мы говорим, допустим, пять раз, пять игр сделать по тысяче шагов. Перед каждой игрой мы этот Environment резетим, просто сбросить его на нулевое состояние. И дальше он выглядит таким образом. Рендер – это команда, которая говорит отрисовать. Ее не обязательно делать, мы ее делаем только в случае, если нам нужен отрисованный экран. То есть, если вы делаете это на компьютере у себя дома, то можно рисовать экран. Правда, это замедляет сильно все. А если вы это делаете на удаленном сервере, то можно не рисовать экран, и просто оно идет. в тихом режиме. Если у вас есть экшен, который вы хотите этому агенту, который вы хотите в Environment послать, то есть если вы знаете, какое действие агент должен сделать, то вы этот экшен ну как-то сами моделируете сейчас покажу еще кусочек своего кода это пока пример а в данной ситуации мы просто генерируем некоторые случайные действия вот экшен space sample мы просто берем рандомное действие экшен это некое рандомное действие и дальше этот экшен мы выполняем мы говорим и 2 сделать один шаг с данным действием. От Environment в ответ в данной игре мы получаем 5 переменных. Это наблюдение среды, это текущий ревард, то есть вознаграждение и там пару системных сигналов дан, это игра закончена или не закончена, инфа это некоторые дополнительные переменные. Если ревард у вас не равен нулю, значит либо очко пропустили, либо забили. Ну и соответственно в конце, когда все эти игры заканчиваются, мы этот environment закрываем. Делаем его environment close. Как это выглядит в жизни? 

S03 [00:33:31] : Вот у меня, допустим, переграмму закрою. 

S02 [00:33:38] : Вот я сейчас нахожусь в этой директории, где лежит у меня этот экзампл. Команда у вас простая. Нужна третья версия Python, а текущий Jim работает с третьей версией. Я думаю, что у меня Python какой-то версии, допустим 3.8. У меня довольно старый питон, но я противник обновляться. Мы запускаем этот питон-экзампл и видим, что возникло окошко. Извините, оно возникло гигантское. Вы видите, что оно возникло. Началась игра. А тут вот пошли, видите у меня здесь были принты написаны, принт экшен вот такой, вот он пишет вот такой экшен и дальше он пишет принт обсервейшн. который он получил. Нас сейчас интересуют две переменные, можно что угодно печатать. Мы видим, что action это просто цифра, в данном случае это вектор из одной цифры, а observation в данном ситуации это массив из 128 чисел. Соответственно мы от environment получаем и мы отправляем ему какой-то action мы можем ему отправлять action сам, мы можем получить этот action от него и так далее если вы это делаете допустим не хотите это делать в режиме чтобы он показывал то мы просто убираем Убираем этот RenderMoveHuman, здесь убираем этот рендер тоже комментируем, он нам что это не нужен. И если мы перезапустим в таком формате, то просто модель будет работать как бы без отрисовки экрана. Она видите работает намного быстрее, но зато экран не видно. Я, например, люблю делать, у меня есть промежуточное на эту тему решение некое. Вот сейчас здесь вы видите, работает моя моделька. Видно, да? Здесь, значит, справа работает вот этот питоновский скрипт, который работает с Pong, а слева работает моя моделька, но в левом окошке. Я, например, люблю делать следующее. Я делаю себе флаги, у меня там все мои скрипты обычно смотрят в директории, я использую там файлы в качестве флагов. Если сделать какую-нибудь команду типа, вот у меня такое есть там, я создаю флаг, ну то есть файл с некоторым названием, ну в данном случае маркет флаг рентер. Скрипт устроен так, что он после окончания каждой игры смотрит директорию и смотрит, есть ли там определенные флаги, в зависимости от них дальше что-то делает. Лог начинает выводить или визуализацию делает. В данной ситуации он сейчас дойдет до конца игры, И вот эта конкретная модель, которая сейчас работает в тихом режиме на этом конкретном компьютере, она будет рисовать. Сейчас покажу, как это делается в скрипте, это совершенно несложно. Можно взять и мой скрипт подробнее посмотреть, как у меня все это устроено. В случае рендеринга у меня есть некая переменная, связанная с рендерингом. В зависимости от того, есть эта переменная или нет, мы создаем этот файл или не создаем, потому что для себя она будущая. В зависимости от этой переменной, если она есть, то создается environment с рендерингом human, а если ее нет, то environment без рендеринга. Ваша модель от этого никак не зависит. Если вы создадите новый Environment и в нем начнете играть, вы просто бесшовно в него переходите и это совершенно не важно. Дальше, когда этот флаг меняется, если он поменялся, то, грубо говоря, пересоздается. Если, допустим, не было этого рендеринга, Он появился, а потом пересоздается новый environment с отрисовкой и наоборот убирается. Можно подсмотреть, когда вам удобно, за моделью что она там делает. Теперь играет так. Потом выключите, он дальше учит себя. А например на сервере это сделать невозможно. Сейчас я про это тоже поговорю. Мне интересно, а где брейки никак не превратятся? Потому что игра длинная, а компьютер медленный. Сейчас показываю на своем компьютере, на котором я тестирую. На нем реальная модель не работает, она не работает обычно. А, ну вот, собственно, рендер. Она закончилась, рендер закончился, и вот, собственно, пошел. Он начал теперь отрисовывать. И вот это он сейчас показывает. Уже как, собственно, ну вот, к сожалению, гигантские в последнее время картинки, я не понимаю, как их уменьшить. Наверное, зная, как их уменьшить, надо там еще библиотеку поставить. Вот это уже модель немножечко обученная, она, видите, чуть-чуть там уже, чуть-чуть играет, отбивает. Но там всего тысячи игр, она пока довольно плохо это делает. Самое начало пути, так сказать. 

S01 [00:40:22] : Смотри, такое впечатление, что она медленно играет. Это потому что обсчет идет? Обсчет тормозит? 

S02 [00:40:29] : Да. В данной ситуации эта конкретная машина довольно медленная. Тот компьютер, на котором мы играем. Это сейчас реальная скорость, с которой у меня обсчет идет. 

S01 [00:40:42] : Получается, что у тебя обсчет не позволяет в реальном времени играть? Он подтормаживает? 

S02 [00:40:48] : зависит от машины вот вот смотри вот на этой машине на этой машине конкретно на этой машине смотри вот слева ты видишь слева мы видим слева мы видим вот это вот как бы вот на этом экранчике картинка меняется каждый раз, когда она получает очко или проигрывает очко, то есть примерно каждые 50-100 шагов. А вот здесь, например, на сервере удаленном она бежит, и это еще не самый быстрый сервер, но вот она там бежит как видишь, на порядок быстрее. 

S01 [00:41:29] : Слушай, я просто почему спрашиваю, потому что смотри, для понимания, я обнаружил, что сейчас у меня не очень сложная логика, я пока простую логику проверяю, и я вижу, что если я играю в human режиме, то оно играет в достаточном виде, но с некоторой реальной скоростью. режим отключая, оно пролетает очень быстро, и отлаживать, подбирать гиперпараметры проще с выключенным Human. В варианте, который ты сейчас показал, я правильно понимаю, что если отключить режим, оно все равно с такой же скоростью будет считать? 

S02 [00:42:04] : Нет, нет, нет, у тебя как раз в этом и прелесть, ты как бы выключаешь этот рендер и у тебя, ведь здесь в этом месте смотри, На сервере у себя я не в режиме рендера работаю, потому что удаленной сервер там экрана нет. И ты там в принципе делаешь без рендера. Я покажу сейчас, как я там подсмотрю. 

S01 [00:42:37] : Игорь, еще вопрос. Я правильно понимаю, что здесь получается вот этот трюк с переключением среды по ходу тестов, ты естественно делаешь только между играми? Потому что иначе ты просто, грубо говоря, у тебя там идет серия, зарядил пачку 10 тысяч игр и после каждой, между любой из этих тысяч игр можешь прерваться и переключить. 

S02 [00:43:01] : Верно, верно. Поэтому если ты заметил, когда я написал сделать этот файл флаговый, то минуты три ничего не происходило, а только потом возникла эта визуализация. И сейчас, если посмотреть, наверняка эта визуализация еще тут вот идет. И она исчезнет только когда этот 21 будет, а флаг я уже убил. Это вот что касается, когда у тебя локальный компьютер, но жизнь показываешь на локальном компьютере крайне неэффективно, потому что локальный компьютер всегда довольно, мало у кого есть очень мощный локальный компьютер. Я арендую сервера в облаке и у меня работает все на сервере в облаке, а там экрана нет. Но у Джима есть совершенно замечательная штука, он может создавать картиночки. И выглядит это так. У меня есть флажок, я его включаю или выключаю. Допустим, если я его включил, такой флажочек, и он включился, то если он включен, то на каждом шаге игры Мы делаем следующее. Есть такая команда, она называется env-env-ale-save-screen.png и он ее сокращает тебе в некий файл. Мы формируем название файла, в данном случае я его формирую как номер игры и номер шага. и просто его сохраняю. И когда я включаю такой флажочек на сервере, то он начинает игры, которые он играет, он начинает их сохранять. И дальше я их просто выгружаю из сервера и получаю себе, допустим, вот скажу вот такой набор картиночек и выглядит это дальше так вот значит вот у тебя есть пнг и ты можешь эти пнг просто между ними поперемещаться и посмотреть что происходит вот я сейчас просто руками нажимаю кнопочку вправо сверху если вы видите там написано шаги В данной ситуации я смотрел на игру, которая проиграла, хотя вообще-то не должна была. Мне было интересно, почему. И вот это позволяет подсмотреть за моделью на сервере даже в тихом режиме. И это сильно быстрее работает, чем рендеринг. Немножечко все замедляет, но не критично. Правда, надо приловчиться, чтобы... Нюанс этого состоит в том, что тебе нужно дальше научиться связывать картиночки, их номера картиночками с тем местом в реальной модели, где это реально произошло. Но тем не менее. Это очень полезная фича. Когда я начал пользоваться, стало все сильно лучше. Потому что когда ты тренируешь модель на сервере, вообще непонятно, что там происходит. А так ты можешь подсмотреть. Этот кусочек мы тоже обсудили. Теперь я покажу немножечко логику того, как у меня этот скрипт работает. Прямо с самого начала условно покажу, останавливаясь на каких-то важных вещах. Понятно, что там есть некий блок параметров, который так или иначе запускает. Вот есть важный параметр Environment. Прям очень важный. И вот для него я сейчас не вернусь, я сейчас вот здесь покажу. Есть супер важная вещь про эти игры Atari. Я про это говорил в прошлый раз в докладе и здесь обязательно скажу. Есть очень много версий каждой игры. Пять основных версий и у каждой из них есть свои подверсии. И там есть два параметра совершенно важных, которые нужно контролировать. Это параметры Frame Skip и Repeat Action Probability. Repeat Action Probability – это параметр для OpenAI Gym, для среды. задает вероятность, с которой Environment вместо того, чтобы выполнить действия, которые вы ему послали для вашего агента, он может с этой вероятностью выполнить предыдущие действия. То есть в breakout версии 0, если вы говорите ракеточке двинуться вправо, то с вероятностью 25% она вас не послушает и сделает предыдущее действие. И вы с этим ничего не сделаете. Я делал некую механику, вот она здесь в создании есть. Когда вы создаете этот environment, ему передаются вот эти параметры frameskip и repeat action probability. Я на всякий случай их везде ставлю, там единичка и нолик, но у меня уже подозрение, что это на самом деле не работает. нужно еще правильную версию использовать. В данной ситуации, да, вот это repetition probability, а frameskip – это сколько кадров, в течение скольких кадров ваше движение будет повторяться. И вообще правильный ответ один. И в этом смысле правильная версия называется вот так – breakout no frameskip v4. В этой версии конкретно Environment выполняет именно то действие, которое вы послали для агента, и выполняет его столько раз, сколько вы ожидаете. Вы послали его на один кадр, он выполнил его на один кадр. Если вы три подряд кадра его послали, то он три подряд кадра выполнил. Поэтому правильное создание этого environment выглядит так. Вы делаете GenMake не просто Breakout, а вот здесь надо написать в качестве названия, например Breakout, NoFrameSkip, V4. В случае конкретно Breakout. У меня это убило, я не знаю, года два, наверное, жизни. Считайте, что пока я этого не знал, а может и больше. Просто не буду дальше на этом станавливаться. Второй важный момент это как устроен ваш observation, с которым вы работаете. Тут есть несколько типов этих observation. Я работаю с памятью, то есть со слепком памяти, но большинство людей работает с экраном. Экран с точки зрения картинка. С точки зрения питона картинка это массив, просто матрица 2х2. Квадратная матрица. Здесь написано 210х160. Это размер экрана Atari. Но для цвета он три матрицы передает. У вас в трех цветах это делается. 

S01 [00:51:25] : Игорь, небольшой комментарий, там можно включить Grayscale, где она будет выдавать все в одну цепь. 

S02 [00:51:30] : Да, ну вот здесь есть и Grayscale, верно, и тогда это как бы ОАС-одна матывается. Верно, верно. Но я сейчас, у меня только в планах поработать с этими, с картинками, я пока с ними не работаю. 

S01 [00:51:44] : Слушай, а скажи, какова структура памяти в случае РЭМ, потому что когда ты показывал какую-то другую группу перед этим, там по-моему тоже матрица была в обсервейшене? 

S02 [00:52:00] : Нет, просто она так выглядела, как матрица. На самом деле это вектор. здесь видишь RAM, это Observation. 

S01 [00:52:08] : А ты структуру этого вектора знаешь или ты работаешь просто как с вектором? 

S02 [00:52:14] : Я структуру этого вектора первоначально не знал и считаю, что модель в принципе его не должна знать, но я его потом в какое-то время подсмотрел, просто чтобы понимать, что работает, что не работает. 

S01 [00:52:27] : То есть ты делаешь вид, что ты не знаешь, правильно? 

S02 [00:52:29] : В принципе, Мартин не знает это. Он не знает, но у нас есть возможность подсмотреть, и я сейчас могу показать, где подсмотреть. Единственное, что я сейчас на другой экран переключусь, потому что у меня эта вкладочка... То есть ты знаешь, но Мартин не говоришь? Ну, конечно, да. Сейчас, секунду, я найду, где у меня... Сейчас я расширю на другом компьютере. Прекрасный Йошуа Бенджио в свое время это сделал. У него есть работа, на архиве она опубликована. Я могу найти ссылку. Важно, что там есть гитхаб, на котором есть сверху ссылка. Можно же найти по идентификаторам на гитхайбе. Здесь идет дальше описание игр. И дальше прямо, вот мы сейчас найдем breakout, например. 

S01 [00:53:48] : Ну а ты можешь эту ссылку потом скинуть в чат? Да хоть сейчас. Ну не в этот чат, лучше в телеграм. 

S02 [00:53:55] : Да, давай я тебе эту ссылку потом скину. Хорошо, давай. Потому что у меня, как обычно, 200 непрощенных сообщений. 

S01 [00:54:03] : Хорошо, скинь мне, я потом переброшу. 

S02 [00:54:06] : Это очень полезная ссылочка, потому что ты можешь подсмотреть, как на самом деле этот вектор устроен. 

S01 [00:54:22] : Слушай, а почему ты считаешь, что с памятью проще, чем с графикой? То есть там и там считай вектор? 

S02 [00:54:30] : Я не считаю, что проще, но я думаю, что вычислений меньше. 

S01 [00:54:36] : То есть, ты хочешь сказать, что размерность этого вектора меньше, чем размерность графики? 

S02 [00:54:40] : Да, я только из-за размерности работаю. Еще одно соображение, это к нашей модели конкретно. Мы же с Шумским делаем модель интеллекта, который не у кортекса, а они получают уже предобработанную информацию от зрительной коры. И мы думаем, что в зрительной коре происходит вся эта первичная обработка. Мы знаем, что Александр Хомяков считает ровно наоборот, но сколько людей, столько мнений. Я сегодня видел очередные чатики на эту тему. А мы думаем, что эта сложная картинка предобрабатывается и схлопывается в некоторые объекты абстракции и, соответственно, с этими уже абстракциями работают лобные доли интеллекта и прочее. И поэтому я считаю правильнее работать уже со сжатым вектором. Я в принципе уверен, что Марти будет работать с моделью, если взять энкодер-декодер, обучить ее и потом отрезать и брать середину, то есть энкодерную часть, я уверен, что с ним будет точно так же работать. Руки не дошли. Первый подход не получился, сходу надо было разбираться и я побежал дальше. Я вот это место просто покажу, что есть такое место, где описаны все эти координаты для тех, кто хочет. Йошуа Берджио причем, он считал, что это важно, они там все это выписали. Потому что его, как я понимаю, агент как раз, ну вот они сделали агента, который знал всю вот эту вот кухню и на этом учился. Так, это мы дошли до Observation, правильно? А теперь мы возвращаемся вот сюда, на этот скриптик. Я здесь допокажу какие-то идеи, тоже связанные с Марти. У меня все игры свои логи делают, это понятно, по которым ты можешь потом ориентироваться. Мы обсуждали с Антоном в чатике, что у меня есть жесткое ограничение на длину игры, потому что, ну так в общем делали в тринадцатом году статьи, я не знаю почему восемнадцать тысяч, ну восемнадцать тысяч, ну вот я взял так же, чтобы И у меня, кстати, сегодня днем на сервере, в марте, взял историческую планку, 864 очка. Это максимум. Прямо за час до семинара. Нет, я увидел это за час до семинара. А с другой стороны, появились игры, где он зарезается на этих 18 тысячах. Не хватает ему. Он бы, может, еще играл, но 18 тысяч максимально в пределах. 

S01 [00:58:11] : Подождите секундочку, на 18 тысячах? 

S02 [00:58:17] : У меня игра устроена так, у меня каждая конкретная игра устроена так, есть сколько-то игр, один рейндж от нуля до геймсплей, допустим 100 тысяч игр, а каждая игра устроена так, от нуля до геймленгв. И он всегда закончится на этом геймлингсе, и он может закончиться только раньше, если игра закончена. Вот здесь, например, если дан, если он получил сигнал дан, то он выходит из этого цикла, и игра считается закончена. А дан приходит к нам из джима. когда он считает, что игра закончена в данной ситуации. 

S01 [00:59:06] : Это у тебя breakout? 

S02 [00:59:08] : Это пункт. Это я сейчас пункт показываю. Так получилось, что показываю пункт. В принципе, breakout устроен примерно так же. 

S01 [00:59:19] : Не просто breakout, а там есть еще truncated, там где у тебя unknown. брейк дауне, там трункейтед, да как раз трункейтед, он тебя может срубить в том случае, если ты попал в цифру, что у меня получается достаточно часто. 

S02 [00:59:34] : У меня None, потому что я не знаю, что это такое. 

S01 [00:59:39] : Это ситуация, которую ты получаешь, если ты попал в ситуацию. 

S02 [00:59:45] : Переменная в данном случае не сильно волнует, может быть, да. У меня не нужно было никогда, я не разбирался, что это такое. Но переменная, которая нас реально интересует, это Observation, которую мы получаем. И в данном случае у меня Observation это вектор. ревард, который там либо ноль, либо единичка, либо минус единичка, и соответственно там done это просто true, false, законченная или не законченная игра, и info это там какие-то дополнительные параметры. В Pong они ни на что не влияют, а вот например в Breakout там как раз жизни, потери жизни, сколько жизни осталось. В этом смысле, когда я говорю, что практически, когда показывают эту картинку, что эти 10 строчек условно, 20, что они плюс-минус с любой игры позволят играть, к сожалению, но вот это observation reward инфа для каждой конкретной игры может быть немножечко по-разному устроена, придется может быть немножечко с этим разбираться. но есть всякие форумы, которые про это говорят. я сейчас конкретно про две эти игры знаю, про breakout и pong. плюс-минус все про то, что Джим делает. но с остальными надо дальше разбираться. Ну и в Pong, соответственно, дальше у меня логика как устроена. С самого начала экшен неизвестен и первое время модель играет, идут просто случайные числа. У меня это сделано, если действие неизвестно, то оно выбирается случайно. Причем я у себя еще сделал мануальный режим, можно запустить флагом вручную, чтобы руками потыкать. начиная с момента когда марти уже сформировался и начинает отвечать то вот к этому месту значит экшен получается от марти обратно и он уже может этот экшен посылает в сведу обратно и тогда уже как бы случайные действия не выполняются. Ну вот собственно у меня там такая логика. Что касается моего марки то у меня он конкретно Здесь два окошка. Питоновский скрипт отвечает только за связь с жилом, а маркер у меня написан на яве и он связывается с питоном через сокет. Мне было так удобно. Это позволяет на двух разных серверах запускать или на кластере серверов. Но это можно делать через файл, как угодно. Так, а рендер у меня все еще идет, да? А я думал, что я его выключил. А, я его не выключил, да. Вот. Ну, собственно, что еще рассказать про этот код? Да, наверное, Такая была логика. Мы создали environment и потом столько игр, сколько нам нужно, мы просто подряд запускаем. Мы каким-то образом сгенерировали action, запустили шаг в environment с этим action, получили от него в ответ наблюдения, что-то с этим наблюдением сделали, подумали, сгенерировали новый экшен и снова отправляем в Environment очередной степ с этим экшеном. Получили от него новое наблюдение и вот это вот вечный цикл, в котором модель крутится, получает наблюдение, думает, что делает возвращать какое-то действие в инваре мы возвращаем только действие какое-то данной ситуации действие это это цифра до продействующих что-то скажу так меня слышно антон я чуть перестал все слышать так антон пропал ну да все слышно игорь 

S04 [01:04:12] : И у меня вопрос, пока тут пауза. Action определен на каком множестве? Это Integer? Минус один, ноль, один? 

S02 [01:04:21] : Прекрасный вопрос. 

S03 [01:04:22] : Я как раз тоже хочу к нему перейти, потому что он пока не понял, что происходит. Так, сейчас, секунду. А, вот, да. 

S02 [01:04:39] : для каждой игры action определен своим образом но для breakout action это как бы 4 возможные движения 0 ничего не делать единичка на самом деле тоже ничего не делать по праву влево одна из особенностей этого джима такая неприятная состоит в том что в некоторых играх эти действия как бы задвоены или там вообще многократно за Вот, например, смотрите, допустим в Понге действия задвоены все. Там реально есть только, ну как бы, условно говоря, вверх-вниз, вы можете двигаться только вправо-влево и ничего не делать. То есть, как бы, три движения. Но каждый из них реально, ну как бы, вот там, 0 и 1 это ничего не делать, значит, 2 и 4 это вправо, 3 и 5 это влево. И вот с этим ничего не сделаешь, к сожалению. И это сильно ухудшает обучение. А есть еще более опасная история, что есть такая переменная в этом GMA, называется Full Action Space. И если, не дай бог, она будет поставлена как true, то он будет 18 потенциальных экшенов использовать. Что такое вот эти экшены? Как моя модель с ними работает и почему это плохо? Когда вы, я возвращаюсь к своему скрипту, пока Марти еще не делает никаких действий, он получает случайные действия от среды. И в зависимости от той настройки той настройки, вот с этими действиями, она ему присылает, допустим, для Понга, она может присылать ему шесть действий, а может присылать восемнадцать. Если, не дай бог, будет настроена вот эта фулл-версия. А смысл в том, что все эти действия, они как бы бесполезные, они бессмысленные. Они либо дублируют, либо просто ничего не делают. И получается, что у вас модель будет получать кучу шума лишнего. Вот эти действия будут просто шумом, который ни к чему не приводит. И в этом смысле в Pong'е и так-то вот эти шесть действий это уже шум, потому что вообще три надо делать, а там их шесть. За таким фигом я, честно, пока не понял, но вот такая игра. к сожалению. В брейкауте хотя бы это чуть-чуть лучше, потому что там всего четыре действия и одно из них всего лишь задублировано. То есть вам, когда вы делаете... Подождите, какое задублировано? 

S01 [01:07:44] : Ноль. Ноль один. Почему? Один это фаер, один это мячик. 

S02 [01:07:50] : Верно. Нет, нет, он ничего не... А, ну да, да, да, кстати, в брейкауте, да, вот это... Про это расскажи еще. Да, это и правда. Вот я чуть не забыл, да. В брейкауте есть совершенно волшебная история, на которой я тоже там потратил, наверное, пару недель, пока в форумах не полазил и не обнаружил. вот конкретно в этой игре, когда агент потерял мячик, то запустить как бы новый запуск мячика можно только действием единичкой. И это надо за хардкодить. У меня вот в моем питоновском скрипте на брейкауте это за хардкодить. То есть когда мячик потерян, нужно обязательно единичку, а нет, не в питоновском, 

S01 [01:08:35] : Игорь, а можно вопрос, а почему это обязательно надо хардкодить? Ну так сказать, нету мячка, нет, ну случайно же он рано или поздно нажмет единичку и получит подкрепление. 

S02 [01:08:44] : Почему обязательно хардкодить? В случае, допустим, Марти, у меня ситуация такая, у меня модель как бы зависает, потому что когда мячика нету, он получает на вход некий вектор, И этот вектор, давай мы его сейчас, я сейчас прям по-быстрому это сделаю, мы это прямо увидим. Сейчас я вот здесь напишу breakout. 

S01 [01:09:16] : То есть как бы в моем понимании это просто одно и еще одно состояние environment, ну просто некоторое более сложное поведение среды, ну а мы же знаем, чем сложнее среда, тем выше интеллект нужен для работы в ней. 

S02 [01:09:28] : это верно но тут есть как бы нюанс сейчас смотри вот я сейчас просто возьму и сделаю вот такую вот вещь и мы будем не рандомный экшен делать а просто 0 посылать в breakout посмотрю, что будет. это за один цикл. 

S01 [01:09:50] : кстати, мой агент, который сейчас у меня учится, ему, конечно, до марти как до луны. особенно после того, что ты сказал по 864, это вообще космос. но у меня как бы агент и никакого хардкода нет. там хардкодится только самый первый мячик в игре. а при потере мячика там ничего не делается. 

S02 [01:10:12] : Ну, вот смотри, я сейчас покажу, что будет происходить. Action. А, блин, пардонте, это питон дурацкий. 

S03 [01:10:23] : Господи. Хотя, вообще нет, по идее, все правильно. 

S01 [01:10:33] : Предыдущий отступ. А, нет, все вроде правильно, да. 

S02 [01:10:36] : Да, вроде, мне кажется, правильно, да. Это вот питонские штуки. 

S01 [01:10:46] : Видимо, у тебя там пробелы, а ты табуляцию сделал. Или наоборот. Не-не-не, это именно в отступе проблемы. 

S02 [01:10:54] : То есть, у тебя где-то пробелы, а где-то табуляция. я небольшой а да мы смотрим. А, ну собственно он уже мячик потерял. Вот он мячик потерял, экшен 0. Видишь, что происходит? 

S01 [01:11:43] : У тебя сэмпла нету. Так нет, правильно, у тебя сэмпл-то не делается. То есть ты не случайное действие, у тебя всегда одно и то же действие. А если действие случайное, 

S02 [01:11:54] : верно, верно. тогда там будет единичка. я сейчас вот про что. 

S01 [01:11:59] : рано или поздно появится единичка. я вот про это говорю. да, да, да. 

S02 [01:12:03] : но я сейчас по-другому смотрю. представьте, что у тебя пока единички нет. и у тебя идет observation постоянный. с точки зрения модели у тебя она как бы получает, как будто бы пауза такая возникла. просто вот она висит. И она будет висеть бесконечно. Почему бесконечно? Она же что-то делает. 

S04 [01:12:28] : Она может начать висеть, посчитая это идеальным решением этой задачи. Такая фигня. Я с этим тоже сталкивался. Это как раз exploitation и exploration. Если она будет считать, что все, это хорошо, а что? Это мы знаем, что смотреть, двигать пустой ракеткой без мячика, это плохо. А агент этого не знает. Для него это вполне приемлемый результат, вполне нормально. Зачем еще что-то делать? 

S02 [01:12:58] : В общем, Антон прав в том, что если будут какие-то случайные действия, то рано или поздно он делает эту единичку, не так уж много вариантов, и он недолго зависает. А вот, например, моя модель, в особенности, когда она обученная уже, она воспринимает эту ситуацию просто как некоторую паузу. ну короче меня это приводило просто к таким зависаниям, я это захаркодил, может это неправильно, короче у меня захаркодило, что после его проигрыша надо обязательно сделать это действие единичку, потому что без него игра не запускается дальше 

S01 [01:13:46] : В общем, здесь как бы есть ощущение, что это не совсем правильно. Ну ладно, окей. 

S02 [01:13:52] : Наверное. Наверное ты прав, что это может быть не совсем правильно. Так. Ну вот, собственно, я думаю, что я плюс-минус все тут рассказал, что хотел. У меня уже время заканчивается. Какие еще есть у нас? 

S01 [01:14:07] : Ну, я не знаю. Может быть, давайте, так сказать, вопросы, какие есть у коллег. Вот. 

S02 [01:14:16] : есть какие-нибудь вопросы? 

S00 [01:14:20] : хотел спросить еще вот когда марти обсуждали на прошлом семинаре вот у вас берется получается статистика за некоторое количество шагов и потом убираются потенциальные векторы которые потом собственно нарезаются на кластеры и по ним собственно обучается модель Как выбираются вот эти вот векторы потенциальные, которые потенциально могут описывать состояние вот этого агента? 

S02 [01:14:57] : А, ну это вопрос уже именно к предыдущему семинару. Давайте я здесь покажу. здесь сейчас покажу вот если мы допустим сейчас а неважно делать рендер не делать вот я запускаю значит этот эту игру и получаю вот он отменить это выше Это 128 байт памяти, которые как-то устроены. Как они устроены, я не знаю, и Мартин не знает. Но он делает следующее. Первые 5 шагов. Честно, я делал некоторое количество как это сказать, действий, которые позволили мне делать модель, чтобы она меньше считала, просто уменьшать пространство счета, потому что иначе длинные вектора считать, классифицировать все сильно дольше. Поэтому, вот если вы видите, а, нет, не видите, да, я подумал, что видно, сейчас, секунду, секунду. сейчас расшарю, что у меня там шаренка, я говорил, думаю, что видно. вот я запустил опять, вот я запускаю еще раз, вот он пошел играть, я его сейчас минимизирую и мы смотрим вот здесь видите на экране бегут циферки, видно циферки. Это observation, которую мы видим, модель получает от отжима. И видите, что здесь некоторые цифры стоят постоянно, а некоторые меняются довольно быстро. Просто так устроен вектор. И одной из очень старых решений, давно сделанных, было в том, что выбросить из рассмотрения цифры, которые не меняются. Просто чтобы уменьшить вектор. Раз они не меняются, мы их просто игнорируем. И, грубо говоря, берутся только активные цифры, которые что-то из себя представляют. и по ним модель кластеризуется, и каждая из этих цифр, которая активна, каждая из них становится гипотезой, что это может иметь какое-то отношение к репрезентации самого агента. В данном случае, в случае рам памяти, Это очевидно, потому что там какая-то из этих цифр, вероятно, координата. Ну, может, какие-то две цифры-координаты. Вот, собственно, модель делает это. Она, во-первых, уменьшает это пространство до только активных цифр, те, которые там меняются. Я не понял, там стоит какой-то порог на отсечении, те, которые медленнее чем что-то, они просто дальше не рассматриваются, мы без них обходимся. те которые активные каждый из них делается по каждой из них строится сколько-то колонок в данной ситуации вот у меня pong сейчас здесь на этой модели на этой машине работает и вот здесь значит один коннектор работает совсем полным вектором а еще два это коннектор движений, а вот еще два работают с цифрами, которые вроде как отражают агента. Причем, кстати, интересный момент, я про него еще думаю, как он устроен. Если посмотреть на вот эту страничку, которую группа Джошуа Бенжио написал с цифрами, где реально что, то странным образом моя модель еще какую-то цифру нашла и считает ее репрезентацией агента. Я не знаю, что это. И она действительно, я смотрел, она как бы тоже действительно... Страничка это вот то, что ты послал, да? 

S01 [01:19:29] : Вот это? 

S02 [01:19:30] : Да, страничка то, что я послал, вот я ее могу... Ну, потому что у меня на разных компьютерах открыто... Ну, в общем, там есть просто перечень координат, грубо говоря, которые можно подсмотреть. ну и в общем у меня почему-то странным образом есть какая-то еще координата в общем короче модель посчитает что агент репрезентативен там другими цифрами не совсем теми которые вот ее же были написано не знаю но работает вот видите там вот оно работает она там как-то учится но пока довольно слабо но это пока только тысячи игр тысячи игр это в общем для марта немного вот антон когда я сказал что выбил исторический рекорд сегодня это 33 тысячи игр и это заняло на сервере у меня две недели вот он сейчас 14 дней играет, то есть это довольно долго, потому что вот эти игры под конец, когда там уже там больше например 500-600 шагов, 500-600 очков это уже где-то 15 тысяч шагов 16, то есть это прям длинные игры и это долго к сожалению. Так, еще есть какие-то вопросы? 

S01 [01:21:07] : У меня, знаешь, какой вопрос есть? На самом деле, правда, немножечко его смарт для меня потерялся, потому что... Короче, вопрос следующий. Я когда предыдущие эксперименты делал 3-4 года назад, которые я докладывал тоже с Шариком, тоже своей собственной моделью, я там обнаружил следующий эффект, что если есть отрицательное положительное подкрепление, Я обнаружил, что если модельке не давать отрицательное подкрепление, а давать только положительное, то в целом обучаемость повышается. Я это интерпретировал так, что отрицательное подкрепление снижает исследовательскую активность. И чтобы не снижать исследовательскую активность, поэтому отрицательное подкрепление надо не давать, а давать только положительное. То есть, грубо говоря, подкрепляется известный широко популяризируемый тезис, что только пряник работает лучше, чем кнут и пряник. Вот, и когда я сейчас вот пару недель назад погрузился, значит, вот уже в этот самый жил. Все эксперименты, за исключением окончившегося буквально 15 минут назад во время нашей встречи, мне подтвердили тот же самый эффект. То есть при всех наборах гиперпараметров я всегда запускаю эту систему так, что в двух вариантах. Первый вариант – это и положительный, и отрицательный фидбэк. А один случай – это только отрицательный, только положительный. Вот и у меня всегда, вот я могу даже показать, все прогоны получают стабильно, что перекрытие есть по разбросу, то есть результаты близкие, но всегда стабильно. Обучение только пряникам, оно лучше. И вот только последний прогон, вот буквально сейчас закончился, показал почему-то, что с кнутом может быть лучше, чем без кнута. 

S02 [01:23:01] : Вот смотри, тут у меня есть на эту тему Я тоже с этого начинал. Я начинал только с положительных предпринятий. И это довольно долго работало. Но потом я пришел к выводу, что... Но у меня тогда была Это была довольно специфическая история. То есть, когда у тебя такое положительное подкрепление, то в играх типа Понга, в брейкауте, когда ты отбил мячик, ты всегда забьешь, ты всегда получишь очко. Ну, почти по началу. Пока у тебя модель только учится, уже в конце, допустим, когда у тебя экран почти очистился, у тебя осталось там пару-тройку кирпичей, ты, значит, агент отбивает мячик, а он может там по кругу крутиться, а очков не приносит, но модель у тебя уже обучена. Но в понге не так. Когда ты один раз отбиваешь мячик, то есть же противник, который мячик тоже отбивает и далеко не всегда ты получаешь положительный реворд. И оказалось, что в этом случае только положительные реворды недостаточно, потому что они не всегда есть. И я там многократно потом перестудировал книжку этого Сатана «Обучение с подкреплением». Я, кстати, всем советую, всем, кто… Я покажу, она у меня тут, можно сказать, одна из моих настольных книжек. совершенно прекрасная книжка, вот такая. Мне ее подарила цифра. Это ДМК Пресс. Они у нас на конференции продавались. И, короче, студируя ее, я понял, что Если делать внутреннее подкрепление на каждый шаг, типа минус 0.1, то в этой точке рефуркации, отбил-не отбил, будет сильная разница в прогнозах. Будут траектории с большим минусом и с маленьким минусом. А если ты даешь положительное подкрепление, то этого нет. То есть, получается, что с положительным подкреплением она рано или поздно научится, но время обучения будет на порядке быстрее, по крайней мере, в игре типа Понга. Не в рейкауте. В рейкауте этого не видно. Вот такая неприятная штука. И мне самому не нравилась эта идея. Я долго к ней свыкался, но в результате понял, что ну как бы да. Но Сатун про нее пишет в очень жесткой форме. Он считает, что да, обязательно давать модели нужно. маленькое отрицательное подкрепление на каждый шаг, потому что это позволяет из тупиков потом выходить, не делать бессмысленные циклы. То есть когда ты делаешь маленькое отрицательное подкрепление и у тебя модель впадает в цикл гриппинга, то ты можешь Кубович там реворт колоссальный, отрицательный будет и так далее. 

S01 [01:26:27] : Вот, кстати, здесь есть Владимир Крюков, если еще не ушел, я просто прокомментирую. В моем понимании это очень хорошая история, которая вписывается вот в ту Концепцию, которую мы в сознании представляли, что мы можем считать... Опять-таки, я, может быть, притягиваю это за уши, но, может быть, Сатан это по-другому объясняет, не читал, но одобряю. Но идея в том, что на каждый шаг мы на самом деле тратим какую-то энергию. Грубо говоря, если мы сделали 100 шагов и получили положительный результат, то каждый из этих шагов с этим результатом соотносится как 100 с 1. А если мы за 10 шагов достигли того же самого результата, Вот, то соотношение того же самого результата получается, что мы в выигрыше. То есть мы должны учиться не просто получать положительный результат, а мы должны получать положительный результат за меньшее число шагов. И в этом смысле как бы учет количество шагов, отрицательным подкреплением с потерей, условно говоря, энергии сопоставлены с выигрышем, так сказать, той добычей, которую мы добыли на это количество шагов. Это вот некоторое экономическое соотношение, которое мы можем чисто экономическими методами рассматривать, как мы должны не просто обеспечивать максимизацию прибыли, но мы на самом деле, чтобы у нас как бы прибыльность получалась, чтобы чтобы мы были в большем плюсе, получали большую прибыль на единицу затрат. 

S02 [01:28:08] : Антон, тут есть еще один вопросик в чате, я на него отвечу и уже пойду. Если еще не говорили, расскажите про внутренние подкрепления и возможное применение ВГРЭВДЖИН. Это, на самом деле, суперский вопрос. Один из, мне кажется, ключевых вопросов, потому что внешние подкрепления – что-то редкое. И в жизни тоже. А вот внутренние подкрепления – это как бы… у меня одно из любимых высказаний такое победы надо праздновать даже маленькие иногда ты делаешь какой-то маленький пускай шаг на пути там какой-то большой цели ты цель не достиг еще но ты же понимаешь что вот уже как бы уже продвинулся это можно рассмотреть маленькую победу маленький праздник себе там чем-то вознаградить мы так люди делаем и это в принципе правильно как это делать с моделью вот это цинично на самом деле большая сложность у меня долгое время работала я там сделал так называемые сюрпризы такое удивление модели и его использовал как внутреннее как внутреннее подкрепление у меня это в первой статье описано про марте который вот найджа и там была Но с тех пор я отказался. В какой-то момент времени я отказался от этих удивлений, потому что они работали не совсем четко. Иногда шумели, и я не нашел, как избавиться на тот момент времени. Сейчас я подумал, что я даже знаю, как это избавиться теперь. То есть это было связано просто с недостатками модели, а не с идеей. Идея этого удивления была на примере ракеточки. Если он не отбивает, то из этого состояния А у него следующее состояние B, которое минус 1, большой минус. И вот он из этого состояния А, отбивая, у него все время минусы, допустим, он много раз пропустил в этом месте. И он уже знает, что основное вероятное продолжение из состояния А это B и большой минус. И он уже по это знает. И вдруг в какое-то время он двигается в другую сторону и попадает в состояние С. которая маленький минус дает и вот в этом месте я как бы у меня модель удивляется потому что она не текущая, предыдущая версия, но я это потом верну, мне кажется, очень классная идея. Удивляется в том смысле, что вдруг она видит, что несмотря на большую статистику и очень вероятный большой минус, вдруг мы получили прогноз сильно лучше, а значит наверное, было сделано что-то правильное. И я вот в этом месте давал внутреннее подкрепление, которое было там сильно меньше, чем внешнее. Нет, сперва такое же, потом должно было делать меньше. И вот это очень сильно позволяло ускорять игру. Но здесь надо очень точно рассчитать, в какой момент это делается и очень точно с оценками ситуации работать, потому что, другими словами, если у вас вот эта единичка не пришла, которая будет еще долго потом, то есть в Понге в том же, эта единичка будет еще потом, а может даже и не будет. компьютер отобьет и марки пропустит, то венички не будет. Но вот факт того, что прогнозы изменились, прогноз был сильно отрицательный, а вдруг он стал неотрицательным, это обозначает, что вероятно сделано что-то хорошее. Я вот это назвал для себя удивлением и в этом месте давал внутреннее подкрытие. Мне это кажется, ну, правильно. Я до сих пор считаю, что это правильно, но просто я не сумел это сделать. 

S01 [01:32:53] : Вадик, а подскажи, пожалуйста, такой момент, если еще не убегаешь. Если прогноз очень отрицательный, то почему ты это сделал? Или там просто вариантов не было, все варианты были с одинаково плохим прогнозом, и поэтому ты выбрал наименьшее-наименьшее зло, но получил, вместо наименьшего зла, получил добро. По-разному. 

S02 [01:33:14] : Может быть такая ситуация, да. А может быть просто еще не все движения перебраны. По-разному. Как раз в данной ситуации это скорее всего не все движения перебраны. не все возможности, и тут он лишь находит возможность, которая еще раньше не была, он ее пробует, и оказывается, что оно... несмотря на плохой прогноз Да, а плохой прогноз, грубо говоря, когда он у тебя 10 раз в этой ситуации побывал и использовал несколько разных движений, у него уже статистика появилась, и у него уже 10 условий есть, а одиннадцатый может оказаться другой, или 51-й может оказаться другой. Когда у тебя статистика уже, я не знаю, тысяча случаев, то маловероятно, что она изменится, хотя всё бывает. Окей. Ладно, спасибо, друзья, я уже всё говорил. 

S01 [01:34:12] : Ну, наверное, тогда давайте, Игорь, отпустим, значит, если вдруг какие-то ещё вопросы есть, можем обсудить. Игорь, всё, спасибо тебе огромное. Спасибо огромное. Полезная информация. 

S02 [01:34:21] : Надеюсь, было полезно. 

S01 [01:34:22] : Да. Хорошо, коллеги, спасибо Игорю. Если какие-то еще вопросы есть, обменяться мнениями, там может быть какие-то вопросы, я могу ответить. Если нет, то будем запругляться уже. Но если нет, тогда всем спасибо за участие и интересная встреча у нас будет в следующий четверг. Будем говорить как раз про то, как это все работает в живом мозге. у спортсменов с человеком, который с одной стороны разбирается в спортивной физиологии и отчасти нейрофизиологии, а с другой стороны занимается импульсными нейросетями. Поэтому до следующего четверга. Всем пока! 







https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html

