## 3 апреля 2025 - О моделях, задачах, нечетких "знаниях" и "ризонерах" - Антон Колонин — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/X5gQ6LUMsMU/hqdefault.jpg)](https://https://youtu.be/X5gQ6LUMsMU)


Суммаризация семинара:

## Введение и концептуальная основа

Семинар был представлен Антоном Колониным, который продолжил серию обсуждений, ранее касавшихся систем онтологии и предметно-ориентированных языков (DSL). В текущем семинаре основной темой стали "нечеткие знания резонеры" (fuzzy knowledge reasoners), с предварительной дискуссией о функциональной эквивалентности различных подходов к искусственному интеллекту.

## Функциональная эквивалентность и аппроксимация

Спикер начал с концепции функциональной эквивалентности, указав, что одну и ту же задачу можно решить разными способами. Используя аналогию "описания слона", он продемонстрировал, как различные математические подходы могут аппроксимировать одну и ту же функцию:
- Гармонические функции
- Степенные функции/полиномы
- Вейвлеты
- Даже белый шум

Докладчик подчеркнул, что любую функцию можно хорошо аппроксимировать на конкретном интервале обучения, но экстраполяция за пределы этого интервала ненадежна. В качестве реального примера был приведен опыт с финансовыми временными рядами, где увеличение числа признаков улучшало точность на тренировочном наборе, но ухудшало результаты на тестовом.

Для описания сложных объектов (например, трехмерного слона) требуется структурированное машинное обучение, где отдельные функции описывают отдельные части, а функции более высокого уровня структурируют их в целостную модель.

## Геофизическая томография как аналогия для задач ИИ

Опираясь на свой опыт в геофизике, спикер представил томографию как аналогию для задач ИИ:
- Томография представляет собой систему уравнений, где мы пытаемся восстановить свойства среды, пропуская через неё лучи
- Существует два подхода к решению:
  1. Аналитические методы (свёртка обратной проекции)
  2. Итерационные методы (градиентный спуск)

В медицине аналитические методы работают лучше из-за небольших вариаций плотности тканей и регулярной сетки измерений. В геофизике, где вариации параметров могут достигать сотен процентов, необходимы итерационные методы.

Это проводит параллель с вопросом, когда использовать графовые или нейросетевые подходы в ИИ.

## Представление знаний

Докладчик обсудил два основных способа представления моделей:
1. **Матрицы/тензоры** (используются в нейронных сетях)
2. **Графы/гиперграфы** (используются в символьных системах)

Для эффективной работы с графовыми представлениями знаний необходимы четыре компонента:
- **Базовая онтология** (мета-онтология) - фундаментальное описание объектов
- **Представление времени** - часто отсутствующий, но критически важный компонент
- **Язык** для манипулирования графами (Turtle, JSON-LD, Cycle Language, и т.д.)
- **Резонер** (inference engine) для логического вывода

## Системы логического вывода (резонеры)

Спикер представил три основные системы логического вывода:
1. **NARS** (Non-Axiomatic Reasoning System) Пей Ванга - с открытым кодом, включает управление ресурсами
2. **Discovery** Евгения Витяева - закрытая система
3. **PLN** (Probabilistic Logic Networks) Бена Гёрцеля - математически похожа на NARS, интегрирована с OpenCog AtomSpace

В системах NARS и PLN логический вывод строится на двух параметрах:
- **Strength** (сила) - вероятность связи
- **Confidence** (уверенность) - насколько достоверна оценка

Четыре базовых правила вывода:
1. **Ревизия** - агрегирование множества связей между одноименными вершинами
2. **Дедукция** - вывод транзитивных связей
3. **Индукция** - выявление связи между двумя объектами, имеющими общую связь с третьим
4. **Абдукция** - обратное рассуждение о причинах

Важным аспектом NARS является приоритизация вычислений по важности связей и ограничение времени на логический вывод.

## Реализация и OpenCog AtomSpace

Докладчик подробно описал реализацию гиперграфов в OpenCog AtomSpace:
- Базовый элемент - атом, который может включать другие атомы
- Атомы имеют значения (values), включая strength и confidence
- Атомы могут быть разных типов (направленные/ненаправленные связи)
- Пространства (spaces) позволяют разделять атомы по сегментам

Был представлен пример вероятностного рассуждения о погодных явлениях (облако → дождь → наводнение), где каждая связь имеет свою силу и уверенность.

## Практические вопросы и проблемы

Спикер затронул практические проблемы применения графовых представлений:
- Необходимость хранения полного графа исходных знаний для точного пересчета вероятностей
- Вопросы доверия к различным источникам информации
- Сложность ручного создания структурированных графов для предметных областей

В ответах на вопросы были отмечены:
- Ограниченное практическое применение таких систем (хотя Discovery Витяева используется в закрытых проектах)
- Ограничения предсказательной способности на несколько шагов вперед
- Важность контекста при аппроксимации и интерпретации данных

Семинар завершился дискуссией о необходимости учета контекста в функциональных аппроксимациях и о том, насколько эффективно различные подходы могут решать сложные задачи ИИ.







S02 [00:00:09]  : Коллеги, всем добрый вечер. Сегодня мы начинаем очередной семинар русскоязычного сообщества разработчиков общего и сильного искусственного интеллекта. И сегодня изначально планировалось продолжить разговор, который мы начинали с антологии, с систем редактирования антологий. редакторов онтологии, потом говорили о DSL. И остался хвостик, который я хотел чуть-чуть рассказать о своем видении того, что называются нечеткие знания Ризенгера. Но поскольку хвостик оказался маленький, я решил, чтобы было интереснее сделать некоторую вводную которая мне кажется обозначить некоторые принципиальные вещи вот поэтому будет две части сначала будет некоторая такая разнообразная вводная а потом собственно перейдем немножечко и поговорим про нечеткие знания резонеры вот поскольку народу у нас немного то можно в свободном режиме, если будут возникать какие-то вопросы по ходу, то перебивать и задавать вопросы. Итак, сейчас мой экран видно, да? Да, видно. А с чего хочется начать? С того, что есть такое понятие, как эквивалентность. В том числе и некоторая функциональная эквивалентность. Речь идет о чем? О том, что мы тут спорим, прыгая заранее вперед, нужно ли решать задачу с помощью графов. задачу нашего построения общего искусственного интеллекта с помощью графов или с помощью нейронных сетей. Нужно ли обратное распространение или не нужно обратное распространение? Если строить системы вероятность на выводы, то на основе какой логики их строить? Байосовской или нечёткой логики? Но надо помнить, что существует ряд задач, которые можно с тем или иным успехом решить разными способами. В том числе, одну и ту же функцию можно аппроксимировать тоже различными функциями. И уж коли в некоторых случаях и в некоторых кругах говорят, что весь искусственный интеллект и весь Data Science в частности это просто аппроксимация, давайте посмотрим на аппроксимацию с точки зрения вот некоторой возможной эквивалентности различных методов аппроксимировать какую-то функцию. Предположим, что мы хотим описать слона. Которого разные философы и разные математики могут осматривать с разной стороны. И предположим, что одной из функций, которую описывает слона, является функция, которая описывает его поверхность. Вот мы смотрим на слона и видим, например, что у него такой хобот. Слева внизу, направо вверх поднимается такой приподнятый хобот. И мы хотим этот хобот чем-то аппроксимировать. И выясняется, что если мы этот хобот аппроксимируем на некотором конечном отрезке, ограниченном этими двумя точками, мы его можем аппроксимировать гармонической функцией на определенном интервале с определенной частотой. Или можем аппроксимировать некоторой степенной функцией, опять-таки на определенном интервале с определенной степенью, с определенным коэффициентом. Можем аппроксимировать кусочками Вейвлета. с определенными коэффициентами. Более того, мы можем найти некоторый кусочек белого шума и, вырезав кусочек этого белого шума, аппроксимировать вот эту целевую функцию. При этом, естественно, если мы задумаемся о том, что на самом деле функции могут быть не такие красивые и не такие гладкие, они могут быть всякие разнообразные, то например временные ряды, которые очень любят предсказывать люди, которые занимаются финансами, то нам придется эту функцию описывать не одной гармоникой и не одной степенной функцией, не одним вавилетом, а рядами. То есть либо рядами Fourier мы аппроксимируем какую-то функцию, либо аппроксимируем эту функцию каким-то набором полиномов, Либо мы ее аппроксимируем каким-то набором вавилетов, либо мы ее аппроксимируем каким-то набором функций белого шума. Причем мы должны понимать, кто занимается наукой о данных, наверное, это очень хорошо понимает, что мы очень можем хорошо аппроксимировать каким-то набором различного рода функций или различного рода рядов любую функцию на каком-то интервале. на котором мы можем осуществить подбор коэффициентов, но за рамки этого интервала нам ничто не гарантирует. То есть, если вот эта целевая функция, если хобот слона на самом деле окажется частью синусоиды, то полиномы не подойдут. А если он окажется, допустим, наоборот, экспонентой, то не подойдут ряды фурьер. на достаточно большом интервале. Кстати, насчет белого шума есть очень хороший показательный пример, с которым мы столкнулись. Мы занимались построением предсказательной модели для финансовых рядов. И для того, чтобы увеличить точность наших предсказаний, мы делали все больше и больше фич. И в какой-то момент мы обнаружили, что чем больше фич мы включаем в алгоритм, Причем не важно какой алгоритм, это могла быть и линейная регрессия, это могли быть и нейронные сети типа Multilayer Perceptron. Чем больше мы фич подключаем, тем больше, тем более точно мы можем построить модель, которая будет предсказывать нашу функцию на трене. Но как только мы начинаем уходить на тест, то там все разваливается, да? При этом, чем больше мы берем фич, начиная с какого-то момента, тем лучше мы описываем трейн, и тем хуже у нас все осуществляется на тесте. Тем хуже у нас предсказание на тесте. То есть у нас мы, по сути, да, при этом оказалось, что предсказать данную функцию фактически невозможно, по крайней мере, на том наборе данных, с которыми мы работали. Что мы делали? То есть на самом деле мы брали много фич, каждая из которых по сути была белым шумом. И множеством этих разнообразных фич, просто за счет увеличения большого числа этих фич мы аппроксимировали любую функцию на том интервале, на котором мы тренировались. А на тестовом интервале мы не могли ничего разумного предсказать. Кончилось все тем, что просто мы одним из гиперпараметров ввели количество фич. которое мы берем, максимальное можно взять, и метрика, по которой мы эти фичи отбираем, с тем, чтобы каким-то образом сбалансировать ухудшение качества при увеличении числа фич. Таким образом, алгоритм обучения заключался не только в подборе фич и неподборе коэффициентов самой нейронной сети, но и в алгоритме отбора этих фич и в их количестве. Но когда мы говорим о предсказании целевой функции, допустим, хобота слона на конкретном интервале, очевидно, что, описав хобот, мы обнаружим, что если мы начнем описывать спину слона, то эта функция не подойдет, и нам придется брать какую-то более сложную функцию. А если мы хотим описать самого слона, то нам уже нужно будет какой-то ряд функций Безье использовать, которые будут описывать вот этот вот контур слона на поверхности. Если нам нужно будет описать трехмерного слона, нам понадобится семейство функций Безье, которые описывают слона в трехмерном пространстве. А если этих слонов может быть много, или они будут в разных позах, то у нас возникнет, по сути, бесконечное количество функций без Е, которые мы, в принципе, никогда не сможем подобрать. И поэтому возникает задача структурированного машинного обучения, когда мы отдельными функциями, более простыми, описываем отдельно хобот, отдельно ухо, отдельно каждую из лап, более высокоуровневая функция, их структурируют, описывают некоторую функцию вот этих вот функций. И таким образом на различных уровнях нашей модели мы описываем вот этого самого слона. Забегая опять таки вперед, многие наверное догадываются, а некоторые знают, что вот эта вот иерархия различных структурного описания модели, которую мы описываем с помощью в случае искусственного интеллекта нейронной сетью многослойной определяется как раз вот различными слоями, то есть на различных слоях описания пространственных визуальных объектов находится фича более высокого агрегирующего порядка. Ну а когда мы описываем то же самое гиперграфами, например гиперграфами типа OpenCock Atomspace. Это делается через иерархию гиперграфов. Соответственно гиперграф более высокого уровня включает себя под гиперграфы более низкого уровня и образуется некоторая иерархия. Итак, идем дальше. Я хотел перейти к другому роду эквивалентности. Эквивалентности не функции, а того, как мы эти функции находим. Антон, а вы в чат будете смотреть? В чат мне смотреть очень трудно, потому что мне тогда нужно выходить из режима разделения экрана. 

S01 [00:09:35]  : Можно я маленькую заметку скажу? Да, давайте голосом. Вот когда мы говорим про поверхности, ну это ваш пример просто, но раз мы имеем пример. то там все сплайнами делается и делается любая поверхность. 

S02 [00:09:49]  : Совершенно верно, ну вот сплайн, то есть сплайн это частный случай полинома. 

S01 [00:09:59]  : Да. Ну он частный, но он годится для любой поверхности. Окей. Для аппроксимации любой поверхности. Окей, хорошо. 

S02 [00:10:11]  : Проблема в том, что слона можно описать бесконечным количеством поверхностей. Соответственно, модель слона, если мы каждого отдельного слона опишем отдельным сплайном, то для того, чтобы описать полного слона, нам потребуется бесконечное количество сплайнов, умноженное на бесконечное количество сплайнов, потому что каждый слон может иметь бесконечное количество постов. 

S01 [00:10:37]  : Переход к иметь, то есть возможным вариантам, это совершенно другая песня. А вот когда поверхность уже предъявлена целиком, она целиком может быть сформирована с любой степенью точности сплавить. 

S02 [00:10:52]  : Окей. Я же спорить не буду. Я хотел сказать, что мы, в общем, одну и ту же функцию можем описывать разными способами. То есть вот это был мой тезис. понятно и так идем дальше значит если мы поговорили идти дальше то Я хотел переключиться к тому, что называется функциональная эквивалентность, и перейти к той теме, в которую я на самом деле попал в историю с искусственным интеллектом. В историю с искусственным интеллектом я попал из геофизики. А в геофизике первая задача, с которой я серьезно столкнулся, это была задача томографии. Как выглядит задача томографии в общем случае? Предположим, что у нас есть некоторый объект, допустим, двумерный объект. Этот двумерный объект представляет собой некоторый прямоугольник, в котором находится какая-то среда. Это может быть фундамент здания или это может быть молочная железа. Или это может быть там чья-то нога, которую нужно посмотреть, что у неё внутри. Или это чья-то голова. И коэффициент имюн со значком Ж обозначает плотность вещества в некоторой точке. Ну или не плотность, там это может быть всё, что угодно. Мы можем в принципе любой параметр таким образом описать. И G это по сути координата. То есть можно было бы написать Muxy, но мы пишем MuG, который просто является некоторым индексом любой точки в этом пространстве. И для того, чтобы изучить эту среду, мы эту среду просвечиваем лучиками в различных направлениях. И каждый лучик определяется индексом i. И, соответственно, каждый из лучиков W с индексом i пересекает конкретную точку j. Причем таких точек j много. И в каждой точке каждого лучика, соответственно, появляется некоторая W и j. Это метрика того, что происходит с лучиком, когда он пересекает с собой эту точку. И мы имеем возможность, просвечивая эту среду различными лучиками, получать значение функции, допустим, звука, цвета, гамма-излучения, какого-то другого излучения, то есть любое физическое поле, которое проходит через среду, мы можем померить и дальше составить систему линейных уравнений. В общем случае, система может быть не линейных уравнений. А в случае, если мы изучаем не просто значение амплитуды или интенсивности какого-то сигнала, а получаем полноценное волновое поле, это будет уже система дифференциальных уравнений. Но так или иначе, мы имеем некоторые регистрируемые сигналы G, которые определяются набором некоторых коэффициентов, описывающих то, как эти лучики проходят по среде. Вот эта вот матрица, она описывает то, как эти лучики по среде ходят через каждые конкретные точки. И значение искомого параметра в этой среде. Собственно, вот базовой теории и практике вот этой самой томографии, которая возникла изначально в медицине, известно, что есть два подхода, с помощью которых эти системы линейных уравнений можно решать. Причем я не буду сейчас говорить про различные способы системы решения линейных уравнений. В частности, поскольку я не математик, я не буду брать на себя лишнего. С точки зрения конкретно вычислительной томографии подхода два. Первый способ это метод, что называется свёртка обратной проекции. Это так сказать аналитический способ, который имеет в своей основе некоторую достаточно сложную формулу, включающую в себя дифференцирование свёртку и оператор так называемого обратного проецирования, который по сути, как мы чуть-чуть позже узнаем, является аналогом того, что называется обратное распространение в глубоких нейронных сетях. вот это один способ да то есть мы как бы имеем вот эту вот матрицу которую мы можем построить зная параметры вот того объекта которым изучают и геометрию всех лучиков мы знаем вот этот вот набор измерений, которые мы получили в результате сканирования со всех сторон исследуемого объекта, и мы просто пропускаем это все через некоторую формулу, через некоторый алгоритм конечной сложности, который на выходе нам дает некоторое распределение этих коэффициентов. При этом на практике конкретно медицинской томографии, например, было известно, да, вот я с этой историей столкнулся с позицией применения данного метода в геофизике, вот, о медицине было известно, что, в общем, вот да это первый способ это собственно аналитическое решение а второй способ это решение итерационная методом итерационного обращения матриц то есть на самом деле метод значит итерационный заключается в том что мы постепенно подбираем по сути по тому или иному алгоритму, как правило основанному на той или иной разновидности градиентного спуска, мы итерационно подбираем все более и более точные значения этих коэффициентов, чтобы отклонение расчетной левой части было как можно ближе к полученным результатам. То есть, например, мы вначале говорим, что все μ равняется нулю, узнаем левую часть, рассчитываем значение g, получаем распределение ошибки. Это распределение ошибки мы каким-то образом раскидываем обратно по этим μ, смотрим, стало лучше. стало если значит посчитаем новое распределение ошибки снова его корректируем и так до тех пор пока у нас не перестанет уменьшаться ошибка или пока ошибка не станет существенно меньше заданной. Так вот выясняется, что конкретно в медицине первый метод гораздо более эффективный. Во-первых, он вычислительно более простой. Во-вторых, он дает более высокое разрешение. Второй метод, он вычислительно более сложный, он требует больше времени, поскольку требуется неопределенное количество итераций в общем случае. И результаты получаются просто более размытые и менее качественные. То есть он обладает меньше разрешающей способностью. Поэтому в медицине используется метод свёртки обратной проекции. И объясняется это в частности с тем, что большинство объектов, которые в медицине рассматриваются, во-первых, характеризуются достаточно маленькими вариациями вот этого параметра μ. То есть, отклонения плотности в тканях человеческого тела плюс-минус. Я не беру, сказать, значения, потому что с медицинской частью это я, собственно, плохо знаком. Но там порядка, скажем так, с каких-то процентов. Может быть, первых десятков процентов. То есть, вариации небольшие. А во-вторых, что важно, медицинской томографии можно обеспечить достаточно равномерное подкрытие. То есть, сетки достаточно регулярные. А вот если мы возьмём геофизику, например, в геофизике, то мы напримерим, что у нас перекрытие среды и регулярность наблюдения этих моделей существенно ниже. Например, у нас здесь показан результат изучения некоторого объекта, который находится в середине. Наверху у нас находятся источники сигналов, внизу у нас находятся приемники сигналов. Вот мы просвечиваем сверху вниз, собираем лучи, которые проходят по всем различным направлениям и решаем обратную задачу. И вот на верхней картинке показано, как мы находим карту изменения скоростей исследуемого объекта. На нижней картинке показано, как мы находим карту распределения коэффициента поглощения, в сейсмических данном случае волн, в исследуемом объекте. И мы обнаруживаем две интересные вещи. Во-первых, если мы исследуем такой параметр, как коэффициент поглощения, то нам лучше использовать метод, который вот здесь используется, основанный на этой пресловутой свертке обратной проекции. То есть, когда мы, по сути, обратная проекция – это что такое? Это мы, грубо говоря, взяли, так сказать, что вот если у нас сигнал прошел, значит, отсюда досюда, то мы каким-то образом значение сигнала размазываем по всему лучу. Если он прошел отсюда досюда, то мы значение этого сигнала размазываем по всему лучу. После чего у нас получается очень размазанная картина, но поскольку мы знаем структуру прохождения этих лучей через среду, мы можем построить некоторый фильтр, который это размазывание скорректирует. И наложив этот фильтр, корректирующий на полученную картинку, мы получим как бы уже более чёткую картинку, ожидаемую. И вот практика показывает, что если отклонения параметра среды небольшие, то здесь вот этот вот алгоритм свёртки обратной проекции, грубо говоря, он более эффективен, как и в медицине. А вот если мы будем изучать скорости распространения сейсмических волн, которые могут меняться гораздо в более широких диапазонах, то есть отклонения там могут быть на десятки и сотни процентов от среднего, то этот алгоритм не работает в принципе. То есть там могут работать только итерационные методы. Соответственно, вроде как задача примерно одна и та же, но разные параметры. эти параметры и требуется применение различных алгоритмов то есть в принципе и тот и другой подход основанный на аналитическом обращении функции и основанный на итерационном обращении функции и там и там те алгоритмы и тот и другой применимы и теоретически и практически но с точки зрения практики они дают совершенно разные результаты и в одном случае Лучше использовать один, в другом случае другой. Это как бы заход на то, что какие-то задачи может быть имеет смысл решать с помощью графов и вероятности логики, а какие-то с помощью нейронных сетей и обратного распространения. Двигаемся дальше. Уж коли мы заговорили про геофизику, мне хотелось бы обозначить некоторые интересные аналогии. Первое смысл на эту тему, что вообще говоря, многие вещи, которые на сегодняшний день бурно обсуждаются и рассматриваются в машинном обучении, в искусственном интеллекте и в data science, они на самом деле десятки лет назад изучались в геологии и геофизике без вот этих названий. То есть на самом деле большие данные, они появились когда появилась цифровая геофизика. То, что называется сейчас feature engineering, это было такой рудиментарной и обыденной техникой при геологическом прогнозировании. То есть точно так же, как сейчас, дата-сайентисты подбирают для каждой конкретной задачи, фичи, с помощью которых можно строить предсказывающие или обнаруживающие модели для того или иного объекта. В предмете изучения точно так же геологи и физики подбирали поисковые признаки, которые могут с максимальной вероятностью предсказать наличие на карте или на геологическом разрезе искомого объекта. методы градиентного обучения да как я уже сказал вот поскольку значит и как мы чуть-чуть еще дальше поговоримся к этой теме поскольку и нейросетевые модели да они могут и собственно предметы которые мы Пытаемся описать или распознать с помощью нейросетевых моделей. Они описываются на самом деле не матрицами, они описываются тензорами. Кстати, та же самая геофизическая томография, например. Она в общем случае не сводится к решению системы линейных уравнений в виде матрицы. На самом деле любой трехмерный геологический или физический объект, он описывается как минимум тензором третьего порядка, потому что у нас есть три координаты x, y, z. А если в каждой точке среды у нас есть еще несколько параметров, то это у нас уже получается тензор четвертого порядка. То есть это четвертое измерение. А если мы скажем, что у нас свойства этого объекта меняются во времени, то у нас получается тензор пятого порядка. Ну а соответственно объекты пространственные, они опять-таки у нас описываются в некоторой иерархии. То есть если у нас там ухо слона и тело слона, это у нас уже получается иерархия как минимум двух систем тензоров или двух уровней гиперграфа. Ну а если мы говорим про обращение, то вот точно те же методы градиентного спуска, которые сейчас используются для построения коэффициентов нейросети, они использовались, вот с левой стороны показано, как этот метод градиентного спуска используется для построения поля скоростей сейсмических волн. по наблюдению сейсмических волн на поверхности с помощью вот тех же самых методов. Вот здесь вот видно наверху то, что получается после первой итерации, условно говоря, первая обратная проекция, когда мы получаем некоторое размытое изображение, а потом методом последовательного обратного проецирования. то, что в нейросетях называется обратным распространением ошибки, а в геофизике это называется обратным распространением коррекции. Значит, мы, последовательно распространяя вот эту вот коррекцию, мы постепенно получаем все более и более точный результат, который результаты моделирования поля, значит, сейсмического, на котором дают все более и более результаты, все более и более приближенные к наблюдаем вот здесь даже вот показан вот это тут схема этого алгоритма как по циклу этот подбор осуществляется другой прекрасный пример значит это свертка да вот сверточные нейронные сети вот замечательная вот это вот картинка которая много таких картинок появилась не так давно. В геофизике это давно использовалось, по сути, оператор свёртки, двухмерной свёрки или одномерной свёрки для обнаружения определённого типа аномалий или сигналов. То есть, если мы примерно знаем, как выглядит, условно говоря, кемберлитовая трубка или сигнал, отражённый от поверхности нефтяного пласта, то мы можем построить оператор свёртки, который будет обнаруживать вот эти соответствующие аномалии. И напустить на исходную карту этот оператор свёртки, и он нам вот такими пупырышками покажет все места, где, скорее всего, находится интересующий объект. Точно так же, как при поиске собак на изображениях, мы можем натравить оператор, описывающий собачью морду, на любое изображение, и он найдёт всё то, что походит на собачью морду. Итак, формулируем. Тезис очередной эквивалентности различного рода задач. Хочется сказать следующее. Что у нас есть так называемая прямая задача, которая в простейшем виде описывается на системе линейных уравнений. Но это может быть и система нелинейных уравнений, это может быть и тензор. И это может быть система тензоров. В простейшем виде у нас есть некоторые, если это геофизика, то параметры среды. И это некоторые условия измерений, в данном случае коэффициенты. И у нас есть некоторые результаты измерений, которые в случае решения прямой задачи нам нужно рассчитать. То есть у нас есть известная среда, которая описывается X. Есть условия измерений, которые описываются через A. И мы расчетным методом можем получить то, что мы получаем на выходе при осуществлении измерений. Если мы в эту же рамку загоняем задачу машинного обучения, то у нас входные данные это A, коэффициенты нейросети это X. Соответственно, те предсказания, которые нейросеть генерирует на основе входных данных A с помощью той модели, которую мы в данный момент имеем, дает нам B. Дает нам, по сути дела, предсказание. И дальше возникает вопрос. У нас вот эти вот X, мы их задали вручную или мы их каким-то образом выучили? Есть обратная задача. Если мы берем мою любимую геофизику, в обратной задаче у нас условия измерений А также задаются. Результаты измерений нам также задаются. А дальше у нас задача. Вот эти самые параметры среды x нам нужно как-то рассчитать. Либо аналитическим методом обращения вот этих результатов в x через условия измерений, либо итерационно. Если же мы берем, опять-таки, наше любимое машинное обучение, то у нас в качестве A это входные данные, которые нам тоже известны, в качестве B у нас разметка, а коэффициенты X нейросети нам нужно выучить, опять-таки, либо методом обратного распространения ошибки, то что мы называем в глубоких нейронных сетях. в многослойных нейронных сетях, либо каким-то аналитическим решением, которого вроде как пока для нейронных сетей не предусмотрено по той простой причине, что в подавляющем числе случаев функции искусственных нейронов обычно нелинейные, и поэтому аналитическое решение построить трудно. Ну и обобщенная постановка задачи, таким образом, дальше от которой мы будем плясать, выглядит следующим образом, что у нас есть некоторые входные данные, и у нас есть некоторые выходные данные. Если мы решаем обратную задачу, мы из входных данных и выходных данных, мы осуществляем некоторое обучение, кластеризацию, feature engineering, engineering и поиск паттернов, которые в каком-то виде А дальше будем говорить о том, в каком виде складываем в модель. Если же мы решим прямую задачу, условно говоря, то у нас есть входные данные и у нас есть модель. И на основе входных данных и модели мы генерируем выходные данные, то есть предсказания. И, собственно, на этом оканчивается практически вводная. Здесь какие-то есть комментарии или вопросы? Нет. Прекрасно. Тогда идем дальше. Сейчас, наверное, чтобы пойти дальше, я переключусь на другой слайд. Теперь вопросы, которые возникают. Мы только что сказали, что у нас есть модель. То есть, теперь как нам эту модель... описать. Но, грубо говоря, есть два варианта, как эту модель описать. Первый способ описания модели – это пресловутая матрица, либо тензор, либо некоторая система тензоров, либо это иерархия тензоров. А на самом деле иерархия тензоров – это, по сути, разреженный тензор. Может быть описано как некоторым разреженным тензором достаточно высокого порядка. Либо мы описываем эту модель в простейшем виде некоторым графом, а в сложном случае некоторым гиперграфом. Где под гиперграфом мы подразумеваем, что у нас вершина каждого... Каждая вершина или даже каждая связь, в зависимости от того, как мы на это смотрим, может раскрываться в подграф. То есть вот мы, грубо говоря, строим социальную связь между людей, это у нас граф, так сказать, одного уровня. Но если мы каждого человека разберем на его собственной мотивации, жизненную историю, интересы и модели поведения, это получится подграф. А если мы этого человека, эту группу людей рассмотрим как некоторый социум во взаимодействии с другими социумами, мы получим некоторый суперграф. И вот мы можем строить некоторые иерархии вот этих вот подграфов и надграфов в гиперграфическую структуру в той или иной реализации. Реализация может быть разной, но мы сейчас про реализации как раз поговорим. Это, соответственно, либо мы это описываем графом-гиперграфом, либо это мы описываем матрицами-тензорами. И здесь возникают следующие вопросы. Первое. Какая для нас модель предпочтительная? Или если у нас в разных случаях предпочтительны разные модели, то что мы храним в одной модели, что мы храним в другой модели? А с третьей точки зрения, а имеет ли смысл и когда и как переход от одного представления в другое? Имеет ли смысл, например, допустим, сперва сформировать модель в виде тензора или тензоров, а потом перевести ее в гиперграфы? Или наоборот? Это вот, значит, одна развилка, которая представляет интерес для дальнейшей дискуссии. Значит, второе, это откуда мы эти самые тензора, или графы, или гибрографы берем. Если у нас речь идет о хранении моделей в матрицах слэш тензорах, и если мы формируем эти модели методом обратного распространения ошибки, итерационным подбором, модели под тренировочный корпус или под наблюдаемую реальность то тут вроде как все просто значит обратное распространение значит это у нас обучение а Или прямая задача в терминах геофизики. А обратная, точнее наоборот, обратная задача. А прямое распространение это у нас inference. То есть когда мы формируем модель по имеющейся реальности, это у нас обучение. А когда мы имея имеющуюся реальность модель получаем предсказание этой имеющей реальности или недостающие свойства реальности, это у нас inference. Тут вроде как все просто. А как нам быть, если мы храним данные в виде графа-гиперграфа? С инференсом вроде как все просто. Если у нас есть граф, если у нас тем более этот граф вероятностный, то есть у нас к каждому ребру или к каждой вершине, тут опять-таки возможны варианты этого графа, привязана некоторая вероятностная метрика или некоторая вероятностная оценка, то мы можем с помощью той или иной логики, вероятностной, про которую мы сейчас тоже поговорим, рассчитать, значит, вероятность исхода, допустим, из точки A в точку B или вероятность происходить события B в случае наличия событий X и Y. Вот. А вот откуда эти самые графы или гиперграфы взять? С одной стороны мы можем их гипотетически взять из тензоров. То есть мы можем сказать, что если мы с помощью обратного распространения ошибки построили некоторую модель в виде тензоров, то должен быть какой-то гипотетический алгоритм, который в результате прунинга ненужных связей и лейблинга оставшихся связей даст нам семантический граф или гиперграф. Но пока что успешных и работающих решений подобного рода, по крайней мере мне неизвестно, я не слышал. Это с одной стороны. С другой стороны, если мы говорим про обучение, то вообще есть два вида обучения. значит один вид обучения это что называется обучение на опыте да или то что называется тренировка или experiential learning обучение на собственном опыте обучение методом проб и ошибок есть обучение дидактическое или директивное да то есть в одном случае мне говорят значит вот копай отсюда и до обеда то есть я получаю конкретную установку или мне говорят вот если ты увидишь знак кирпич туда не ездить и я принимаю это знание и руководствуюсь этим знанием то в случае обучения с ошибкой я должен 10 раз проехать на знак кирпич меня 10 раз оштрафуют я в итоге выучу что на кирпич ездить нельзя или я прокопаю 10 раз разные стороны не получу зарплаты за то, что я копаю в разные стороны, пока, наконец, случайно не прокопаю в нужную сторону и не получу за это зарплату. Соответственно, мы можем предположить, что, наверное, есть две формы обучения. Одна форма обучения дидактическая или инструктивная, когда мы получаем некоторые инструкции, воспринимаем их и их выполняем. И в этом случае мы можем знания хранить в виде графовых представлений. С другой стороны, возможно, мы эти графовые представления, которые получаем методом инструкции или методом передачи этих инструкций от других людей или чтением их в какой-то литературе, мы эти знания можем загрузить в тензорное представление для того, чтобы осуществлять инференс. Либо мы в другую сторону получаем вести опыт методом проб и ошибок, загоняем этот опыт в тензорные представления, и потом при необходимости объяснить, например, эти тензорные модели кому-то, или в том числе самим себе, мы извлекаем оттуда эти знания уже в семантическом или в графовом представлении. Итак, мы переходим к вопросу о представлениях. Сейчас какие-то вопросы и комментарии есть? Хорошо. Теперь переходим к продолжению разговоров, которые были на прошлых семинарах. Предположим, мы хотим построить некоторую систему, которая хранит представление в виде графов или гиперграфов, и представляется, что нам для этого нужны четыре вещи. Здесь слайд представляет информацию неполную, он слегка устарел, и часть информации уже не актуальна, но принцип он некоторым образом иллюстрирует. Во-первых, у нас должно быть некоторое описание того, что называется не онтология верхнего уровня, а мета-онтология, или то, что я называю базовой онтологией. То есть, мы должны договориться о том, как у нас описываются некоторые базовые объекты, из которых мы собираем всю остальную. Ну, например, допустим, Базовая антология Java, языка программирования Java отличается от базовой антологии языка программирования C++. Потому что в языке программирования Java различную функциональную нагрузку несут классы интерфейсы. В классах множественного наследования нет. Но там есть и методы, и функции. А интерфейсы предполагают множественное наследование, но там нет переменных, а есть только функции или методы. А в СИ наоборот, а в СИ по-другому, в СИ базовое множественное наследование может быть и у классов, может быть у классов множественное наследование, но там нет интерфейсов, и у классов с множественным наследованием могут быть переменные и, соответственно, методы. Точно так же разными структурными свойствами и структурными возможностями описания объектов прикладного уровня характеризуются антологии, как Psyche, антология Freebase, есть еще Wikidata, есть DBpedia. Есть верхняя мета-онтология OpenCog, есть верхняя онтология моего проекта WebStructor. Есть случаи, как, например, в проекте Николая Рабчевского, который у нас на нескольких семинарах докладывался, или Константина Дьяченко, который у нас недавно вошел в сообщество, но развивает свой проект достаточно давно. Значит, у них принцип такой, что никакой фундаментальной оснологии вообще нет. То есть, у Николая Робчевского всё строится на бинарных связях. То есть, ничего кроме бинарной связи нет. То есть, мета-онтологией у Николая Робчевского является бинарная связь. Точка. Вот. Соответственно... И похожее решение в мета-онтологии Константина Диченко, но я не буду говорить, пусть он лучше сам в какой-нибудь момент расскажет о том, как у него все устроено, но там ситуация похожая. То есть, если у Николая Робчевского все есть связь, то у Константина Диченко тоже все есть связь. Но только смысл той связи разный. У Воробьевского она бинарная, а у Дьяченко она н-арная. В связи с чем я сильно предполагаю, что конструкция Константина Дьяченко на самом деле соответствует революционной алгебре кода, где просто всё есть в отношении. Это первое, что нам нужно. Второе, что нам нужно. Нам нужно некоторым образом работать со временем. И здесь, к сожалению, нет у нас Александра Балдачева, который на предыдущем семинаре рассказывал про свою систему. И приятной и полезной функцией решения Александра Балдачева и его так называемой событийной онтологии является то, что там без времени нет вообще ничего. С моей точки зрения, там у него понятие времени несколько даже гипертрофировано. Не будем сейчас это без него обсуждать. Но, тем не менее, любая система для работы с какими-то реальными знаниями о мире и для построения моделей, которые с этими знаниями о мире работают, и вывода на этих моделях, она обязательно должна некоторым образом иметь информацию о времени. Чего нету, насколько мне известно, в большинстве решений, связанных с графами. Большинство существующих графовых систем позволяют хранить трипл-стор. Трипл-стор мы можем загнать либо простой граф, либо в лучшем случае какой-нибудь гиперграф. А загнать гиперграф, который описывает динамику изменения чего-то во времени мы не можем, потому что это там из коробки не предусмотрено. Соответственно, если мы хотим строить какую-то систему, которая будет работать с событиями, изменяющимися во времени, то нам нужно либо брать базу данных, Вот где предусматривать поля времени и некоторые способы индексации триплетов или энерных отношений, если мы хотим работать с гиперграфами. Либо мы должны брать некоторый triple store и туда каким-то образом прикручивать время индексирования по времени. Либо мы должны брать базу данных, ориентированную на временные ряды, и к ней прикручивать триплеты. Либо мы должны брать гипотетически OpenCoke Atomspace и его соответствующим образом использовать, как вариант. В моем проекте e-agents я использую систему временных графов, ограничением которых является, что работа возможно только на уровне дней. То есть единица измерения времени в e-agents temporal graphs является день, что устраивает для анализа новостей на том временном горизонте, на тех временных масштабах, с которыми в моем проекте это работает. Но для многих практические задачи это слишком грубо и неприменимо. Во всем случае, эта задача является нерешенной в промышленности, как мне кажется. Третья составляющая того, что нам нужно, это некоторый язык. который будет позволять с этими графами манипулировать, если не на стадии конечного пользователя, то на стадии разработки и квалифицированного пользователя. И тут практика такая, что на каждую систему со своей мета-онтологией или онтологией верхнего уровня Есть свой собственный язык, да, то есть для OOL, RDF OOL есть, соответственно, Turtle и JSON-LD Spark OOL. У Cycle, у него был Cycle Language. В моей прошлой системе, про которую я рассказывал в прошлый раз, был язык URL, Object Relational Language. В системе семантического моделирования новосибирских коллег Италия Гумирова, Дмитрия Сверидьенко. У них используется язык D0SL. В OpenCog до недавнего времени был специальный язык Atomis, который сейчас заменен на язык Meta, который на самом деле является просто некоторым существенным расширением и более, скажем так, расширяемым расширением языка Atomis. В системе неосиматического вывода переванга используется язык нарсиз. В системе, в моем текущем проекте e-agents используется e-agents language. Есть аналогичный осиматический язык в системе Майкла Миллера, премис. Ну и, наконец, самая важная часть, которой предполагалось посвятить сегодняшнюю дискуссию, это пресловутый Reasoner, это пресловутый Inference Engine, которых мне на сегодняшний момент известно три. Первая система – это система NARS. неоксимосистема неоксимотическая вывода пиванга про которую я поговорю более подробно чуть-чуть дальше вот я поговорю чуть-чуть просто вот в рамках общего наброса но более подробный доклад о том как устроен нарс делась у нас на семинаре и я ссылку давал форум можно его нашу группу можно посмотреть Основным достоинством этой системы является то, что она является системой с открытым кодом. Второе, она реализует функцию управления и планирования ресурсов. При осуществлении логического вывода, что с моей точки зрения чрезвычайно важно, с точки зрения EGI, потому что по мнению как меня, так и таких товарищей, как Ян Ле Кун, экономические или энергетические соображения при принятии решений, они чрезвычайно важны. Вторая система, которая мне также хорошо знакома, это система Discovery Евгения Витяева, которая, к сожалению, доступна только в закрытом коде. Ну и третья система, мне известная, про которую мы тоже сегодня чуть-чуть поговорим, это OpenCorp Probabilistic Logic Network. Бена Гёрделя, со товарища, с точки зрения математики она по сути повторяет НАРС, но там есть своя собственная техническая реализация, собственно, движка и, в общем, своя целая, собственно, реализация инфраструктуры, интегрированная с OpenCock от Amspace. Итак, мы задаемся вопросом, если мы хотим строить нейросимвельный искусственный интеллект, в котором будет в том числе симвельная предсоставляющая, что нам нужно? Нам, соответственно, нужно некоторый inference engine, нам нужен некоторый семантический язык, нам нужна некоторая графовая база данных с с возможностью индексирования по времени. Есть некоторые базовые или фундаментальные элементы антология. Ну и здесь показана некоторая табличка тех проектов, которые мне известны и мне близки, с указанием тех лицензий, с помощью которых эти проекты могут быть доступны и могут распространяться. Собственно, теперь перейдем к тому, как работает логический вывод, логический резонер с точки зрения систем Пиванга и Бена Герцога, то есть это NARS и PLN. Предположим, что у нас есть некоторая графовая структура. И что мне теперь делать? слайдик перевернулся так как как обратно слайду вращать так сейчас мы его будем вращать обратно вернулся Предположим, что у нас есть некоторая графовая конструкция, которая определяет происхождение некоторых событий. То есть у нас есть явление плохого питания, которое может быть как-то связано со слабленной иммунной системой, которое может быть как-то связано с обострением или проявлением инфекции, которое может быть как-то связано с переохлаждением. И мы имеем некоторую историю наблюдения и фиксации, насколько те или иные события в жизни некоторых гипотетических выборок пациентов совстречались совместно. И мы можем, например, обнаружить, что в 9 из 10 случаев, когда мы одновременно фиксировали наличие плохого питания и ослабление иммунной системы, у нас совпало, что действительно из 10 раз, когда у нас была ослаблена иммунная система, 9 раз при этом перед этим еще было плохое питание. 9 из 10, окей. При этом мы знаем, что у нас в 90 случаях из 100, когда была инфекция, обострил физофиксировано, у нас произошло ослабление иммунной системы. Обратите внимание, тут 9 десятых, да, и тут тоже 9 десятых, но тут 9 из 10, а тут 90 из 100. То есть соотношение с точки зрения вероятности одно и то же, но объем evidence, да, объем, так сказать, доказательности, он с левой стороны и с правой стороны разный. Наконец, мы можем наблюдать, что также, когда мы совместно анализировали переохлаждение и плохое питание, мы в 6 из 10 случаев переохлаждения, у нас к переохлаждению предшествовало плохое питание 6 раз. Соответственно, тоже наблюдаем некоторую определенную связь. Ну и опять-таки мы можем зафиксировать, например, что у нас из 100 случаев инфекции в 60 случаях до этого наблюдалось плохое питание. И мы, глядя на эти стрелочки, можем выделить несколько ситуаций. Первая ситуация. У нас есть ситуация, когда у нас есть высокая связь. Точнее вот у нас высокая связь. То есть 90 и 100 это высокая связь. Причем 100 это достаточно много. Будем считать, что 100 это представительная выборка. Соответственно у нас есть высокая сила связи и высокая уверенность в том, что эта оценка достоверна. С левой стороны у нас опять таки есть достаточно высокая связь 9 из 10, но поскольку 10 это выборка менее представительная чем 100, мы можем сказать, что у нас сила связи высокая, но уверенность она низкая. Если мы возьмем связь между плохим питанием и инфекцией, мы обнаружим, что у нас уверенность высокая, выборка достаточно большая, но 60 из 100 это не очень много, это почти что 50 из 100. Можно монетку бросать. Ну и когда мы берем плохое питание и переохлаждение мы видим 6 из 10 что в общем у нас и confidence уверенность маленькая и соответственно вероятность на связь тоже маленькая. И вот эти два понятия, strength и confidence, это два фундаментальных базовых параметра, на основе которых строится вероятностная логика Пиванга и Бена Герцеля и его проекта Probabilistic Logic Networks. И на основе этих двух параметров строится алгоритм логического вывода, который позволяет осуществлять вывод, оценку вот этого, что называется truth value, то есть пара, комплексное значение силы и уверенности. strength and confidence это так и называется truth value или compound truth value составное значение истинности вот для всех для четырех типов вот этих вот логических операций собственно определяются вот то как формируются составные значения истины Соответственно, первое правило – это правило ревизии, которое позволяет агрегировать множество связей, которые связывают одноименные вершины. Соответственно, если мы зафиксировали много связей между плохим питанием и ослабленной иммунной системой, то у нас правило ревизии позволяет построить агрегирующую связь, которая агрегирует индивидуальные связи с индивидуальными значениями истинности в одну. Это вот один способ построения связи. Вторая связь – это дедукция. Соответственно, если у нас есть связь от плохого питания к ослабленной иммунной системе, а из нее от ослабленной иммунной системы есть связь к инфекции, то правило дедукции позволяет, соответственно, объединить эти две связи и построить на основе составного значения истинности этой связи и этой связи составное значение истинности более протяженной связи соответственно индукция это вот от плохого питания к ослабленной иммунной системе и от плохого питания к переохлаждению мы можем попытаться построить связь о том, как переохлаждение связано с ослаблением иммунной системы или наоборот, ну и абдукса это в обратную сторону, как от ослабленной иммунной системы или переохлаждения приводящего к инфекции, можно сделать, с какой вероятностью можно сделать заключение о том, что одно является причиной другого. Соответственно, наборы из этих четырех формул определяют как раз базовую аксиоматику систем Пиванга и Бена-Герцеля. Здесь какие-то вопросы есть? 

S03 [00:55:21]  : Да, есть вопрос, Антон, можно такое? Конечно. Количество параметров достаточно маленькое, их получается всего три. Плохое питание, слабая иммунная система, инфекция. А если параметров будет значительно больше, потому что, допустим, здесь не учитываются факторы какие. Если плохое питание, ослабленная иммунная система, но, допустим, человек потом принимает какие-то лекарства, либо изменяется погода у нас с холодной на теплую. Когда у нас не четко разделены эти параметры, а их достаточно больше, и на каждое из этих трех может влиять еще по десятку, допустим. У нас эта сеть будет гораздо большего размера. Как в этом случае оценки эти будут интерпретироваться, скажем так. 

S02 [00:56:16]  : Ну это уже вопрос следующий, то есть здесь вопрос в следующем. Первый вопрос это как осуществляется, в каком порядке осуществляется вот эта попарная связь. Да-да-да, вот именно порядок. Это причем порядок 2. Во-первых, с точки зрения структуры, То есть, откуда мы идем, от связей каких типов, к связям какого типа мы идем, это первое. И второе, как определяется приоритетность этих связей. То есть, допустим, у нас есть много разных связей, которые, допустим, ведут к инфекции. И мы хотим… Вот у нас, к примеру, есть одна связь к инфекции, у нас другая связь к инфекции. И у нас вязанка таких связей, которые гипотетически ведут к инфекции, у нас их может быть миллион, условно говоря. И к ним ко всем идет связь, к примеру, от плохого питания. И теперь мы хотим попытаться методом дедукции построить все возможные связи от плохого питания к инфекции. И подобных вопросов в графе вычислить мы можем очень много. И как мы их будем перебирать. Фишка проекта Пиванга заключается в следующем. что мы в первую очередь обрабатываем те связи, которые для нас более важны. Есть понятие импотенса, которого на этом слайде нет. Есть понятие важности связи с точки зрения организма. Насколько эта связь сейчас первая находится в фокусе внимания? Насколько эта связь ассоциирована с выживанием организма? Насколько эта связь ассоциирована с какой-то жизненно важной функцией? И соответственно предметом общета, предметом вывода являются в первую очередь те связи, которые имеют этот самый высокий импотенц. Это первый принцип. А второй принцип, что мы ограничиваем процесс логического вывода временем. То есть, ситуация простая. Нам нужно принять решение. Мы идём по лесу и видим, что что-то полосатое в кустах. И вот мы начинаем обсчитывать, что это такое. То ли это просто матрас там в кустах лежит, то ли это там тигр. Но у нас время для принятия решения конечное. Нам нужно в какой-то момент либо броситься бежать, либо спокойно идти дальше. Возможно разность. Или мы видим, что в нас что-то летит. И у нас конечное время. Либо упасть, либо отмахнуться рукой, либо пригнуться. И мы должны очень быстро принять решение, что мы падаем, машем рукой или пригибаемся. Так вот, мы начинаем с учетом приоритета пообсчитывать разные связи, проигрывать разные теории, на выходе которых является принятие некоторого решения, что нам делать, но в какой-то момент у нас уже время для принятия решения окончается, мы пересчет обрываем и принимаем то решение или делаем то действие, которое оказывается в данный момент максимально вероятным. Это вот как раз фишка Пи-Ванга. 

S03 [00:59:47]  : Понятно. Это, кстати, очень важный момент с этим приоритетом. 

S02 [00:59:52]  : Да, да. Вот как раз за что я люблю товарища Пи-Ванга, это вот именно за это, потому что он этот принцип сформулировал еще, по-моему, то ли в 93-м, то ли в 95-м году, а Ли Кун про это начал писать только в прошлом году. вот идем дальше да вот идем дальше значит теперь собственно это вот как раз вот на самом деле это сейчас было самое интересное с моей точки зрения часть разговора потому что это как раз было про резонера но теперь давайте проговорим про то что над чем эти самые резонера работают вот и что же такое пресловутый гипер граф варианте атом спейса Вот, например, еще раз напомню, что в модели OpenCock Atomspace, это графическая гиперграфовая база данных проекта SingularityNET, предполагается, что у нас любая связь во-первых, может иметь не только один аргумент или три аргумента, то есть бинарная связь ассоциативная, она там просто, если А, то Б, вот триплет она еще имеет тип, допустим, А является родителем Б, или там а является частью б или а является результатом б то есть есть еще тип связи это триплет вот в случае с гиперграф у нас каждая связь может иметь любое количество элементов вот и кроме того каждая вершина она в свою очередь также может быть объектом связи Таким образом мы можем строить сложные иерархические структуры. Например, вот здесь показана гиперграфовая структура, описывающая следующую конструкцию. Например, у нас есть причинно-следственная связь. Точнее, не причинно-следственная связь, а имеется связь типа последовательность, которая проходит через три события и соединяет их единой цепочкой. Клауд, появилось облако, пошел дождь, случилось наводнение. Это вот единая последовательная связь, которая говорит о том, что три события случаются одно за другим в определенной последовательности. Один вариант. Дальше. Есть другое понятие. Это классическая связь триплет. Это связь с определенным типом, у которых есть четыре аргумента. Есть унарная связь, которая является в чистом виде концептов. В open quantum space нет связи, там есть атомы. Соответственно, вот эта связь унарная, Fog, это просто, по сути, концепт с точки зрения обычных графов. А вот эта связь из трех элементов в последовательности, это уже связь третьего порядка с типом или связь четвертого порядка, если тип считать одним из аргументов. Есть связь более высокого уровня, это ассоциативная связь, по сути гиперсвязь, которая объединяет любое множество элементов. У нее нет последовательности, но она может объединять много элементов. светло-серая, она объединяет в себе одну унарную связь фок или концепт фок, что есть туман, и вот это вот последовательность, то есть она говорит о том, что вот в какой-то момент времени имел место туман и облако, приведшее к дождю и последующему наводнению, а туман типа вот он безотносительно к этому все время существовал на протяжении всей этой истории. Это вот одна трехуровневая уже конструкция. Первый уровень это мы на трех концептах построили одну связь. Это уже второй уровень. Потом мы рядом положили еще один концепт и все это объединили общим контуром. Получилась связь уже третьего уровня. Дальше мы говорим, что у нас, независимо от этого, была ещё одна конъюнкция. Высокая влажность и низкое давление. Соответственно, вот у нас есть совпадение высокой влажности и низкого давления, которое мы объединяем в N. А дальше, это как бы конструкция второго уровня. Тут у нас конструкция двухуровневая, тут у нас конструкция трехуровневая. И мы вот эти вот две конструкции соединяем связью типа implication. Следствие. Соответственно, наличие высокой влажности и низкого давления приводит вот ко всему этому букету. К туману и облаку, переходящему в дождь, переходящему в наводнение. Но это не всё, потому что вот эта связь, объединяющая вот эти два явления слов составных, это всё относится к некоторой предметной области. И эта предметная область объединена в некоторый набор, который называется set, или множество. И вот все это множество подобного рода импликаций, или средств, или функциональной зависимости, относятся к предметной области географии. Соответственно, inheritance link говорит о том, что вот все это множество явлений относятся к разделу знаний географии. Ну и, наконец, на языке атомиз это все описывается вот такой конструкции соответственно вот все что мы сейчас только что написали описывается одним выражением на языке атомиз на языке мета, наверное, это выглядит похоже. Причем это упрощенная формулировка, потому что поскольку любому овальчику и любой стрелочке вот на этой конструкции может быть ассоциировано вот то пресловутое составное значение истинности и truth value, соответственно, полная семантика языка Atomis, она предполагает, что на каждой строчечке справа в квадратных скомочках еще будет комплексное число. truth, strength, confidence. В языке нарсиз, кстати, примерно похоже. Велико разнообразие различных конструкций, которые мы можем записать с помощью атомиза, нарсиза, метты и в терминах гиперграфовой структуры Atomspace. Более того, согласно принципу функциональности, функциональной эквивалентности или просто эквивалентности, мы можем одни и те же жизненные ситуации, природные явления описать различными конструкциями. Если мы хотим сказать, что у нас предметом к области географии относится то, что ветер приводит к волнам, а облака приводят к дождям, мы можем либо сделать импликейшн-линк между wind и wave и cloud и rain, объединить это в некоторый сет и с помощью inheritance-линк присобачить это все к разделу география. А можем сделать по другому. То же самое WIND приводит к WAVE, CLOUD приводит к RAIN, а дальше все это через некоторый membering отнести к разделу география. вот то здесь все как в революционных базах данных то есть когда мы на самом деле вот как я недавно пошутил нынче какую базу графовую базу данных не делай все равно получается революционная когда мы строим значит описываем антологии когда мы описываем гиперграфовая база данных в конечном итоге все сводится к инженерии структур данных с пониманием того какую нормализацию или денормализацию ты должен сделать точно так же как ты все все точно так же как ты это делаешь когда строишь реализационную базу данных значит архитектура atom space То есть как собственно вот устроена под капотом вот та база данных OpenCog AtomSpace, которая позволяет хранить вот эти вот гиперграфовые отношения. Базовым элементом является атом, который с моей точки зрения математически эквивалентен отношению товарища кода. И к этому атому ассоциированы некоторые значения, properties. Эти значения являются системными или техническими свойствами этого атома. Они не входят в отношения, то есть в этих values относятся вот эти вот вероятности на оценки. Как правило, обычно в этих велиусах хранится Strength и Confidence, но также, гипотетически, могут храниться другие метрики. Например, счетчик, на основе которого может быть рассчитан Strength и Confidence. Моя гипотеза заключается в том, что если бы я, с учетом понимания того, как это должно делаться с точки зрения реалии и задач сегодняшнего дня, когда я говорю, что данные графовые структуры должны обязательно индексироваться по времени, я бы эти системные свойства также затолкал еще временной таймстэмп. К сожалению, сейчас я близко никак не занимаюсь OnnumSpace, как это устроено, не знаю, но вот я бы это так сделал. Кроме этого, кроме атома и технических свойств, каждый атом, естественно, может включать в себя множество других атомов. Соответственно, если атом себя других атомов не включает, это концепт. Если атом включает в себя 2 атома, это бинарная связь. Если атом включает в себя 3 атома, это, соответственно, уже триплет. Но только надо не забывать, что у атома есть еще и тип, поэтому для того, чтобы построить триплет с точки зрения классической семантической базы данных, то у этого атома внутри должно быть 2 атома, потому что тип у него есть независимый. Соответственно, у нас есть тип и некоторое количество отношений, которые этим типом соединены. В зависимости от типа, кстати, атом может быть одного из четырех видов. Либо это направленная связь или асимметричная связь, у которой есть начало и конец, например, последовательность. Это типичная асимметричная связь. Или причинно-следственная связь. Она тоже направленная. Что является причиной чего? Так, значит направленная связь. И второй тип связи это ненаправленная связь. А, сорян, всего два. Всего два разделения базовых типа. Это направленная связь и ненаправленная связь. Например, set, он ненаправленный. Энд – он ненаправленный. Есть еще связь типа Ор. Допустим, если бы мы хотели сказать, что у нас есть либо высокая влажность, либо низкое давление, тогда бы это была Орлинг. Ассоциация. Просто что-то ассоциировано с чем-то. Это тоже асимметричная связь. Соответственно, типы вот этих связей могут быть симметричными и асимметричными. Ну и, соответственно, количество атомов может быть разным. Но еще есть интересная фишка в этом атомспейсе, которая может использоваться по-разному. Это спейс. Он позволяет атомы разносить на различные сегменты. Соответственно один и тот же атом может присутствовать или отсутствовать в разных сегментах. Это свойство, насколько я знаю, очень активно не используется на сегодняшний день, но оно может быть использовано, допустим, для того, чтобы, к примеру, если мы строим многопользовательскую систему, где в контексте разных пользователей одного агента эти атомы работают в разных и встречаются в различных контекстах гипотетически мы можем вот на каждого пользователя завести свой сегмент например или значит какое-то другое назначение может быть ну и собственно то, как работает пример, как работает без формул. Соответственно, если интересуют формулы, можно посмотреть Non-Axiomatic Logic или Probabilistic Logic, Ben Guerdzel или Non-Axiomatic Logic, Pivank и посмотреть, как работают эти самые, как выглядят эти формулы, но логика примерно следующая. значит вот предположим что мы хотим позаниматься моделированием вот вот этого вероятность на моделирование вот этой вот ситуации с облаками дождем и наводнениями и вот мы строим такую табличку из атомов OpenCock, где у каждого атома есть идентификатор, есть тип, есть space, к которому относятся, но space у них у всех один. У атомов могут быть имена или лейблы. Обычно эти имена и лейблы существуют у нарных связей или у связей, которые по сути являются концептами, допустим Cloud, Rain и Flood. значит у них в метаграфе уровень нулевой, то есть они как бы ни в кого не себя не включают, они находятся на верхнем уровне, хотя в общем случае если мы начнем там облака разбивать на подоблака, дождь разбивать на дождинке, а флут разбивать там на отдельные потоки или там на молекулы воды, естественно мы можем значит они станут структурами более высокого уровня, но вот в нашей модели они являются структурой нулевого уровня. значит arety соответственно у них 0 да то есть они ни в кого ни в себя не включают то есть 0 arety вот ну и значит мы у них да аргументов соответственно у них нету они ни в кого ни в себя не включают потому что arety нулевой и уровень нулевой вот а вот strength и confidence них уже есть да причем тут я значит завожу три параметра есть каунт то есть сколько каунт это сколько раз мы наблюдали данное явление а strength и confidence это собственно вот те исходные метрики Или производные метрики. Обращаю внимание, что Strength и Confidence могут быть как исходными, так и производными. То есть, если нам событие поступило на вход системы, то Strength и Confidence задается, исходя из параметров того устройства ввода. которые эти факты, наблюдённые в систему ВИЛУ. Сейчас я расскажу на примере, как это выглядит. В общем, у исходных концептов или отношений атомов, точнее, вот эти стренды Confidence задаются. Например, допустим, мы с помощью объективного контроля наблюдаем факт облачности. да причем наблюдаем его стопудово да вот мы видим это вот мамой клянусь облако и оно вот у нас есть соответственно если оно у нас есть уверенность в том что есть сила значит проявление этого облака 1 0 и то что мы его точно видим мамой клянусь а то что тоже 1 0 соответственно из страны confidence у нас 1 0 вот и наблюдали мы это облако 44 и так было 44 раза дальше а также мы 22 раза наблюдали при этом при наличии облак мы наблюдали еще и дождь тоже методами объективного контроля так сказать там калитка количества осадков было не нулевое вот и тоже приборы у нас были исправными да соответственно тоже из trend и confidence 1 0 А вот то, что при этом еще и наводнение случилось, нам рассказали. То есть вот нам одиннадцать сорок четыре раза были облака, двадцать два раза при этом был дождь и в одиннадцати случаях нам соседи говорили, что затопило подъезд. Но сами мы в подъезд не ходили. Мы вообще были в командировке, про облака и дождь мы наблюдали в видеокамеру, про что было в подъезде мы не знаем, но вот нам говорили. Причем говорила нам там какая-то странная соседка, которая вообще могла по нашему опыту галлюцинировать, поэтому вот говорила, что точно было наводнение 1.0, но Мы этой соседке не очень верим, поэтому мы назначаем данному устройству ввода информации соседка Confidence 0.5, что вот такая вот не очень уверенная новость. Ну а дальше у нас осуществляется логический вывод, который исходя из этих трех фактов нам выводит вероятность двух связей. Первая связь это то, что в случае облака у нас случается дождь вот она первая да соответственно sequential end link уровень в метографе 2 потому что она находится над двумя другими связями ну и соответственно arity 2 потому что она соединяет cloud и rain вот аргументы соответственно 13 и 14 потому что 13 это у нас облако 14 это у нас дождь вот соответственно аргументы 13-14 вот вы посчитали значит счетчик 22 поскольку было 22 дождя значит соответственно вероятность или точнее сила вероятности или силы вероятности у нас 50 процентов там 22 на 44 ну а уверенность у нас 100 процентов то есть confidence тоже единичка ну потому что действительно да вот и тут было было 1 0 и тут было 1 0 тут факт тут факт вот и 22 и 44 соответственно вероятность 0 5 с уверенностью 1 0 Ну а если, да, значит, вероятность, это я, так сказать, язык мой сказал, потому что как раз вот с Пивангом, когда разговариваешь, он специально говорит, мы работаем с невероятностью, мы работаем с неаксиоматикой, не путай, не называй мои оценки вероятностными, у меня неаксиоматическая логика, а вероятность, потому что вероятность это отдельно, вот, а я работаю, так сказать, это, это сила неаксиоматической связи. Соответственно, когда мы считаем силу неоксиматической связи и уверенность неоксиматической связи для тройственной связи облака, дождь и наводнения, то у нас получается уже счетчик 11. Вот, соответственно, сила 0.25, одна четвертая. Вот она, 11 на 44, 25 процентов. Вот. А вероятность уже пробовали, да, потому что вот сосед, мало ли что соседка скажет, поэтому вот мы не очень уверены. ну и соответственно после того как мы на базе вот этих вот и как бы это уже на уровне на базе вот этих вероятности будем строить уже какие-то вероятности более сложные соответственно вот эти числа будут меняться соответственно то есть любая формула пересчета составной truth value из других составных truth value она всегда приводит к изменению как strength так и confidence вот причем что важно что эта формула она зависит от характера операции то есть например при abduction confidence страдает максимально при revision confidence страдает минимально, то есть при revision confidence имеет тенденцию к сохранению, при deduction confidence начинает страдать, при induction confidence страдает еще больше, а при abduction confidence опускается ниже принтукса, потому что абдукция это самый ненадежный способ логического вывода. Ну и, наконец, последняя пара слов. Я уже подошел к итогу. И то, что я буду рассказывать дальше, у меня рассказано в другом семинаре, который я делал где-то во второй половине прошлого года, про свою модель того, что называется модель социально доказательная модель основанная на методы ограниченных ресурсов вот значит идея вот этой вот модели она возникла следующим образом проблема с которой значит я столкнулся когда пытался строить какие-то практические приложения на основе вот вот этой вот логики пиванга заключалось следующим что у нас все хорошо когда мы пытаемся исходя из некоторых исходных стран и confidence, которые у нас откуда-то возьмутся, делать более высокоуровневое рассуждение, пересчитывая на основе исходных strength и confidence, производных strength и confidence. Но откуда взять эти исходные strength и confidence? И с практической точки зрения я не придумал ничего лучшего, что просто реально мы должны хранить с одной стороны весь доступный вот этот исходный evidence, То есть, поскольку если мы не будем хранить весь доступный исходный evidence, то мы не сможем по нему при очередном пересчёте пересчитать вот эти вероятности. Объясняю ситуацию. Предположим, что у нас случилось вот эти 44 дождя, 44 облака, 22 дождя и 11 наводнений. И мы на основе их произвели вот эти логические выводы. А теперь предположим, что мы, поскольку мы не можем, у нас память не бесконечная, мы вот какую-то часть вот этих наблюдений стерли. Но у нас появляется новое знание. У нас там погода поменялась, появляются новые наблюдения по поводу дождей, облаков и наводнений. И нам нужно бы вот эти значения пересчитать. И у нас появляются новые счетчики со своими новыми оценками. И нам нужно как-то из них пересчитать вот то, что мы наблюдаем сейчас, но нам-то нужно пересчитывать, как бы полную модель строить, исходя из того, что было и из того, что пришло. Соответственно, по идее, нам нужна полная историческая эпизодическая память о том, что после чего происходило, для того, чтобы вот эти современные актуальные оценки всегда пересчитывались учетом прошлых знаний. Это с одной стороны. А с другой стороны, если у нас есть куча информации, типа одна бабка сказала, или одна газета написала, или в таком-то чате Телеграмм написали то-то, а в Фейсбуке написали всё совсем по-другому, то у нас возникает проблема различного уровня доверия к различным источникам информации. которую тоже надо учитывать. И, соответственно, далась такая концепция, что у нас элементом нашей, условно говоря, структуры, где мы работаем с информацией, представленной графами, с одной стороны у нас должен храниться полный граф всех исходных знаний, с которыми мы работаем, И если мы эти знания каким-то образом архивируем, если мы их каким-то забываем, то либо нам нужно их каким-то образом агрегировать, да, так, чтобы, допустим, много жизненных опытов однозначных, значит, каждый со счетчиком, со своим счетчиком confidence и evidence. агрегировался в какой-то новый счетчик с какими-то другими метриками. Допустим, count, evidence count, к примеру, должен быть, но тогда логика пиеванга не будет работать, то есть должны быть уже какие-то более сложные формулы, в которых еще и count используется. это с одной стороны, да, то есть если мы это не эгрегируем, нам нужно хранить полный граф, всю информацию, из которой мы имели место. С другой стороны у нас должен быть социальный граф, где мы имеем как бы оценки доверия ко всем участникам информационного обмена ко всем субъектам от которых мы получаем информацию или новостных источников от которых мы получаем информацию кому мы верим больше кому мы верим меньше у нас есть некоторый набор аксиом которые не подлежат сомнению там что такое хорошо что такое плохо и под наши понятия текущие о том что такое хорошо и что такое плохо мы подгоняем свое текущее поведение ну и наконец то что собственно мы думаем о существующем существующей реальности и И те действия, которые мы совершаем относительно существующей реальности, это просто предмет некоторого логического вывода, inference резонера, который берет аксиомы из фундаментального графа, значит, берет evidence, который мы получаем на входе, перемножает это все по некоторой формуле на те веса, которые содержатся в социальном графе, и по некоторой формуле проецирует это все на нашу текущую картинку об окружающей реальности и о тех рекомендациях или мотивах для тех действий, которые мы должны предпринять в следующий момент. но вот это вот моделька более подробно что называется смотри доклад с моего семинара в прошлом году вот все что я хотел сегодня рассказать если есть какие-то вопросы или дискуссии еще два вопроса первый вопрос 

S03 [01:26:29]  : практически мы используем вот эту вот модель графа, она где-то уже используется, то есть есть какие-то оценки практически, что она предсказывает, допустим, вот с такой-то вероятностью? 

S02 [01:26:40]  : смотри, значит, вот есть, давай вот Конкретика. Система Витяева, конкретно, давай я начну с того, что я знаю хорошо. У Витяева и Евгения Евгеньевича система, она используется, что называется, в промышленности. Это закрытый проект. На основе которого делаются какие-то решения. Я знаю, что есть решение по управлению бизнесом и есть решение по медицинской диагностике. Решение закрытое, проект закрытый. У меня есть информация о том, что эти решения есть. Если интересно, у Витяева у нас на канале есть много семинаров, можно посмотреть. Но там информации тоже не очень много. Это раз. Дальше. Система Нарспи-Ванга. значит она тоже значит она вообще открыт открытым доступом можно скачать из гид хаба вот вроде как до недавнего времени даже можно было собрать я просто знаю людей которые эти ее скачивали и собирали есть все работают Основная проблема вот всех вот этих вот систем, возвращаясь к концу первой части моего доклада, что если для того чтобы построить систему нейросетевую на основе регулярных тензоров которые формируются методом обратного распространения ошибки на размеченных или неразмеченных данных нам не нужно заморачиваться откуда эту модель взять мы просто берем много графических карточек загружаем интернет архив или много видиков. Сжигаем много электричества и в итоге получается куча тензоров, которые что-то делают, предсказывают какие-то токены, хорошо или плохо. Как сейчас утверждается, тюринг-тест уже на 70% по-моему последняя новость была проходится. А в случае, когда нам нужно делать подобные логические выводы, нам эти графы кто-то должен накодить. То есть мы должны взять какую-то базу данных, уже заполненную, правильно структурированную, и описать правильные структуры верхнего уровня для конкретной предметной области на основе некоторой мета-онтологии, описать доменную онтологию в терминах какого-то языка, где у нас сущности, где у нас отношения, где у нас причины, где у нас следствие, где у нас частное, где у нас общее. И вот только после этого мы, собственно, эту логику можем реализовывать. Так вот, проекты что Витяева, что Пиванга, они работают либо на конкретных примерах, либо задачах, где вот кто-то берет и это конкретно описывает. И вот теперь пример. Допустим, у Бена Герцеля система с открытым кодом, к сожалению, она не настолько доступна с точки зрения скачать, запустить с примерами как Нарс и Пиванга. Вот, например, ты хорошо знаешь человека, который сейчас пытается в эту систему в рамках одного закрытого нам с тобой известного проекта загрузить подобные графовые отношения, относящиеся к финансам. Вот для того, чтобы и как бы долгосрочная цель, то есть ради чего этот человек делает. Он это делает для того, чтобы когда мы наберем достаточно большое количество данных, чтобы подобную логику можно было применять, чтобы для того, чтобы можно было задавать вопросы. А вот скажи, пожалуйста, вот если там, к примеру, Билл Гейтс станет генеральным директором NVIDIA или если HUANG из NVIDIA перейдет в Intel, взлетят ли акции Intel или нет? вот для того что принимать инвестиционное решение вот как бы катидея значит заталкивание графовые базы данных этого самого эдгара в соответствующую а там space для того чтобы мы можно было значит на нем делать с одной стороны логический вывод, а с другой стороны результаты этого логического вывода запрашивать в разговорном интерфейсе с помощью ЛЛН – это одна из понятных задач. 

S03 [01:31:38]  : И, собственно, второй вопрос. Мне кажется, что эта система будет хорошо предсказывать только на один шаг вперед, может быть, на два, когда у нас есть три предыдущих состояния. И мы примерно можем по этой схеме вычислить вероятность четвертого. А если нам, допустим, надо предсказать на два или на три? шага вперед, там у нас получается вероятность будет вообще расходиться, потому что мы допустим, мы читаем вероятность, если она там не близка к единице, а там 0,7 допустим, то через два состояния, ну подальше в будущее, то есть она будет там падать еще до 0,5, может быть, 0,3, опять же, если там не сильно связь. То есть, в плане предсказания, мне кажется, куда-то вдаль она будет не очень хорошо предсказывать. Судя по схеме, я так смотрю. 

S02 [01:32:28]  : Тут я не берусь прокомментировать. Скажу честно, убедительных примеров предсказания чего-либо-то ни было на основе ПЛН я не видел. На основе НАРСа я видел примеры с ПОНГом и с роботом. То есть, минимальные, самые простенькие примеры из OpenAI GYM с помощью НАРСа, они решаются. Элементарным примером этого самого OpenAI GYM с помощью Discovery я тоже видел решаются. Например, знаменитый пример нематода. С помощью Discovery нематода может научиться ползать. то есть вот это вот последовательное сокращение мышц ну там сказать я не знаю можно ли сказать то что у нее мотодом являя мышцы или нет или это просто доскать клеточная поверхность и клетки где по моему там даже не Короче, нематода что-то там у себя сокращает и в итоге совершает вот эти волнообразные движения, благодаря которым она может ползти к источнику повышенного химического градиента или наоборот избегая повышенного температурного градиента. Либо за едой, либо от жары к холоду, либо наоборот от холода к теплу. Вот. Нематода с помощью вот этой вот конструкции может научиться полностью. Но единственное, что у Витяева не вот эта вот логика, у него там вероятностная логика, чистая логика, типа Бависовской. 

S03 [01:34:08]  : Понятно. Все. У меня вопросов нет. 

S02 [01:34:11]  : Так, коллеги, еще? Будет у нас сегодня дискуссия или нет? Владимир? Вам в прошлый раз не дали задать вопросы? Сейчас вам полный карт-бланш. Но я вижу только ваше горло. И чувствую себя Кисой Воробьяниновым. Так, вас не слышу. Владимир, вас не слышно. Так, всё равно не слышно. 

S01 [01:34:54]  : Ну, хорошо. 

S00 [01:34:58]  : Вы же, в принципе, в прошлый раз не дали мне задать вопросы. 

S02 [01:35:01]  : Ну, видите, как бы, да. Там регламент был. 

S00 [01:35:06]  : Я не думаю, что дело в регламенте. Я думаю, что вы догадываетесь, что и там, и здесь я ничего хорошего не спрошу. Общее впечатление от вашего рассказа, хотя я там не могу сказать, что полностью и внимательно все время слушал, но смысл такой, что аппроксимации можно осуществлять различными методами. Вы же с этого начали, так ведь? 

S02 [01:35:32]  : Ну да, это скорее была провокация о том, что да, что может быть можно, но не всё. Что-то можно, а вот теперь давайте разбираться, что можно и что нет. Ну давайте, хорошо, давайте вопрос. 

S00 [01:35:54]  : То, о чём вы дальше стали рассказывать, я это воспринял то, что вот есть и такие методы, о которых вы там несколько разных рассматривали, ну и ими можно воспроизводить. Причём вот последнее, о чём вы говорили, это примерно о том, что вот если человек умеет водить машину, а тем более занимается извозом, то раньше считалось, что он может руководить государством, а теперь выяснилось, что нет. Если человек может водить машину и считает, что знает всё про политику, это не значит, что он будет хорошо руководить государством. Примерно такое же моё отношение к тем примерам, которые привели. Если там пинг-понг-система играет, то в сложной среде, насколько она сможет жить, это совсем не очевидно. Поэтому я не против вашего тезиса, что аппроксимацию можно осуществлять различными средствами. и там можно выбирать разные языки, но проблема не в том, что давайте так попробуем, а давайте так попробуем, вдруг получится, а все-таки хотелось бы сформулировать, какую проблему мы выбираем, решая языки, в чем состоит проблема, почему мы не берем любой язык, можно же чем угодно аппроксимировать или описывать, в чем проблема с вашей точки зрения? 

S02 [01:37:03]  : С моей точки зрения проблем нет. Я высказал то, что я думаю по этому поводу. Я согласен с вашим тезисом насчет неуверенности, что можно простыми способами решить сложную задачу. Это про что сказал Владимир. Извини, Евгений. Евгений сказал, что мне кажется, что на один, на два шага максимум оно будет считать, а на более сложное не будет. Вот, это принимается. Второе, да, второй тезис, значит, у меня есть ощущение, да, вот как бы вот в процессе, ощущение, которое у меня, скажем так, сформировалось в процессе подготовки сегодняшнего рассказа, значит, его изложения, которое у меня сформировалось, это то, что мы не можем строить сложные вот эти вот иерархические модели с помощью того что я называю аналитических решений то есть с помощью некоторой аналитической процедуры там а-ля свертка обратной проекции с помощью которых мы можем там для слабо неоднородной среды построить картинку медицинской томографии с помощью нее мы не можем там сказать по восстановить структуру там какого-то трехмерного здания по набору космоснимков Для этого нужны другие методы. И в этом смысле может оказаться, что итерационные методы, опять-таки, которые, а-ля обратное распространение ошибки, или другие методы, например, про которые Сергей Шумский рассказывал на прошлом семинаре, и, надеюсь, он нам еще расскажет, структурированное обучение послойно, они являются более эффективными. То есть мы не можем, грубо говоря, сказать, в один проход, каким-то простым способом получить и всю структуру трехмерных, и всю структуру иерархических тензоров, и каждый тензор в отдельности. Либо мы должны каждый тензор по отдельности собирать простыми способами, а потом как-то эту структуру компоновать, либо какие-то другие сложные методы нужны. 

S00 [01:39:11]  : Примеры, конечно, не очень мне удается соотнести с тем, что вы рассказывали. Вот. Но, значит, ситуация какая? Представление о том, что давайте обучим нейросеть условным рефлексам, а потом она запомнит много условных рефлексов, а из них сформируется система, и все будет работать. Вы согласны с таким утверждением? Потому что я-то против такого утверждения. 

S02 [01:39:37]  : Я не делал никаких утверждений. 

S00 [01:39:40]  : это хорошо вопрос значит вот как понимать то о чем вы рассказывали это что надо свести вот к такому утверждению с которым я не согласен или все что хотел сказать я сказал и понимать это никак не надо я вас понял хорошо тогда у меня больше нет вопросов хорошо коллеги 

S04 [01:40:05]  : Антон Борисович, добрый вечер. Добрый вечер. Антон, у меня в одной части по поводу аппроксимации есть мысль, но я не знаю, как это немножко сформулировать. Получается, была попытка аппроксимировать фрагменты разных кривых – синусоиды, или это фрагмент шума, и попытаться восстановить, что это такое. Но является ли это фрагментом хобота слона? Мне кажется, это будет фрагментом слона только в том случае если вот эта кривая будет являться контуром слона то есть если у нас есть контур слона и тогда уже форма хобота не будет иметь никакого значения хобот может быть даже завязан скажем так в узел, но по контуру мы поймем, что это хобот слона. Вот тут какого-то контекста не хватает. 

S02 [01:41:30]  : Конечно, конечно, конечно, да. Это может быть хвост, это может быть не хобот слона, это может быть хвост осла. Совершенно верно. 

S04 [01:41:39]  : Ну как вот этот контекст? 

S02 [01:41:43]  : это собственно говоря это вот то чем attention is all you need то есть вот как раз что в чем заключался прорыв трансформеров в том что они научились объединять не только исходные некоторые данные Но для построения вывода они научились брать контекст. То, что мы аппроксимируем, оно включает в себя контекст тоже. То, что мы видим плюс контекст, это все вместе взято и является предметом аппроксимации. Грубо говоря, давайте под другой пример приведу, тоже из другой истории. Предположим, что вы хотите аппроксимировать нечто серое на аппроксимацию отображения некоторого объекта на некоторое животное. Соответственно, если вы будете отображать нечто серое либо на кита, либо на слона, то у вас будет всегда неоднозначность. Но если вы, допустим, будете отображать нечто серое на нечто синим, то вы скорее будете отображать вот это вот серое на синим на кита. А если вы будете отображать нечто серое на чем-то зеленом или на чем-то желтом, то вы будете отображать это на слона. До тех пор, пока слон не окажется в море, и тогда вы будете получать ошибку. Либо кита не выбросят на берег, и тогда вы тоже будете получать ошибку. 

S04 [01:43:31]  : Да, получается, это вполне очевидная вещь, но почему-то в этой логике рассуждений у вас как-то не просвечало то, что необходимость контекста как-то я не увидел. 

S02 [01:43:46]  : Я показывал, просто давал элементарный пример. То есть мы берем некоторую элементарную функцию, ее пытаемся аппроксимировать. А функция может быть сложной. То есть эта функция может быть двухмерной, эта функция может быть трехмерной, эта функция может быть n-мерной. 

S04 [01:44:11]  : Понятно, но все-таки результат мы же не получим. 

S02 [01:44:17]  : Почему? Если у нас будет функция, которая будет исчерпывающим образом описывать исходные данные и возможные контексты, мы получим. То есть, если мы будем учитывать как и наличие ушей, хвостов, лап, плавников, так и воды, либо песка, либо кустов, мы сможем понять, либо это у нас кит в море, либо это у нас слон на берегу. Просто аппроксимирующая функция будет очень сложной, не полиномиальной, и не гармонической, и даже не сплайновой. хорошо спасибо коллеги еще вопросы тогда на сегодня все и до свидания до новых встреч всем спасибо 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
