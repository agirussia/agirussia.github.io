## 8 октября 2020 - Виктор Носко - Разговорный ИИ с сознанием  — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/x_8PdHFCc0I/hqdefault.jpg)](https://youtu.be/x_8PdHFCc0I)

Суммаризация семинара:

Семинар посвящён теме разговорного ИИ и сознания. В ходе обсуждения были затронуты следующие ключевые моменты:

1. Разговорный ИИ и сознание: Докладчик поднял вопрос о сознании в контексте чат-ботов и разговорного ИИ. Обсуждался, что современные чат-боты могут имитировать вежливость и логичность, но не обладают настоящим сознанием.

2. История чат-ботов: Были рассмотрены исторические аспекты развития чат-ботов и их влияние на современные технологии.

3. Сознание и нейросети: Докладчик отметил, что нейросети могут создавать видимость сознания, но на самом деле это лишь эмуляция. Проблема в том, что нейросети не обладают пониманием контекста и человечности.

4. Эксперименты с архитектурой трансформера: Компания, с которой сотрудничал докладчик, экспериментирует с архитектурой трансформеров для улучшения эмуляции человечности в чат-ботах.

5. Критика и вопросы аудитории: Аудитория задала вопросы о сознании и его определении, а также высказала критику в отношении некоторых подходов к разговорному ИИ.

6. Бенчмарки и сознание: Обсуждалась проблема бенчмарков, которые не всегда могут точно оценить уровень сознания в ИИ. Использование нейросеток для оценки текстов и их суммаризации обсуждалось как способ улучшения бенчмарков.

7. Коэволюция языка и сознания: Некоторые участники семинара поддержали идею коэволюции языка и сознания, предполагая, что сознание может развиваться вместе с языком.

8. Новые архитектурные принципы: В дискуссии поднимался вопрос о необходимости новых архитектурных принципов для создания систем, способных к настоящему сознанию, и о гибридах нейросеток и других подходов.

В заключение, семинар показал, что вопрос сознания в контексте ИИ остаётся открытым и требует дальнейших исследований. Участники обсудили различные подходы и методы, но признание того, что настоящее сознание у ИИ отсутствует, остаётся общим для всех.



S03 [00:00:00]  : Так, запись пошла. 

S01 [00:00:01]  : Виктор, пожалуйста. Так, я сейчас включил демонстрацию экрана уже, и вам должно быть видно. Так, сейчас проверю, как переключение идет. Да, все видно. Вот, вот так. Окей, смотрите, давайте сделаем таким образом. У меня есть какие-то тезисы основные, которые изложены вот в такой короткой презентации. Я их просто проговорю. Для меня был такой основной вопрос. Какая аудитория здесь будет? Потому что я полагаю, что где-то половина информации, которую я буду рассказывать, она будет вам известна. Особенно то, что касается философской части. Потому что вы здесь, я смотрю, сколько я вас видел многих в группе Гираша, вы достаточно умудренные люди. Скорее всего, что вы читали те же книжки, что и я, и даже больше. Другая половина, которая касается современных разработок, я думаю, что вы, скорее всего, ее не знаете. Я поэтому об этом тоже расскажу, что мы делаем и какие есть проблемы в Conversational AI. Я люблю говорить по-русски. Ну вот. И, соответственно, тема доклада «Чат-боты и сознание». И мы сейчас рассмотрим вообще, что такое чат-боты, какие они бывают, и есть ли там у них сознание. Сразу спойлер. Я думал, Стоит ли называть таким образом доклад? Потому что те люди, которые, допустим, считают, что в чат-ботах сознания нет, им может не понравиться просто название доклада. Я видел такой комментарий, что ты, не имея очень хорошей определенной терминологии, которая даже в литературе очень спорная, Называешь таким образом доклад, как будто бы заявляешь, что в чат-ботах есть сознание, и эти люди могут разозлиться и отключиться. Но с другой стороны, я предлагаю просто порассуждать на эту тему. Никакого утверждения или контрутверждения в названии доклада не заложено. Но мое личное мнение, я его скажу ближе к концу. План следующий. Просто расскажу немножко про историю чат-ботов конфиденциального АИ. Немного про сознание, то, что я знаю. Рассмотрим кейсы и рассмотрим будущее. Но первое, о чем бы я хотел сказать, что мы в данном докладе не будем рассматривать чат-боты, которые являются так называемыми ассистентами. ассистенты, которые помогают вам выбрать продукт в магазине, или там заказать билет, или те же самые Олег, или там, в частности, также и Алиса сюда относится, на самом деле. Я бы не хотел рассматривать Алису, Маручу, всех этих ассистентов типа Siri, которые выполняют определенные которые дают вам ответ на определенные вопросы, очень жестко зашитые, несмотря на то, что там нейросетки все-таки есть. Все-таки они больше работают по принципу rule-based, то есть на правилах. А на правилах, вы понимаете, этот подход устаревший, мы работаем сейчас с 80-х годов, и даже были мысли построить систему перевода на правилах. И вы знаете, чем это все закончилось. Закончилось ничем. И появился нейромашинный перевод, который только стал нормальным и хорошим. Так вот, мы будем рассматривать чат-боты, которые являются человечными. То есть это те чат-боты, у которых есть способность эмулировать личность конкретного человека или персонажа. Такой скилл персоналити мы его называем. эмпатия, то есть что-то типа эмоционального интеллекта, то есть это некоторая вежливость, это проявление эмоций и так далее, сопереживание возможно, вовлеченность собеседников в диалог, то есть человечность общения. И еще важное такое свойство, все-таки для того, чтобы человек-блог был человечным, мы считаем, мы в нашей компании, коммерческой компании. Мы считаем, что должно быть несколько скиллов, то есть учат бота для того, чтобы внешняя среда, сообщество и вообще СМИ, которые об этом пишут, чтобы они стали об этом вообще что-либо писать, необходимо, чтобы было несколько скиллов, они были очень плавно интегрированы, ну то есть очень хорошо переключались. И тогда Мы называем это человеческим чат-ботом. Я здесь сразу скажу, что вот что мы сделали. У нас есть продукт, это Newton-чат-бот. Это чат-бот, который является эмпатичным, он open-domain, то есть он разговаривает на большое количество тем. Мы используем архитектуру Transformer, нашу кастомную модифицированную архитектуру, то есть это не GPT-2, которую наверняка вы читали в интернете, и в общем все о ней знают, и также сразу, сбегая вперед, скажу, что это не GPT-3. По недостаткам этих моделей мы пройдемся чуть дальше. В чем суть, в чем фишка этого чат-бота? Он обладает эмпатией, он проактивный, у него есть определенные знания о мире, он может отвечать на вопросы про определенных людей, про определенные события, и он понимает контекст. Это наш продукт, то есть мы в этой теме делаем такие продукты коммерчески, делаем такие решения. Если будут вопросы, я могу рассказать, какие есть подводные камни, когда клиенты заказывают, потому что огромный пласт терминологии еще не является устоявшимся. И когда нам говорят, что сделайте вежливость от бота, то не очень понятно, что такое вежливость. У каждого есть понятие о вежливости, оно довольно-таки размыто. В общем, мы лучше Яндекс.Алиса с точки зрения вот этих трех основных качеств. Это эмпатия, это проактивность, то есть способность чат-бота не просто отвечать, но и задавать в ответ вопросы. Обычно чат-боты так не делают. Такая способность задавать вопросы в ответ. У меня есть собака, а у тебя есть Чат-боты обычно так не делают. Это проявление такой человечности и обладание знаниями. Ну и вот здесь по ссылке можно посмотреть скриншоты реальных разговоров. Вообще, я просто расскажу здесь один конкретный кейс, для чего можно использовать такие чат-боты. Применений достаточно много, но на данном слайде показано применение когда можно сделать детского бота, который бы разговаривал на различные темы в рамках определенной области. То есть мы хотели бы, чтобы был детский чат-бот, который рассказывает про мультики что-нибудь, про персонажей этих мультиков, но при этом там нет запрещенных тем, там нет мата, но при этом общение достаточно свободное. Вот, допустим, это было бы круто. Это вот такое коммерческое применение, где это может работать. Вообще, наш чат-бот — это аналог мировых чат-ботов. Известно, да? То есть, это Ося Уайс от Майкрософт, Нина Гугла, и Блендер Фейсбука, и есть еще различные примеры. Давайте сейчас пройдемся по истории. Как вот такие разговорные чат-боты вообще появлялись, конверсиональные они называются? Какие там были проблемы, они были связаны, как правило, с архитектурой. Вот когда появился SeoWise, я читал статьи про все эти чат-боты и знаю, с помощью каких принципов они построены, какие там использовались эти сеты. Ну и вот первые такие статьи, которые стали появляться в СМИ, что достигнут какой-то, ну более-менее, назовем это так, прорыв или какое-то определенное достижение. Это чат-бот от Microsoft, Seowise. Когда я читал статью, что они сделали? Они взяли сначала вручную большое количество датасетов разговоров живых людей, те, которые у них были. Почему они у них были? Потому что у Microsoft есть Microsoft Bot Framework. То есть у них уже были определенные датасеты, но их было достаточно мало. И буквально через небольшое время, месяц-два-три, Когда они выпустили этот продукт, они стали уже переобучать свою простую модель. Это была модель не трансформер, тогда трансформеров не было. Моделька была что-то типа SEC2SEC OSTM. Они стали переобучать уже на вот этих миллионов диалогов, которые продуцировали пользователей данного чат-бота и полностью убрали свою изначальную свой изначальный датасет. К чему это привело? Это привело к тому, что они смогли потом заявить, что их чат-бот впервые делает реализацию эмоционального интеллекта. Что они делают? Они в своей статье показывают примеры разговоров. В частности, когда пользователь разговаривает с этим чат-ботом. Чат-бот – это девушка, говоря сяо айс по-китайски. И как они показывают, что чат-бот проявляет эмоциональный интеллект? Юзер разговаривает и описывает свою определенную ситуацию, которая у него там произошла с девушкой. Чат-бот, спрашивают у него наводящие вопросы, какие у вас там отношения и прочие вопросы разнообразные. И в конце концов он говорит, тебе эта девушка не нужна. Вот молодец, что ты ее бросил, тебе не нужна. Далее авторы статьи от Майкрософта делают вывод. Смотрите, ведь наш чат-бот какой он классный, он не стал там советовать верни эту девушку, он понял, что данному человеку все он правильно сделал, что он ее бросил. И они говорят, что вот это у нас проявление эмоционального интеллекта. Когда я это читал, мне это было несколько смешно. И я думаю, что если вы знаете про уровень развития чат-ботов сейчас, то есть В чем здесь дело? Авторы статьи взяли и интерпретировали ответы чат-бота, они придали им смысл, они придали им определенный смысл. При этом мы же с вами понимаем, что та архитектура, которую они использовали, она не способна была понимать смысл. То есть, что она сделала? Она просто вытащила из датасета наиболее вероятную реплику. Вот что она сделала. Вот с чем мы сталкиваемся, когда мы говорим про chatbot, мы сталкиваемся с некоторой халтурой, и она у нас здесь еще дальше по слайдам будет. То есть даже большие компании допускают такие вещи. Я думаю, Может быть вы еще знаете, что выходила система генерации текста посимвольная, автор Карпаты, известный, не знаю как произносится его правильное фамилия, по-моему Карпаты так и произносятся. Там была посимвольная генерация, тоже это не представляет какого-либо интереса большого для генерации текста. Почему? Потому что она побуквенная, там просто считаются статистики, и когда Когда генерируется текст слаженный и когерентный, и там правильные окончания, и там правильное употребление определенных имен, то это связано только лишь с тем, что накоплена огромная статистика, что окончания должны быть такими. Почему? Потому что предыдущее слово вот такое. То есть там ни о каком понимании смысла речи не идет. Соответственно, конечно же, никакого сознания в этих ранних чат-ботах нет и быть не может, потому что есть у нас архитектурные сложности с этим. Вышел недавно достаточно чат-бот Мина. Здесь на слайдах вы видите, что датасеты существенно выросли, используются сотни гигабайт текста, используется архитектура трансформера, модифицированная в GPT-2. И что важно здесь сказать, что осознанность диалога и специфичность поддержания тематики, она уже была достаточно близка к человеческой. То есть у человека это 86%. Что означает вообще число 86%? То есть, грубо говоря, в 86% случаев группа людей, которая оценивает логичность диалогов, она говорит, что продолжение диалога является логичным. Вот этот показатель. является неким усредненным показателем логичности и специфичности. Ну а специфичность — это когда вы говорите о той же тематике, о тех же субъектах и объектах, которые были в диалоге. И в данном случае я могу сравнить с Алисой. Если вы попытаетесь пофлиртовать с Алисой, то она будет постоянно вас сбрасывать. Ну или я называю это отбивки. Это когда она вам говорит, что она не хочет флиртовать, что она не в настроении сегодня. Задайте, пожалуйста, другой вопрос. Это некие общие фразы, которые не являются специфичными. Алиса Плохара. Еще одна работа — это Blender от Facebook. Здесь о чем стоит сказать? О том, что использовался микс датасетов в обучении, и было использовано внутри три подхода. то есть использовать конвои-датасет, World of Wikipedia, по-моему, называется, и эмпатичные идеологии. Был достигнут достаточно хороший показатель perplexity. И вот эта работа, она превзошла Нина по бетчмаркам. То есть работы идут, но недостатки тоже есть. а здесь вот сравнение фич, но я здесь не буду сильно углубляться, я просто скажу, что для стороннего наблюдателя или, допустим, для тех наших зрителей, которые Мне в этой теме для них достаточно тяжело и сложно определить о каких чат-ботах, собственно, делает компания. То есть в чем специфика, в чем разница. То есть я вот когда разговариваю с клиентами, я им постоянно это объясняю, что есть чат-боты, которые простые, условно говоря, слабый искусственный интеллект. Здесь я использую такое объяснение и в принципе люди понимают, потому что они где-то слышали про слабый искусственный интеллект, И в принципе им понятно, что да, конечно, распознавание интентов чат-ботами на мере них, это слабое, это просто классификация. Ее тоже можно сделать на мере сеткеров типа BERT, но это не очень круто. А генерирование диалога, не вытаскивание отдельных фраз на генерацию диалога, это уже нечто более крутое. Теперь давайте приступим к критике. Некоторые из вас, конечно же, знают о том, как критикуют GPT-3. В чем фишка? Здесь есть ссылка на статью на Хабре, но такие статьи, их вышло прямо несколько, которые критиковали. conversational модели. Фишка GPT-3 – то, что она few-shot learners. То есть впервые удалось добиться такой штуки, когда вам для того, чтобы зафайнтюнить, то есть настроить нейросеть на определенную задачу, вам нужно достаточно мало примеров для обучения. Утверждается, что Бывает, сотни примеров может хватать, а раньше требовались тысячи, сотни тысяч таких примеров. К сожалению, проблема в том, что нейросеть совершенно не понимает, что же она генерирует. И в этой статье достаточно много примеров. Я здесь поставил просто один пример, когда университет не смогла понять, что смешивание двух соков дает напиток, который не является отравой. И там в статье еще множество таких примеров. Конечно же, GPT-2 тоже страдает этим. Производные модели. которые мы используем также страдают этим. Но я бы здесь сказал, что все-таки вот этот хайп по поводу модели генеративных GPT он У него есть, конечно, фундамент, то есть определенные достижения здесь есть, но все-таки говорить о том, что есть сознание, что есть какое-то понимание, ну или хотя бы зачатки сознания в этой модели, все-таки мы не можем этого говорить никак. То есть людей, которые не в теме, это удивляет, им нравится, какая происходит генерация текста, но тем не менее все-таки от какой-то осознанности, понимания что происходит, у кого какое целеполагание, если вы разговариваете с этой моделью, у нее просто задают вопрос, она вам генерирует ответ. Но генерирует она все-таки, к сожалению, статистически. То есть, что она сделала? Она взяла кучу гигабайт датасетов, Википедии, новостных статей и так далее, и она их слепливает в кучу, таким образом, что каждое отдельное предложение и каждый отдельный абзац вроде бы имеют какой-то определенный смысл, но все вместе это не имеет, то есть никакой цели написания данного целостного текста все-таки нет. И когда вы видите новость, в Коммерсанте выходила новость, что вот нейросеть написала статью, И в конце я читаю, что, оказывается, человек вручную правил эту статью и менял местами абзацы для того, чтобы была какая-то логичность. Это еще одна проблема. Есть уже нейросети, которые могут генерировать тексты достаточно длинные, и при этом у них не теряется контекст и понимание. Здесь есть такая проблема. Есть еще подход, достаточно новый подход. Я видел уже, что его обсуждают в Телеграме. Оказывается, на самом деле можно применять аугментацию данных. Вот здесь, вот в этой работе, я ее сам еще не читал, честно скажу, но здесь применялась, насколько я понимаю, аугментация данных и определенного рода дистилляция. Дистилляция — это когда вы можете достичь того же самого качества модели, но при этом в ней будет намного меньше параметров, соответственно, меньше, она будет меньше по размеру, она будет работать быстрее. При этом качество модели будет чуть-чуть хуже, но не сильно. Вот заявляется, что авторы этой работы смогли обучить нейросетку, похожую на GPT-3, и она на бенчмарке SuperGLUE смогла победить GPT-3. Бенчмарк SuperGLUE это такая штука, которая проверяет знание языка у нейросеток. Допустим, такие вопросы как read comprehension или логический вывод определенный нужно сделать. Вы прочитали текст, вам нужно ответить на вопросы по данному тексту. Примерно как мы делали в школе, когда были школьниками. Вот эта модель победила. Код открытый – это очень хорошо. Теперь переходим к собственно, теме, да, то есть есть ли все-таки сознание и вообще может ли оно появиться, вот таких как Conversational AI, а если оно появится, то как вообще мы проверим, что оно там появилось? Это тоже большая проблема. И вот есть позиция философов, но вообще я думаю, что многие не любят философов. Почему они их не любят? Потому что Потому что философы проводят, к сожалению, у них достаточно часто. Результат их работы — это проведение мысленных экспериментов. А эмпирические исследования, то есть научные исследования, они почему-то редко ими применяются, но, может быть, потому что они не сильно разбираются в математике, физике. Но такие товарищи, как Чалмерс и Пенроуз, вот они более-менее разбираются. Пенроуз недавно английскую премию получил. Вот они разбираются, поэтому на них я могу еще хоть как-то ссылаться. Вот есть позиция Чалмерса, насколько я ее понимаю, она в чем заключается? В том, что сознание, вообще говоря, это некая феноменологическая вещь. То есть это такая вещь, которая сколько бы вы не усложняли какую-то техническую систему, допустим нейросетку, или там какой-то суперкомпьютер вы собрали из большого количества видеокарт, процессоров, как-то их очень круто соединили, то все равно в такой мега крутой системе сознание не может возникнуть. Это насколько я понимаю его позицию. И такая же позиция, когда читал Пенроза «Тени разума», По-моему, у него тоже такая позиция, я еще книжку прочитал. А позиция такая, причем там эта позиция разделяется на четыре разных позиции. То есть в каких случаях мы бы могли все-таки получить сознание, в каких случаях мы бы могли получить видимость сознания, но не смогли бы этого проверить. Мое личное мнение по этому поводу заключается в том, что есть проблема комнаты Серла. То есть действительно то, с чем мы сталкиваемся, с вот этими всеми нейросетками, о которых мы поговорили на предыдущих слайдах, проблема здесь в том, что вы получаете всё время видимость, то есть вы получаете всё время эмуляцию того, что сознание это есть. Сознание, можем заменить его другими словами, что понимание контекста есть, что понимание, что такое вежливость есть, что понимание, что такое логичность есть. Но, к сожалению, все нейросетки описаны, какими они не были бы крутыми, всё-таки понимания этого нет. Мы находимся на таком этапе развития науки и техники, что этого понимания нет. Поэтому мы действительно сталкиваемся с этим парадоксом китайской комнаты, когда мы получаем видимость, эмуляцию вежливости, но мы не получаем действительную вежливость, то есть целеполагание этой вежливости, то, что есть у людей. Есть также у Днепрова автор в 61 году написал рассказ-игра, где он продемонстрировал на таком простом эксперименте, он создал из людей на стадионе аналог вычислительной машины. Какой-то относительно простой. И с помощью этой вычислительной машины смог перевести текст португальского на русский. При этом никто из людей на стадионе, которые друг другу передавали определенную информацию бинарную, 0-1, они не понимали, что происходит. Никто из них не понимал, что происходит, а при этом текст оказался переведенным. Это действительно иллюстрирует то, что система может очень круто работать, очень правдоподобно, однако неким знанием и целеполаганием, что происходит, она не обладает. Но при этом я бы сказал, что мы как компания, чем мы занимаемся? Мы действительно сейчас пытаемся сделать очень правдоподобно эмуляцию вот этого осознания и понимания человечности, то, как мы называем, то, что не режет людям слух, мы пытаемся сделать правдоподобную эмуляцию человечности. Для этого мы экспериментируем с архитектурой трансформера, делаем другой энкодер, декодер и прочие вещи. Но в апогее давайте представим такой эксперимент. Идете вы с девушкой по улице, по парку, и тут где-то в США идете, допустим, и тут идут люди из компании Boston Dynamics. И с ними рядом идет собака. Их вот эта собака, которую, как вы знаете, многие любят потолкать, попинать ногами, чтобы она вставала. И эта собака очень хорошо встает, она там может делать сальто. Эту собаку они сейчас пытаются продавать. Вот. И представьте, что Вот идёте, и вдруг вы начинаете эту собаку пинать. Больно. Вот прям больно. То есть с озлоблением. Вы её начинаете пинать. Как отреагирует ваша девушка? Я думаю, что в апогее, если мы представим, что эта собака будет очень реалистичной, то есть она будет обладать шерстью, она будет не металлической собакой, она будет шерстью, она будет со слюнями, она начнет скулить, ваша девушка закричит, не бей. Вот я уверен, что она закричит, не бей, пожалуйста, не бей, ты что? И так далее. То есть что это означает? что когда мы достигнем определенного уровня эмуляции, у нас будет действительно происходить вот этот перенос. То есть мы будем отождествлять очень сложные системы с такими системами, которые обладают сознанием. И когда люди говорят о том, что нам нужно подумать о правах для роботов, То есть пока нам рано об этом думать. Но когда роботы станут настолько антропоморфными, как это показывают в фильмах, которые вы все знаете. Мне нравится из машины фильм. Вот тогда возникнет эта проблема. Проблема с правами, проблема с сознанием. И, соответственно, моя позиция заключается в том, что я не знаю, когда технология дорастет до того момента, когда мы смогли бы утверждать, что в сложной системе есть сознание, однако преодолев определенный порог вот этой сложности, я бы сказал, что для нас уже станет неважно, есть ли там по-настоящему сознание или нет. Ну и есть еще проблема проверки, то есть, в частности, есть такие мысли по поводу непредсказуемости мышления, то есть примерно как погоду мы не можем предсказать. Я бы сказал, что действительно, наверное, это и есть такое. То есть есть explainable AI, такая штука, которая уже сейчас позволяет обеспечивать объяснение того, как нейросетки принимают решение. Но когда мы разрежем нейросетку и узнаем, почему она так ответила, не являемся мы тогда случайно наблюдателями, которые вмешиваются в эксперимент и тем самым нарушают его течение. Ну вы знаете про эффект наблюдателя. И соответственно мы поймем почему так нейросетка ответила, но у нас исчезнет вся магия того, что у нее вообще есть сознание. Как только мы понимаем, почему она так ответила, нам кажется, что это некая механистическая штуковина по типу комнаты китайской или по типу человека, который сидел в машине, которая играла в шахматы. То есть мы разрезали машину, посмотрели, а там человек. Ну все, это чепуха. То есть это никакой не AlphaZero, не AlphaGo, и стоит на это обращать внимание. Это такая немножко философская проблема. На этом слайде я немного расскажу про проблемы индустрии, какие вообще существуют. Дело в том, что недавно вышел такой называемого доклада про искусственное сознание. Я очень быстро про это скажу, потому что не хочу пиарить этих людей, потому что они не в этой индустрии, они не в индустрии искусственного интеллекта, они несколько в иной индустрии. Так вот, вышел этот доклад, и я его весь посмотрел. Что происходит в этом докладе? Авторы утверждают, что они создали искусственное сознание, и они его в этом докладе тестируют. Они привлекли людей, психиатров, психологов, по крайней мере одного человека, который есть на видео. И он тестирует это искусственное сознание. И нужно сказать, что тестируют они его следующим образом. Они ему задают постоянно вопросы. ну, якобы каверзные вопросы, типа, а что такое свобода, какие у тебя желания, они спрашивают у этого чат-бота, который за кадром отвечает, точнее, отвечает, как они говорят, сознанием, ну, в общем, отвечает чат-бот. Они спрашивают эти каверзные вопросы, и чат-бот очень хорошо отвечает, очень хорошо отвечает, ну, не подкопаешься, как человек. Они говорят, что он отвечает не то, что как человек, а круче, чем человек. И поэтому вот у нас прорыв, у нас научная вообще революция. Мы создали такое сознание, которое может всё изучить и так далее. И вот я написал такую короткую статью по этому поводу. Мало того, что они не публикуют никакой технической информации, ну ладно, можно было бы сказать, что это их секрет, ноу-хау, не хотим публиковать, как коммерческое применение, допустим. Но проблема там методологическая. Это основная проблема. То есть способ проверки на сознательность данного чат-бота, он не выдерживает никакой критики. То есть, по сути, Вся их проверка заключается в том, что они проверяют просто чат-боты. Когда мы говорили про все предыдущие нейросетки, если вы откроете статьи про эти нейросетки, то там будут бенчмарки. Там будут множество тестовых диалогов с этими чаботами и проверка, логично они отвечают или нелогично. Вот они сделали ровно то же самое. Они просто разговаривали с этим сознанием и делали определенные выводы и интерпретации. А так делать неправильно. Как правильно делать – дискуссионный вопрос. Очень большой вопрос, как правильно делать. Но так делать точно неправильно. Потому что мы абсолютно никак не можем проверить и верифицировать, грубо говоря, не сидит ли там за кадром человек, который все это отвечает. Ну, в общем, я должен был об этом вам рассказать, что вот есть такая попытка пиариться на сознании и искусственном интеллекте. Ну и здесь же проблемы эмуляции. В частности, в докладе они говорят, что вот наше сознание понимает, что врать плохо, и она никогда не врет. Но как они пришли к этому выводу? Совершенно никакого ответа нет. Как они отличили ложь от правды? И почему они считали, что раньше, что им Бот отвечает вообще правду, а не все врет? Каким образом они приходят к своим выводам? Никакого ответа нет, ну и поэтому, в общем, мнение индустрии об этом не очень хорошее. Здесь можно поговорить про пути создания думающей машины. Есть такая штуковина, как Neural Architecture Search. Вот мы сейчас пытаемся, мы только начинаем этим заниматься. Почему? И что это такое? Это такой подход, который позволяет находить эффективные архитектуры, новые архитектуры, в частности трансформеров, но и других нейросеток. Но впервые Google опубликовал статью, что поиск оптимальной архитектуры был применен на трансформере и такая архитектура была создана. И об этом есть статья, можете погуглить, почитать. В чем здесь проблема? В том, что Но, как вы понимаете, нейросетки были созданы для того, чтобы не писать самим правила. Нейросетки генерируют сами правила. А теперь мы говорим, а можно ли то же самое сделать с архитектурой? Потому что до сих пор люди сами создают архитектуру в программном коде. Вопрос, а можно ли сделать так, чтобы нейросетки сами создавали архитектуру, и после этого эта архитектура уже делала следующую вещь обычную, которая ей и присуща, то есть создавала сама правила. И в чем здесь фишка, в чем здесь крутость, что нейросетка тогда будет оперировать более общими концепциями. Потому что, когда я начинал доклад, я говорил вам о том, что был подход очень ущербный по символной генерации текста, то есть по буквенной. Ущербный подход. Почему? Потому что система не понимает, что значат эти буквы. По словной генерации уже лучше. Появились нейросетки типа BERT, они могут уже сравнивать смысл двух предложений. Даже если все слова там отличаются, если смысл один и тот же, Можно обучить нейросетку сравнивать смысл. Теперь вопрос, как сравнивать смысл по определенным осям. То есть вы хотите сравнивать вежливость двух текстов, или вы хотите сравнивать стили двух текстов, или вы хотите сравнивать определенные характеристики, которые не являются информационными. То есть не информацию из этих текстов вы хотите сравнивать, а вы хотите сравнивать схожи ли у них стили определенные. То есть тогда вы уже переходите на уровень семантики. И вот как раз поиск архитектуры позволяет сделать и создать такие нейросетки, но при этом автоматически с помощью эволюционных алгоритмов. То есть технология тоже не новая. И это лучше. Это лучше, в принципе, чем чем делать это вручную, и есть уже фреймворки от Майкрософта, Google там что-то представила, AutoML. И в итоге что я хочу сказать? Я хочу сказать о том, что мое мнение человека, который обучает эти нейросетки и работает с клиентами, оно заключается в том, что я думаю, что сознание все-таки не может возникнуть, вот в текущих архитектурах это точно, но не может возникнуть. А где бы оно могло возникнуть? Если бы наша архитектура вот этих электронных вещей всех, она была бы близка, хотя бы функционально, вот хотя бы функционально к архитектуре мозга. Чтобы у нас там, чтобы были нейросетки, которые бы исполняли мозжечка. чтобы у нас была некоторая зрительная определенная кора. Почему? Потому что, когда мы говорим про Conversational AI, почему он так плохо работает? Потому что он оперирует словами и понимает их в рамках ограниченного векторного пространства. То есть, а ребенок, когда он учится, он имеет зрительную кору, тактильные ощущения, запахи, и поэтому То есть архитектура его мозга намного круче, он только тексты не воспринимает. Он воспринимает множество зрительных, обонятельных и прочих каналов восприятия. И, соответственно, он намного круче, чем текущие нейросетки. Если мы добавим в Condensational AI такие части, такие архитектуры, такие надстройки определенные, не знаю, что-то типа модулей, тогда мы могли бы обучать нейросети оперировать образами. Примерно то, как мы мыслим, что определенный образ, он взаимодействует с какими-то другими образами. У этого всего есть какая-то цель. Если мы добавим также сюда какую-то эмуляцию, программную или аппаратную, вопрос отдельный, наверное, можно делать отдельную серию докладов по этому поводу, какая она должна быть на самом деле, вот гормонов, я имею в виду гормональный фон, потому что принятие решений человеком, оно во многом менее регулируется, то тогда мы бы могли получить хотя бы некую видимость вот этого сознание в этих говорящих машинах. Ну и вот здесь скриншот такого советской короткометражки, называется «Над той за стеной», где два экспериментатора, они тестировали, пригласили человека и они предлагали ему задавать очень каверзные вопросы двум людям, один из которых был искусственным интеллектом, но визуально он выглядел в точности как человек. И в чем суть этой короткометражки, я не буду там спойлерить, там есть такой кульбит в конце, но в чем суть ее? В том, что она неправильная. То есть в этой короткометражке, она снята давно, И на тот момент не было правильного понимания, как проверить, есть ли самосознание у искусственного интеллекта. То есть они считали в этой короткой нейтражке, что когда нейросетка, в частности, один из этих людей, которых они тестировали, когда она отвечает, что у нее есть самосознание или что у нее нет, они это интерпретируют как правду. А мы с вами уже знаем, что это может быть просто вытаскивание определенной реплики из датасета и, к сожалению, это совершенно ничего не означает. Это не означает о том, что нейросеть поняла этот вопрос, что она ответила сознательно. То есть можно посмеяться над этим видео. Оно прикольное, но хочется над ним смеяться. Ну вот это вкратце. Спасибо. 

S03 [00:41:01]  : Виктор, спасибо. Так, ну, коллеги, значит, у нас тут есть уже вопросы. У меня есть один вопрос, который я начал писать, но давайте я его тогда не допишу, а скажу вслух, и дальше можно будет пойти поначалу. Вот вы сами, в своем чадьботе, что делаете с контекстом? То есть вы храните историю диалога. Вы храните какую-то историю диалога, диалога, так сказать, зато с удалением из нее каких-то устаревших элементов? Или вы храните просто некоторый какой-то текущий контекст, или предыдущую реплику там, или три предыдущих реплики? Вот как вы, что вы делаете в этом плане? 

S01 [00:41:44]  : В текущем боте мы не используем никакой настройки типа внешней памяти. Ничто не хранится в какой-то базе данных и оттуда не достается. Каждая следующая реплика генерируется исходя из того, что на вход в нейросетки подается контекст, то есть группа реплик. Группа предыдущих реплик, разное количество можно подавать, и 3, и 5. Но желательно не сильно много, потому что потеряется понимание. Это все дело проходит через голову attentions, внимания, и она генерирует ответ, который получается достаточно логичным, но при этом мы же с вами понимаем, что каждый диалог можно продолжать где-то двадцатью разными способами, и каждый из них будет логичным. То есть можно спрашивать уточняющие вопросы, можно какие-то утверждения делать и так далее. И вот эта способность есть, и все-таки она достаточно ограничена. То есть только лишь там три, ну я бы сказал, что где-то три варианта логичности у нас реализованы сейчас. Это высказывание своего мнения, это ответ на мнение человека, которое находилось в предыдущей реплике, и намного реже, когда оно было три реплики назад. И это какой-то встречный вопрос, список, перечень вопросов он тоже не очень большой. 

S03 [00:43:19]  : Спасибо. Так, давайте вернемся к началу вопроса. Первый вопрос от Алексея Егорова. Вы когда вначале говорили про эмуляцию и имитацию, вот он просил уточнить, что вы имеете в виду, но, по-моему, по ходу доклада вы как раз это пояснили, но, может быть, еще раз. 

S01 [00:43:41]  : Да, собственно, это и есть канва, канва всего доклада, что мы можем делать только имитацию, вот сейчас. Только имитацию, ну или там эмуляцию. Эмуляцию видимости, психических каких-то структур, которые просто проявляются в тексте. И как бы благодаря, ну вы знаете, да, что там был создан чат-бот, который прошел тест-твиринга. Он был создан достаточно давно, это был 13-летний мальчик, но при этом, насколько я помню, в статье, когда я читал, были ограничения для тестеров. Им было сказано, что вы не можете спрашивать абсолютно все, что вы хотите, вы можете спрашивать в определенный спектр тематик, которые может знать этот мальчик. вот этот тринадцатилетний мальчик, то есть им было сказано, что вот попытайтесь отличить тринадцатилетнего мальчика от тринадцатилетней, грубо говоря, обученной на этих датасетах. Ну и там была система на правилах. То есть вот был пройден тест Юринга, был пройден абсолютно кустарным, плохим способом, и поэтому Вот мы все, я думаю, и у Антона тоже говорили в одном из докладов, видео, где я был, что тест Юринга устарел. То есть его нужно совершенно по-другому, причем он кардинально устарел. Потому что все время мы сталкиваемся с вот этой эмуляцией. 

S03 [00:45:10]  : Спасибо. Следующий вопрос от меня. Как вот вы себя сравнили бы с репликой, как с точки зрения функциональности, так и с точки зрения архитектуры, если это известно? 

S01 [00:45:23]  : Да, конечно. Про реплику, кстати, часто спрашивают. Но смотрите, я, конечно же, хочу покритиковать конкурентов. Вот, можно немножко это сделаю. У них до того, как они сейчас используют чужую разработку, то есть GPT-3, то есть это публичная информация, они подключили GPT-3, они ее затюнили. В интернете есть доклад, где разработчик рассказывает, что они применяют определенный pipeline обработки данных. классификацию, ну дополнительно, помимо самой GPT-3, которую они заинтюнили, для того, чтобы течение разговора было еще более крутым. А до того, как они встроили и стали использовать GPT-3, у них была не знаю какая архитектура точно, но она работала не очень хорошо, и я видел публикации в интернете, где люди жаловались на то, что разговор совершенно не логичный и он не специфичный. То есть примерно то самое, что вы видите, когда вы общаетесь с Алисой. Что у нас? У нас архитектура не GPT-3, у нас кастомная архитектура, то есть мы взяли, мы тоже используем чужую технологию, скажем так, но все разработчики, абсолютно все разработчики, просто для зрителей поясню, который там совершенно может быть не в теме, все разработчики искусственного интеллекта используют чужую технологию, используют ли они какой-то фреймворк, используют ли они определенную архитектуру, но, как правило, это предобученная нейросетка. Почему так? Потому что обучать нейросеть с нуля это очень дорого. Это десятки, а может быть и сотни тысяч долларов. Вот мы взяли такую предобученную нейросеть и затюнили ее, то есть мы ее доработали, поменяли там, не буду говорить что, но кое-что мы поменяли. У нее другая архитектура, не GPT-2, также не скажу что за архитектура, в общем у нее другая архитектура немножко. и она больше затюнена на большом количестве диалогов. И эта архитектура, этот энкодер и декодер очень хорошо подходят именно под диалоговые задачи. Не под генерацию статей, не под что другое, а именно под диалоговые, то есть это все про вопрос и ответ. Грубо говоря, это вопрос-ответ. И поэтому именно бот может отвечать достаточно логично. Но если его просить, а расскажи что-нибудь, допустим, да, расскажи что-нибудь, он не расскажет, потому что этого нет. Нет такой способности, нет этого скилла, и он скажет какую-нибудь чепуху. 

S03 [00:48:34]  : А ваш бот может что-то рассказать? 

S01 [00:48:37]  : А я же говорю, что наш МОД нет, не может рассказать. Понятно. 

S03 [00:48:42]  : Хорошо, спасибо. Так, здесь поздравляю Петрову за Снобелевской премии и комментарий от Бориса Новикова, что аргументация китайской комнаты опровергается многими философами и мной тоже. Тут есть что-то прокомментировать? 

S01 [00:48:58]  : Я читал критику, разнообразную критику. по поводу того, что вот эти символы, которые там подаются, то есть иероглифы, что рассматривать нужно не самого этого человека, который как бы не знает этого языка, а что нужно рассматривать некую общую систему. И в этом плане у общей системы можно будет сказать, что есть некое осознание. Я наполовину соглашусь с этим утверждением, потому что я все-таки в конце как раз и говорил о том, что если будет очень крутая эмуляция такой системы, можно будет признать, что там сознание есть. Но это будет, я бы сказал, что это нечто типа другого типа сознания. То есть вот когда инопланетяне прилетят к нам, Где-то я видел такой прикол, или есть такой фантастический рассказ, где инопланетяне, прилетающие, обладающие другим типом сознания и другим способом мышления, ну и архитектурно построенная, то есть не белковая, скажем, жизнь, у них будет научно доказано, что люди, вот эти товарищи, у которых в голове мясо, они мыслить не могут. Точно так же, как примерно мы думаем насчёт муравьёв, что у них очень низовое, несмотря на то, что сознание, кстати, было обнаружено недавно у муравьёв, они там стирали с себя метку, подходили к зеркалу и стирали с себя метку. Это означает, что они понимали, что в зеркале, конкретно муравей, видя себя в зеркале с меткой белой, начинает её стирать, и значит, он понимает, что это он. Вот такой эксперимент. Но в целом, в общем, Спорная вещь. Наверное, можно спорить. 

S03 [00:50:45]  : Следующий вопрос от Владимира Смолина. Если мы не закладываем в машину механизм целеполагания, откуда могут появиться цели, в том числе быть вежливыми? 

S01 [00:50:59]  : Вот ниоткуда. Текущие архитектуры нам придется закладывать. То есть нам нужно будет делать две вещи, которые делают все разработчики. Выбирать подходящую архитектуру под задачу, а вторая часть выбирать датасет под задачу. Тогда у нас происходит transfer learning, и мы можем эту задачу решить, и она будет решена хорошо. в принципе. Если не закладывать этого, то бывают такие случаи, ну я бы так сказал, что все-таки трансформеры вот эти большие обученные, там есть побочные эффекты, которые есть и которые хорошие. Ну в частности это то, что они могут генерировать на любую тему. И в принципе здесь я бы еще добавил, что когда они генерируют на другую тему, все-таки они понимают и стилистику тоже. Потому что они генерируют в этой стилистике. Когда они берут новостную статью, они эмулируют, что такой-то журналист сказал. То есть, они вставляют эти слова, и это является стилем новостного текста. То есть, побочные эффекты возникают, хорошие. 

S03 [00:52:26]  : Но все-таки целеполагания нет. Ну, то есть, если у вас нет целеполагания вежливости, то откуда у вас берется вежливость? Это просто style-трансфер? То есть, просто fine-tuning сетки через старт-трансфер на вежливые стилы общения, там типа, там ты не дурак, а дурачок, например, да, заменять слова на дурак, на дурачок. 

S01 [00:52:50]  : Это трансфер. Трансфер просто стиля. Ну, что еще? Ну, то есть, как это можно делать? Наверное, можно долго рассуждать, можно можно было бы прикручивать какую-то reinforcement learning. То есть, грубо говоря, обучать нейросетку, если она общается невежливо, то чтобы она понимала в реальном времени, если мы рассмотрим некоторую обучающую в реальном времени, чтобы она переобучалась. Но эту историю мы видели также, по-моему, в Твиттере была система, которая генерировала, и она от Microsoft опять же. и она стала генерировать хейтерские записи. Почему? Потому что люди стали ее троллить различными вещами, расовыми, нехорошими словами. И она обучилась этому и стала генерировать это в ответ. Ну, это уже такая этическая сторона. 

S03 [00:53:45]  : Спасибо. Так, следующий вопрос от Бориса Новикова. Цель вежливости критерии заложены авторами? Ну, я так понимаю, на этот вопрос отвечено уже, что никакие цели вежливости. То есть, видимо, они неявно заложены через те диалоги, через которые вы делали style transfer, да? 

S01 [00:54:03]  : Да, датасет очень важную роль играет. Во всех отраслях, не только в текстах, в картинках тоже. Есть еще такая тема, как по картинкам, генерация описания картинок. Вот это можно, в частности, использовать для понимания мира какого-то. когда вы распознаете объекты, то, в принципе, вы можете сказать, что на картинке изображено даже в динамике, то есть в действии, что не просто там кучка объектов, а что они связаны вместе, то есть они находятся в определенных частях картинки, по пиксельным расстояниям, раз посчитали, сделали какие-то свербки, и уже мы понимаем, что этот футболист, оказывается, гол забивает, потому что у него нога около мячика находится, и, соответственно, можно такие штуки делать. 

S03 [00:54:58]  : Спасибо. Так, дальше, значит, замечание, что нет способности строить свои цели, если сами цели строить не можем, значит, и понимания никакого нет. Тут, видимо, комментариев не будет у вас, да, наверное? 

S01 [00:55:15]  : Ну, мне бы очень хотелось, чтобы понимания было. Вот. 

S03 [00:55:20]  : Но его пока нет. Хорошо. Значит, следующий вопрос, значит, уже. Если у вас определение сознания, я вот даже, значит, на два раза его разобью. У вас, вот вы говорите про сознание, да, и говорите, исходя из того, что у всех участников, значит, примерно такое же понимание сознания, в контексте этого, значит, ведете разговор но вот просто из опыта общения там у нас половина участников свое индивидуальное неповторимое понимание сознания поэтому вот ваше что вы подразумеваете под сознанием техническая или математическая формулировка строго можете сформулировать 

S01 [00:56:01]  : Строго нет, строго нет, но я бы сказал, что это какая-то совокупность человеческих качеств, то есть в частности это характер, у каждого человека он есть и есть исследования, что оказывается люди делятся на различные типы, ну в частности там социологика, есть MBTI, то есть есть деление людей на различные типы и мы могли бы создавать системы, которые бы разговаривали со стилистикой этих типов, чтобы они отражали характер, чтобы какой-то бот был очень деловой, какой-то бот очень медлительный, но очень творческий, к примеру. Получается, что это набор инстинктов, естественно, то есть желание жить должно быть, желание кого-то любить, продолжение рода, вот такие вещи. Если эти все штуки есть, и если есть осознание и саморефлексия, что я понимаю, что это есть, тогда я бы сказал, что данная машина обладает признаками сознания. Но вот этот подход, он обсуждается в литературе, как бы менталистский, ментальность рассматривать и функциональность. И я бы сказал, что это спорная такая часть. То есть видимость сознания будет, если вот эти все вещи будут. внешняя, но я не уверен, что она будет внутренняя. Все-таки она зависит от достижений науки, от того, что будут ли у нас вот эти железячные аналог нейронов, которые очень близко к устройству мозга. Вот недавно выходила новость, что создан аналог мозга, который по количеству нейронов близок к мозгу мыши. То есть здесь нужно рассматривать еще и железячную часть. Количество коннекшенов. Насколько хорошо эмулируются вообще вот эти взаимодействия, передача сигналов. Потому что ведь у нас же нейроны очень упрощенные. Во всех нейросетках очень упрощенные нейроны. И как они передают эти сигналы, очень слишком просто. При том, что мы знаем, что в мозге нейромедиаторы есть, есть центр удовольствия. Много всего есть. И вот эта передача сигналов зависит от множества химических процессов. Все эти химические процессы, которые происходят в мозге, пока никак я не видел в статье, где хотя бы близко разработчики подошли к тому, чтобы эмулировать. Но это мы рассматриваем, конечно же, человеческое сознание. Можно создать какое-то другое. 

S03 [00:58:47]  : Хорошо, спасибо. Вот тогда вторая часть вопроса. То есть, допустим, у вас есть некоторое понимание того, что такое сознание, а когда вы подбираете архитектуру своей сети, вы файнтуните эту архитектуру, какие у вас формальные критерии? То есть, есть, грубо говоря, критерии, по которым вы корректируете ошибку, если критерии по наличию сознания не удовлетворяются? 

S01 [00:59:20]  : Это вопрос про бенчмарки, наверное. 

S03 [00:59:22]  : Да, бенчмарки и наличие сознания, совершенно верно. Бейзлайн по наличию сознания у вас есть? 

S01 [00:59:32]  : Вот мы используем просто то, что есть мысли, как усовершенствовать текущие. Пока что мы этим не занимались, скажу так. Но есть мысли. Почему? Текущие бенчмарки, автоматические бенчмарки, они статистические. Я приведу пример какой-то с суммаризацией. Не помню, как он называется, но его суть в следующем, что правильно ли, хорошо ли, круто сделана суммаризация текста. то есть его краткий пересказ. Можно посчитать количество слов. Если количество слов в пересказе сильно много, на 8% пересекается со словами из исходного текста, будем считать, что хороший пересказ. Много же слов попало в итоговую выжимку. Но это, конечно, полная чепуха. Вот этот статистический подход является полной чепухой, потому что там не перейдется смысл. Поэтому статистический подход использовать нельзя. Но во многих местах он еще сохраняется. Вот этот бенчмарк, о котором мы говорили, SuperGLUE и русский SuperGLUE, который недавно вышел от Сбербанка. Было же, да, у нас мероприятие по Сбербанку. Когда там были более сложные более сложные примеры. Но это уже лучше. В общем, бенчмарки идут в сторону, когда в них самих используются нейросетки, которые обучены на человеческих примерах. То есть на огромное количество текстов. То есть уже сейчас в бенчмарках используются, собственно, вот эти трансформеры, потому что ничего лучше нет. И эти уже бенчмарки ближе намного. То есть бичмёрки зависят от текущих стейтов за арт архитектур. И, конечно, да, что происходит, когда мы анализируем, насколько хорошо наш бот разговаривает, вообще любой другой бот, можете посмотреть статьи, это человеческая оценка по многому, потому что применив автоматическую оценку, даже с помощью нейросеток, мы просто получим число, некоторое число 0,8, которое как бы считается нормальным. То есть вот 80% Но, к сожалению, пока мы не прочитаем, мы не можем понять, логичный ответ был или нет. То есть только человек оценивает логичность разговора. Потому что нейросетка будет проверять схожесть по смыслу, а у нас логичность – это некая другая более сложная система, структура. Поэтому в чат-ботах сейчас проверка человеческая, ну и пытаются ее оцифровать. В общем, группу людей оценивают логичность. 

S03 [01:02:30]  : Спасибо. Комментарий от Алексея Егорова, что раз нет моделей самопоставленных задач, значит нет никакого сознания. Ну вот да, то есть это разные понимания того подсознания, кто что подразумевает. Значит, следующий, да, вот теперь еще вопрос тоже про определение. Вы в какой-то момент, когда рассказывали про кого-то из конкурентов, говорили, что значит они могут в своей нейросетевой архитектуре сопоставлять смыслы и выяснять что у двух разных предложений один и тот же смысл и что-то с этим делать вопрос тогда что такое смысл смысл но в данном контексте то что о чем мы говорим смысл там имелся ввиду что 

S01 [01:03:20]  : одно и то же говорится в этих двух предложениях. То есть, грубо говоря, там происходит сжатие или классификация по какой-то одной шкале. Как правило, эта шкала действия, то есть одно и то же действие или намерение. Это слово «интент» еще, то есть намерение. То есть, грубо говоря, если происходит действие, движение с объектами-животными, то смысл что кони бегут за кем-то, и что гепард охотится на антилопу, смысл здесь считается близким. Насколько он близкий, 0,7 или 0,8, вопрос спорный, потому что мы не выбрали определенную шкалу. Мы можем выбрать более тонкую шкалу, про охоту, когда происходит охота. Тогда здесь близости совершенно никакой нет по смыслу. То есть в одном месте просто бегают, а в другом месте охотятся. И смысл у нас тогда зависит от конкретных объектов, от их типов, от семантики и прочих свойств. 

S03 [01:04:26]  : Спасибо. Вопрос от Бориса Новикова. Мозги разных животных с сознанием устроены по-разному физически, но похожи функционально. Почему ориентация только на мозг человека, который только часть его нервной системы? 

S01 [01:04:43]  : Ну, я просто не являюсь специалистом по мозгу. Я просто читаю книги, как и все, наверное. Но я просто думаю, что, наверное, человеческий мозг наиболее круто изучен. Все-таки томограмма мозга есть. Все-таки есть там какой-то Илон Маск со своей нейролинкой. Есть примеры, я знаю, опять же, ростовский проект, который занимается, у нас есть не физики, где разработчики, какие в том числе и мои знакомые, занимались такими вещами, как распознавание сигналов, идущих от мозга, то есть это те самые такие проводки, которые в голове присоединяются, и что-то они там улавливают. Я не специалист в этом, что они там улавливают, но при этом можно было давать сигнал мышке, и мышка двигалась по экрану. А по поводу мозга животных, мне кажется, он просто хуже изучен. И все-таки там нет многих отделов. Опять же, не специалист, но мне кажется, каких-то отделов речевого нет. Или он устроен очень по-другому. Есть, конечно, эксперименты, когда на шимпанзе проверяли, что шимпанзе действительно может разговаривать с символами. В принципе, он простые вещи может делать, может просить еду. И даже, как считается, достаточно сложные может делать. Ну, наверное, еще какие-то этические моменты. 

S03 [01:06:21]  : Спасибо. У вас вот на слайде про архитектуру, которая сейчас на экране, есть слово «шумский». И, по-моему, про это пояснения этого слайда не было. Что означает это слово? Вы используете какие-то технологии Сергея Шумского? 

S01 [01:06:40]  : Ну, технологии не используем, но мне просто интересно его разработки, потому что мы когда с ним общались, ну у него вообще книга есть, Она тоже в процессе чтения, просто очень много всего я не все еще досконально изучил. И в этой книге, насколько я понимаю, хорошо описано устройство мозга с точки зрения математического аппарата. как можно было бы, исходя из того, как мы понимаем, как мозг работает, переносить это в технические системы. И, опять же, вот я не знаю, насколько секретно, не секретно, но, в общем, говорилось о том, что ведутся разработки другой принципиально иной архитектуры. Я не назову сейчас конкретно, как это все называется, может быть это вообще секрет. Но ведутся. То есть это не глубокие нейронные сети, это нечто другое. И поэтому я добавил, может кто-то захочет это поизучать. 

S03 [01:07:56]  : Спасибо. Следующий вопрос от Ильи Гизара Талипова. Существуют ли чат-боты, способные в процессе диалога выстраивать некоторую модель собеседника и вести нить беседы с ее учетом? Это могло бы быть первым приближением к демонстрации понимания. Извиняюсь, прежде чем на этот вопрос ответить, я забыл прокомментировать. Вы упоминали Евгения Гусмана, который прошел тюринг-тест под видом 14-летнего мальчика. Причем это был мальчик из Одессы, который плохо владеет английским, потому что он вырос в Одессе и недавно переехал в Лондон. И я просто был на докладе, где разработчики рассказывали об этой архитектуре, и у них там был не совсем такой чадбот все-таки. То есть у них там было не просто дерево диалогов, у них были как раз цели. И у них как раз было встроено, так сказать, большим количеством костылей. реализована именно технология целеполагания, причем цели можно было переключать. То есть, если чат-бот понимал, что сейчас мы находимся в каком-то другом контексте, то он мог переключиться с одной цели на другую, но уже разговор с собеседником вести с точки зрения того, насколько данная ветка диалога может приблизить к достижению той цели, которая сейчас является текущей. Ну, и, соответственно, в каком-то смысле, то получается уже некоторая, если не модель собеседника, то нить беседы или шаблон беседы. Соответственно, вопрос. Существует ли, с вашей точки зрения, чат-боты, способные выстраивать вот эту модель собеседника и вести нить беседы с его учетом? Это могло бы быть первым приближением к демонстрации понимания. 

S01 [01:09:52]  : Ну, существует. Это просто штуки или разработки, собранные из нескольких модулей. Потому что есть статьи, что нейросетки являются сжатыми базами данных, ну или базами знаний. Такие статьи есть, их можно нагуглить. То есть если вы возьмете архитектуру, которая заточено на то, чтобы быть вопросно-ответной системой. Такие архитектуры, обученные, предобученные университетки, УДИ Пауэллов есть, Хэгген Фэйс, можете поискать, скачать. Все это в открытом доступе. Вот они заточены, в принципе, нормально отвечают. Почему они нормально отвечают? Потому что они очень хорошо запоминают эти знания. А, допустим, трансформер не сильно заточен на этом. Он немножко запоминает, но не особо. И таким образом получается, что для того, чтобы создать вот эту систему, которая обустроила некий образ собеседника, на текущий момент, насколько я знаю, все-таки нужно сделать хак. То есть программно создать базу знаний, в эту базу знаний загонять данные о собеседнике, то есть просто собирать их сначала вопросами, затем загонять, А затем уже эти знания можно действительно вытаскивать очень хорошим способом, нейросетевым, очень хорошо классифицировать, в какой момент нужно ответить этими знаниями, а в какой момент можно не отвечать и генерировать. Но сама система все-таки не является чем-то очень крутым с точки зрения современного развития науки. То есть такая система не будет каким-то прорывом. Она будет крутой с точки зрения коммерческого использования, да. Она будет создавать видимость, понимание. Но сильно крутой она не будет. 

S03 [01:12:00]  : Спасибо. Есть комментарии и вопрос у Игоря Пивоварова. 

S02 [01:12:11]  : Коллеги, всем добрый вечер. Виктор, спасибо. Прошу прощения, я опоздал немножко, но какую-то часть слышал. И у меня сперва такой комедий, коллеги, для всех нас, не столько для Виктора, сколько для всей этой аудитории и, в частности, к Борису Новику, любителю задавать вопросы про то, что такое сознание. Есть надежда, что каждый следующий докладчик знает ответы на все вопросы. Давайте мы зададим ему главный сакральный вопрос, но и, как обычно, получим ответ, что он этого не знает. У меня в детстве была такая дурацкая привычка, я заглатывал книжки очень быстро любые, очень часто в конец заглядывал. Я не мог долго понять, почему я так делаю, а потом я понял, что я там искал некий ответ на некий главный вопрос. Но ни одна книга его не содержит. Это надо прочитать много книг, чтобы в конечном итоге привести самому к каким-то выводам. Поэтому я предлагаю не троллить докладчиков. ну, как бы, знаете, вопросами, которые, честно говоря, не имеют смысла. Дайте жесткое определение сознания, и давайте потратим полчаса на то, чтобы пообсуждать, что оно неправильно. Ну, мне кажется, это бессмысленно. Это мой комментарий, я на нем не настаиваю, но, честно говоря, если бы я моделировал семинар, я бы такие вопросы бы запрещал. И чтобы немножко конструктивно… Борис, я просто, вы знаете, ну, как бы, я не первый доклад такой слушаю, и не первый вопрос. 

S03 [01:13:50]  : Борис, давайте по порядку, Борис. 

S02 [01:13:52]  : Давайте. Я готов на эту тему подискутировать, конечно. Просто, Виктор, на самом деле, комментарий к Виктору, прежде чем выходить на аудиторию с докладом, надо хоть немножко, ну, посмотреть на аудиторию. Потому что вы, конечно, Вы рассказываете про вещи, которые люди, сидящие в этой аудитории, многие из них знают очень хорошо. Но это не суть важно. У меня конструктивное предложение такое по поводу сознания, что все-таки надо пытаться договориться в сообществе о хотя бы некоторых критериях, может быть, о некоторых пунктах, из которых это состоит. и оперировать не целым термином сознания, который непонятен, который разное определяет по-разному, а все-таки какими-то кусочками отдельными. Например, постановка целей является необходимым, но недостаточным условием. Точно совершенно. Даже самостоятельная постановка целей – это один из кусочков. Меня тут названивает товарищ. Вопрос, Виктор, к вам. В плане того, что вы делаете, понятно, что там никакого сознания нет и об этом говорить не имеет смысла, но в плане того, что вы говорите о нарабатывании человечности и доведении этого бота до вида, когда как бы извне производит впечатление понимающего. В этом плане у меня, знаете, какой вопрос. Вот есть в компьютер-вижн такая технология, как GANы, да, генеративные сетки, там с дискриминатором, когда дискриминатор, ну, в общем, вот такая, ну, игра не игра, ну, в общем, короче, просто технология, Сетка обучается с помощью дискриминатора. А вы не пробовали свои архитектуры разговорные обучать? Знаете про такие работы, кто-то, может быть, так делал, обучая диалогов с дискриминатором? То есть, пытаясь отличить фразы с генерированной сетью от реального человека, и дальше, может быть, это приведет к некоторому приличному скачку, скажем. 

S01 [01:16:15]  : Так, ну мы не пробовали, ответ первый, мы не пробовали. Но сама идея в принципе хорошая. Почему? Потому что мы пробовали использовать другую вещь, которая похожа, когда у нас данных не хватает в датасете, аугментацию. А аугментацию можно сделать используя нейросетку и подавая ей несколько нейросеток. И другие. которые в данном конкретном предложении тексте заменят некоторые слова на синонимы, на антонимы, но при этом в зависимости, естественно, от задачи. То есть в каких-то задачах можно заменить на антонимы, в каких-то задачах нельзя заменять только на синонимы, в каких-то задачах можно добавлять описание. То есть там такой-то стол стоит, а мы добавляем там синий стол стоит. И тогда мы аугментируем таким образом улучшаем, увеличиваем наш датасет и получим в итоге более крутой результат. В принципе, можно было бы и диалоги точно так же, каждую реплику увеличивать. То есть увеличивать их количество и получать большую точность, большую когентность. Но здесь, конечно, есть такая опасность, что мы потеряем потеряем смысл, то есть это как бы надо делать осторожно. 

S02 [01:17:46]  : Понятие смысла, оно достаточно условно, его и надо отдельно давать, но я, знаете, я хотел просто пояснить еще свой вопрос, дополнить его, чтобы вы ну, как бы, еще надо это вам наподумать, раз вы поднимаете такую тему, как разговорный интеллект и сознание. Значит, просто в этой группе достаточно много людей, которые, ну, там, ну, в частности, Антон, например, поддерживает идею, ну, я, собственно, тоже вполне ее поддерживаю, о коэволюции языка и сознания. что бы ни обозначало сознание, многослойный пирог, допустим. И в этом смысле у меня есть предположение, что, допустим, если у вас есть разговорные модели, которые друг с другом будут разговаривать, дискриминатор – это тоже в некотором смысле разговаривать друг с другом, но только в специальной форме. то может быть мы, экспериментируя с такими моделями, можем прийти к каким-нибудь любопытным моделям поведения. 

S01 [01:19:04]  : Вот мне кажется, я просто не читал еще PET, то что было на одном из слайдов, мне кажется то, что вы говорите, это близко к тому, потому что они там использовали не совсем Supervised, а Unsupervised, то есть они дополняли датасеты. Просто вот когда мы смотрим, как две Алисы разговаривают, да, смешные ролики в YouTube, там проблема-то в том, что если взять этот разговор и использовать его как некий разговор, на котором обучать нейросеть, то все-таки же мы не знаем, какого качества этот разговор. Его же все равно нужно инспектировать с помощью человека. Это основная такая проблема. Я понимаю, о чем вопрос. А можно ли автоматически проинспектировать некую логичность всего этого разговора? 

S02 [01:19:57]  : хотелось бы. Этот вопрос подразделяется на два. Одна история это действительно что-то типа автоматического оценки качества и попытки поднять это качество за счет использования архитектуры дискриминатора, но другая часть она как раз но как бы она немножко про другое. Если Алиса с Алисой это бессмысленная история, потому что Алиса заведомо думает, что, ну не думает, запрограммировала алгоритм, запрограммировала так, что на той стороне человек, который с ней разговаривает, и она как бы этот алгоритм пытается ну как бы соответствовать некоторым представлениям человека об алгоритме. Если от этого очистить, и сделать так, чтобы просто алгоритм, грубо говоря, говорит, что не думает, он не думает, но говорит, что ему на нейросеть пришло. И такие алгоритмы там друг с другом будут разговаривать. Я к тому, что мне кажется, что некое поле для экспериментов. Антон, это может быть даже и к вам вопрос. То есть вот подумать на такую тему, на тему, можно ли мы смоделировать эту самую но не людьми разговаривая с нейросетями, а чтобы они друг с другом массово общались. 

S01 [01:21:25]  : Я понимаю почему к Антону, потому что я знаю, что у Антона есть проект по поводу графов, а я просто тоже графами занимался. Если бы мы придумали некоторую систему когда бы мы разным специалистам направляли ответы, что это типа распределенная некая система, чтобы не человек, прочитывая реплику, говорил, насколько она хорошая, а была бы какая-то штука, которая как reinforcement learning, чтобы не читая, она дала ответ от системы, что данная реплика хорошая. Например? Тогда окей. Тогда да. Но я все никак не могу придумать, что бы это могло быть за автоматическая система, которую я не читая могла бы рассказать. 

S02 [01:22:09]  : Это вопрос на подумать. Ну да. 

S03 [01:22:13]  : Хорошо, спасибо. Давайте продвинемся дальше. Я хотел бы тоже откомментировать на комментарии Игоря по поводу того, чтобы запретить модераторам спрашивать вопросы про сознание. Я, с одной стороны, согласен, ровно по той причине, что у каждого своё определение сознания. Но я позволю оставить за модератором право уточнять позицию спикера по данному вопросу, потому что мне как раз было непонятно, что именно подразумевает Виктор под сознанием. Я этот вопрос задал, я этот ответ на него получил и понимаю, что многие претензии к докладу с этой позиции, они просто не имеют смысла. Поэтому вот давайте, значит, есть еще заявка на комментарии от Алексея Егорова, но у нас есть три коротких вопроса, к сожалению, тоже про термины, но давайте их быстро пройдем. Значит, во-первых, от Бориса Новикова. Вы допускаете диалог на любую тему, специальную или только бытовую? 

S01 [01:23:15]  : В нашем Боте? Да. Там есть несколько тем, на которые лучше Бот общается. В частности, на бытовые. Про музыку может хорошо поговорить. Знает Стива Джобса. Но если вы будете разговаривать про некого другого Стива, то он ошибется. Он уже не может понять, какой имеется в виду Стив из контекста. Слишком мало информации. То есть, некоторые темы есть хорошие, в частности, мы разработали, тестили, сказали, что удалось убедить, что бот религиозный, бота удалось убедить, что он религиозный, удалось убедить его в определенной ориентации сексуальной. То есть, есть еще такие вещи, как все-таки там проскакивают то есть плохие шутки, которые встречались в диалогах, очень мерзкие, они есть там. Это проблема датасета, естественно. Они могут вылезать. 

S03 [01:24:23]  : Спасибо. Теперь ещё вопрос. Я надеюсь, что будем считать, что это вопрос на уточнение, а не троллинг. Ответ был про психику, а не про сознание. От Бориса Новикова тоже. Но я не совсем понял вопрос. Борис, можете уточнить? Так, Борис, мы вас не слышим. 

S04 [01:24:48]  : Алло. Говорила, что нужно стремление к выживанию и так далее. Докладчик говорил в ответе про сознание. Я считаю, что это относится к психике в целом, а сознание только часть психики. И безусловные врожденные рефлексы не относятся к сознанию. А упоминалось это. 

S03 [01:25:19]  : Спасибо. Вопрос от Сергея Московского. Если в человечном чат-боте Ньютон есть понимание, то что это и как оно реализовано? 

S01 [01:25:40]  : Понимание чего? Понимание каждой конкретной реплики. О чем она? Тематика? Тематика есть. Понимание вежливости, ну то есть как бы те скиллы, которые мы их называем там скиллами условно, которые были там на одном из первых слайдов. Ну опять же, это понимание очень кустарное. Ну то есть просто оно основана на датасете, который был специальным образом подобран, таким образом, чтобы там были режливые диалоги. А если бы их не было, то тогда бы и понимания не было. 

S03 [01:26:17]  : То есть, понимание – это у вас не элемент архитектуры, а это некоторое качество системы с точки зрения стороннего наблюдателя? 

S01 [01:26:30]  : Наверное, да. Мне легче сказать «да», чем разбираться в этом вопросе. 

S03 [01:26:34]  : Спасибо. Хорошо. Ну и вроде у нас все вопросы. Остался комментарий Алексея Егорова. Пожалуйста, Алексей. 

S00 [01:26:43]  : Раз, два, три. Слышно меня? Да, да. Пожалуйста. у меня достаточно короткий комментарий я хотел бы прежде всего сказать спасибо автору и прежде всего за искренность потому что техническая недосказанность во время доклада который было с моей точки зрения достаточно много вот она была с успехом компенсирована искренностью и правдивостью ответа на вопросы и я испытал в этом смысле очень много удовольствия потому что Виктор, Виктор, да, Вик это Виктор, вот, потому что Виктор очень-очень честно отвечал на вопросы и фактически после его ответов уже недосказанности вот этой не осталось. Я хотел бы отметить, что для обсуждения четботов, вообще говоря, очень хорошая практика была бы использование и юзание демки перед докладами, потому что мы сейчас занимаемся очень странной вещью. Мы обсуждаем со слов автора четбота, что может четбот вместо того, чтобы взять и просто хотя бы 15 минут практики иметь. Я понимаю, что, может быть, технически это какие-то возник... могут возникать какие-то сложности, но это было бы сразу... сразу же огромное количество вопросов бы отпало, и мы бы, может быть, не задавали бы их, или, может быть, бы ушли в конкретику какую-то. Потому что у меня есть просто некоторый план, потому что я люблю чат-боты, и я знаю, что их спрашивать для того, чтобы понимать, как они устроены. А я очень бы хотел знать, как устроен ваш чекбот, и, к сожалению, из доклада я этого не получил. Поэтому демка была бы очень полезна. 

S01 [01:28:21]  : Да, окей, я сразу отвечу. Просто было одно мероприятие, где, оно на фейсбуке у меня есть, видео, где как раз эта демка рассматривается. Сразу скажу, что там демка маленькая модель. Маленькая модель, там... миллионы параметров, скажем так. То есть она там будет отвечать не сильно круто, как я рассказывал, как она отвечает, скажем так. У нас есть более крутая модель. Мы сейчас это все дело перезапускаем, потому что для нее нужно GPU. Но она дороговато стоит. Она будет в ближайшее время перезапущена, так что там вопросы с сессиями есть. В общем, разная загрузка, когда много людей подключаются. Она будет, и я вам дам эту демку. Вы поговорите со своим футботом. Вот ответ простой. 

S00 [01:29:07]  : То есть, мы можем попросить вас выложить ссылку на эту демку в HDI in Russian, группу с тем, чтобы мы могли ее попробовать. 

S01 [01:29:17]  : Ну, в принципе, да, но я думаю, лучше, чем просто в личку направить, чтобы там не обрушили нам все. 

S00 [01:29:25]  : 17 или 23 человеком. 

S01 [01:29:29]  : Чтобы распределить во времени, но чтобы они не все сразу заходили. 

S00 [01:29:33]  : Хорошо, в принципе можно координировать эти вещи через Антона Германовича, то есть я в принципе не против, потому что это как бы может быть, если вы хотите, чтобы это были вещи администрированные, то почему нет? И мой третий пункт моего короткого комментария заключается в том, что здесь довольно странная происходит вещь, потому что мы обсуждаем мета. То есть у нас обсуждение на самом деле не предмета. А у нас мета-обсуждение, мета-предмета. Это очень странно по причине того, что мы уходим от обсуждения темы, что такое сознание, что такое понимание, что такое смысл. Мы понимаем, что понимание мы не сможем сказать какого-то четкого определения. Мы понимаем, что Когда мы говорим о боте, мы не говорим о том, что бот имеет модель, несмотря на то, что Антон пытался продавить активно и говорить о том, что у вас модель пользователя есть, как она устроена, что вы с этим делаете. Мы не говорим об этих вещах по причине того, что мы понимаем, что это в некотором смысле игрушка. Игрушка, которая настраивается на уровне той же самой статистики. Вот вы предлагали подключить какую-то базу знаний, условную, да? И вы говорите о том, что у вас есть контекст, который, скорее всего, определен некоторым корпусом обучения и так далее. И мне кажется, что это очень хороший итог нашего обсуждения, потому что Боты перешли, боты и ну и в принципе как бы диалоговые агенты, перешли из разряда вещей, которые обсуждаются как реальные какие-то паникийные сервисы, да, в разряд того, что это некоторая игра некоторая словесная статистическая игра, которая находится в рамках, ну я сейчас скажу, вы меня может побейте, в рамках дистрибутивной семантики, по большому счету. И мне кажется, это очень хорошим итогом. И в этом смысле я готов как бы еще раз сказать вам спасибо за этот доклад, потому что я в этой мысли уверился. 

S01 [01:31:39]  : Спасибо. Да, да, да. Окей, спасибо вам, да, что Конечно, когда был вопрос по поводу того, что образ человека, у модели просто нет образа самой себя. Не говоря о том, что у нее какой-то образ человека. Но опять же, почему нет образа самой себя? Это архитектурно. Нет просто никакой реализации. Не то, что железячная, а даже просто в программе. Программы архитектурные нет. Поэтому, конечно, не крутые штуки, типа сделать модульную систему, которая там все это дело переключает, диалог менеджер, который классифицирует. Окей, мы такое делаем. А хотелось бы в какой-то одной монолитной системе это все сделать. Но это такой ресерч, то есть научная работа. Спасибо. 

S02 [01:32:38]  : Антон, да, я хотел комментарий к Алексею Егорову. Он очень правильную вещь сказал, на мой взгляд, то, что мы не обсуждаем сложные, некоторые очень принципиальные вопросы, на которые у многих есть своя позиция. Так, слушай, у меня сегодня, как только я начинаю говорить, Я Романа Душкина просил, как раз предложил ему сделать такой отдельный семинар, на котором можно будет как раз про эти вещи поговорить, чтобы попробовать выработать в сообществе некоторое там понимание вопросов, что такое, ну не понимание, а некоторое определение для себя, что такое понимание, что такое там постановка целей, что такое смысл именно с точки зрения проектирования сильного «и», а не с точки зрения философского или мировоззренческого вообще. Мне кажется, это будет полезно, просто поразговаривать об этом, но конкретно об этом, а не в применении к чадо-боту. Именно об этом. Это важно, потому что действительно в сообществе есть на эту тему запрос. Не знаю, что из этого получится, но мне кажется, упражнение может быть полезным. 

S01 [01:33:50]  : А можно я вопрос спрошу? Просто в Игоре, я когда говорил про Шумского, я собственно имел в виду вас, ну как бы, потому что как раз я просто смотрел ваш доклад, один из докладов, где вы как раз говорили. 

S02 [01:34:02]  : Сейчас могу рассказать, сейчас дайте мне паузу, у меня тут просто сейчас ремонтник придет, один момент, сейчас вернусь. 

S03 [01:34:20]  : На всякий случай, поскольку, как понимаю, мы уже перешли больше к координационным вопросам. Для тех, кто не досидит до конца, еще раз спасибо большое Виктору. за новый поворот нашей истории, наших семинаров. Немножко с других позиций, в другом свете и на другом уровне немножко поговорили. Так что, Виктор, большое вам спасибо. Будем общаться дальше. Давайте дождёмся Игоря. А еще вопрос тогда пока между делом. Скажите, вот у вас, когда вы последовательность вот этих реплик закладываете, то есть я понял про память, то есть памяти нет опять-таки явной как элемента архитектуры, но есть неявная память, поскольку, так сказать, реакция идет на последовательность этих самых реплик. У вас, когда последовательность реплик идет модель, Различает, что это реплика машины и реплика человека, реплика машины и реплика человека, или идет просто чисто последовательность реплик? Или с вашей точки зрения это не важно? 

S01 [01:35:29]  : Вообще важно, но насколько... Я думаю, что нет. Я думаю, что сейчас не различают. Нужно еще с разработчиком поговорить. Спасибо. Игорь? Я на самом деле сам не все понимаю, честно говоря. 

S02 [01:35:50]  : Да, я в принципе тут сейчас более-менее в доступе. У меня сейчас там работа дома делают, но не важно. Виктор, ваш вопрос, задавайте. 

S01 [01:36:02]  : Я просто видел доклад ваш про другую систему. То ли там было… Про модель Шумского, да-да. И просто мы с ним общались на эту тему. Никаких спойлеров там не было особых. Но я знаю, что в МФТИ разрабатывают другую архитектуру. Почему? Потому что огромные компании захватили вот этот диплернинг. Выпускают все эти фреймворки, модели, коммерческие уже выпускают. И там, как я понимаю, цель и задача захватить какую-то другую часть рынка с помощью других архитектур и очень эффективно решать. И тем самым создать конкуренцию в рамках вот этой концепции, которую Путин в январе провозглашал по поводу искусственного интеллекта. Как я понимаю, с чем занимается ФТИ. Вот как вы можете про этот проект и какие там результаты, естественно. Какие там задачи, какие результаты. 

S02 [01:37:04]  : Смотрите, в МФТИ много проектов разных. Я знаю, что есть, например, Александр Панов, замечательный, который пару докладов или на прошлый раз, или позапрошлый раз он был и рассказывал про свои вещи. У него подход такой архитектурный. У Сергея Шумского Основная мысль, которую мы с Сергеем, философия, которую мы исповедуем, как физики, состоит в том, что для каждого... У нас есть в сообществе такой очень активный Алекс, Александр Бур, не знаю его фамилию, Алекс Бур, у него ник, который любит говорить, что все мышление – это моделирование. В этом смысле физики оперируют моделями, моделями окружающего мира. И мы знаем, что модель тогда эффективна, когда она обладает определенной предсказательной силой. На наш взгляд, сегодня люди ушли слишком в нейросети, не понимая, что это… Знаете, есть люди, которые создают модели, а есть люди, которые используют модели. Люди, которые используют модели, часто не понимают границы их применения. А любая модель, как инструмент, у нее есть границы применимости, за пределами которых она бессмысленна. Но если человеку дали молоток, он научился работать молотком, но дальше он будет везде видеть гвозди во всем и будет пытаться все забивать этим молотком. Строго говоря, концепция этих глубоких сетей Она, конечно, воспроизводит частично мозг, но на слишком детальном уровне. И у нас есть представление, что если пытаться этот уровень, волочить сложность системы до того уровня, на котором там действительно будут возникать разумные разумные с точки зрения нас, как людей, которые говорят про сильный искусственный интеллект, целеполагание, сознание, поведение, такие вещи, то эта система будет еще раза, на три порядка больше. Сейчас обучить нейросетку с нуля, вы знаете, это уже стоит несколько миллионов долларов. А если представить себе, что это будет реально система еще на три порядка больше, это будет просто невозможно. Поэтому нужны просто другие модели. То, чем сейчас люди занимаются, это как вот представьте себе, у компьютера есть процессор, который исполняет программу, но люди вместо того, чтобы понять архитектуру процессора и как работает программа, моделируют транзисторы. и моделируют 3 миллиарда транзисторов в Intel Pentium, чтобы понять, как он складывает 2 плюс 2. Это, конечно, круто, но бессмысленно. Люди, у которых много денег, типа Mark Ram, могут себе это позволить. Замечательно. но Шумский думает по-другому. Я тоже думаю по-другому. И, собственно, Сергей сделал некую модель, то, что он рассказывал про свои парсеры, модель как бы гиперколонки, которая Это на порядок более компактная и простая модель части мозга, которая исходит из предположений, что базовым элементом является не один нейрон, а некая группа нейронов, которая выполняет некоторую задачу. И если мы будем моделировать эту группу нейронов, и возьмем ее за базовые элементы, и поймем, как она оперирует, и как они складываются друг с другом, и надстраиваются в иерархии, то мы получим на порядок более компактную, быструю и работоспособную модель. Проблема только того, что делает Сергей, что когда физик модель взял, она в матаппарате обычной, и там все элегантно. Когда ты в код это кладешь, Ну, это тут все знают. Вы сами это прекрасно знаете. Когда вкладываешь в код, все сильно меняется. У тебя там миллионы развилок, в которых нужно принять какое-то решение, и дальше начинаются всякие тонкие вопросы. Вот я могу сказать так, что Сергей сам изначально свою модель делал для текстов, и на текстах она работает отлично. 

S01 [01:41:48]  : как классификация. Что она делает с текстами? 

S02 [01:41:53]  : Модель это просто следующее, грубо говоря, вот этот маленький модель маленькой гиперколонки, мы его называем парсером, неважно, у нее на входе есть некоторый сигнал, неважно какой, ну допустим вектор цифровой, потому что в конечном итоге текст – это тоже цифровой вектор, очень примитивный, из одного числа. И вот этот цифровой вектор у него на входе, и все, что делает этот маленький модуль, он смотрит сигнал на входе, изучает его и пытается вычленить в нем закономерности некоторые. и предсказать следующее, что будет дальше. Он пытается научиться тому, что будет дальше. Одна из концепций сознания, которую я, в частности, тоже придерживаюсь, вернее, один из слоев сознания, очень важный, состоит в том, что сознание – это способ предсказывать до некоторой степени будущее и благодаря этой способности предсказывать будущее выбирать свою модель поведения. И, собственно, мы как люди, мы именно поэтому являемся доминирующим видом, потому что мы моделируем у себя в голове варианты происходящего. Нам не надо прыгать с крыши, чтобы знать, чем это кончится. Мы без этого обходимся. И в этом наша сила. Гипотеза Шумского, модель Шумского, состоит в том, что если мы возьмем маленький парсер, который на своем микроуровне умеет предсказывать как бы свой сигнал, а дальше его предсказание как бы передадим парсерам следующий уровень, который уже с этим предсказанием будет работать на другом временном масштабе, потом следующим будем наращивать эту иерархию, то что называется некая там разностная… есть такая концепция, что мозг это некая разностная машина, которая все время пытается оптимизировать ошибку предсказания на каждом уровне. Вот, собственно, модель Шульского об этом. А я занимаюсь сейчас тем, что присобачив к ней reinforcement learning, это оказалось нетривиальным, потому что изначально в этой модели reinforcement learning в форме математического аппарата не было. И, в общем, когда мы ее туда встроили, это вызвало некоторое количество сложных всяких, например, много разных решений маленьких, И на самом примитивном уровне там моделька есть, которая работает на очень простой задаче, но она не интересная. Она маленькая, мы ее сделали, а сейчас мы делаем другие задачки. 

S01 [01:44:45]  : А какой у вас способ обучения? Обратного распространения ошибки или какой-то совершенно иной? 

S02 [01:44:51]  : Обратное распространение ошибки – это термин, применимый только к нейросетям. Потому как, собственно, это способ корректировки весов, который был предложен. Которого, конечно, в мозгу нет. Вы понимаете, что в мозгу нет bad propagation, точка. Хотя, кстати, полгода назад Хинтон лекцию сделал, в которой он пытался рассказать, что в мозгу есть bad propagation. 

S01 [01:45:18]  : Я просто считаю, что есть. Да ладно. Пропагация в мозгу? Я просто думаю, что мы по-житейски используем этот брек-прогресс. То есть, грубо говоря, смотрите, как девушка принимает решение о том, чтобы встречаться. Она статистически накапливает. Подарил то, подарила это, повел туда и накапливает. Потом, бах, что-то плохое сделал. 

S02 [01:45:45]  : У нее происходит поправка. Вы сейчас перескакиваете несколько уровней модели. Вы сразу от нейроны перескочили к человеку, который, безусловно, использует обратные связи для того, чтобы что-то делать. Но backpropagation имеет отношение... Это термин, имеющий смысл на уровне единичных нейронов и связи между ними. Исходя из моего знания, как работают нейроны, синапсы, связи между ними, я знаю, что весов там никаких нет и корректировать там нечего. И никакого обратного импульса распространения ошибки там нет. Я биофизикой занимался много лет. И занимался математическим моделированием метаболизма клетки. Там нет этого повышения. Я в этом уверен. Там другие механизмы работают. Я более-менее представляю, какие. Но это выходит пока за рамки модели, которую мы шум стимулируем. Но ваш тезис, я думаю, что ваш тезис был правильный по поводу того, что нужны другие архитектурные принципы для того, чтобы продвинуться на пути. Просто я думаю, что эти архитектурные принципы, если широко рассматривать, это не должны быть новые архитектурные принципы нейросеток. Должны быть новые архитектурные принципы создания ну вот там, системы. И реально, скорее всего, придем к некоторым гибридам. Уже сейчас понятно, что некоторые задачи очень эффективно решаются нейросетками, очень эффективно, типа computer vision. А некоторые ими вообще не решаются, вообще невозможно. 

S01 [01:47:35]  : Я, на самом деле, подал к вам на конфу. К вам на конфу подал мой друг, вы организатор, да? Да, да. Я подал про поиск. Я, по-моему, видел ваш доклад. Там такой доклад сыроватый. 

S02 [01:47:57]  : Мы слоями просто собираем. Антон тоже уже заявил доклад. У нас есть люди, которые в первых рядах это делают, и это здорово. Но мы это делаем такими партиями. Поэтому ответим еще, наверное, не скоро, но ответим. 

S01 [01:48:18]  : Все, я понял. Хорошо. Ну, в общем-то, хорошо, вот вы приделали reinforcement learning к этой системе. 

S02 [01:48:25]  : Виктор, смотрите, я очень, вот знаете как, я много лет считаю себя конкретным человеком, я еще не готов к докладу на эту тему, я Антону говорил. Потому что у меня нет такой работающей системы, которую я могу показать и рассказать, и не быть голословным. Я что-то рассказал, я не готов пока рассказывать, я не чувствую себя конкретным человеком. Я начинаю ходить в спекуляции, это некомфортно. Давайте, я еще там, я надеюсь, что я, может быть, до конца года ее там доведу до какого-то уровня, если повезет, и тогда я собственно расскажу, потому что эта тема прям крутая. Ну, когда она заработает. 

S03 [01:49:11]  : Насчет Сергея Шумского. Виктор, там у нас же Сергей выступал, и у нас есть видео семинара, где он рассказывал о своей последней разработке, которая, как я понимаю, вместе с Игорем делаются, и отвечал на вопрос. Вы можете найти запись на YouTube и посмотреть. Окей, окей. Да, я еще не смотрел просто. Игорь, еще короткая ремарка, тогда, наверное, уже, значит, чтобы закругляться. Если у Романа ничего не вызревет, то, может быть, вы поможете в этой ситуации? У нас же ваш семинар где-то там через месяц примерно планируется, плюс-минус или через полтора. Мы хотели поговорить про безопасность AGI, но, может быть, мы там займемся терминологическим аппаратом. Ну, как бы подумайте на эту тему. 

S02 [01:50:00]  : Я просто разговаривал с Романом на эту тему, потому что я вижу, что у него есть энтузиазм прояснить понятия. Я лично уверен, что это итеративный процесс, итерация будет работать. Готов поспорить, что здесь важны два фактора. Во-первых, важна одинаковая группа участников. Мы должны поговорить и включить в эту группу какое-то число участников разных, но ограниченное, чтобы она потом не менялась. Потому что если через два раза к нам добавится еще один человек, вся работа пошла на насмарку, мы не договоримся, это факт. Но все это знают, все прекрасно знают, лучше меня. И поэтому нам нужно сперва обсудить кто будет в этой группе, разумное число человек. Наверное, те, кто сейчас присутствуют, плюс-минус 5-10 человек. И потом в несколько итераций поговорить, потому что я думаю, что это, знаете, за один раз не сделаешь. будут какие-то аргументы, что-то против, потом вернулись к этому, обдумали, желательно, чтобы каждый что-то написал к следующему разу. Это такая, на самом деле, это некая работа все-таки, но групповая. И я льщу себя надеждой, что, может быть, нам удастся хотя бы в небольшом сообществе договориться о каких-то, ну, нескольких конструктивных пунктах. Ну, я, допустим, от себя набросал там какой-то текст, я не знаю, там, ну, там, читали, не читали, но там, ну, мне кажется, он последовательный, какие-то мысли, ну, вот как стартовый, допустим, или как своя линия там. Я в чат тогда бросал, в общем. по поводу всяких определений, как я их вижу, ну так, сходу. И вот за несколько итераций договориться с тем, чтобы дальше мы оперировали, ну, по крайней мере, какими-то… хотя бы было ядро, которое, ну, как-то более-менее, ну, как бы согласно, с каким-то набором этих определений. Понятно, что всегда будут критики. Не знаю, Владимир Смолин на нас слушает или нет, он на нас яростный критик обычно. Всего-всего. И Борис Новиков, конечно. Но он задает правильный вопрос, я не спорю. Но здесь нужно, если мы хотим в проективном ключе двигаться, именно в проективном, не просто в дискуссионном. Потрепались и каждый ушел довольный собой. Нет. А если мы хотим сделать нечто, вот мне в этом смысле очень как раз у Миколы Рабчевского понравился доклад, он был очень такой конструктивный, то есть проективный, в очень правильном проективном как бы направлении. Вот нам также нужно эти вот понятия как бы попытаться, поэтому я надеюсь на роман, но если роман не сделает, ну грубо говоря, или там в обозримое время не сделает, ну по-другому как-нибудь сделаем, сам может быть там модерируем. 

S03 [01:53:01]  : Хорошо, коллеги, еще есть последние вопросы, комментарии к Виктору? Ну тогда спасибо спикеру, спасибо всем участникам и до новых встреч. Спасибо огромное. Пока. До свидания. 








https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
