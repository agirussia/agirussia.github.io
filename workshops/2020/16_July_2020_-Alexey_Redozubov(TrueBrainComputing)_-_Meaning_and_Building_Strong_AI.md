## 16 июля 2020 - Алексей Редозубов (TrueBrainComputing) - Смысл и построение сильного ИИ — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/mcFOl-TKheM/hqdefault.jpg)](https://youtu.be/mcFOl-TKheM)

Суммаризация семинара:

Основная тема семинара касается механизмов работы мозга и их аналогов в искусственном интеллекте, а также разработки новых подходов к обучению и обобщению опыта.

## Суть
Ключевые концепции

Семинар затрагивает такие ключевые концепции, как обучение подкреплением, обобщение опыта, и использование контекста для понимания и обработки информации. Особое внимание уделено описанию сути вещей и методам кодирования информации.

Основные идеи

Основные идеи семинара включают в себя:
- Использование контекста и смысла для кодирования информации.
- Принципы обобщения и обучения подкреплением для решения вопроса "комбинационного взрыва".
- Разработка новых методов обработки информации, которые позволяют эффективно работать с короткими описаниями и выявлять закономерности в данных.
- Создание моделей, которые могут адаптироваться к различным ситуациям и контекстам, а также обучаться формировать разные архитектуры для работы в разных ситуациях.

Важные выводы

Важные выводы семинара касаются биологической обоснованности разработанных моделей и их способности работать с различными типами информации, включая зрительную информацию.

## Детали
Значимые технические аспекты

Семинар рассматривает технические аспекты, такие как использование вертикальных колоночек коры мозга как модулей для обработки информации, а также разработку алгоритмов обработки информации, которые могут работать с короткими описаниями и выявлять закономерности в данных.

Существенные примеры

Примеры, приведенные на семинаре, включают в себя:
- Аналогия с языком и его правилами для соотнесения слов и создания более богатой системы обработки информации.
- Примеры работы с зрительной информацией и контекстно-смысловыми моделями.
- Описание работы обучения с подкреплением и методов, таких как Q-learning, для поиска наилучших действий в различных ситуациях.

Принципиальные особенности

Принципиальные особенности семинара включают в себя:
- Обсуждение идеи о том, что сильный искусственный интеллект (AI) и общий AI являются одним и тем же понятием для участников семинара.
- Подтверждение предположения о том, что концепция общего AI предполагает возможность обучения формировать разные архитектуры, которые будут работать в разных ситуациях.

## Результаты
Главные достижения

Главные достижения семинара включают в себя разработку новых подходов к обучению и обобщению опыта, а также создание моделей, которые могут адаптироваться к различным ситуациям и контекстам.

Ключевые выводы

Ключевые выводы семинара касаются возможности создания вычислительной модели, которая может реализовать не только заложенные в архитектуру нейронной сети, но и учиться формировать различные архитектуры для работы в разных ситуациях.






S01 [00:02:59]  : Давайте тогда слово Алексею Рядозубову. 

S00 [00:03:04]  : То, чем занимаемся и то, о чем пойдет рассказ, это сильный искусственный интеллект. И вот именно с упором на слово сильный, то есть попытка понять, что принципиально отличает сильный искусственный интеллект, в чем его алгоритмическая суть, в чем его стилософская суть и как он реализуется. При этом, когда сама концепция формировалась, делался упор на составляющие какого рода, что есть понятные требования к сильному искусственному интеллекту, То есть это должна быть конструкция, которая где-то приблизительно мыслит так, как мыслит мозг. Не с точки зрения реализации, а с точки зрения тех возможностей, которые мы видим. Понятно, тест Тьюринга, то есть сможем общаться и не сможем понять, кто там, вот значит сильный искусственный интеллект. А вторая часть уже как раз именно, что биологически это должно быть обосновано. То есть мы должны понимать, сам проверяя себя, что действительно биология мозга, она позволяет вот те идеи, те алгоритмы реализовать, что они действительно правдоподобны. Потому что мозг показывает очень много всяких феноменов. Некоторые упоминаются часто, некоторые малоизвестны, но тем не менее, в смысле известны нейрофизиологам, но обычно не упоминаются в суть просто для того, чтобы Зачем упоминать то, что не можешь объяснить? Но тем не менее, когда много накапливается всевозможной информации и фактов, то да, хотелось построить такую концепцию, которая объяснит одновременно и как, и что происходит именно там уже в голове. В основе всего лежит представление о понятии смысла. То есть любая информация, которую мы воспринимаем, изначально смысла не несет. Смысл появляется только в результате того, что мы оцениваем эту информацию. Мы каким-то образом воспринимаем и, восприняв, говорим о том, что смотрите-ка, за этой информацией стоит какой-то смысл. в этот момент он появляется. Это все очень хорошо в декларативной части, то есть хорошо сказать, но пока не сформулируюсь, а что же этот смысл с точки зрения, а как он выглядит и как он появляется, это все пустые слова. Это первая такая часть, то есть хотелось бы из того, что воспринимается, будь то зрительная информация, будь то текстовая информация, понимать ее смысл. Это раз. Второе. Мы оперируем понятиями, явлениями какими-то, которые нас окружают, мы им присваиваем понятия и обозначаем эти явления понятиями. А затем мы говорим, что за каждым понятием стоит некий смысл. Древние называли суть вещей, некая суть. И чтобы нам можно было вообще как-то строить концепцию, хорошо бы ответить на вопрос, а что такое суть вещей? Потому что, когда мы в это упираемся, мы, оказывается, приходим к очень интересной развилке. Что вот описать, что есть явление, можно каким способом? А давайте мы возьмем набор признаков. который характерен для этого явления. Опишем этот набор признаков и скажем, что вот через него мы это явление определим. Потом мы столкнемся с тем, что бывает это явление как-то по-другому выглядит, не в этих признаках. Хорошо, скажем, давайте еще сделаем набор признаков, которые будут характеризовать это явление. Затем мы столкнемся с тем, что может так получиться, что вот похожее явление имеют как бы общие признаки, но чем-то отличающиеся. И тогда мы говорим, давайте введем метрику и скажем, насколько вот это признаковое описание позволяет нам сравнивать между собой явления, чтобы сказать, относятся они к нам, к классу или нет. Потом мы скажем о том, что нет, есть еще кроме вот этого мерка, мы ввели, значит, расстояние, в меру близости. И затем говорим, есть еще распределение, соответственно, давайте параметры распределения. Затем мы говорим, что у нас много явлений, давайте смотреть, относятся ли к какому-то явлению, исходя из... ведем некую функцию близости, которая будет учитывать характер распределения. Этот подход предельной понятия в математике очень хорошо проработан. Но он весь строится на том, что давай Давайте мы попробуем через набор признаков, а дальше можно усложнять немножко эту пробегную или сильно усложнять, но тем не менее через набор признаков описать явление. Другой подход, а давайте попробуем уловить суть и по сути узнавать явление. Как только мы это говорим, мы приходим к тому, с чего начинали. что как понять, что такое суть, как понять, что такое смысл и что такое вообще смысл информации. То есть вот получается, что куда ни кинь, хотим ли мы что-то понять или хотим ли мы что-то узнать, мы переходим к понятию смысл, к понятию суть, и пока мы не дадим четкого определения, что это такое, все наши телодвижения, они оказываются немножко такими скованными, то есть на уровне слов. В отличие от этого, описание, построенное на признаках, оно работает, на нем строятся нейронные сети во многом, и в нем все понятно. Теперь я переключу камеру, буду рисовать. Я попробую через такой наиболее простой пример, который обычно, когда мне надо объяснить, что такое смысл, пытаюсь через него. Из истории есть Вторая мировая война, и во время Второй мировой войны немцы шифровали свои сообщения с помощью шифровальной машины Enigma. Порядка миллиона кодовых комбинаций. На входе у шифровальной машины есть некий текст, последовательность букв. Это зашифрованное сообщение. Дальше. Есть сама машина. Известно ее внутреннее устройство. По крайней мере, то, что смогли англичане сделать, когда вот эта лаборатория, Тюринг и они расшифровывали сообщение. Значит, было известно внутреннее устройство. И было известно, что есть порядка одного миллиона различных кодов, ключей, которыми можно будет закодировано сообщение. Соответственно, получается так, что вот это зашифрованное сообщение поступает на машину. Если мы знаем ключ, мы подставляем, у нас на выходе получается расшифрованное сообщение, то есть знаки заменяются одни на другие, и мы это сообщение можем прочитать. Но если нам ключ неизвестен, Так, раскрыть видео. А, это не мне, это кому-то надо раскрыть. Если у нас ключ известен, мы просто его подставляем, но если ключ неизвестен, то что можно сделать? И что делали англичане? Они сказали, давайте так, мы возьмем и поставим миллион вот таких расшифровывающих машин. Естественно, был не миллион машин, они просто поставили машины, которые перебирали ключи один за другим, но делали это там распараллельно. Давайте мы миллион раз попробуем вот это зашифрованное сообщение расшифровать. Каждый раз при применении нового ключа у нас получается новая расшифровка. Вот это будем называть исходной информацией. А вот то, что получается, будем называть трактовкой. То есть каждая попытка расшифровать, применить ключ к зашифрованному сообщению приводит к появлению некого варианта букв, который мы называем трактовкой в этом ключе. И теперь вопрос, а как понять, какой из этих ключей верный? Ну, ответ практически очевиден. Там, где мы угадали ключ, зашифрованное сообщение вдруг превращается в набор знакомых нам слов. То есть везде получается некая билиберда, и только в одном ключе, который правильнее, вдруг эта трактовка, вот, например, здесь, превращается в набор осмысленных слов. Что сделали англичане? Они вычислили, что утренние метеосводки заканчиваются у немцев всегда словами «Хай Гитлер». Они брали утреннюю метеосводку, запускали ее в свои бомбы и перебирали ключи до тех пор, пока в конце не появлялась «Хай Гитлер». Как только появлялась «Хай Гитлер», они говорили, смотрите, мы узнали код, которым закодировано сообщение. Соответственно, было важно, что узнав код, можно было теперь расшифровать все остальные сообщения. Дальше, если бы там не было Хайль Гитлер, а было бы что-то другое, то в принципе не было бы проблем, технически была бы проблема, но сейчас мы идеологически понимаем, что можно было бы ввести слова немецких слов и крутить до тех пор, пока не появится некое сообщение, где максимально будут встречаться немецкие слова. То есть, которое будет выглядеть наиболее правдоподобно. Что такое, значит, определить правдоподобие в данном случае? Это посмотреть, что получившаяся трактовка состоит из слов, которые мы знаем. Более того, что если мы, например, немецкий язык изначально не знаем и знаем только слово «Хайль Гитлер», то по мере того, как мы начинаем читать сообщение, мы можем пополнять свой вот этот словарный запас. и, соответственно, читать все новые и новые сообщения, в которых будут встречаться какие-то новые слова, которые мы уже узнали. Так вот, теперь немцы поменяли код, мы теперь уже в другом ключе найдем эти слова и, соответственно, сможем понять, какой код. Вот в этой цепочке проследилась какая-то терминологическая история, что есть информация, есть ключ, то есть некая ключ, которому соответствует преобразование, то бишь правила трактовки. Мы взяли и по определенным правилам применили и функцию преобразования, которая нам дала трактовку. Вот этот ключ и вот этот механизм назовем теперь словом контекст. Мы взяли информацию. применили правила преобразования, свойственные контексту, получили трактовку в этом контексте и теперь проверили, насколько эта трактовка соответствует памяти. С точки зрения известного нам, выглядит ли она правдоподобно. И вот эта память, она одинаковая. во всех вот этих контекстах. То есть у нас есть миллион контекстов, и каждый обладает одной и той же памятью. Причем это не общая память, которую они все обращаются, а это у каждого из них своя индивидуальная память, но эта память одинакова у всех. За счет этого мы можем, обращаясь к этой памяти, проверить правдоподобность трактовок. И вот, по сути, то, что я сейчас описываю, это и есть контекстно-смысловая модель работы. Когда из исходной информации, которой сама смысл пока не несет, она зашифрована, мы находим ее трактовку, используя правила преобразования контекста, и эта трактовка оказывается смыслной. Но вот теперь главный фокус всей вот этой истории. С одной стороны, нам кажется, что главное здесь это то расшифрованное сообщение, которое мы нашли. То есть места трактуры мы увидели, с одной стороны. А с другой стороны, для шифровальщиков самым главным это был не прогноз погоды. Их совершенно не интересовал этот прогноз погоды. Их интересовало найти ключ, которым зашифрованы все остальные сегодняшние сообщения. И когда им удавалось через прогноз погоды найти это, они узнавали ключ. То есть сам ключ, который был вычислен, это была самая ценная информация, которая при этом получалась. Так вот, оказывается, что вот таким способом можно не просто узнать трактовку, но вычислить контекст, в котором эта информация приобретает смысл. И вот этот контекст сам по себе становится ценнейшей информацией. Так вот, если теперь эту историю переносить на наш мир, на, соответственно, информацию любого рода, оказывается, что любому понятию, которому мы оперируем, можно сопоставить контексты и обучиться правилам преобразования информации в этом контексте. как информация будет выглядеть в присутствии вот этого некого явления. Затем можно, получив информацию, сделав преобразование во всех известных нам контекстах, получить многочисленные трактовки. Причем число их будет удивительным образом совпадать. То есть если брать реальную зону коры и количество одновременно рассматриваемых вариантов, которые там происходят, а забегая вперед, надо сказать, что за это отвечают мини-колоночки коры. Каждая мини-колоночка, где 100 нейронов, отвечает за отдельный контекст. Порядка 1 миллиона мини-колоночек содержит типичную зону коры. То есть в одном миллионе контекстов рассматривается информация одновременно, в одном из контекстов она оказывается осмысленной, но в этот момент мы говорим, мы узнали сам контекст, а контексту соответствуют понятия, потому что пространство контекстов – это пространство понятий, которыми мы оперируем. И в этот момент мы узнаем явление. То есть понимаете, какая интересная вещь? узнаем явление не по его признакам, а мы узнаем явление, потому что, исходя из нашего опыта преобразования в этом контексте, информация вдруг встала в смысл. Ну, применительно так, на практике, если мы о чем-то говорим, то мы можем из контекста разговора, что этот гад устроил революцию и из-за него вся страна покатилась кувырком, мы понимаем, что разговор идет о лени. И при этом никакого Владимира Ильича, его имени, фамилии, роста, веса, еще чего-то не прозвучало. Мы по-любому, по какому-то конственному относящемуся к нему можем понять, о чем идет разговор. Хотя ничего к нему конкретно относящегося может не прозвучать. Вот это вот суть контекста. Но не были бы контексты таким мощным инструментом, если бы не было вот этого механизма, что все контексты обладают общей памятью. Вот вернемся к этой машине, к ее расшифровке. Предположим, мы сначала знаем только Хайгитлер. С помощью этого Хайгитлера мы расшифровали первое сообщение о погоде, узнали еще пяток или десяток новых немецких слов. Если бы мы эти слова, которые узнали, вот только в том контексте, который его узнал, в памяти сохранили, то только бы этот контекст теперь и мог бы узнавать следующие сообщения. Но мы сказали так, давайте мы эти слова возьмем и отдублируем во всех остальных контекстах. И теперь уже другой контекст может узнавать сообщения по тем словам, которые были обнаружены здесь. Так вот общий принцип системы контекстов, что информация, полученная в одном контексте, может использоваться для того, чтобы узнавать явления в совершенно другом контексте. И за счет этого удается побороть тот самый комбинаторный взрыв, который возникает, когда мы хотим узнавать много явлений этого мира, и для каждого явления мы хотим понимать, как оно выглядит во всех возможных ракурсах, вариантах и так далее. И нейронные сети пытаются сейчас это сделать и делают мучительный перебор на огромных датасетах. Давайте-ка мы попробуем. Обучить, обучить, обучить, как каждое явление выглядит в массе своих признаков. Оказывается, этого можно не делать. История с контекстами позволяет за счет обучения правил преобразования для контекста, за счет общей памяти, многократно, радикально уменьшить комбинаторную нагрузку на систему. она очень похожа на сверточные сети. Потому что что такое идея сверточных сетей? Мы хотим... Вот у нас есть картинка. На этой картинке у нас есть какой-то крестик. Мы увидели крестик в этом месте, а теперь мы хотим узнать этот крестик вот в этом месте или вообще в любом другом. Мы научимся делать ядра свёртки, который будет, по сути, образ этого крестика. У нас заранее уже задан алгоритм, по которому мы можем применять эти ядра свёртки к различным местам изображения. В том месте, где совпадет и опять ядро свёртки, покажет большой результат, мы скажем, значит, мы узнали. По сути, это и есть та же самая история. только в ней правила преобразования, по сути, не получены за счет обучения, а заданы. Мы знаем геометрию, говорим, давайте-ка мы эти правила обучения просто сразу имплицируем в эту систему, как будто бы как знание априори. Более того, в этом подходе мы говорим о том, что ядра свертки – это общая память, которая ко всем точкам применяется. Но это именно некая память, некая информация, и мы теперь просто знаем, как вот эту информацию взять и применить сюда. Но можно же по-другому это все сказать. То есть вот эту историю сверочных сетей можно вывернуть на «не знал». Потому что, когда мы говорим «а давайте мы вот этот крестик будем знать, как применить сюда», точно так же можно сказать «а давайте мы». Возьмем и научимся, имея ядро свертки, переместить картинку сюда так, чтобы она совпала с этим крестиком. Мы крестик таскаем по картинке, или крестик неподвижен, а мы картинку таскаем мимо этого крестика. Казалось бы, разницы нет, но фокус-то в том, что как только мы переходим именно, давайте мы картинку будем таскать мимо крестика, то мы тогда говорим, давайте опишем смещение в различных направлениях на различную длину и покроем все пространство картинками вот этим набором таких смещений. И для каждого смещения, которое в данном случае будет контекстом, вычислим правила преобразования, то есть как будет смещаться картинка. И если мы это сделаем, то окажется, что мы можем легко узнавать крестики, которые мы видели в одном месте, запомнили, в другом контексте, когда в другом контексте, в другом смещении. Причем здесь вроде как на примере именно сверочных сетей это все кажется очень очевидно. Но когда мы формулируем это именно давайте попробуем сделать правило преобразования, то оказывается, что это можно легко сделать. Каким образом? Мы сначала говорим, у нас просто есть априорная информация о всех возможных смещениях, которые могут быть. Откуда она берется? У человека есть глаз. Глаз еще в утробе совершает вот эти движения микросакады и сакады. Движения сопровождаются мышечной активностью, которая генерирует специальные коды. И вот эти коды позволяют взять и разметить первичную зрительную кору именно по вот этим кодам смещения. Создав контекст, каждый контекст знает, что я соответствую определенному смещению. А вот потом, когда уже глаза открываются, мы можем взять и посмотреть. Вот происходит смещение глаза. в момент смещения глаза, код которого мы знаем, одна картинка, которая была, моментально сменяется другой картинкой. И мы можем, исходя из двух картинок и того, что это одно и то же изображение, вычислить правила изменения вот этой картинки, то есть как ее, грубо говоря, передвинуть. И вот таким образом мы можем не воспользоваться априордным знанием, мы знаем, как устроена геометрия, и поэтому пишем свертки, а построить пространство контекстов просто самообучением, наблюдая информацию, которая у нас присутствует. И вот как только мы переходим к этому, оказывается, что вот такой самоорганизации, самообучением можно построить пространство контекстов для любой информации. То есть не только для зрителей, где это достаточно очевидно, где есть вот эти пространственные координаты и мы знаем, как это организовано, а для абсолютно любой информации. Будь то семантическая информация, будь то звуковая. Короче, любая информация, которая есть, вот при таком подходе можно для нее построить пространство контекстов. При этом свойство пространства контекстов оказывается, что похожие контексты мы должны расположить рядом в этом пространстве. Для зрителей, когда это хорошо наблюдается, то есть там многочисленные измерения, как расположены эти колоночки, говорят о том, что колоночки с одинаковыми свойствами находятся рядом друг с другом, образуя так называемую pinwell-конструкцию. Зачем это надо в контексте вот этой модели? Когда есть похожие контексты, похожие слова, которые дают похожие интерпретации и так далее, то мы как контексты, в которых можно истолковать, видим, Если просто случайным образом эти контексты расположить на плоскости, мы увидим картину такого снега, когда разбросаны везде соль и перец какие-то контексты, которые сработали. И мы не знаем, какой из них выбрать. Но если мы их располагаем пространственно организованно, что похожие рядом, то выясняется, что теперь у нас будут не отдельные контексты зажигаться, а будут зажигаться яркие пятна. И мы можем максимум просто выбрать и вот этот максимальный контекст и сказать, как контекст, который нас интересует. Более того, если информация оказывается многозначной, двухсмысленной, то мы можем выбрать, мы увидим два-три пятна. Вот эту историю мы смоделировали на семантических моделях. Так получилось, что пришлось заниматься древними языками. Это древний еврит, это древний шумерский язык. И, соответственно, там построить пространство, контекст для языка, когда есть несколько смысл-праздник, мы видим вот эти яркие пятна, и там, соответственно, с этим удобно работать, выбирая разные значения, разные понимания одного и того же текста. Переход к такой модели, контекстной, и опять же желание сделать так, чтобы это все было похоже на то, как работает мозг, заставила пересмотреть и еще один фундаментальный принцип — это как нам работать, вообще как должна выглядеть информация. Потому что классический подход часто говорит о том, что давайте-ка мы представим информацию в виде вектора признакового описания, то есть вектор, и каждому признаку сопоставим какой-то элемент этого вектора. 0,1 – значит, есть признак, нет признаков. А может быть, некое число, которое будет говорить нам о выраженности этого признака. Очень удобно, потому что, во-первых, все сразу у нас есть, все признаки, которыми мы работаем, все понятия, которые мы описываем. А во-вторых, удобно то, что мы можем перемножать такие вектора, искать между ними близость, вводить любую метрику, которую хотим. Хотим евклидов, хотим скалярные произведения. Математически очень удобно. Но когда мы делаем такое описание, оно становится невероятно громоздким. Работать с ним во многих случаях становится просто невозможно с точки зрения, опять же, того же комбинаторного взрыва, который вы знали. Та история, с которой, как мы считаем, работает мозг, это описание, которое ближе к семантическим описаниям, когда есть некие понятия, обозначенные кодами, и дальше, как предложение состоит из слов, мы из этих понятий формулируем достаточно лаконичные мысли. Вот есть еще объем внимания, понятий, которые человек удерживает в голове, к которым может оперировать. Это тот средний размер информационного пакета, к которым мы можем оперировать. Но когда мы из общего набора признаков, к которым мы хотим оперировать, выхватываем несколько, например, мы оперируем 20000 числом, если бы вектор некого признаков описания, мы должны были бы сделать вектор 20 тысяч элементов, Но при этом это было бы просто перечисление тех слов, которые нам нужны. Если слова повторяются, мы не знаем, как с этим работать. Сам язык обладает не только словами, а еще содержит все правила, с помощью которых предложения формируются, с помощью которых устанавливаются связи между слов и так далее. То есть система более сложная. И как это закодировать таким описанием, возникают определенные проблемы. Потому что само описание Вот именно в таком признаковой истории оно сильно ограничено. Если мы описываем именно с помощью понятий, выстраивая свой уже язык со своими правилами языка, то тогда мы можем делать и соотнесение слов мы начинаем гораздо богаче с точки зрения, что мы можем с этим сделать. Но тогда нужны другие алгоритмы обработки. И вот эти другие алгоритмы обработки – это отдельный класс, недавно мы рассказывали об этом, у нас было выступление, 40 минут где-то пришлось на это. Я сейчас бы эту часть сократил, сказал, что просто да, они разработаны, мы их тоже показали программно, как они работают. Основная идея это строится на, когда все-таки само описание имеет размерность достаточно короткий, то есть 128 бит в общей сложности можно его сделать, или 256 бит, то есть не огромные вот такие 10 тысяч слов, то поиск закономерности в них может уже идти через комбинаторное пространство, когда мы Делаем пространство различных комбинаций, проецируя этот вектор в элементы, каждый из которых видит только ограниченную часть. например, 25 бит из вот этого набора, и строить гипотезы о том, что какой-то элемент соответствует чему-то, а потом эти гипотезы проверять. Огромное количество, соответственно, фальшивых гипотез, которые отбрасываются буквально после первой проверки 90%, после второй проверки отказываются отбрасывать 99% и так далее. Очень быстро такие алгоритмы приходят к тому, что выискивается гипотеза, которая действительно отвечает за результат. Но она работает с коротким описанием. Исходя из этого, и построена модель, когда у нас возникают на пространстве коры вот эти яркие пятна, каждая из которых соответствует определенному контексту, в контексте возникает определенная трактовка, то мы можем описать то, что видит вот эта зона коры, сняв самые яркие пятна в последовательности, просто от яркого к менее яркому идеа, выстроив семь таких элементов, которые описывают, что мы видим, При этом нам важен сам контекст и трактовка в этом контексте, что мы увидели. Если говорить про зрительную информацию, то это выглядит так. Вот у нас есть, например, цифра какая-то, и мы говорим, что контексты – это смещение. А то, что внутри контекста видим, это какие-то общие для всех изображения элементы. И тогда мы говорим, мы увидели вот этот хвост вот в этом месте, мы увидели вот такой элемент вот в этом месте, мы увидели вот этот элемент вот в этом месте, вот этот вот в этом месте, вот этот в этом. И таких вот мы набираем. Но поскольку в вариации что можно выбрать огромное количество, то мы можем дополнительно делать такую вещь – обучать. систему, запоминая образы, которые она видит, давать оценки этим образом. И вот тут мы переходим уже ко второму этапу, это обучение с подкреплением. Потому что оказывается, что вся эта конструкция идеально ложится на обучение с подкреплением. Например, для того, чтобы выбрать, какие же элементы должны попасть в описание, мы говорим о таких, которые наилучшим образом позволят дальше с этим изображением работать и получать наилучший результат с точки зрения, например, распознавания. Тогда мы вводим функцию оценки качества ситуации. и можем обучать, запоминая образы, что вот такая трактовка, которую мы увидели, она действительно дает нам некий полезный результат, а такая, она как бы бесполезна, она что есть, что нет, не дает особо никакого результата. И тогда мы можем уже выбирать с точки зрения наилучшего оценки качества ситуации того описания, которое у нас получится. Ну а теперь вообще, значит, к обучению с подкреплением. Оказывается, если мы посмотрим, есть в обучении с подкреплением на самом деле те же два подхода, которые вначале я озвучивал как подходы описания сути вещей. То есть описываем ли мы через наборы признаков или мы пытаемся найти некую суть, но тогда надо эту суть описать. И вот в обучении с подкреплением есть вот это знаменитое Q-обучение, Q-learning, когда мы говорим, что в любой ситуации, давайте так, вот у нас есть агент, есть среда, и мы говорим о том, что эта среда, агент ее видит, видя нее этого описания. И вот это описание картины S, которую он видит, мы хотим найти некое действие, которое приведет к максимальному результату. И тогда мы говорим, что неплохо бы нам построить вот эту оценочку Q от картины, которую мы видим, действия, которые мы планируем совершить, И теперь тогда, оказавшись в некой среде, мы должны найти такое действие, для которого оценка будет максимальной. То есть неплохо бы разметить пространство всех возможных ситуаций, в которые мы попадаем, всеми возможными действиями, дать этим действиям оценку и дальше, соответственно, неким оптимальным способом сделать. И все это называют некой стратегией. А для того, чтобы получить какой-то опыт, да, надо вести исследовательское поведение, иногда совершать поступки, которые не совсем оптимальны, но в итоге, набрав статистику, мы это ведем ко чему-то. Да, действительно, этот инструмент работает, но он, по сути, говорит о том, что давайте мы запомним признаки, которые определяют ситуацию, давайте мы перечислим все возможные комбинации, все возможные места, куда мы попадем, Перебрав это все, получим что-то вот работающее. Другой подход в адаптивных критиках – это V-критики. И там ситуация начинает резко меняться. Там говорят так, что подождите, а давайте мы научимся делать… Вот у нас есть описание среды S, и мы попробуем сделать модель, которая нам переведет это описание, в новое описание при условии, чтобы было применено действие А. А затем попробуем научиться давать оценку вот этому результату, который получится. И тогда сам алгоритм становится уже такой. Вот у нас есть различные действия, которые могут вообще быть нами совершены. Есть модель, которая умеет применять к текущей ситуации вот это действие и строить прогноз, что будет. И есть оценка, которую мы можем дать. И тогда нам просто потом надо построить все возможные варианты вот такого прогноза для разных действий, применить к ним оценки. и выбрать то действие, которое нам сулит наилучшую оценку качества ситуации, то есть тот мир, в который мы попадем, окажется самый выгодный для нас. Ну, плюс хорошо бы сделать случайный выбор, чтобы опять сохранить исследовательское поведение. Вот когда мы в таком виде это рисуем, то оказывается, что модель, по сути которой мы обучаемся, естественно, это правило преобразования информации. Мы взяли исходное описание, и научились в присутствии некого действия, явления, вычислять, как эта картина будет выглядеть. Это ровно то, что я вам рассказывал про контексты в самом начале. Есть входная информация, есть ее трактовка. Что такое трактовка, еще раз? Это то, как выглядит информация в присутствии вот этого явления. Что произойдет, если это явление действительно присутствует? Мы осуществляем это преобразование, получаем трактовку, а дальше даём ей оценку. А как мы даём оценку? А мы сравниваем с памятью, потому что у нас есть память. В памяти у нас есть некие картины, которые могут возникнуть. С оценкой этих картин, которая дана памятью, в смысле, которая получается из опыта, когда память сохраняется, вместе с оценкой вот этой ситуации. И вот модуль оценки — это и есть такая самая память. И он же модуль, который говорит о посмысленности, что результат, который мы получим, это будет результат осмысленный, который вообще имеет для нас какое-то значение. А когда мы получаем, что он имеет значение, то мы еще смотрим, а хорошо или плохо это значение. То есть я, например, могу сунуть руку в огонь? Да, могу. Но хорошо ли это будет? Нет, это будет плохо. Вот такие вещи. И вот мы в докладах рисуем обычно, оказывается, что сама вот эта контекстно-смысловая модель Она в точности совпадает, если немножко перерисовать историю адаптивных критиков и критиков, с ней, но дополняет ее тем, как должна кодироваться информация, как это должно выглядеть с точки зрения оценки ситуации через память и так далее, и пространственной организацией, потому что, оказывается, действия могут быть похожи, могут быть близки друг к другу. И нам надо из возможных близких действий выбрать наилучшее. Более того, возможно, несколько альтернатив, рассмотреть их, возможно, дальше еще рассмотреть продолжение, что после этого действия, что будет. То есть на этом, безусловно, не ограничивается история. Но тогда нам нужна пространственная организация, возможность увидеть вот эти яркие пятна и понеслось. И вот дальше вот эту историю, Нам удалось показать, что она абсолютно биологически обоснована, что каким-то волшебным или удивительным образом мозг как будто бы специально устроен, ну я посчитаю, что действительно специально, что это и есть, удалось нащупать именно то, как работает мозг. Он реализует вот все то, что сейчас было сказано. что кора, которая основное наш мыслительный инструмент, она везде, куда ты ни ткни, состоит из одних и тех же вертикальных колоночек. То есть неких модулей, которые 50 микрометров, приблизительно диаметр имеют высота 4 миллиметра в зависимости от толщины коры, где порядка 100 нейронов располагаются вертикально друг под другом. и связи между этими нейронами гораздо поднее, чем между соседними. И когда наблюдается активность мозга, что же там происходит, мы видим активность как раз колоночек. Когда возникает не отдельный нейрон, а когда комбинация нейронов, и вот я показываю, что комбинация нейронов именно и кодирует код того, информационный код, что эта колоночка увидела, той трактовке, которая возникает. И дальше описывается, как память устроена, а основной элемент памяти, оказывается, это кластеры рецепторов, которые находятся на поверхности нейронов, то есть на гандритах. у каждого нейрона 10000 синапсов, на каждом синапсе порядка 1000 кластеров рецепторов образуется. То есть представляете вычислительную мощность одного нейрона. И вот эти кластеры рецепторов превращаются в некие простые вычислительные машинки, которые в состоянии зафиксировать информацию. Что такое рецептор? Есть поверхность нейрона, есть белковые молекулы, которые с одной стороны находятся снаружи нейрона, а с другой стороны внутри. И вот они там дали сигны какими-то. вот эти трансмембранные домены, дальше у них есть возможность связываться с легандом, то есть нейромедиаторами, которые для них характерны, и менять дальше свою конформацию, передавать сигналы внутрь клетки, открывать ионные каналы, разные вещи. И вот эти рецепторы, они способны за счет того, что у них вот эти вот кончики, объединяться в кластеры, то есть Несколько рецепторов берут и, объединяясь вот такими электромагнитными защелочками, образуют цепочки. И более того, могут дальше как некие нанороботы выполнять определенные операции. И вот элементы распознавания памяти, элементы как раз элементарных вычислений — это и есть вот эти рецептивные кластеры. За счет этого удалось посчитать, что память одной миниколонки — это порядка 500 мегабайт семантической информации. 500 мегабайт — это колоссальное число, если это говорить о текстовой информации. А именно так, как я утверждаю, скажем, мозг и хранит информацию. И за счет этого получается, что на пространстве коры формируется вот это пространство контекста, где каждая мини-колоночка – это отдельный контекст, который рассматривает информацию в своем виде. Они пространственно организованы. В каждой колоночке получается трактовка. И вот эти кластеры рецепторов — это та самая память, которая… память активная. То есть для того, чтобы сверить что-то с памятью, есть два варианта. В компьютере мы должны взять, запросить, пришлите-ка нам процессор и содержимое памяти, мы его сверим, если надо, обратно что-то запишем. Другой вариант — вот мы вам показали всем, И каждый элемент памяти сам уже смотрит на информацию и знает, что с ней делать. Сверится с собой, предназначает какие-то вычисления делать и так далее. И активная память, она и есть вычислительный элемент, который позволяет мозгу оперировать, обрабатывать поступающую информацию. То есть смотреть, насколько она похожа, адекватно, фиксировать ее, запоминать. Но для этого необходимо вот это вот распараллельное свойство, что каждая мини-колоночка, из которой состоит кора мозга, хранит всю копию памяти для одной зоны коры. И это вот ответ на тот удивительный вопрос, как так получается, что нигде не найдена локализация памяти. То есть, что бы ни поражалось, какие бы участки мозга не вырезали и так далее, память сохраняется. Ну, если уж тотально не вырезать все. То есть, если мы возьмем зону коры, уничтожим половину колоночек, мы уничтожим половину контекстов. Но вся память останется цела, потому что каждая память продублирована в каждой мини-колоночке. То есть можно 80-90% коры уничтожить, память, субытийная память, она сохранится. Исчезает только контекст. Вот опять же такой интересный пример, что если брать, например, как у человека происходят действия. Вот рука находится здесь, я хочу переместить ее сюда. Ведь на самом деле что происходит? Я же не могу запомнить все возможные положения руки, как мне из любого положения прийти в другое положение. Но у нас есть моторная кора, а потом есть мозжечок. Можжечок содержит несколько раз больше нейронов, чем вся кора главного мозга. И состоит тот же самый из маленьких колоночек. То есть там нейронов поменьше, по 60 нейронов, по-моему, на колоночке. И что делает мужичок? Он рассчитывает для каждого возможного движения мышц или комбинации движения мышц результат, который мы получим, когда он будет сделать. И если этот результат нас устраивает с точки зрения того, куда мы хотим сейчас рукой попасть, мы это движение совершаем. Более того, постоянно во время движения идет перерасчет этих комбинации, перерасчет вариантов, за счет чего у нас и появляется вся вот эта удивительная моторика, которую просчитать, запомнить невероятно, но сделать пространство контекстов и вот как вы критики это делают или как делают контексты, соответственно реализовать. Вот я пробежался очень быстро телеграфным текстом по истории с контекстами, пространством контекстов, с тем, как это похоже на то, как работает мозг, опять же, очень быстро. Ну а теперь отвечу на вопросы уже, может, более подробно. 

S01 [00:46:06]  : Так, коллеги, давайте тогда начнем с тех вопросов, которые пришли в Фейсбуке и как бы были заранее. 

S00 [00:46:13]  : Вот. Вопросы есть у слушателей? Давай мы на них ответим, потому что... Давайте тогда по ходу, по чату. 

S01 [00:46:23]  : Первый был комментарий, что смысл — это неправдоподобие. Ответ на вопрос, зачем? 

S00 [00:46:32]  : Поясните вопрос. Кто задавал? 

S01 [00:46:36]  : Александр. Можете прокомментировать? 

S05 [00:46:50]  : Он по-моему убежал. он в фейсбуке был и в телеграме был. а можно все-таки, поскольку я здесь есть, можно я влезу со своими вопросами из фейсбука. первые два вопроса как вы это соотносите с одной стороны с тем, что Хокинс предлагает, потому что сейчас то, что вы рассказывали, у меня тоже было такое дежавю, что очень похоже на то, про что говорит Хокинс. И сразу второй вопрос, как вы это соотносите с санухинским когнитоном? Противоречит ли, противоречит ли, как ложится? 

S00 [00:47:27]  : Смотрите, давайте проще с Санухиным, потому что когда он формулирует когнитон, ну это страшная история. Он говорит о том, что мы не знаем как, но если много чего запутать, то там что-то возникнет. Он не предлагает никакого конструктивизма с точки зрения, как это реализовать. Он даже не пытается объяснять, как это работает. Он просто говорит о том, что количество переходит в качество, и если слишком много всего запутать, где-нибудь само что-то возникнет. И что ключевое положение, что связи определяют наше «я». Главное – это связи. то полностью противоречит тому, что действительно наблюдается в коре мозга. Это полностью противоречит тому, что я говорю. Потому что не на связях нейронов запоминается информация. Она запоминается на вот этих рецептивных кластерах. А это абсолютно другое. И самое главное, что когда идут вычисления, оно идет в масштабах мини-колонки. И миллион мини-колонок повторяет эти вычисления, просто каждая в своем контексте. То есть нельзя вообще говорить о связях, которые что-то фиксируют. Представление о связях, которые что-то фиксируют, это по сути нейронная сеть. Да, там есть леса нейронов, там есть нейроны бабушки. И вот смотрите, мы как только мы говорим нейрон бабушки, мы переходим сразу к тому самому первому подходу. Мы опишем явление набором признаков. Потому что нейрон бабушки – это и есть нейрон, который сам своим определил признаки. Дальше мы говорим, ну хорошо, не один, а два нейрона, три нейрона, комбинации нейронов, но все равно они своими весами определяют признаки. А как только весами определяют признаки, дальше мы приходим к этому, да, что вот эти веса с этими связями, они определяют дальше взаимосвязи всего остального. Нет, это совсем не так. С моей точки зрения. Это даже близко не похоже. Теперь, если говорить о Хокинсе, то кроме того, что там упоминается слово миниколонки, больше ничего общего по сути нет потому что там темпоральная память там по сути говорить давайте мы будем перечислять все что можно на вот этих вот элементов там как на которых будет запоминать то есть они тут ключевой момент это именно понятие контекст контекстного преобразования и памяти, который сравнивается для того, чтобы определить осмысленность получившейся трактовки. Потом есть то, что результат — это сам контекст, который мы узнали, и понятия, которые с ним связаны. Может быть, я этого не проговорил, Опять же, удивительная вещь, что когда мы узнали контекст и связанное с этим понятие запустили в оборот мозга, то есть вот мы что-то узнали, дали понятие, теперь оно тоже как информация, потом следующее, идет рекуррентный процесс, когда новая и новая информация появляется, в какой-то момент, опа, мы к чему-то пришли. то в этот момент эта информация превращается в понятие, которое само по себе может быть многозначным, многосмысленным и тоже также подвергается трактовкам и так далее. Вот ничего такого не описывается, соответственно, в этой темпоральной памяти даже близко. Поэтому там нет инструментов, которые бы... То есть здесь я говорю, что за счет этого я решаю вопрос, проблему перечисления огромного количества опыта, которого мы получаем, и обобщения этого опыта. То есть я показываю сам инструмент обобщения. Как уйти от комбинационного взрыва не говорится, вопрос обобщения, ну то есть опять же все переваливается на обобщение, но обобщение воспринимается как давайте ведем меру близости, ведем класс, вот все, что похоже внешне, будем относить к этому классу. Понимаете разницу? То есть здесь обобщение не есть введение меры близости и говорить все, что похоже будет относиться к этому классу. 

S05 [00:51:34]  : Спасибо. Короткая ремарка. Мне все-таки кажется, что то, что вы отвечали, это имеет отношение больше к тому, что у Хокинса в HTM, действительно, в HRHLTM, но последние его работы по поводу теории тысячи мозгов, они как раз не совпадают, но где-то ближе, достаточно близко к тому, что вы говорите. Это просто ремарка какая-то. 

S00 [00:52:00]  : Да-да-да. Когда он говорит про тысячи мозгов, он говорит о том, что давайте мы действительно распараллелим и будем в тысячи вариантах смотреть. Но вот это распараллеливание, первое, он говорит о тысячах, а реально разговоров будет миллион. То есть это совсем другая, это три порядка разницы. Дальше он не формулирует главного, зачем это распараллеливание. Если мы не говорим об общей памяти, которая общая у всех контекстов, и о правилах преобразования, то мы описываем совсем другую модель. У него этого нет. Есть входная информация, она смысла не имеет. Есть контекст, он позволяет дать трактовку этой информации, преобразовать ее в другую информацию, которая в тех же понятиях, в тех же терминах и так далее, но она другая, она совсем другая. Мы взяли картинку, мы ее передвинули, то есть получилась новая картинка, мы ее сдвинули, это если применить к зрительной истории. И вот эту картинку новую мы теперь будем уже проверять на соответствии с памятью, но эта память, опять же, общая для всех контекстов. Вот это я вроде бы повторяю как некую мантру, но если этого нет в модели, это уже другая модель. 

S01 [00:53:17]  : Антон, давайте перейдем к вопросам других коллег. 

S00 [00:53:26]  : Можно еще вопрос? 

S03 [00:53:31]  : А как формируется контекст? Какие алгоритмы формируют контекст? И еще один вопрос. Связано ли это как-то с процессом идеализации? 

S00 [00:53:41]  : Вот формирование контекста это и есть идеализация. 

S03 [00:53:44]  : А тогда более точно в чем состоит определение идеализации? 

S00 [00:53:50]  : Смотрите, когда мы говорим понятие стол, то мы сначала хватаем внешние признаки и говорим вот четыре ножки, вот столешница, это у нас стол. Вот стол, это стол, это стол, они все похожи, вот это столы. У этого три ножки, у этого там столешница какая-то круглая, у этого квадратная, у этого одна ножка. Но постепенно у нас возникает одно, что всегда, когда есть стол, когда кто-то нам показывает на стол, некое свойство, что оказывается за столом удобно оперировать расставленными на нем предметами. И теперь уже у нас есть операционный стол, пенек в лесу, который стол, где мы стакан поставили, есть рабочий стол компьютера и все это стол. И, опять же, тот же Платон говорил, что нет стола, а есть столешность, некое свойство, которое присуще всем столам. Нет стола, а есть столешность через это свойство. Это и есть идеализация, как он это воспринимал. Я абсолютно нахожусь на этих платоновских позициях, что на самом деле, когда идет формирование контекста, то есть мы сначала, завязываясь на внешние приписники, наблюдаемые, Этот контекст формируем именно по внешним признакам, размещаем пространство контекста по внешним признакам. Мы так и делаем в наших моделях, когда у нас зрительная история. Мы размещаем пространство по движениям глаз, которые доступны, которые легко наблюдаем. Затем мы наблюдаем. что меняется, вот когда глаз двигается, была одна картинка, становится другая. Есть у человека такое наружное коленчатое тело, которое состоит из многих слоев, и эти слои в себе как раз и накапливают информацию, произошло движение, вот было то, вот было после. Теперь оба эти информации Посмотрим, сравним, найдем зависимости. Вот найти зависимость одна информация и вторая, что связывает эти две информации, что повторяется всегда. Это те алгоритмы, о которых я вкратце упомянул. Они идеально подходят именно для этого, когда есть два кода. Один входной код, выходной код. И мы хотим в присутствии некого явления, когда нам известно, найти, что связывает один код с другим. Я показал на демонстрационных примерах, что эти алгоритмы очень быстрые. Вот есть кодовая задача, один код сложный, по сложным правилам сформулирован, минус соответствует другой код преобразования. Давайте попробуем. Буквально сотни правил, которые связывают одно с другим. Вот 600 правил, которые были у меня в тестах. И еще за 2-3 тысячи демонстраций они 600 правил внутри находят и создают контекст. То есть это действительно очень эффективные, очень быстрые, красивые методы обучения. 

S03 [00:56:10]  : Мне тоже есть методы обучения, которые одновременно являются и формированием понятия, одновременно являются формированием столешности в смысле активного действия и в смысле циклического процесса восприятия по Найсеру, и в том числе процессом идеализации в смысле Ильенкова. Ильенков в бывшем Советском Союзе был главным ведущим специалистом о проблеме идеализации. У меня есть замечательная книжка, где он описывает, что это такое. Я могу точно аргументировать, что те алгоритмы, которые я могу предложить, они в том числе формулируют детализацию. В этом случае было бы очень интересно сравнить эти алгоритмы и эти возможности с теми, которые у вас есть. Это было бы достаточно интересно. Может быть. 

S00 [00:56:56]  : Я с ним не сравниваю. Есть классический подход, это адаптивный резонанс. который очень во многом, скажем, является таким фундаментальной самой идеей, которая описывает, что нам делать, когда мы хотим сделать некую вот в том числе идеализацию, то есть как нам построить классы. И он как раз очень четко вот эту идею саму формулирует по поводу признаков описания, И давайте мы дальше построим расстояние и будем смотреть, что рядом. Если что-то далеко, значит, новый класс введем. И будем таким образом строить это пространство классов. Дальше с ним можно любые чудеса делать, сети какие-то строить и так далее. Но сама идея, что класс – это что-то близкое, а если что-то не попадает в новый пасс, сформулируем, она очень взуалирована, она сквозит в очень многих алгоритмах. Главная разница – это понять, Имеем ли мы дело с описанием, с центром класса, с идеальным понятием класса, с его представителем идеальным, который мы вычисляем, и потом мерой близости, или мы вообще этого не касаемся и говорим, давайте-ка мы. Вот мы не знаем ни одного самого описания. Мы просто построим правила преобразования, просто зная, вот есть учитель, который говорит, что это явление присутствовало, причем сначала этот учитель именно строит принципы признаков внешних. По мере того, как мы накапливаем опыт, мы начинаем узнавать это даже при отсутствии этих внешних признаков. Потом мы начинаем дальше приобретать опыт. И уже можно вообще от них уйти так, что о них не вспомним никогда. Но вот эти правила преобразования и память, которая общая для всех контекстов, нам позволят узнавать. Вот алгоритмически это уже становится абсолютно другая история. Когда мы применяем, например, вот то, что успешно удалось сделать, это модель парсинга русского языка. которая разбирает этому NLP классические задачи, но она делается не просто чтобы показать, что мы тоже можем выделить роль слов в предложении, а она позволяет реализовать вещи, когда неполные предложения, когда в предложении одно слово только сказано, восстановить его до конца, понимание контекста, происходящее понимание предыдущего. Ну, то есть оно показывает, как мозг действительно может вот это все делать, как может из этого мышления происходить и так далее. И это все строится как раз на вот этой истории, когда память шарится между всеми контекстами, и нам не надо ее дублировать. Главное, в одном получили, в другом уже все знают. Вот этот перенос — это мощнейший инструмент. 

S03 [00:59:26]  : Да-да-да, вот на самом деле адаптивный резонанс, он может реализовываться разными способами. У Грозберга один, вот у меня формализация тоже в точности адаптивного резонанса, причем она не связана с близостью по признакам. это, так сказать, особая конструкция. И в этом случае это было бы действительно очень интересно. И вот у меня тоже была идея, но я ее не проверял, что адаптивный резонанс на словах это действительно выделение смысла. Но это надо, так сказать, отдельно проверять. 

S00 [00:59:53]  : Но резонансы, я как раз и пытаюсь объяснить, что вот адаптивный, вот в том плане, как его понимали, адаптивный резонанс это хороший лук и раунд, но это не решение проблемы. 

S01 [01:00:04]  : Коллеги, позвольте вмешаться, то дискуссия превращается уже в диспут двух человек. Давайте ответим на следующий вопрос. У нас вот Алексей Егоров пытается прорваться. Алексей, вам слово. 

S06 [01:00:17]  : Да, я как русский индийский, я думаю, что здесь У нас в последнее время такая тенденция в США, что мы за национальное меньшинство, поэтому я, у меня нет вопроса в явном виде, да, я хотел бы сформулировать некоторые комментарии. Комментарий первый. Я заметил достаточно много плохих слов в докладе, потому что мне кажется, то есть мне они режут ухо, вот, уши у меня большие, то есть я считаю, что среди плохих слов есть информация, тот же самый контекст, опыт, память и узнавание. Эти слова, они верхнеуровневые, и если мы занимаемся тем, что употребляем их в рамках объяснения механизмов того, что такое смысл, мы должны, во-первых, их достаточно строго определить, во-вторых, мы должны определить их на каких-то более нижнеуровневых значениях. Второй момент, который бы я хотел отметить, это связанный с достаточно часто звучащим комбинаторным взрывом. Я не согласен ни с этой формулировкой, не комбинаторный взрыв, он, ну, фактически это просто оценка, да, то есть если мы рассматриваем задачи, которые у нас существуют, скажем, в машинном обучении, да, и мы имеем дело с NPC, да, с неполиномиальный Более того, для большого количества задач мы до сих пор имеем либо открытые вопросы, либо доказаны, что они не были реальны. Имеем комбинаторный взрыв, как brute force решения задачи. И полный перебор – это просто один из единственных вариантов поиска решения. Все отличные варианты – это оптимизационные варианты. Приближенные, оптимизационные. И так или иначе, допустим, Говорить о том, что комбинаторный взрыв – это какая-то проблема, с моей точки зрения, не очень точно. И здесь я бы хотел отметить, что мне тоже резонуло ухо то, что нейронные сети в рамках решения задач с помощью нейронных сетей возникает комбинаторный взрыв. С моей точки зрения, это совсем не так, по причине того, что, скажем, тот же самый B-propagation и прочие решения, оптимизационные задачи в нейронных сетях, они как раз оптимизируют исходное состояние между input data и, скажем, классификацией. Мы здесь не имеем дела с комбинаторикой. Особенно мы не имеем дела с комбинаторикой, когда применяете 

S00 [01:02:55]  : Я комментирую, это не вопрос. 

S06 [01:02:57]  : Третий аспект, третий комментарий, вот здесь я хотел бы поговорить, потому что первые два здесь не о чем говорить. Это ваше ключевое понятие смысла. На мой вкус, смысл — это незначение, когда мы пытаемся найти, как я вас понял, баланс между символьным значением и смыслом, как некоторой, ну скажем так, философской категории, мы сразу попадаем в два контекста. И здесь единственное, что может нас разрешить, что может нам позволить куда-то двинуться, с моей точки зрения, это семиотика. В рамках семиотики смысл очень близок к понятию. И в этом смысле у нас возникает такая картина, что ваши построения, с моей точки зрения, как я понимаю семиотику, они не очень хорошо отражают семиотику. И поэтому понятие смысла, как некоторого идентификатора, в рамках некоторого, это может быть любая символьная структура, это может быть сеть, это может быть гиперграф, какая разница. Но попытка приписать смыслу значения сразу означает то, что мы уходим от понятия, от семиотического понятия смысла, и мы разрушаем, в принципе, всю когнитивную картину восприятия. 

S00 [01:04:36]  : что запомнил, начну откатывать, прокомментирую. Первая сама идеология, что когда вводится понятие контекст, то понятие контекст содержит в себе знак, слово, то, что этот контекст обозначает, и то, как этот контекст проявляет себя, когда мы хотим из него построить описание. Но что есть сам контекст, это правило преобразования. Плюс, что объединяет все контексты, это общая память. И когда контекст не может существовать изолированно, вот есть пространство контекста, Вот когда оно сформировано, когда оно обычно именно на том многообразии явлений, с которыми мы имеем дело, вот тогда эта конструкция становится работоспособной. Выхватывается из нее какой-то один элемент, по отдельности не работает. Дальше. Когда мы говорим о том, что мы нашли в информации некий смысл, то разговор идет о том, что мы сформулировали название контекста и сформулировали ту трактовку, которая в нем возникла. Вот когда мы берем эту пару, мы сохраняем как бы то, что, ну некую суть, которая была, которую мы хотели передать. то есть она здесь не разваливается. Но говорить о том, что понятие это просто вот знак, смысл, который стоит и там как тезариус какой-то сделали, там вот зафиксировали, нет, потому что каждое понятие, которое возникает, оно все равно потом многозначное. Оно, как только мы, мысль изреченная и сложная, как только мы сняли с пространства контекстов некую информацию и представили ее в некой записи, там вот последовательности понятия, то в этот момент опять это получилась информация, которую заново надо трактовать. Очень хорошо это мы прослеживаем, когда на зрительную историю делается, то есть то, что на первом уровне сняли как описание цифра 2, она состоит из таких элементов, каких-то позиций. А теперь на втором уровне мы начинаем говорить о том, что подождите, цифра же может бегать. Давайте пространство контекстов теперь позиции посмотрим. Теперь оказывается все в новых контекстах, смещается. То есть опять на каждом уровне происходит новая трактовка этих же понятий. Нет ничего с гвоздями прибитого. И вот это основной принцип, что как только мысль изреченная, она все, уже ее надо трактовать. То есть нигде ни в какой момент не происходит фиксации понятия. Потому что когда мы говорим о симметрике и так далее, очень хочется, это всегда попытка привести описание информации к чему-то фиксированному и понятному. Вот нигде не происходит в этой модели. Дальше, давайте на вопросы проговорю. Когда идет разговор о комбинаторном взрыве, да, это именно комбинаторный взрыв, ну, с моей точки зрения, потому что есть, ну, вот то, что называют квантовым превосходством и так далее. То есть мы понимаем, до какого-то момента мы это посчитать можем, а с какого-то момента очень трудно, а с какого-то момента не хватит времени жизни Вселенной, чтобы это посчитать. И это очень быстро наступает. Вот этот водораздел, он присутствует. И когда мы говорим про нейронные сети, мы себя немножко успокаиваем. Мы говорим, а что может эта нейронная сеть распознать? Котиков, собак, пешеходов, велосипедистов, там 20 явлений, да, вот мы натренировали. А если не 20, а если 200? А если 500? А если 1000? И вот в этот момент оказывается, что это нелинейная история. Что тут идет экспоненциальная сложность и уже не работает. Поэтому комбинаторный взрыв возникает. Поэтому когда мы говорим, давайте мы на простых каких-то ограниченных вещах, компьютерной игре и так далее, на простых признаках научимся это делать, да, до какого-то момента можно. Но с какого-то момента количество обучаемых данных даже становится настолько колоссальным, чтобы это обучить, что мы этого уже сделать не можем. Вот тогда и возникает необходимость какой-то оптимизации. Дальше. Если мы говорим, давайте оптимизацию делать за счет Вот то, как в итоге делают внутри себя нейронные сети. За счет того, что будем рождать нейроны бабушки, которые своими весами типичного представителя класса выражают, описание бабушки, нечто среднее, усредненное. Вот в этот момент мы приходим к другой парадигме. Я и пытаюсь эту парадигму критиковать и говорить о том, что она имеет свои ограничения. Она работоспособна, безусловно, на ней держится огромный математический аппарат, но есть альтернатива. Это вот история с контекстом. Остальные вопросы были, комментарии были, я забыл. Ну-ка, комментарии мы зато все услышали. 

S01 [01:09:04]  : Алексей, поскольку здесь еще много вопросов поднасыпалось, я немножко... 

S06 [01:09:12]  : Я хотел бы обратить внимание, что, опять же, если возвращаться к синематической модели, и я сейчас не буду остальное комментировать, самое важное, здесь мы имеем два пространства. два пространства, которые даны нам, грубо говоря, в образе. Это пространство феноменальное, это образы феноменального пространства, это когда вы уже рассуждали про изображение, это вот оно. И у нас есть понятильное пространство. Когда мы говорим о смысле, мы говорим о нем. То символьное пространство текста, моделей – это всего лишь навсего референция этих двух пространств. Они открыты оба, понимаете? И когда вы начинаете смешивать как бы право-слева, возникает некоторая путаница, которая в конечном итоге не позволит сделать алгоритм. 

S00 [01:09:56]  : Ключевой момент, это и подчеркиваю, что нету двух пространств. Вот нету в этой модели двух пространств. Есть одна входная информация, которая определяется в неких понятиях. И есть трактовка, которая получается, которая в тех же самых понятиях. В смысле, меняются понятия, но это те же. То есть, трактовка и исходная информация в одном и том же пространстве существуют. 

S06 [01:10:24]  : Да-да-да. Так нет, я об этом и говорю. Я говорю о том, что вы полагаете, что эти пространства существуют. Так нас на этом не удалось показать. Законы природы, они хороши тем, что их невозможно не исполнять. Ну да, ради бога, я не против. 

S00 [01:10:40]  : Вот на этом, в суть, удается сделать всякие забавные вещи, то есть чудеса. Потому что понятие контекста, есть формальный анализ понятия, теория решеток и так далее, и там понятие контекст вводится именно как, вот у нас есть некие понятия, есть контексты, а понятие содержит набор признаков, то есть вот понятие определяется его набором признаков. И вот давайте мы теперь сделаем контекст, как то, где понятие с одним набором признаков переходит в то же понятие, но с другим набором признаков. И дальше из этого будем строить математику, смотреть, соответственно, какие-то характерные для контекста признаки и так далее. Вот здесь предлагается другое переосмысление понятия контекста. Вот такая вот история. 

S01 [01:11:27]  : Алексей, следующие вопросы немножко сгруппирую, потому что похожие вопросы несколько участников задало. Во-первых, определение сильного искусства интеллекта. Возможен ли он и нужно ли ему сознание? 

S00 [01:11:41]  : Вот нужно ли сознание, с конца начну, не знаю. Мы на эту тему Федософский семинар по пятницам проводится. По-моему, уже два раза возвращались к этому вопросу. Один на лекции была полностью посвящен. У меня есть теория того вообще о роли сознания, что это такое. Но это отдельный, долгий разговор. Но вот нужно ли сознание для того, чтобы реализовать синий искусственный интеллект? Такое ощущение, что можно и без него. Но при этом это будет синий искусственный интеллект. Что отличает сильный искусственный интеллект от слабого? Это как раз работа со смыслом. То есть, если интеллект может оперировать смыслом, ухватывать смысл, суть, а не просто пытаться имитировать какую-то деятельность, если при этом он имеет систему оценок, то, что сажда, мы называем системой эмоциональных оценок. Ну, а по сути просто оценок хорошо-плохо, оценок качества ситуации, возможность оценить. И на базе вот этой системы оценок может формировать свое адаптивное поведение, то я вот такую конструкцию называю сильным искусственным интеллектом. А уже критерий того, действительно ли это так, это тест Тюринга, это вопрос пройдет, не пройдет. 

S01 [01:12:58]  : Спасибо, надеюсь, ответили на вопрос, на эти вопросы. 

S00 [01:13:06]  : Хорошо, давай все-таки присутствующих послушаем. Наверняка кто-то еще что-то хочет спросить. 

S05 [01:13:14]  : Я вклинюсь в паузу. По поводу памяти. Посмотрите, у вас прозвучало вначале, как я услышал, что память у каждой колоночки своя, и они эту память копируют друг другу. Теперь, значит, вопрос называется, внимание, вопрос, является ли наличие индивидуальной памяти, то есть, если я правильно вас понял, является ли наличие индивидуальной памяти у каждой из колоночек, скажем так, некоторым важным условием реализации этой архитектуры или можно на самом деле просто сделать так, чтобы память была общая, да, и все работали с общей памятью? 

S00 [01:13:52]  : Конечно, тогда мы программируем мы делаем общую память, потому что скорости тактовой частоты мозга, скажем, 15-30 Гц, может, 50 где-то, а частота компьютера – это гигагерц. Поэтому мы можем позволить себе просто-напросто одну и ту же колоночку моделировать, имитировать и так далее. Но вот именно в архитектуре мозга и в том, как мозг это делает, ему нужна активная память. То есть там нет вот этих вычислений, которые делает компьютер. Там эти вычисления, то есть сверка информации с памятью, ложится на саму память. И поэтому не остается мозгу, кроме как в каждой колоночке, иметь собственную копию памяти. Да, на компьютере мы можем, имея одну память, использовать ее для каждого из контекстов. Но это особенность не на архитектуре, которую мы используем, потому что даже если мы параллельные вычисления делаем, мы все равно можем шарить память и так далее. 

S05 [01:14:45]  : Я правильно понимаю, что вы рассказываете некоторый принцип и некоторую реализацию этого принципа в коре головного мозга, и дальше вы говорите, что мы можем этот принцип реализовать на другой архитектуре вычислительной, где будет общая память, и в результате этого получить все равно сильный искусственный интеллект на основе вот этого же принципа, но в рамках другой вычислительной архитектуры, правильно? Да. 

S00 [01:15:09]  : Спасибо. Хотя хотелось бы архитектурно, имеется в виду с точки зрения, если делать какие-то процессоры, которые специально рассчитаны на эту модель, делать именно активную память в каждой колонке. Потому что понятно, что тут весь вопрос – это вопрос распроллеливаемый. То есть когда мы должны в миллионе контекстов посчитать, миллион причем хорошо распроллеливаемых, очень хорошо распроллеливаемых процессов, то грех как бы считать последовательным. 

S02 [01:15:45]  : У меня еще вопрос. 

S03 [01:15:55]  : программы. Но дело в том, что в Reinforcement Learning можно сформулировать ту же самую теорию функциональной системы Анохина, как Reinforcement Learning. И при этом мы сами проводили такие эксперименты, что формализация, которую мы сделали, она принципиально точнее и сильнее, чем чем Q-learning. И поэтому, по крайней мере, по отношению к тому примеру, что вы приводили, можно было бы предложить использовать ту формализацию функциональных систем, которую мы делали и более того, экспериментально проверяли. 

S00 [01:16:36]  : Есть много хороших примеров того, как обучение с подкреплением творить чудеса. В принципе, каждый из них достоин того, чтобы описать, почему удалось что-то достойное сделать. Но вот когда все-таки задаешься вопросом, как мозг работает, я пытаюсь настаивать на то, что именно та история, которая возникает в адаптивных В-критиках, она наиболее близка к этому. а там все-таки очень четко идет именно разложение, первая модель мира, которая нам позволяет для действия, не действие оценивать, не пару, вот что мы оцениваем? Пару, ситуация, действие или мы разделяем, мы говорим отдельно, давайте мы позволим, сделаем новую ситуацию и отдельно оценку этой ситуации. За счет этого у нас, понятно, сложность возникает на этапе создания модели мира и на этапе создания системы оценки, но зато мы теперь уже практически не ограничены вот этим количеством комбинаций. Ситуация, действие. То есть мы развязались. 

S03 [01:17:49]  : Ситуация, действие новой ситуации и получающие результативнейшие правила, которые можно обучиться и получать оценку в виде оценки вероятности предсказания. 

S00 [01:18:01]  : Всегда отвечаем на вопрос, что мы в итоге оценили? Пару? Ситуация, действие или мы отошли от этого? Вот если мы приходим, если все равно действие в ситуации, то мы остаемся где-то в районе правильного курьеринга. 

S03 [01:18:23]  : Нет, ну там, так сказать, в той системе, что у нас главная оценка – это прогноз достижимости цели. Ну ладно, это, впрочем, опять же дискуссия. 

S00 [01:18:32]  : Ну вот прогноз достижимости цели – это есть ли есть цель? Да, конечно. Это тоже вопрос такой хитрый, потому что откуда берется эта цель? Если смотреть на всю архитектуру, когда у тебя много зон коры и так далее, то оказывается, что сама и цель, она возникает как результат обучения с подкреплением, исходя из ситуации. То есть мы в этой ситуации увидели возможности, она стала нашей целью. Вопрос, когда есть дальше пространство, которые реализуют возможные действия, то если в каком-то из этих пространств, они же не вычисляют по запросу, ты меня вычислил, они молотят все одновременно, нужны они или не нужны. И если вдруг какое-то действие нам говорит о том, что надо запереть, а я могу оказываться в вашу цель. а цель сообщена этому пространству. Реализовать, и значит, соответственно, оценка результата моего возникает высокой, потому что под эсселью, так давайте меня не сделаем. Вот оно как бы, мы к тому же и приходим. 

S01 [01:19:31]  : Так, коллеги, я вот вижу поднятую руку Сергей Шумский. 

S04 [01:19:37]  : Тогда я могу задать вопрос, да? Давайте, пожалуйста. Спасибо. Алексей, спасибо большое. Мне показалось, что основную идею я понял. Она хорошо объяснили на примере расшифровки, но в этом примере Ключевой все-таки элемент это то, что каждая копия машины имеет свой словарь немецких слов. Переходя к мозгу, каждая колонка имеет один и тот же словарь. одной и той же памяти. И вот здесь я теряюсь, я не могу понять, как могут все колонки иметь одну и ту же память, поскольку история каждого нейрона уникальна, и то, что он видел, уникально. Как организовать вот эту общую память у колонок с помощью концептивных кластеров? Я не понял. 

S00 [01:20:48]  : Если у нейрофизиологов спросить, как информация гуляет по карету, вы услышите, наверное, неожиданную историю, давно ее уже сформулировали, что нейроны не кричат, они шепчут. Дело в том, что то, что мы наблюдаем в виде спайков, когда потенциал скачет, сама амплитуда порядка 100 мВ, и когда эти спайки возникают с какой-то периодичностью, то это уже вершина айсберга. И более того, сами эти спайки и то, что они делают, это вершина некоего химического процесса. Потому что то, что мы видим как спайк, это одновременно еще и выбор из колоссального количества нейромедиаторов во всех синапсах этого нейрона. То есть вот у него 10 тысяч синапсов, там среднем это для коры, и вот в 10 тысяч синаптических щелях выбрасываются несколько пузырьков этих нейромедиаторов, а каждый пузырек — это 6-7 тысяч, по-разному, молекул. То есть это масштабный процесс, который идет, когда у одного нейрона идет спад. И это абсолютная вершина. А основные информационные процессы — это на уровне одного милливольта. когда идет квантовое испускание нейромедиатора, когда в синаптическую щель выбрасывается один-единственный пузырек, когда боссинаптический потенциал, они называются миниатюрные потенциалы, токи миниатюрные, которые идут на уровне одного милливольта, которые распространяются под эндритом, которые дальше вызывают определенную активность и так далее. То есть основной обмен информации идет на этом уровне. а уже потом для того, чтобы масштабно сделать какие-то вычисления, которые будут химические характеры носить. Так вот, показано, что может информация распространяться по коре, вот из зоны коры, и одна и та же информация распространяется неким условным образом радиовещания, так что каждая колоночка получает одну и ту же информацию кодовую в виде кодов. А коды – это некая, если взять объем, колоночке некая пространственная активность в этих бентритах, которые что-то кодируют. И вот она может распространяться, переносить информацию. Каждая колоночка получает информацию, потом с помощью этих спайков делает вычисления. А когда выясняется, где смысл найден, где и так далее, то она может также распространиться обратно для того, чтобы то, что обнаружено в одном контексте, эта трактовка запомнилась теперь в других контекстах. То есть вот как это на уровне химии и электрики мозга сделать, есть на эту тему у меня даже отдельные рассказы. По крайней мере, это показывается. Утверждать, что это точно так делается, нельзя, потому что это надо мерить. Но то, что для этого у мозга есть весь инструментарий, это да. Тут главное отойти от того, от этого представления, что нейрон на своих связях что-то формирует. В связи мозга ничего не формирует, то есть этого даже нигде не существует. То есть есть так называемая когда смотрят, синаптическая пластичность, да, но есть четыре типа синаптической пластичности, краткосрочная, долгосрочная и так далее, но все это такое окукал, все как это происходит, как это наблюдается, это надо знать в условиях этих экспериментов, это абсолютно не то. Это не связано ни с памятью, это не связано ни с формированием каких-то информационных процессов. Все, что происходит, да, изменяется синапс, но это не меняет нейроны какой-то своей там весов синапсов для того, чтобы что-то запомнить. 

S04 [01:24:37]  : Правильно я понимаю, что рецептивные кластеры, они в синапсах находятся? 

S00 [01:24:42]  : Они по всей поверхности дендрита находятся. Они и в синапсах, они и вне синапсов находятся. Кроме нейронов, существенный объем, даже больше, чем сами нейроны, занимает по площади так называемые глиальные клетки, плазматические астроциты. И на поверхности этих астроцитов находится огромное количество тоже этих рецептивных кластеров. И эти астроциты имеют короткие росточки, значит, они принимают участие в работе синапсов. Есть такая концепция, называется трехстороннего синапса, где показано, что Вот все, что происходит в синаптической щели, какие нейромедиаторы выбрасываются, как это все определяется не только самим нейроном, сколько и окружающими астроцитами и так далее. Это хитрый процесс, конструкция очень сложная. Но какие-то основные фундаментальные вещи более-менее понятны. Ну, это как, если мы берем, скажем, первые компьютеры, которые делали, они были еще более-менее, там, посмотрел, понятно. То есть, если сейчас начать обратную, там, пытаться процессор, как он работает, ну, столкнемся с огромным количеством всяких нюансов, деталей и так далее, которые, ну, да, они все понятны, все нужны, но они сложны для того, чтобы говорить о простой структуре компьютера. 

S04 [01:26:00]  : Ну, хорошо. Еще вопрос номер два. а уже не по деталям молекулярным, да, а по принципу. Ну, если этот принцип работающий реально, да, то, ну вот как говорил Фейнман, что не могу вас создать, того не понимаю. То есть у вас должна быть, ну это уже вопрос к… такой научной программе. То есть у вас должна быть какая-то научная программа, как этот принцип развивать в сторону сильного интеллекта. То есть сильный интеллект, в конце концов, это есть некая программа или искусственная психика такая-какая-то. Вы должны тогда каким-то образом двигаться в этом направлении. У вас должна быть какая-нибудь моделька, которая показывает какие-то когнитивные способности. Сначала слабые, потом все более сильные и так далее. У вас в этом направлении вы куда идете? 

S00 [01:27:05]  : Сейчас в свое время было сделано несколько программ, которые показывали работоспособность отдельных элементов в концепции, которые показывали, вот память как на этих кластерах, вот смоделирована колоночка, вот как память, вот она как работает. Там Гиппокамп, его роль описывалась как не место, где воспоминания хранятся, а роль Гиппокампа именно как генератора ключей воспоминаний. А уже само воспоминание хранится сразу в колоночке, то есть ключ просто несет информацию, место, время, такую вещь. Ну и было показано, как два кода пространственных, как некая такая интерференция этих кодов позволяет создать в колоночке на вот этих кластерах отпечаток этого воспоминания. Причем такой, который с одной стороны вот как он записывается, а вот когда ключ повторили, вот как он воспроизводится информацией. Вот это запрограммировал, это я показал. Затем было показано, как распространяются информационные волны. Запрограммировал, показал, как информация передается в виде кода по пространству коры, как разбегается, как передается через волновые каналы. Те связи, которые существуют, система связей, то, что соединяет зоны коры, это не отдельные аксоны, а это пучки. То есть это некое такое, как оптоволокно, когда не одно, а вот не таким пучком идет. передает сразу картиночку. Вот такие вот зоны на зону идут, и вот как с помощью них передается эта информация, по зонам распространяется. Затем запрограммировал, показал, как идет обучение. по правилам преобразования, когда сложные коды на сложные коды через вот это пространство-контент. Пространство, комбинаторное пространство, вот сделал. Но это отдельный разговор, это не алгоритм обучения. Это работает, это показал. Потом сделал модель семантическую парсинга русского языка, которая без самообучения для правил языка, где просто контексты, они руками задаются и просто исходя из знаний, ну, уже русский язык хорошо описан и так далее, описывается контекст, описывается память, и за счет этого получается вот эта система распознавания партинга. Но, тем не менее, что, ну, и она, что интересно, она показывает, что вот то, что Хомский описывал как формальные грамматики для, там, соответственно, описания языка, оно просто один в один реализуется в рамках вот этой контекстной модели. Что это по сути те же яйца, только вид сбоку. Вот это удалось показать. Дальше есть сейчас ребята, которые работают в Австралии, которые на это гранты получили, показывают, как в зрительной системе реализуется та же самая контекстная смысловая модель, как строится пространство контекстов. Насколько сил хватаем, делаем. Я показал, сделал для древних языков примеры, где пространство контекста позволяет, которое уже теперь на самообучении строится. которое позволяет читать древние тексты. Ну, как всегда, проблема, что есть слово, слово имеет много значений. Есть там знак слова еврита, которое записано консонантным письмом, то есть безогласным, и может быть прочитано 10 разными способами. Есть слова шумерского языка, которые записаны или написаны символом, или сочетанием символов, которые имеют несколько вариантов прочтения. Для того, чтобы понять, нам надо понять смысл. Один из вариантов, мы можем взять наиболее вероятные, наиболее часто встречающиеся, ну и, соответственно, мы что-то получим. Но если мы построим пространство контекстов, а пространство контекстов — это пространство слов языка, сделаем их порядочным, организуем так, чтобы похожие были рядом, а это очень похоже на то, как получается результат в самоорганизующейся картке кофе, когда они там со словами это строят. Вот когда мы строим это пространство. Затем мы говорим, давайте-ка мы для каждого слова поймем, насколько оно часто встречается с другими словами. То есть не просто статистику, а это мы говорим, давайте от вероятности априорной к апостриорной перейдем, то есть баясовская вот эта история. И каждый контекст, по сути, вычисляет нам уже вот эту вероятность того, что вот такая-то фраза встретится, но при условии, что реализовалось это событие. Причем не слово появилось, а при условии, что об этом говорится, об этом понятии говорится, то есть в этом контексте. Это очень важный момент. Практически всегда, когда о чем-то говорится, само слово не называется. И вот в этом контексте строится некая формально-грамматически возможная конструкция из вот этих исходных слов и проверяется, насколько она вероятна в этом контексте. И когда мы строим, мы видим яркое пятно в том месте, где действительно слова оказываются вместе употребимые и так далее. И мы выбираем это значение. И оно, оказывается, отличается от того, если мы будем просто делать вероятностный выбор по максимальной вероятности. Если есть, а шумер из-за красив, то что тексты шумерологи неправильно перевели вообще ничего, то есть там огромное количество афоризма. А что такое афоризм? Это два смысла фразы, которые надо читать либо так, либо так. И ты видишь, когда вот два пятна, когда вот это прямое прочтение, это вот смысл, это все красота. Это начинает сразу очень похоже на то, как действительно мозг работает, как мы наблюдаем в мозгу вот эту активность, когда там ее снимаем. Ну, то есть, короче, есть некие результаты. Постепенно идём. То есть было бы больше денег финансирования, наверное, взяли бы ещё там людей и так далее. Сейчас пытаемся с Сбербанком эту тему говорить. Вот True Brain Computing создали для того, чтобы там, ну, сайт где-то там, что-то выложили, материалы. Мало ли кого-то привлечёт. Да, очень хочется обратить на это внимание и как бы заниматься более масштабно. К сожалению, то, что своими силами удаётся сделать, там вот у нас три-четыре человека работают, сколько успеваем, столько делаем. 

S04 [01:32:49]  : Спасибо большое. 

S01 [01:32:51]  : Антон, вот вижу ваш вопрос. Давайте вы. Еще у нас Алекс висит давно с поднятой рукой. 

S05 [01:32:58]  : Значит, я с конца начну. Вот то, что вы про НЛП сказали, есть ли у вас какие-то публикации или результаты, которые можно показать? Просто вот мы тоже занимались этой историей, правда занимались немножко других позиций, но просто интересно было бы сравнить результаты. 

S00 [01:33:13]  : Есть программы, которые можно показать, которые работают. к сожалению, публикациями не дошло, потому что там много надо описывать. 

S01 [01:33:21]  : — Готовим сейчас с Алексеем как раз в процессе, но таком, неблизком. 

S00 [01:33:24]  : — Дорогие вещи не публиковали, а вот то, что касательно языков, там Шумерский и так далее, это все идет в рамках более масштабного проекта, грандиозного там абсолютно. Там уже это все как-то в рамках вот этого, когда будет итоговая публикация, то есть отдельно описать именно вот этот результат. Ну, наверное, тоже, если руки дойдут по пробкам. Но программа есть, то есть показать есть что-нибудь. 

S05 [01:33:52]  : Антон, еще вопрос был про… А, да, еще про 7. Значит, вот волшебное число 7, вы про него говорили, соответственно, вопрос, есть ли понимание, с чем оно связано в рамках… Абсолютно нет, нет понимания, с чем оно связано. 

S00 [01:34:06]  : Абсолютно четко есть понимание, с чем она связана. Когда мы начинаем смотреть, с какой длиной кода мы можем работать. Идея такая. Вот у нас есть код какой-то длины. И мы хотим… есть один код, есть другой, в который мы должны перейти. Есть какая-то закономерность, которая связывает одно с другим. Почему она может быть хитрая, с какими-то хитрилогическими правилами, с какими-то… вот мы не знаем даже заранее, что это такое. Количество комбинаций, которые могут связывать вход и выход, оно фантастическое, оно настолько фантастическое, что, ну понятно, никакими числами его не переберешь. но, вот это количество правил, чтобы найти, но оказывается можно взять и сделать разбитые случайные пространства, каждая из которых будет видеть только часть входного и часть выходного вектора. И вот оказывается, что вот эту вот исходную комбинаторно сложную задачу, которая практически нерешаемая, чтобы связать вход с выходом, перевирая комбинации, что может быть там внутри. Если разбить на вот эти комбинаторы относительно простые задачи, то есть вот входной вектор 256 бит, а мы разбиваем на вектора по 30 бит, случайные, но делаем их много, делаем их миллион таких векторов. В смысле вот этих подпространств, иначе говоря, точек комбинаторного пространства, так я это называю. Если брать по аналогии с мини-колоночкой, то там миллион синапсов, и вот каждый синапс видит не все 100 нейронов, он видит только 25 аксонов, которые проходят мимо него. И вот нейромедиаторное воздействие на один синапс оказывает 25 нейронов этой колоночки. И вот получается так, что каждый синапс по тому, какие нейромедиаторы, а нейромедиаторы каждый нейрон разный спускает, как деле нейромедиаторов. он видит картину от 25 случайных нейронов из вот этих 100. Но зато таких мест миллион. И вот оказывается, что то, что надо увидеть, один, может быть, две, может быть, три вот этих места да заметят. Но вот чтобы гарантированно увидеть, чтобы любая комбинация, которая где-то свое место нашла, мы ограничили по вот этому входному размеру, по длине кода. Потому что в противном случае, вот скажем так, 100 и миллион, и это работает для очень сложных кодов. Ну, размер 100 и миллион вот этих точек энергетического пространства. Если их не 100, а уже 200, с трудом, но можно. Зрительная корара – это 180 нейронов в колонке. начинается уже все, комбинаторный взрыв, больше уже никак. Больше там начинает экспоненциально расти количество вот этих точек комбинаторного пространства, которые тебе надо. Их уже надо не миллион, не два, а 100 миллионов, 200 миллиардов и так далее. То есть ты начинаешь уходить. Причем это именно экспоненциально растет. И дальше оказывается, что если ты теперь берешь длину этого кода, который ты работаешь, 128 бит, это разреженный код, и смотришь то многообразие, сколько ты можешь туда запихать информации, то оказывается 7 понятий, если они будут закодированы некими 5 битами, случайным образом в этом коде набросанными. Вот есть код 128, 5 бит, такое разреженное достаточно кодирование, то если мы теперь 7 понятий сложим, мы получим в среднем 30 бит, и 128 будут активны. Некоторые просто совпадут случайно, если брать. От 30 и 128 с этим можно работать. Больше, если 8 понятий, 9 понятий и так далее, плотность начинается такая, что очень много количества возникает, и уже дальше с этим работать нельзя. То есть тут возникает две вещи. Первая – это размерность кода, с которыми мы можем работать, чтобы они не могли быть длинными. А вот эта размерность кода определяет количество понятий, с которыми мы можем одновременно иметь дело. И оказывается, что в тех моделях, которые работоспособны, действительно порядка семи понятий до семи понятий – это то, с чем можно комфортно работать. 

S05 [01:38:34]  : Можно я попробую уточнить вопрос? То есть я понимаю логику, как ограничение связано с размерностью и с комбинационным взрывом, а вопрос в чем? Что это ограничение происходит из определенной архитектуры колонки, сложившейся эволюционно, И в этом случае, если мы используем этот принцип, о котором вы рассказали, для того, для построения другой архитектуры, где будут другие параметры колонок, соответственно, на компьютере мы можем реализовать интеллект, который будет работать с более сложными, более насыщенными колонками, где там будет не 25 нейронов, а 50 или 100, и мы сможем работать с более длинными кодами. и мы сможем работать с большим числом вот этих вот объектов в фокусе внимания, и тогда на самом деле мы сможем сделать действительно интеллект, который будет не сильный, и это будет супер, и потому что там мы можем сделать архитектуру, где будет не семь, а двенадцать или пятнадцать. или все-таки вот это вот 7 плюс-минус 1-2 это ограничивается какими-то математическими фундаментальными. 

S00 [01:39:48]  : Нет, я бы сказал так. Это мое мнение. 7 ограничивается эволюционными вещами, когда выясняется, что больше просто не надо. То есть там, где надо больше, на самом деле мозг делает. Вот зрительная кора — 180 нейронов в колонке. И, соответственно, и количество понятий, которыми… Ну, там количество объектов, которые нужно для распознавания, больше 7. Мы просто этого не осознаем и так далее. Но когда мы поднимаемся выше, оказывается, что то, с чем мы имеем дело, 7 более чем достаточно. Если мы смогли... Более того, есть же такая штука, крах-то сестра таланта. Наш мозг учится, и вот обучение подкреплением один из элементов, в смысле главный элемент в этом. учиться так формулировать даже внутреннюю информацию, не то, что я говорю внешне рассказывая, а внутренне для себя, чтобы она была наиболее ярко, наиболее емко и наиболее коротко. То есть кратко смогли сформулировать, все, мы сами для себя нам с этим удобно работать. Длинное описание, оно по определению будет плохим описанием, а в нем будет всегда куча мусора. Нет, иногда без этого не обойтись. То есть он в зрительной истории говорит, что, ну нет, к сожалению, чтобы отделить на Бога Уша от Бога Пыща, надо иметь там вот много важных деталек, потому что тут надо не только самые вот эти там горизонтальные, вертикальные, но должны быть еще какие-то там, а вот для каких-то других вещей. Опять же, почему я так говорю, я не пробовал, не экспериментировал, но если бы мозгу было надо, мозг бы создал зону коры, где колоночек было бы 200, 300 и так далее. В смысле, в колоночке нейронов. Если сделал 100 и ограничил с этим, значит, ему этого хватило. При этом точно известно, что мозг наш человеческий, он не экономил с точки зрения, мы не будем этого делать, потому что это трудно. Например, для того, чтобы шикарно видеть, а мы, например, посмотрим на птиц, да, птицы шикарно видят. Достаточно зоны коры размером сантиметра на сантиметр. У нас, вдруг, стариарная кора сто раз больше площади будет, но она не нужна для того, чтобы видеть на самом деле. Она, основная функция первичной коры, там обнаружены колонки глазодоминантности и так далее, это колоночки, это то, что позволяет, сопоставляя в разных контекстах параллакс от левого и правого глаза, там, где картинка совмещается, определять, где, в каком параллаксе совпадет, строить карту глубины пространства. Вот только для того, чтобы построить карту глубины пространства, мозг создает колоссальную, 10 на 10 сантиметров, представляете, конструкцию, это колоссальная карта, в 100 раз больше, чем надо просто для зрения, только для реализации этой функции. Но зато это удобно. То есть если бы в каких-то других вещах Было бы удобно работать с более длинными описаниями, это было бы осмысленно, может быть, это эволюционное создание. Раз не сделал, значит, скорее всего, просто этого не было. Ну, мое такое мнение. 

S01 [01:42:53]  : Коллеги, у нас пропала рука была у Алекса. Алекс еще хочет задать вопрос? 

S05 [01:43:05]  : Пока Алекс думает, я последний вопрос свой задам. Значит, вот как раз то, что вы говорили. Смотрите, вот вы говорили, значит, тоже вначале приводили хороший пример про, значит, параллакс, да, и про узнавание крестиков с помощью, сверточных сетей. А вот скажите, в рамках вот той модели, которую вы описываете, как решается проблема инвариантности, допустим, к вращению или к размеру? То есть у нас же крестики могут быть вертикальные, а могут быть углом 90 градусов, 45 градусов. 

S00 [01:43:42]  : Никаких чудес не делается. Все делается за счет того, что строится пространство контекста вращения. Кстати, опять же, как оно строится? Вот если пространство, которое мы делаем, от глаз двигается, и мы в этот момент имеем информацию, что будет, Изображение сместилось. Физически у нас есть для этого инструменты то, что позволяет обучиться. Так вот, оказывается, что есть мышцы, просто не видна их работа, которые еще и вращают глаз. То есть глаз, оказывается, еще и поворачивается. И вот эти повороты позволяют построить пространство контекстов на обученной ноздривке. На вращении их, точнее. Вот такая вот история. 

S01 [01:44:28]  : Так, коллеги, Виктор Сенкевич давно задавал вопросы. Пожалуйста, слово. Виктор, с нами? 

S00 [01:44:41]  : Видимо, мы так замолвно все рассказываем, что уже народ разошелся чай пить. 

S05 [01:44:44]  : Вы тогда, Дмитрий, может просто зачитаете те вопросы, которые есть, а авторов нету? Да, давайте. 

S00 [01:44:52]  : Но ты только интересный посмотри. 

S01 [01:45:00]  : Так, ну тут скорее уточнение, а Юрий Бабуров с нами? 

S00 [01:45:05]  : Юрий видел где-то там я. 

S02 [01:45:09]  : Да, с вами, но у меня много вопросов, поэтому я даже не знаю. Наверное, разумно будет один или два спросить. Очень хотелось бы, конечно, попробовать вашу теорию взять нейросеть сверточную и попробовать как раз вашу теорию применить на ней. То есть просто понять, похожа ли она на мозг в этом плане или нет. Потому что во многих вопросах окажется, что похоже, правильно? 

S00 [01:45:32]  : Нет, они похожи. То есть понятно, что свертки и сети возникли не от хорошей жизни, а от того, что вот стандартная парадигма, она как бы не могла побороть вот эту комбинаторию, не запоминать же образ в каждом месте. И для этого появилась вот эта идея свертки. Она действительно потрясающая, она замечательно работает. Я-то просто говорю о том, что можно ее обобщить, вывернув наизнанку. что не применять ядро свертки, а двигать картинку под это ядро. И тогда мы получаем вот эту историю с контекстами. Но как только мы вот это поворачивание делаем, мы ее можем теперь использовать и для другой информации, потому что появляется сразу алгоритм, позволяющий построить вот это пространство сдвигов, исходя из архитектуры глаза, вот у которого есть вот эти движения. А это можно делать и на компьютере, то есть когда, ну понятно, электронным образом можно делать DIG. То есть весь смысл в том, что сверточная сеть, она как бы пропускает этот вопрос, она его проскакивает. Она говорит о том, что как применять игру свертки, мы просто это знаем, мы что, математику научили? 

S02 [01:46:40]  : Нет, подождите, почему вы считаете, что о нейросети этот вопрос проскакивает и в представлении нейросети, которая генерируется на последующих уровнях, в них нет этой трансформации? 

S00 [01:46:51]  : Нет, я к тому, что что делает вот первый уровень, уровень Шверц. Он говорит, я знаю, как мне ядро свёртки применить к позиции 1-1, 1-2, 1-3 и так далее. 

S02 [01:47:04]  : Смотрите, ядер же много, у каждого фильтра есть своё ядро свёртки, допустим 64, эквивалентно 64 нейронам, которые ещё и расположены в разных местах. 

S00 [01:47:18]  : Тем не менее, вот как одно и то же ядро применить сначала, к первой позиции, к первому самому пикселю с координатами 1,1, а потом как применить его к пикселю с координатами 1,3, 1,5, 1,25. Мы это знаем просто потому, что мы так запрограммировали, изначально заложили это в сверточную сеть. Вот на этом моменте я останавливаюсь и говорю, подождите, а ведь можно это не заложить просто правилом, а это можно обучить. 

S02 [01:47:51]  : Ну а нейросеть в каком плане она закладывает, а не обучает? 

S00 [01:47:57]  : Когда пишется алгоритм свертки, мы просто говорим. 

S02 [01:48:01]  : Что значит пишется алгоритм свертки? Ну какая-то зона, ну и у мозга есть тоже у нейрона соседние зоны, соседние нейроны, которые он видит. 

S00 [01:48:20]  : Я как раз пытаюсь объяснить, что мозг работает не так, как сверточная сеть. 

S02 [01:48:24]  : Давайте вот это различие попробуем как раз понять, прослеживая вот эту вашу теорию, ровно повторяя, только для сверточной нейронной сети, и поймем, что отличается. 

S00 [01:48:34]  : Вот все-таки сначала, опять же, принципиальное отличие. Сверточная сеть говорит, вот у меня есть ядро сверки, которое я каким-то волшебным образом получил, не получил. в нем содержится кружочек, в этом другое ядро сверки, в нем крестик содержится и так далее. Дальше. Я говорю, вот у меня есть позиция на картинке с координатами какими-то, если вот это ядро сверки 20 на 20, вот предположим, позиция 10-10. И я говорю, я знаю, как взять математически и применить сюда это ядро сверки. Мне надо этот элемент 1-1 умножить на элемент вот этот и так далее. Дальше. Как применить это вот к этой точке? Я тоже знаю. Мне надо использовать знание координат этой точки и так далее. Перемножать, только теперь уже используя вот этот зверь. Правильно? Вот откуда я это знаю? Это же геометрия пространства. 

S02 [01:49:33]  : А, в этом плане. 

S00 [01:49:35]  : Исхожу из того, что я знаю геометрию пространства, я просто вот ее здесь запрограммировал. 

S02 [01:49:40]  : Я правильно понимаю, что вас смущает то, что ядра одинаковые, свертки получаются? Меня это не смущает. Меня смущает то, что отличается. 

S00 [01:49:51]  : Откуда берется вот этот алгоритм, как применять? ядро свертки к вот этому месту изображения. 

S02 [01:50:00]  : Смотрите, это просто вычислительная оптимизация. То есть это не значит, что ядра свертки не могут быть по произвольному адресу сделаны. Это все может быть сделано и будет работать точно так же. И не значит, что нельзя их обучить по разным адресам. Фокус-то в том, что мы 

S00 [01:50:22]  : вот эти ядра-свертки применяем везде и при этом говорим, что мы ядро вычислили в одном месте, мы теперь можем в другом использовать, даже если она никогда этого раньше не видела. В этом фокус. То есть это архитектуры, которые просто каждый нейрон зачем-то смотрит, не реализовать, потому что этого переноса ядер тогда не будет. 

S02 [01:50:44]  : Да, но вы это решаете как раз тем, что у вас в зоне общая память. 

S00 [01:50:52]  : Теперь, вот мы сделали нейрон, который смотрит каким-то образом, предположим, за вот этим местом. Но, предположим, нейрон, который смотрит за этим местом, Я про мозг сейчас. Нам ему удалось каким-то волшебным образом передать вот эту память, вот это ведро сверки. Но как мы знаем, что у него вот эти точечки попадают в те же самые места, что они также расположены пространственно? Они могут быть перепутаны. Нет никакого вот этого механизма синхронизации. Как нам это засинхронизировать? 

S02 [01:51:29]  : Ну подождите, а он нужен вообще? Мы же глазом фокусируемся на объекте, если мы хотим его рассмотреть и понять его. То, что мы фокусируемся. А если мы не фокусируемся, мы максимум можем только посчитать объекты в фоне. То есть мы многого не можем. А фокусирование как раз эквивалентно тому, что у нас отличаются ядра свертки. И свертка как раз механизм как раз для того, чтобы, ну то есть механизм, которого нету у человека, но это механизм. 

S00 [01:52:01]  : Смотри, мы сейчас можем сами себя немножко запутать, потому что если мы говорим, О свёрточных сетях это одна история. Если мы говорим, как можно попробовать а-ля свёрточные сети сделать не как свёрточные сети, а на каких-то нейронах и так далее, это вторая совершенно другая история. Потому что тоже интересная, но она другая. Но если мы говорим именно про свёрточные сети, то в них есть вот этот ключевой момент. Мы просто знаем, как применить ядра свёртки, Потому что мы говорим, что это из геометрии мира следует. 

S02 [01:52:36]  : Я утверждаю, что это несущественные моменты. Мы могли бы тогда рассмотреть фулли-коннект обычный. Вас бы тогда этот момент не беспокоил? или мы можем применить рекуррентные сети и реализовать у них точку взгляда. То есть это просто особенность реализации? Нет. 

S00 [01:52:59]  : Когда мы узнали что-то каким-то нейроном, условно, который смотрит, нам важно, чтобы этот нейрон имел свои координаты. Хорошо, мы ему каким-то чудесным образом присвоили, он узнал свое местоположение там и сейчас. Хорошо, мы сильно-сильно, скажем так, озаботились и все-таки такую историю создали. Но дальше у нас задается вопрос, а она будет работать с другим видом информации, который вообще никак на это не похож? Как нам тогда построить вот эти проекции, на что их надо проецировать? Я как раз показываю, что есть общий алгоритм, который через представление о контекстах и изменение информации позволяет быть как реализованным вот здесь, как реализовать историю со сверочными сетями, так и в любом другом типе информации. 

S02 [01:54:00]  : Сердечные нейросети могут также работать для разных типов информации? 

S00 [01:54:05]  : Как они для разных типов? Разные типы, их начинают применять. В чем? Если это к тексту, то это условное применение, порядок слов. Если это не первым, а вторым словом идет, там начинается конструкция. Это совсем примитивная история. 

S02 [01:54:24]  : Ну, в смысле, ну, решают нейросети, такие же сверточные нейросети. Вот я вот применяю для аналогии сверточные нейросети. 

S00 [01:54:32]  : Ну, тут идет вопрос по-другому. Когда у нас есть изменение смысла. Вот если говорить про язык, то слово меняет свой смысл. 

S02 [01:54:42]  : Смотрите, Алексей, вы снова переходите к уровню парадигмы, то есть вы рассказываете, что то же самое, но у него просто есть другой смысл. Вы вкладываете теперь другой смысл, рассказываете, что... не мячик катится потому что его толкнули, а потому что мир смещается, чтобы мячик переместился в данном направлении. То есть на уровне парадигмы, а суть от этого никак не меняется. 

S00 [01:55:15]  : От этого меняется суть. 

S02 [01:55:17]  : Ладно, мы уже много времени заняли, так бы, конечно, хотелось докопаться до сути, в каком месте эта суть меняется. То, что просверчены нейросети, это все-таки особенность реализации. Есть много других реализаций, в том числе Касательно реализации для зрения, было показано, что с очень хорошим качеством также распознаются цифры, последовательности цифр, номера картинок, если мы делаем рекуррентную сеть, которая может смотреть область 10 на 10 пикселей и перемещать глаз. Есть такая история? Соответственно, это намного ближе к вашей модели и к тому, как мозг делает. Но это для меня как особенность реализации. То есть оно принципиально ничего не меняет в применении парадигмы. И точно так же нейросеть может сверточные координаты выучить. То есть когда координаты точек лица в анимированных современных преобразованиях лица у всех это учит. Обычная нейросеть, та же сверточная, нейроны учат со своей координатой на выходе. 

S00 [01:56:31]  : И координаты можно научить нейронам. 

S02 [01:56:34]  : Да, но это фактически преобразование контекста. Получается, тогда нейросеть полностью подпадает под ваше определение механизма работы мозга. 

S00 [01:56:43]  : Нет. Вернемся вот к этой истории. то, что я рисовал, там этот нейрон смотрит. Вот мы увидели вот здесь вот кружочек. Мы увидели его один единственный раз в жизни. Как нейросеть узнает его теперь вот в этом месте? Сверточная сеть, я знаю, как узнает. Она просто знает правила применения свертки, применит его здесь и скажет, это кружочек. Так вот такая полносвязанная сеть скажется, увидев кружочек здесь, теперь я его сбежу здесь. 

S02 [01:57:18]  : Видели один раз. Но разве человек так, так этот вопрос? Не сменой сакадами, микросакадами? 

S00 [01:57:29]  : Нет, ничего подобного нет. Мы никогда никакими сакадами не приводим к центру там все изображения. Мы увидели здесь, теперь мы должны это узнать здесь. один раз здесь никогда не видели. это формулировка разумблата, вот именно так он формулируется. 

S02 [01:57:50]  : хорошо, есть еще один механизм, который используется в предтренировке сверточных сетей, особенно в последнее время, но использовался, потом некоторое время не использовался, потом снова использовался. Когда мы берем и нейросети даем какую-нибудь задачу, в частности, например, как раз задачу даем восстановить контекст преобразования. Например, картинка сдвинулась на несколько пикселей. Пиксели нейросети, ну, показываем прошлую картинку и следующую, нейросети даем задачу восстановить вот это преобразование. Нейросеть показывает, что в таком случае нейросеть как раз обучается ровно тем же картинкам. Это эквивалент вашему механизму обучения. Но этот механизм не требует общей памяти. 

S00 [01:58:39]  : Это идет разговор об обучении одного контекста. То есть, вот один контекст научился прям сбегать картинку на такой расстоянии. 

S02 [01:58:48]  : Один тип контекста, может, стоит назвать типом. 

S00 [01:58:51]  : Просто один конкретный контекст, потому что вот нейросеть учится сдвигать ее влево на 3 пикселя, скажем. 

S02 [01:58:57]  : Не, ну произвольный сдвиг она учит. 

S00 [01:58:59]  : Произвольный сдвиг, она уже начинает биться на сегменты сеть, то есть, один как бы на один сдвиг, другой на другой, там и так далее. То есть, каждый начинает отвечать за свой контекст. При этом каждый из этих сегментов должен учиться на всех видах картинок. То есть если один сегмент научился сдвигать кружочек, то другой сегмент не знает, как этот кружочек сдвигать. Он должен увидеть этот звук этого кружочка и теперь тоже научиться сдвигать кружочек. 

S02 [01:59:32]  : Но смотрите в натуральных картинках, то есть в картинках, которые встречаются в реальном мире, которые мы подаем на вход такой сети, когда она изучает преобразование. Есть кружочки, есть антропологи и так далее. 

S00 [01:59:47]  : Если мы будем данные готовить так, чтобы все в них встретилось, мы брутфорсом можем действительно много чего научить. Всегда можно привести примеры, когда никакого объема не хватит. Если говорить про человека, то тут очень простой критерий. Мы очень быстро приходим к тому, чтобы научиться тому, что мы знаем, не хватит, если брутфорсом учить, не хватит просто человеческой жизни. Поэтому что мы можем делать? Мы, узнав информацию в одном контексте, теперь можем ей оперировать во всех остальных контекстах. И, собственно, вот идея некого творчества. Вот что такое творчество, которому человек способен? Это когда мы берем и то, что видели в одном контексте, теперь реализуем в другом, в котором никогда раньше этого не было. И получается что-то принципиально новое. То, чего раньше никто никогда не видел. 

S02 [02:00:43]  : Это способность, вот это способность. Я бы предпочел на этот уровень не переходить, потому что я вижу разные механизмы реализации этого. Я боюсь, что это связано конкретно с вашей реализацией, с вашей парадигмой. 

S00 [02:00:54]  : Юрий, смотри, ты очень четко сформулировал вот именно разницу подходов. Я же не говорю, что они не работают. Работают. Нейронные сети работают. 

S02 [02:01:03]  : Но я боюсь, я сам ее до конца не осознал. 

S00 [02:01:07]  : Надо ли нам продемонстрировать, Все примеры, чтобы нейронная сеть на них обучилась, чтобы кружочек и здесь увиделся, потому что он присутствует в изображении, мы здесь научимся. Или можно многократно, в миллион раз, разговаривать именно о таких вещах, уменьшить количество примеров, которые нужны для обучения. Чтобы, увидев здесь, мы уже теперь знали здесь. Вот сверточные сети это и реализуют. Но в сверточных сетях применен трюк, по которому знание, как применять вот это ядро, зашито просто изначально на берегу. Я рассказываю, как это знание сделать методом обучения то же самое, чтобы система могла этому научиться, чтобы не надо было зашивать, но чтобы это зато можно было применять теперь ко всем видам информации, чтобы вот это пространство сверток, оно так и получается, пространство контекстов, это по сути пространство сверток. оно формировалось автоматически методом самообучения. 

S02 [02:02:14]  : Я как раз приводил в пример метод самообучения для сверточной сети. Различие тогда получается в том, что у человека миллион сверток, а у компьютера сотни. 

S01 [02:02:35]  : Коллеги, мне кажется, в сравнении с верточными сетями интересная тема, и можно тогда просто отдельно провести про это вебинар. Вы сейчас, наверное, уже существенно насытились. Я вот вижу, Антон Машин. 

S05 [02:02:48]  : Да, мне кажется, что мы подошли к заключительному вопросу, который Дмитрий Иванович Серединко задал. Из этой дискуссии я понял, что концепция Алексея предполагает, что мы имеем вычислительную модель, которая может реализовать не только то, что заложено в архитектуру, некоторой нейронной сети, но мы можем учиться формировать разные архитектуры, которые будут работать в разных ситуациях. То есть мы на самом деле описываем концепцию общего искусственного интеллекта. 

S00 [02:03:27]  : В котором можно уже зоны делать, которые комбинировать в взаимосвязи. 

S05 [02:03:32]  : Да. И вот здесь как раз вопрос чисто терминологически. Как вы соотносите определение сильного искусственного интеллекта и общего искусственного интеллекта? Это для вас одно и то же или есть какие-то нюансы? 

S00 [02:03:46]  : Я нюансов этих не вижу. Для меня это, я так понимаю, одно и то же. Просто я привык говорить сильно искусственный интеллект. Сколько лет его говорил, поэтому как-то продолжаю вместо общего говорить сильно. Спасибо. 

S01 [02:04:01]  : Коллеги, ну мне кажется, что на сегодня можно закончить. Всем участникам большое спасибо. Была интересная дискуссия. Алексей, спасибо за доклад и ответы. Ждем всех вновь с новыми вопросами, с новыми темами. До новых встреч. 

S04 [02:04:26]  : Спасибо. До свидания. 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
