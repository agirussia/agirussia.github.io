## 5 ноября 2020 - Антон Колонин - Структуры Данных и Языки для Общего Искусственного Интеллекта, часть 2 — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/TPISkLZnndk/hqdefault.jpg)](https://youtu.be/TPISkLZnndk)

Суммаризация семинара:

Общая тема и контекст:
Семинар посвящен структурам данных и языкам для искусственного интеллекта (ОИИ). В частности, обсуждаются вопросы онтологии и когнитивных архитектур, а также методы автоматизированного решения задач и взаимодействия между различными модулями в универсальной когнитивной архитектуре.

Основные тематические блоки:

1. Онтология и когнитивные архитектуры:
   - Обсуждается концепция онтологии как системы понятий и отношений, описывающих реальный мир.
   - Рассматривается идея когнитивных архитектур, которые позволяют системам ОИИ имитировать человеческое мышление.
   - Приводятся примеры фундаментальных онтологий, позволяющих строить доменные онтологии.

2. Методы автоматизированного решения задач:
   - Обсуждается возможность автоматического построения знаний на основе индукции-дедукции-абдукции.
   - Анализируется вопрос о том, на каком языке говорить и как решать терминологические споры.

3. Взаимодействие модулей в когнитивной архитектуре:
   - Рассматривается задача создания механизма для взаимодействия между модулями с унифицированным языком.
   - Приводится пример использования антологии для представления знаний в разных модулях.

4. Локальные и глобальные обратные связи:
   - Обсуждается концепция локальных обратных связей, таких как подкрепления, в контексте восприятия и понимания.
   - Анализируется роль глобальных фидбэков в процессе обучения и развития.

Заключение:
Семинар показал, что разработка универсальных когнитивных архитектур для ОИИ требует глубокого понимания онтологии, когнитивных процессов и механизмов взаимодействия между модулями. Важной задачей является автоматизация процесса построения знаний и решения задач, что требует развития методов индукции-дедукции-абдукции и других когнитивных алгоритмов.



S04 [00:00:00]  : Итак, в прошлый раз начались определения, и в рамках этого определения хотелось бы остановиться на небольшом обсуждении, которое было в WhatsApp. Какие виды активности, которые могут напоминать интеллектуальную, могут попадать под это определение? Да, ну вот если идти снизу, то вроде как у нас отдельно взятая клеточка, в данном случае белое кровяное тело, может охотиться за бактериями, да, и вот по этой ссылке на презентации можно запустить видео и посмотреть, как это происходит. Но есть сомнения, что данное поведение попадает под категорию как минимум обучаемости. Есть сомнения, что если эту клетку поместить в другую среду, то она в состоянии будет в этой новой среде обучаться каким-то подобным функциям. Есть вообще предположения о том, что данное поведение каким бы интересным и разумным не казалось, оно, скорее всего, предопределено генетически. Опять-таки, если мы возьмём многоклеточный организм, в данном случае нематоду, которую, кстати, моделирует система Евгения Евгеньевича Витяева Discovery, если мы возьмём эту самую нематоду, то она тоже в состоянии целенаправленно избегать тёплых предметов, И можно тоже спекулировать на тему того, в состоянии ли она этому обучаться, или это предопределено генетически. млекопитающих со сложной нервной системой, которая в состоянии выполнять сложные моторные функции, например, применение камней для добывания орехов, то в общем наблюдения с помощью видеокамер показывают, что они этому учатся социально, то есть они учатся этому именно как когнитивной программе методом transfer learning от своих родителей в принципе следуя вот тем схемам примерно про которые мы говорили в прошлый раз и в этом смысле можно говорить что и зная о том что те же самые обезьяны они в состоянии вообще говоря на То есть, они в состоянии составлять сложноподчинённые выражения с двумя, тремя, четырьмя частями речи, включая имена собственные, и строить предложение, состоящее из трёх-четырёх слов. То вот можно предполагать, что какие-то зачатки того, что было в прошлый раз определено как сознание или способность к осознанному поведению, по крайней мере, у млекопитающих есть. Значит, дальше еще один комментарий, который хотелось бы сделать, это вот по поводу сегодняшнего опроса, который организовал Юрий Бабуров в группе, что, значит, когда мы говорим о верхнем уровне когнитивной активности, можем ли мы говорить, что это осуществляется на уровне векторов или символов. С моей точки зрения вопрос некорректен, потому что, во-первых, что мы подразумеваем под верхним уровнем? Наверное, под верхним уровнем мы подразумеваем уровень абстракции. То есть, чем выше абстракция, тем, значит, чем больше абстракция, тем уровень выше. Если рассуждать так, и рассуждать в терминах, например, модели вертикальной нейросимвольной интеграции, которая представлена на данном слайде, когда на нижнем уровне у нас совершенно распределенные представления с неразмеченными графами. представленные неразмеченными графами, а на верхнем, на абстрактном уровне у нас размеченные графы и символьные представления, то тогда, наверное, да, на верхнем уровне тогда у нас, наверное, мы работаем символьными представлениями. Значит, во-первых, тут тогда нужно разделять понятия. Во-первых, работаем мы с размеченными графами или неразмеченными. И если на самом деле где-то внизу на уровне, так сказать, моторной активности у нас есть нажатие каких-то кнопок или реализация каких-то, так сказать, совершенно конкретных функций, которые имеют некоторое символьное и конкретное представление, в принципе, мы и на нижнем уровне можем оперировать с этими символами. И во-вторых, даже на верхнем уровне, в моем понимании, мы можем из этих самых символов строить те же самые вектора. Допустим, на вопрос того, работаем мы ли на верхнем уровне с символами или с векторами, я могу дать следующий ответ. Мы на 90% работаем с символами, на 10% с векторами. А кто-то скажет, допустим, Хинтон, как в том интервью, с которого все началось, Хинтон скажет, что мы на 100% работаем с векторами и на 0% с символами. А кто-нибудь, допустим, Бен Герцель скажет, что мы на 50% работаем с векторами, и на 50% символами. Причем эти 50 на 50 могут иметь смысл как вероятностный, так и смысл уверенности, но так или иначе, вот эти значения, они будут характеризовать для каждого из исследователей некоторую точку в векторном пространстве, и в этом случае мы тоже работаем с векторами, но в общем тоже в символном представлении. как бы мы можем работать и с символами, но работать с ними, используя векторные представления, не говоря уже о том, что современное направление Формал-концепт анализис, тем более в рассмотрении вероятностного подхода к формал-концепт анализис, как Евгений Евгеньевич рассказывал, это те же самые вектора. То есть вот эта матрица, которая описывает пространство, характеристики пространства объектов, ну это тоже на самом деле векторное пространство. координатами в этом векторном пространстве являются вполне конкретные символы. Поэтому тут надо разделять разные вещи. И еще один комментарий связан с когнитивными архитектурами по поводу обратной связи. Сегодня это обсуждали опять-таки в И сегодня как раз мы эту схему разбирали с Евгением Евгеньевичем по поводу того, что такое backpropagation, что такое error propagation, что такое feedback, что такое reward, что такое reinforcement. Тут ситуация следующая. Если мы посмотрим на ссылку внизу, это классическая работа Марвина Минского. где он рассуждает про глобал и локал ревард. Вот там достаточно интересное и внятное рассуждение, где он как раз рассказывает, чем отличаются глобал и локал ревард, и это можно проиллюстрировать на примере, который показан в левой стороне слайда. Здесь взят пример не от Минского, а пример взят у меня из головы, но идея там примерно следующая. Допустим, у нас есть некоторое многоканальное восприятие, с помощью которого мы, с одной стороны, видим некоторое слово, в данном случае слово пинг, у которого последняя буква не очень отчетливо видна, то ли это буква Q, то ли это буква J. С другой стороны, мы видим мячик от пинг-понга. И в этой ситуации, если мы уже прочитали первые три буквы, Вот, то дальше, значит, у нас вот на, скажем так, втором уровне восприятия, где у нас собираются слова из букв, у нас уже есть гипотеза о том, что это либо слово «пинг», либо слово «пинг», да, либо «фиолетовый», либо это «мячик». Вот, и исходя из того, что… Эта буква, которую мы видим, она все-таки больше похожа на Ж, чем на букву К. И поскольку слово «пинг» с буквой «q» не существует в природе, то, в общем, гипотеза о том, что, скорее всего, это буква означает «пинг», она приводит к тому, что если верхний уровень говорит о том, что это буква «ж», то распознавание «ж» в контексте ожидания слова «пинг», оно получает подкрепление. То есть мы можем сказать, да, мы ждали букву «ж» для того, чтобы получилось слово «пинг», нам верхний уровень говорит, что это «ж», и в этом случае этот уровень получает положительное подкрепление. если мы пойдем дальше на следующий уровень где у нас из слов получается значение этих слов и мы при этом еще и получаем на вход образ вот этого мячика от пинг-понга вот по зеленому каналу вот то опять-таки с этого уровня у нас дополнительно идет положительное подкрепление или положительная обратная связь на тот уровень, который распознал это слово «пинг». Вот. И в том и в другом случае это у нас получаются локальные обратные связи, значит, можно их назвать подкреплениями, можно их просто назвать, можно их назвать наградой с точки зрения, вот если взять, так сказать, рассуждения Минского Если у нас на уровне восприятия все совпало, если мы понимаем, что мы увидели мячик от пинг-понга, то тогда тот сегмент коры, который отвечает за восприятие объектов, который решил, что это мячик, а не луна, например, не модель луны, то он тоже получает подкрепление, потому что он сказал, это мячик от пинг-понга, да, это мячик от пинг-понга, ты молодец, ты угадал. А если бы, значит, тут была нарисована луна, если бы уровень зрительного восприятия объектов предсказал бы, что это луна, вот, а тут было бы ожидание пинг-понга, то он бы получил, мог получить, не получить положительного подкрепления или получить отрицательное подкрепление. Ну и предположим, если дальше у нас восприятие слова «пинг» и мячика от пинг-понга приводит к тому, что мы дальше формируем слово и эту форму раскладываем на звуки, а потом эти звуки у нас вырываются изо рта, и у нас при этом есть мама или учитель, который нам это все показывает, и он видит, что мы правильно угадали предложенную загадку, он нам улыбается в ответ, и мы получаем уже глобальный фидбэк. который закрывает как бы весь когнитивно-активный, так сказать, когнитивно-активную схему, то есть мы поняли все правильно, мы отреагировали на это правильно, произнеся увиденное, вот, и все, соответственно, участники этого процесса получают глобальный фидбэк, который возникает в результате воздействия на органы чувств агента через внешнее подкрепление в виде улыбки, которая материализуется в близлежащем окружающем пространстве и приводит к формированию в случае человеческого агента существа или животного, каких-то положительных эмоций, эти положительные эмоции влияют на весь тракт. При этом, если мы возьмем с правой стороны, посмотрим, как это может реализовываться на уровне нейронов и на уровне выброса, например, допамина, который возникает при эмоциональном подкреплении положительным, положительный выброс допамина При эмоциональном подкреплении он воздействует на весь когнитивно-активный тракт, и это является аналом глобальной обратной связи. механизмы могут достигаться тоже разными способами, но вот как, исходя из той информации, которая есть у Евгения Евгеньевича и у меня, обратная связь на уровне нейрофизиологии, на уровне структур нейронных, она осуществляется не через положительную связь, а наоборот через подавление. То есть, если подкрепление идет глобальное по всему каналу, то локально у нас идет не подкрепление, а локально у нас идет подавление или ингибиция, ингибирование тех агентов соседнего и вышележающих уровней, которые дали нам неверную информацию. Ну, это вот второй комментарий. И возвращаясь опять-таки к тому опросу, который предложил Юрий Бабуров, о том, мыслим ли мы в рамках некоторого гипотетического AGI на уровне векторов или на уровне Вот, пожалуйста, пример, что если мы будем говорить о нейросимвульной интеграции не вертикальной, да, когда у нас внизу нейронные сети, а наверху у нас графы знаний, а если мы будем делать, так сказать, вертикальную нейросимвульную интеграцию, либо там для оптического, для визуального распознавания, либо для natural language processing, то в этом случае у нас представления нейросетевые и представления семантически графовые являются эквивалентными. То есть мы можем и в том виде информацию хранить, и в том виде ее хранить. информацию перекачивать из одного вида в другое. С правой стороны у нас происходит то, что Даниэл Канеман называется Thinking Fast, а с левой стороны у нас находится то, что называется Thinking Slow. И, в принципе, эти два модуля внутри архитектуры нашего гипотетического АГИ, More AGI, могут действовать независимо, они могут решать разные задачи, ну и синхронизоваться через механизм explain и transfer. для того, чтобы то, что мы узнали методом медленного обучения, могло использоваться для быстрого распознавания, а то, что мы долго учили в результате обучения на каком-то многократно повторенном опыте, чтобы это могло быть переведено в символиное представление для задачи объяснения этой информации другим участникам общества. Ну вот, собственно, это все комментарии. А теперь хотелось бы вернуться к тому слайду, на котором остановились в прошлый раз. Итак, мы поговорили о том, что у нас есть какие-то возможные когнитивные архитектуры, которые включают в себя либо нейросетевые модели, либо какие-то графово-символьные представления, основанные на предикатах. И мы теперь хотим понять, а как же нам вообще… из чего нам это все строить? То есть, вот мы знаем, что человеческим, так сказать, элементарным кирпичиком, из которого строится человеческий мозг, является нейрон. С левой стороны мы видим, вот есть нейрон, у него там есть тело, у него есть эндрит, у него есть аксон, есть синапсы. И вот из всех нейронов, они, конечно, немножечко разные, но, в принципе, мозг весь сделан из того же материала, что называется, из одного, из этих самых нейронов. Если мы возьмем в серединке вычислительную архитектуру, современную, классическую, то все сделано из одних и тех же транзисторов. Они просто соединены по-разному, там бывают разного размера, бывают интегрированные, бывают неинтегрированные. А из чего же сделана вот та самая гипотетическая когнитивная архитектура, что является элементом этой архитектуры, и из чего ее собирать, чтобы можно было взять, так сказать, один и тот же материал универсальный или, так сказать, набор запчастей, и из этих запчастей собирать когнитивную архитектуру для самых разных задач. И если мы посмотрим на вот эту упрощенную справа внизу модель агента с элементарной когнитивной архитектурой, то можно выделить, с моей точки зрения, четыре функциональных составляющих не говоря о том вообще, является ли внутреннее представление миросетевым, или оно является символьным, или оно является каким-то гибридным. То есть, мы сейчас говорим о некоторых функциях. Значит, одна функция – это собственно, способ представления данных или схема хранения данных. То есть, что это такое? Это у нас графовая база данных или это у нас гиперграфовая база данных или вот у нас какая-то определенная схема в реляционной базе данных и некоторые правила по поводу того, как мы эту информацию храним в базе данных. вот это вот граф сторочь что называется одна необходимая компонента которая нам нужна Вторая необходимая компонента – это некоторая базовая онтология, которая описывает то, как описываются структуры, в которых наши данные будут лежать. Если у нас, к примеру, графовым хранением является революционная база данных, то, очевидно, foundation ontology должна предопределять некоторый набор базовых таблиц, в которые мы все будем расталкивать. Если это гиперграфовое представление или графовая база данных, то может не быть вообще никакой онтологии, но онтология, если у нас предполагается, как допустим, насколько я понимаю, в работе Николая Рябчевского, не предполагается никакая типизация связей, вершин все определяется из контекста, ну тогда ответ такой, что у нас никакой фундаментальной онтологии нет. Допустим, в моем проекте фундаментальная онтология очень простая. Есть вещь, есть свойства, и есть два базовых свойства – это обладание и наследование. В ОПНКОГе базовая фундаментальная антология, она состоит примерно из нескольких десятков, если не пяти десятков различных типов атомов. Ну и так далее, да, так сказать, можно говорить о разных антологиях. Это второе, да. Третье – это, значит, слева inference engine. Это, собственно, та математика, которая обеспечивает Собственно, преобразование одних структур или значений, которые мы ассоциируем с этими структурами, допустим, веса нейронной сети или вероятности тех или иных предикатов, которые лежат в графовой базе данных – это функция inference engine. Это, собственно, тот код, который работает над теми данными, которые хранятся в рамках той или иной фундаментальной И, наконец, то, что я называю semantic language – это некоторый протокол, с помощью которого данный модуль взаимодействует с другими модулями. То есть это может на самом деле быть бинарный протокол, который предопределяет просто некоторую семантику сообщений. как мы передаем сигналы, которые поступают у нас на выходе, и в каком мы виде кодируем сигналы, поступающие нам на вход. Если это многоагентная система в прямом смысле, где агенты взаимодействуют между собой с помощью каких-то сообщений, то это протокол, который описывает это сообщение. Предполагается, в контексте того, что я рассказывал в прошлый раз, что этот Семантик Ленгвич оперирует все-таки с какими-то предикатами, Даже если предикатом является значение пиксела в той или иной точке растра графического изображения или видеоизображения в конкретном фрейме, а время этого фрейма или номер этого фрейма в видеопотоке также является атрибутом предиката. Или это значение частоты на аудиограмме. Соответственно, язык – это то, чем мы общаемся с внешним миром и с другими модулями. Если мы посмотрим, так сказать, рынок того, что есть по каждой из этих четырех компонент, то как я вижу эту картинку, причем я сюда включаю только те компоненты, которые я знаю и которые являются доступными в публичном пространстве. Допустим, если в Inference Engine я не включаю, например, какие-то системы, основанные на глубоких байосовских сетях, ну, наверное, это потому что просто я их не знаю. Если вот кто-то знает готовое опенсорсное решение для Дибайеса, вот можете мне прислать, я в следующую версию этого слайда это дело воткну. Но если говорить про Inference Engine, то на сегодняшний момент есть три решения. Это есть Non-Axiomatic Reasoning System Пиеванга, про который я говорил в прошлый раз, соответственно, система Discovery Евгения Евгеньевича и Logistic Logic из OpenCog, про которые я тоже в прошлый раз говорил. И больше мы подробно это обсуждать сегодня не будем. Если поговорить про, собственно, систему представления онтологии, то на одном конце написано, что есть вариант no ontology. Это, насколько я понимаю, то, как все работает у Николая Робчевского. Ну и с другой стороны, если следовать подходу, про который на прошлой лекции рассказывал Дмитрий Иванович Середенко, то мы для каждого домена строим свою онтологию и работаем в этой доменной онтологии. Ну и кроме того, есть большое определенное количество вариантов. Здесь опять-таки я перечисляю, которые я знаю. Вот есть два закрытых варианта, то есть Freebase и Cyc. Это сейчас Closed Source. Причем Cyc можно получить за деньги, а Freebase никак не получить нельзя. Вот есть базовая онтология OpenCog, здесь моя собственная. Ну и в рамках этих антологий мы где-то можем эти данные хранить. Вот про GraphStorage у меня будет отдельный слайд. Следующий я открою про варианты GraphStorage, мы поговорим. Значит, если же говорить про те структуры передачи семантической информации в виде предикатов, есть стандартный набор, значит, это RDW, OVL, TORTL и JSON-LD для представления знаний, и SparkQL или GraphQL, например, GraphQL сюда можно было еще добавить для извлечения этих знаний. Вот, Cycle – это Cycle Language, старейшее вообще, что есть. Ему, по-моему, там, если не 25, то 30 лет. Вот, и он имеет лиспоподобную структуру. Значит, дальше, вот, языки, так сказать, Сибирской школы искусственного интеллекта УРЛ. Мы с Леонидом Кузиным делали 23 года назад. ДНО-ЛСЛ – это вот язык, который как раз разработан с участием Дмитрия Ивановича Свериденко. Мыши на прошлой лекции показывали, как с помощью этого языка рассказывают друг другу, как правильно сыр искать. Значит, у Бена Гердзалей и команды есть язык АТОМИЗ в ОПЕНКОГе. АТОМИЗ, потому что он оперирует с атомами ОПЕНКОГ. У Пей Ванга и Питера Хаммера в Нарсе есть язык нарсиз. У меня есть свой язык, я просто сегодня про него буду рассказывать. У Майкла Миллера сейчас вышла книжка, в бета-версии у него находится язык примис. Ну и, собственно, если попытаться собрать как бы линейки различных решений, у которых есть почти все из предложенного списка, то вот получается такая картинка. Значит, у OpenCog есть все составляющие, и они доступны по лицензии GPL, и я надеюсь, на следующий четверг Алексей Потапов расскажет более подробно, значит, вот если не по всем этим позициям, то по большинству из них. Значит, у Питера Ванга есть, собственно, система вывода язык, значит, специальной базы данных у них нету, значит, я как раз спытал Патрика Хаммера на прошлой неделе, значит, он сказал, что они просто вот загружают всё в память и работают совсем в памяти, значит, онтологии у них либо нету никакой, как у Николая Рабчевского, либо есть минимальная онтология. на основе нескольких базовых типов связей, типа inheritance и causal relationship, но я его не допытал. Соответственно, лицензия тоже MIT. Связка Discovery Евгения Евгеньевича и Dame Nolle Selle Это система Discovery, язык DSL, специального графа хранилища нету. Антология DSL, он сам язык идет тоже с некоторой минимальной антологией, мета-антологией, которая позволяет строить доменные антологии, ну и лицензия GPL и Commercial. Ну и у вашего покорного слуги есть, собственно, два языка, про которые я сегодня буду говорить. специальная база данных, у которой есть свои плюсы и минусы, есть минимальная онтология, MIT лицензия. Единственное, чего нет, это inference engine. Обсуждаем с Евгением Евгеньевичем, может быть, брать его систему. Значит, если теперь говорить по поводу того, где хранить информацию в предположении того, что мы их представляем в предикатах. Я специально устраивал опрос. Устраивал его в нашей группе в Фейсбуке, но там получилось мало респондентов. И в англоязычной группе AGI получилось респондентов чуть-чуть побольше. Ну и получается так, что, в общем, все делают что-то своё. А, причем важный момент. Важный момент, что в этом опросе я исходил из того, что время должно являться некоторой особенной категорией. То есть, если мы работаем с предикатами, то время не должно являться обычным термом, стандартным термом предиката. И оно может быть, может не быть. Таким образом, если времени нет, то мы делаем некоторый интеллект, который делает какие-то выводы и осуществляет какие-то свои интеллектуальные функции, но вообще без привязки времени. И, кстати, вот именно так все устроено в Опенкоге. Мы специально с Беном эту тему обсуждали, и Бен считает, что время не является какой-то особенной характеристикой. Наличие времени в каком-то предикате является просто некоторым частным случаем. Как бы математически с этим можно согласиться, но с практической точки зрения, если мы говорим о reinforcement learning, который, как мне кажется, является критическим звеном того, что мы в большей части случаев называем AGI. и способностью к обучению, то у нас есть стимул, есть респонс, есть обратная связь от среды, либо положительная, либо отрицательная. Ну, а если у них есть последовательность, стимул, респонс и подкрепление, то или иное, то вот они все равно привязаны как-то во времени, и все развития, все какие-то процессы, с которыми мы имеем дело в системах искусственного интеллекта, они обычно решают какие-то задачи, связанные с автоматизацией каких-то процессов, с какими-то предсказаниями, какими-то рекомендациями, что нужно делать в будущем. И мне кажется, что с практической точки зрения построение систем для AGI, оно должно все-таки поддерживать время некоторым особым образом для того, чтобы просто повысить эффективность этих систем. То есть мы, получается, строим систему narrow AGI, которая предназначена для решения некоторых процессов и сценариев и явлений, которые разворачиваются на вовременной шкале. И если вот задаваться такой задачей и пытаться понять, что люди для этого используют, то получается, что подавляющее большинство респондентов используют какие-то собственные решения, которые позволяют хранить графы во временном контексте. Ну и вот, по сути, здесь вариант Николая Рапчевского тоже попадает в эту категорию. То есть Николай уточняет, что он использует тема антиграфа и temporal sequence, но очевидно, это какое-то кастомные решения, а вот из стандартных решений здесь вот только есть, упоминается TimeScaleDB вот в одном месте, а OpenCock, Grakan, Neo4j не использует вообще никто, все делают что-то свое. Соответственно, если посмотреть с левой стороны, каким образом мы можем теоретически попробовать похоронить информацию о триплетах, аннотированных временем, то есть у нас есть глагол, субъект, объект, что определяет элементарный, бинарный типизированный предикат, трипл, с временной аннотацией, то мы можем это дело либо хранить в обычной базе данных, но тогда вручную нужно обеспечивать правильные индексы на триплетах и времени. Либо можем хранить это дело в любом TripleStore, но тогда нам нужно каким-то образом обеспечивать кастомные индексы, которые будут работать со временем и присобачивать эти временные индексы к тем триплетам, которые у нас хранятся в базе данных. Либо мы можем взять революционную TimeScaleDB, которая оптимизирована для работы с атрибутом время в тех же самых картежах, с глагол, субъект, объект и время, как если бы мы их хранили в революционной базе данных, но с оптимизацией по времени. Можем это хранить в любом виде, вариантов очень много, мы про это чуть дальше поговорим в этом спейсе OpenCog. И опять-таки нам придется решать вопросы производительности каким-то образом. Ну и если взять мою собственную конструкцию, которая ориентирована для работы с блокчейнами и социальными графами, то здесь получается очень эффективно, то у нас архитектура здесь примерно такая, что мы берем множество подграфов и каждый подграф имеет некоторый таймстамп, в большинстве случаев это таймстамп, связанный с конкретным днем, то есть для каждого дня в системе мы получаем свой собственный подграф. И очень эффективно можно выбирать некоторый интервал времени по временной метке, получать объединенный подграф, связанный с каким-то интервалом или с какой-то точкой времени, делать в нем нужные операции. графовые, но если нам нужно взять какой-нибудь объект, субъект или объект их отношения и проследить их по всей временной шкале, это получается не очень эффективно, потому что мы должны делать выборки по множеству вот этих вот временных подграфов. Значит, теперь давайте поговорим о том, что же всё-таки является прообразом вот этого предиката и его структурным представлением с точки зрения того, как мы его поместим в некоторую базу данных. И для начала давайте подумаем… Представим себе, что мы хотим хранить информацию, хранящуюся в нейронной сети. У нас, к примеру, есть нейронная сеть, и мы хотим знания, хранящиеся в нейронной сети, записать не в виде какого-то блоба или бинарного файла модели, А мы хотим положить это в какое-то структурированное хранилище в виде связей, весов этих связей и, собственно, тех искусственных нейронов, которые эти связи осуществляют. Или это у нас граф не нейросетевой, это граф у нас семантический, опять-таки мы хотим записать в граф какие-то концепты и какие-то связи. Если мы делаем это, пытаясь сказать, что ну типа нейронные сети же так и устроены, у нас есть нейроны и между ними есть связи, то на самом деле это не совсем так, потому что нейроны связаны между собой не прямыми стрелочками, то есть нейроны не являются кружочками или точками, и они связаны не стрелочками, они связаны вот такими вот деревьями. И если рисовать, скажем так, правильную топологию нейрона, то нужно на самом деле рисовать все его входы в виде вот таких вот деревьев и стрелочек, и выходы тоже в виде вот таких вот деревьев. Это тогда будет топологически и, возможно, функционально более правильным представлением. И, в частности, это будет соответствовать той модели нейрона, которую строит Евгений Евгеньевич, где нейрон описан именно как совокупность предикатов, где группирование этих предикатов может быть может соответствовать ветвлению на дендритах. Причем в прошлом или уже в позапрошлом году в Nature как раз уже была как минимум одна публикация, которая показывала, что существенная часть вот именно, так сказать, Функционирование нейрона привязано не просто к связям с другими нейронами как таковыми, а именно к тонкой структуре вот этих вот дендритных ветвей, и в зависимости от того, через какие уровни ветвления вот этих дендритных ветвей некий нейрон имеет связь с другими нейронами, это является Имеет определенное влияние на ту обработку информации, на то его поведение, которое наблюдается в эксперименте. Вторая интересная фишка, с этим связанная, это то, что у нас связи в нервной системе, в естественной нейронной сети, не искусственной нейронной сети, связи осуществляются не обязательно между нейронами и нейронами. У нас связи могут осуществляться между нейронами и аксонами, между нейронами и синапсами. И есть такое понятие, как синапс на синапсе, когда некий синапс, воздействующий на тело нейрона, оказывается под контролем некоторого другого синапса от третьего нейрона. Причем в простейшем виде это получается, у нас есть некоторый нейрон, который оказывает либо ингибирующее, либо подавляющее, либо, наоборот, подкрепляющее действие на первичный синапс, и тогда на самом деле мы можем рассматривать вот эту конструкцию по сути, конструкцию одного отдельно взятого транзистора, хорошо знакомого из курса радиотехники. А если посмотреть на эту конструкцию с позиции доклада Алексея Рядозубова, где у нас вот в этом месте находится, что называется, синоптический кластер, то тут, конечно, уже схемой одного отдельно взятого транзистора не обойдешься, тут какая-то уже нужна многомерная конструкция, особенно учитывая возможность тех самых нейромедиаторных коктейлей и возможности интерференции от сигналов от различных медиаторов. Опять-таки, я не нейронфизиолог, я не буду здесь спегулировать про коктейли, но остается факт, что чисто с точки зрения топологии мы имеем дело со связями, которые осуществляются не только между вершинами в виде отдельных нейронов, но связи могут осуществляться между связями, а если связи могут осуществляться между связями, то они могут осуществляться между связями, которые сходятся на связях, а те самые связи, они могут тоже быть на других связях. То есть мы можем вообще представить себе такую конструкцию, когда существуют как бы элементы или атомы с точки зрения Оппенкога нулевого порядка или нулевого уровня, которые связаны связями первого уровня, это простые связи между нейронами. А вот эти связи первого уровня, простые связи между телами нейронов, между точечками и кружочками, они находятся под контролем связи второго уровня, а связи второго уровня находятся под контролем связи третьего уровня, а связи третьего уровня находятся под контролем связи четвертого уровня. И если мы попытаемся вот эту вот структуру вершин в графе и связи между вершинами графа, при том, что связи могут осуществляться на связях, проинтерпретировать с точки зрения логики предикатов, то есть, если считать, что мы, что каждая вершина это терм, а каждая вершина, имеющая стрелочки к другим вершинам, это предикат, то мы получаем так называемую логику higher order logic, называется по-русски, я не знаю, как это перевести, higher order logic, логика более высокого уровня или логика более высокого порядка, когда у нас эти предикаты могут включать в себя как термы, так и другие предикаты, и в свою очередь также быть аргументами других предикатов еще более высокого уровня. Значит, если теперь посмотреть, какого типа объекты могут быть использованы для хранения графовой информации, то мы можем выделить несколько видов объектов. С левой стороны у нас показаны простые бинарные предикаты, которые могут использоваться, в данном случае типизированные предикаты, которые могут использоваться, которые можно хранить в графовой базе данных. То есть это либо направленные или упорядоченные связи, допустим, causality – причинно-следственная связь, что облако приводит к дождю. Либо это могут быть ненаправленные или неупорядоченные связи, вроде симилярити, что там сокол похож на орлат, причем как тут, а в другую сторону это работает. А могут быть точно так же упорядоченные или направленные, неупорядоченные, ненаправленные связи более высокой размерности или большей арности. То есть, по сути, гиперсвязи. Если с левой стороны мы имеем в виду связи обычного графа, то с правой стороны мы имеем связи того, что называется гиперграф. Например, объединение в множество или в сет орла, сокола и ястреба приводит к набору или к гиперсвязи всех хищных животных. или трех хищных животных. И мы можем говорить, что все эти три животных похожи на друг друга примерно в одной степени, поскольку все они являются хищниками, хищными пернатами. Точно так же можем построить гиперграфовую связь, описывающую причинно-следственную связь между облаком сменяющими, который приводит к дождю, который, в свою очередь, приводит к наводнению, и это тоже будет гиперсвязь с арностью равной 3. Но здесь будет важен порядок, и смысл этой связи будет в том, что вот определенную упорядоченность явлений в этой причинно-следственной цепи, она имеет определенную семантику. Если мы поговорим о том, как мы в этой графовой базе данных будем представлять, допустим, бинарные связи или тернарные или бинарно-этипизированные связи, тут тоже возможны варианты. То есть вот, например, схема, в которой, как я понимаю, хранится информация у Николая Рабчевского. То есть связи не имеют типов. или все они имеют тип inheritance, то есть все связи однотипны, и мы из контекста только можем вычислить вообще с чем мы имеем дело. Следующий вариант, если мы строим систему, где каждая связь может являться объектом или наоборот источником связи, мы можем построить бинарную связь, но дополнительная связь в графе, исходящая из этой связи, будет указывать на тип этой связи, что данная связь является причиной. Если кого-то ударили, то ему стало больно, вот дополнительную связь мы строим, чтобы эту связь установить с типом причинно-следственной связи. Дальше мы можем использовать стандартный TripleStore, где у нас есть три аргумента, в каждой связи индексы строятся между ними всеми единообразными средствами. Ну и, наконец, вариант TripleStore, когда у нас тип связи является просто некоторым специфическим атрибутом. Значит, ну вот, обсуждались разные варианты как раз недавно в группе, я просто в качестве иллюстрации приведу, что, к примеру, если у нас есть задача о том, что мы можем Задача ставилась таким образом, что мы, к примеру, хотим описать изначально некоторую схему того, как некоторые пользователи соотносятся к некоторым группам в Фейсбуке или в Телеграме. Вот, допустим, есть пользователь, у которого имя Том, вот от этого пользователя идет стрелочка на вершину том, и этот пользователь является мембром некоторого объекта, у которого имя IGA, I-Russia, и этот объект является группой. Вот идет ИЗА-стрелочка на некоторый объект, у которого имя группа. То есть, получается, есть группа AGR, AASHA, есть пользователь TOM, у которого идет стрелочка на ISA User, то есть он является пользователем, и у этого пользователя есть в системе рейтинг 3, соответственно, вот связь между этим пользователем и циферкой 3, которая обозначает его рейтинг. Если у нас возникает желание сказать, что, к примеру, этот пользователь может быть, мало того, что пользователь может иметь рейтинг и быть членом большого числа групп, мы хотим, чтобы в каждой группе у него был свой собственный рейтинг. И тут у нас возникают разные варианты. В варианте А мы расщепляем сущность пользователя. Мы говорим, что у этого пользователя, которого зовут Том, у него на самом деле есть два экземпляра. подтом, который происходит от Тома и который является мембером группы AGI Russia, и есть другой подтом, который тоже наследует от Тома, но этот подтом является членом группы Social Intelligence. Соответственно, у того и другого, и у этого потома рейтинг может быть 5, у этого потома, у этого супертома может быть общий какой-то универсальный рейтинг 3, а вот у этого потома, который относится к EGRH, может быть какой-то совсем другой рейтинг, 4 там, или может быть он тоже будет 3 или тоже 5. Вариант B – это мы опять-таки в рамках обычного графа построенного на триплетах или бинарных типизированных связях. Мы можем создать дополнительный тип сущности – это членство, то есть вот у нас есть том и вот есть понятие юзер. Мы вводим новое понятие юзер, это некоторая промежуточная сущность, которая связывает между собой с одной стороны юзера, с другой стороны группу, с другой стороны рейтинг. И опять-таки вот у нас юзер, который связывает Тома и рейтинг 3, и группу AGI Russia. Вот у нас есть другой юзер, который связывает Тома, рейтинг 5 и группу Social Intelligence. И эти объекты имеют свой тип. имеет свое отношение и к некоторой вершине, которая имеет имя Membership, то есть эти группы являются экземплярами класса вершин типа Membership. Есть еще вариант, связанный с реализацией связей на связях, того, что называется метаграф. Кстати, вот с точки зрения терминологии графы, которые имеют вершины, объединяющие более чем две связи, называются гиперграфами. Это стандартное определение. А вот стандартного определения в таких топологиях графов, где элементом графа может быть Как вершина, так и связь, устоявшейся терминологии нет, но вот такое вот работает как наиболее подходящее определение – это метаграф. То есть метаграф – это такой граф, где одна вершина может включать в себя другие графы. где элементом вот этого другого графа может быть как вершина, так и связь. Соответственно, если работать в терминах метаграфов и иметь возможность связывать связи, а не только вершины, то мы можем сказать, что мы остаемся в рамках прежней топологии, у нас как была вершина для тома отдельного, так она остается, но мы говорим о том, что вот у нас отдельная связь рейтинг на троечку, И она имеет связь со связью мембершип между AGI Russia и томом. И мы как бы вводим вот эту вторую связь в контекст определенного членства в определенной группе. И мы можем иметь другую связь, которая связывает данного тома с пятерочкой, но эта связь будет в контексте мембершипства этого тома уже в другой группе под названием Social Intelligence. Вот эти все варианты работают в терминах типизированных Если же мы пойдем к схеме Николая Рабчевского, которая работает с нетипизированными графами, то мы увидим, что на самом деле у нас все определяется бинарными связями, что и в конечном итоге, если мы хотим создать отдельную сущность, которая будет описывать некоторый рейтинг некоторого пользователя в контексте некоторой группы, мы просто вот это вот для этой сущности создаем отдельную вершину и просто связываем ее нетипизированными связями со всем чем нужно. Мы ее связываем с определенным рейтингом, мы ее связываем с определенным пользователем, мы связываем с определенной группой, а то что значит эти группы являются группами известные связи на группу, а то, что эти пользователи являются пользователями, связаны с указанием на пользователя. Каким образом подобное решение предлагается осуществлять в системе OpenCock? Теперь небольшой экскурс OpenCock, про который более подробно будет рассказывать Алексей, но я дам некоторый общий обзор. В OpenCog мы можем описывать разные уровни абстракции, используя принцип гиперграфа. У нас есть уровень с правой стороны, зелененьком блоке, у нас есть уровень абстрактных концептов, где мы работаем с абстрактными понятиями столы, там ножки, ручки, поднятие рук, то здесь в данном случае идет речь о том, что кто-то поднимает руку для того, чтобы взять какую-то еду со стола, судя по связям в этом графе. Но вот в правой стороне у нас высокий уровень абстракции, где мы имеем дело как раз с символами в терминах голосование юрия бобурова да вот с правой стороны мы работаем на верховном уровне на верхнем уровне работаем символами стола там руки и поднятие руки как определенного действия вот а вот на среднем уровне мы работаем уже скажем так с неименованными вершинами да и причем здесь мы можем говорить что мы работаем может быть даже и с какой-то нейросетевой моделью, где у нас неименованные графы, хотя мы условно можем некоторый экземпляр некоторого стола ассоциировать с некоторым ID-шником стол номер 754, а некоторое поднятие руки в некоторый момент времени проиндексировать как поднятие руки номер 55 в каком-то контексте. Ну и, наконец, на самом нижнем уровне у нас мы видим определенные пикселы в определенных координатах, определенные даты, как аргументы этих преградикатов. То есть у нас, по сути, мы имеем дело работы на самом детальном уровне, на самом нижнем уровне абстракции, мы работаем с предельными дискретными элементами, описывающими пространство и время, определенные таймстемпы, определенные оттенки цветов по RGB-каналам, определенные координаты пикселов в виде потоки на определенных фреймах. И все это, как можно тут видеть из записи, определяется с точки зрения некоторого языка. оперирующего с предикатами или с элементами графа. Вот те элементы, которые я показывал на примере различного вида представления связи. направленные, ненаправленные, значит, бинарные или н-арные связи, графовые или гиперграфовые связи, вот так вот они представлены в OpenCog, и так они могут быть записаны в Atomize Language, то есть, например, Implication Это бинарная связь, которая соединяет причину и следствие. Вот написано, как на атомизе написано, что облако приводит к дождю. Соответственно, ситуация, описывающая, что облака обычно связаны с дождем, эти случаются вместе, может быть записано через бинарную эндлинк. То есть, эндлинк – это, по сути, логическая коньюнция. Но эндлинк – это такая интересная связь, которая может описывать любое количество аргументов. Соответственно, эта связь является не только ненаправленной или симметричной, или неупорядочной, но она еще и является гиперсвязью в общем случае, потому что Сколько угодно событий может быть определено в одну общую конъюнкцию. Ну и вот так вот записывается это тоже на атомизм. Сколько угодно мы можем определить, сколько угодно аргументов, точно так же для OR-линга, где OR-линг в атомизме и в атомспейсе – это тип связи, или правильно называть, атома, а не связи для дизъюнкции. Ну а sequential link, то есть если мы говорим о том, что облако, дождь и наводнение происходят в какой-то логической последовательности, то мы соединяем их через последовательную связь. Sequential link, которая может объединять несколько концептов. Ну вот того, как мы представляем метаграфовую информацию, когда у нас некоторая последовательная связь может быть объединена с некоторым концептом, и все это вместе может быть объединено в одну гиперсвязь, объединяющую вместе, как вот эти вот концепты и линки, в одном атоме или в одной гиперсвязи. как это может быть связано с другой гиперсвязью. То есть мы говорим здесь, что если у нас случается высокое давление и высокая влажность и низкое давление, то вот это все вместе взятое с точки зрения implication приводит к тому, что у нас случается туман, а потом сначала дождь, сначала облако, потом дождь, потом наводнение. А все это вместе взятое мы можем определить в некоторое множество и все это множество связать некоторым доменом, сказав, что все события подобного рода, все implication links, все подобные причинно-следственные связи, они относятся к области знаний, которая называется география. И можем все это записать на атомизе вот в такой вот древовидной структуре, мы говорим, что у нас inheritance link объединяет вот такой вот set, который включает в себя implication link между вот такой вот конъюнцией и вот такой вот конъюнцией, где в этой конъюнции есть еще вот такая последовательность, и все это попадает под область географических знаний. И, собственно, тем самым мы можем, представляя знания в таком виде и зная, что каждый атом, каждая стрелочка, каждый концепт может иметь за собой некоторый вероятностный смысл, описываемый, например, с точки зрения Non-Axiomatic Reasoning System, Compound Truth Value, где раздельно описывается сила связи и подкрепления или evidence этой связи. Либо это может быть, в принципе, описано с точки зрения вероятностной логики, обычной вероятностью вместо составной, вместо compound truth value из strength and confidence по отдельности. Это уже, что называется, зависит от той системы logical inference, которую мы применяем, но эта система работает не с обычными, не с логикой первого порядка, а с логикой высокого порядка, как здесь изображено. Если мы хотим строить такие конструкции высокого порядка, то даже в рамках одного OpenCog можно это сделать двумя разными способами. С левой стороны показано, как мы это делаем с помощью гиперсвязей типа SET. Ну, например, можем сказать, что ветер приводит к волнам, облако приводит к дождю, связать все это, значит, причинно-следственными связями типа implication и, значит, объединить все это вместе взятое и сказать, что через inheritance link, сказать, что все это имеет отношение к географии, допустим. Ну и вот как это записывается с помощью языка Atomist. А мы можем то же самое сделать по-другому. и сказать, что вот у нас есть такие вершины и такие связи между ними, как implication между in the wave и cloud, implication link на rain, и их все через специальную связь под названием member привязать к концепту географии, и то и другое, и записать это в виде всего такого текста на языке Atomisk через отдельные member links. Ну и здесь возникает, в общем, это уже что называется проблема разработчика конкретной доменной области, как мы одну и ту же информацию закодируем, так мы, собственно, с ней будем работать. Здесь возникает такой же проблема нормализации и проблема оптимизации, которая возникает у разработчиков приложений на SQL-таблицах, как мы по SQL-таблицам все разложим, какие индексы мы построим, и так оно все у нас и будет работать. Значит, здесь я, наверное, не буду уже для экономии времени рассказывать, как устроен OpenCog внутри, и как работает вероятностная логика на примере OpenCog. Просто дальше пробегусь, какие еще интересные Скажем так, доменные онтологии существуют для работы с тем, что мы называем системой AGI. Кроме базовых мета-онтологий, которые работают на уровне объект-класс или явления-отношения или вещь и свойства, существует несколько таких доменных онтологий, которые немножко более продвинутые, работают с более специализированными понятиями, продвинутыми понятиями, которые, тем не менее, могут с точки зрения разработчиков этих онтологий описывать достаточно широкие классы задач. Вот, например, скажем так, узкая широкая онтология, разработанная Беном Гёрдзелем и Лайносом Вепстасом для задач Nature Language Processing, которая позволяет описывать лингвистические модели с помощью грамматики связей. Что интересно, что по утверждению Лайноса Вепстаса с помощью вот этой вот модели можно описать все, что угодно. То есть тут можно спорить или нет, но вот эта теория имеет право на существование. Точно так же, как мы можем описать все, что угодно с помощью объектов, классов и атрибутов, точно так же мы можем все описать с точки зрения дизиджиантов Например, он показывает то, как мы можем всю химию или всю физику описать с помощью вот этих вот связей, вот этих вот структур. Что является базовыми элементами с точки зрения грамматики связей? Значит, это отдельные части речи. применительно к естественному языку. Но если мы возьмем химию, это могут быть химические элементы. Дальше. Есть типы связей, которые эти грамматические элементы образуют либо в правой части по тексту предложения, либо в левой части по тексту предложения. Те типы связей, которые идут направо. обозначаются плюсом, те типы связи, которые идут налево, обозначаются минусом. Если мы возьмем химию, как говорит Лайнесс, этому будут просто валентности соответствующих химических элементов. Ну и, наконец, для каждого элемента, который мы здесь описываем, или для каждой грамматической категории, есть набор слов, которые под эту категорию подпадают, и набор связей. Кроме того, эти связи могут группироваться либо через конъюнкции, что вот мы можем, допустим, если у нас есть какое-то существительное, характеризующее, допустим, какое-то животное, то у нас слева может быть детерминер. D-, а справа, и при этом слева справа может быть субъект, а справа может быть объект. И вот давайте посмотрим, вот, например, cat. Вот у слова cat с левой стороны находится детерминер D, а с правой стороны находится субъект. А если мы возьмем snake, то вот это вот в структуре грамматической связи одинграммера На левой стороне тоже находится детерминер, связь D, только уже детерминер типа A, а не типа V. И слева же находится объект с точки зрения грамматической структуры предложения. Ну, а у глагола, в данном случае честь, у него все просто. С левой стороны находится связь типа субъект, а с правой стороны находится связь типа объект. Ну и в продвинутой версии, значит, товарища Лайнесса у нас вот эти вот грамматические элементы, допустим, вот возьмем слово i, so, de, so, вот, они еще могут объединяться между собой в секции, то есть последовательность вот таких вот конструкций, где у нас имеется некоторое, так сказать, грамматическая единица или структурная единица, соединенная, имеющая налево и направо некоторые связи некоторого определенного типа, состоящие из коннекторов. И вот совокупность связей вокруг некоторой грамматической или структурной единицы называется дижанк, а последовательность вот таких вот элементов, соединенных связями в некоторой множество, в некоторый последовательный набор называется section. И вот если мы подойдем к этой истории с точки зрения, к примеру, вот той саботинной антологии, про которую рассказывал Александр Балдачёв, про который чуть-чуть позже, на следующих слайдах я сейчас буду говорить, мы можем сказать, что вообще любые события, происходящие в жизни, мы можем описывать с точки зрения этой грамматики. То есть то, что происходило перед чем-то, может быть связана связью последования направо, а то, что произошло после, может быть связано связью предшествования налево. И, соответственно, любая причинно-следственная цепочка, она может быть описана вот такими вот секшенами. И на самом деле мы можем свести грамматику связей Лайна-Савепса с той процессной онтологией, про которую я рассказывал где-то месяца три назад. Я, значит, тоже в качестве примера сейчас обозначу основные понятия для тех, кого не было в этом семинаре. То есть у нас в онтологии яджинс базовой вещью является вещь, А дальше у нас есть два уровня понятий, которые, в принципе, могут описывать любые события, неважно, то ли мы имеем дело с робототехникой, то ли мы имеем дело с грамматикой, то ли мы имеем дело с какими-то физическими процессами, которые мы хотим как-то описывать и предсказывать. То есть на верхнем уровне у нас являются абстрактные понятия, то есть сценарий, ситуация, которая возникает в этом сценарии, и явление, из которой состоит ситуация. некоторая роль, которую какая-то сущность принимает в том или ином явлении. Это абстрактные понятия. А есть абсолютные понятия, то есть экземпляры этих абстрактных понятий. Соответственно, экземпляром сценария является процесс, который привязан к определенному временному интервалу. Экземпляром ситуации является совпадение какого-то числа событий, и это совпадение тоже привязано к определенному моменту времени. Каждое событие отдельно взятое, оно также привязано к определенному моменту времени и входит в совпадение событий точно так же, как и явления, входит в ситуацию. Ну и, наконец, само событие, оно включает в себя некоторое число экторов, либо физических, либо либо каких-то живых людей, либо и живых людей, и физических людей, где живые люди совершают какие-то воздействия на физические объекты, и все вот участники этого события, человек там встал, вот человек и пол являются экторами, потому что человек встал, стоя на полу. Ну и Я не буду на самом деле останавливаться на примерах, это можно посмотреть на записи семинара, который был летом. Теперь немножечко слов про фундаментальные онтологии, с помощью которых можно описывать вот эти вот антологии доменные. Тут вот несколько интересных вариантов есть. Например, вот здесь вот показаны два варианта. Один вариант, который я делал в проекте в 1995 году, и идея заключалась в том, что мы описываем, значит, на абстрактном уровне мы описываем некоторые сущности и отношения, которые между ними могут иметь место. И здесь, на уровне экземпляров мы описываем вершины и связи, которые эти вершины могут связывать. Ну и, соответственно, между связью и отношением есть отношение типа, то есть каждая связь может быть какого-то определенного типа, и каждая вершина может быть какого-то, является экземпляром какой-то сущности. А с правой стороны нарисована антология Freebase, сделанная в 2007 году и купленная Google в 2013 году. И прикол этого слайда заключается в том, что на конференции AGI 2018 в Праге, я разговорился за завтраком с товарищем, который, собственно, вот на тот момент, я не знаю, где он сейчас, но на тот момент, два года назад, он, собственно, возглавлял группу Knowledge Graph в Google, вот, и наш разговор за завтраком начался с того, что он рассказывал, почему вот эта вот модель Saks Почему эта модель SACS, почему она не работает, и почему он бьется со своим руководством, чтобы эту модель отправить в топку и заменить на более правильную модель, и что ему руководство не дает, потому что на базе бывшего Freebase'а, сейчас Knowledge Graph'а, у Google работает, собственно, существующая версия его поискового движка, и если там начать рефакторинг, то упадет вообще все. Вот. А проблема, с которой на самом деле они столкнулись, мы с этой проблемой столкнулись в 2001 году, и мы там пришли к тому, что вообще неправильно на уровне фундаментальной онтологии разделять понятия сущности и вершины с одной стороны и отношение связи с другой стороны. Или у Гугла это называется разделение между типом и объектом и сущностью, свойством и связью. На самом деле это все одно и то же. То есть на самом деле у нас есть некоторая отдельная вещь, есть некоторое отдельное свойство. Вот, и они являются как бы конечными, то есть на уровне мета-онтологии мы не можем сказать, является ли что-то абстрактной или неабстрактной сущностью. Все является сущностью, в той, обладающей той или иной степенью абстракции. И на определенном уровне развития системы нечто, что казалось нам, так сказать, конечной и терминальной сущностью, может оказаться абстрактной сущностью, и нам придется у этой новой, так сказать, сущности, обретшей качество абстрактности, делать экземпляры этой сущности, как вот, например, в том примере, где я показывал на представлении пользователей в группе. Можем сказать, что вот данный пользователь в группе, в каком-то смысле, в конкретном реальном случае является классом и на базе его нужно вырастить различные экземпляры вот этого пользователя, которые будут представлять этого пользователя в различных контекстах. В рамках вот такой фундаментальной онтологии вы этого в принципе не можете сделать. Потому что объект у вас уже по определению объекта не обладает свойствами класса. А если у вас все является вещами, то вы от любой вещи в любой момент можете произвести детей с одной стороны, а с другой стороны можете сказать, что вот этой вещи есть какой-то еще более высокий его уровень абстракции суперкласс. экземпляр или класс экземпляром которого эта вещь оказывается является. И вот на базе как раз вот этой антологии переходим уже дальше к тем языкам, которые могут быть использованы для описания различных графовых представлений в рамках различных базовых антологий. Вот, например, пример языка URL, Object Relational Language, который мы сделали разработали в 1997 году с Леонидом Кузиным для создания возможности универсального описания бизнес-процессов. Вот здесь с левой стороны показаны различного типа стейтменты или утверждения на человеческом языке, с правой стороны показано, как эти утверждения записываются на языке РЛ и показано, что мы, как любые запросы, интеррогативные так сказать, statement, interrogative statement, так и любые ответы, как declarative statement, можем выражать с точки зрения, ну, так сказать, в рамках одного и того же языка, в отличие от того, допустим, как вы используете, значит, такие языки, как SparkQL или GraphQL, когда, так сказать, в ответ вы пишете вопрос на одном языке, типа SparkQL, а в ответ вы получаете просто RDF-выход. Вот другой пример. Это пример как раз показан с помощью системы визуализации графов, который был разработан на базе нашей энтологии, на базе этой энтологии в проекте WebStructor. Причем здесь визуализируется утверждение, что называется higher order logic, то есть утверждение логики высокого порядка, взятое из системы PSYCH. То есть в своей системе мы могли интерпретировать выражения, как написанные на языке RL, так и утверждения, и стейтменты. из системы PSYCH. И вот, например, утверждение на русском языке, что мама животного является самкой, является mother of an animal is a female animal – это вот выражение на естественном языке. А вот как это записано в языке Higher Order Logic, где то, что мы видим с вопросиками – это переменная Вот, соответственно, мы пишем, что, значит, если есть конъюнкция, значит, с одной стороны у нас есть, если есть конъюнкция, объединяющая то, что между A и M, значит, есть отношение mother, и между M и female animal есть отношение is a, то есть M is a female, is a mother, и если и то и другое выполняется, вот, и, значит, мы можем сказать, что существует такое m, при котором выполняется вот это вот все, потому что есть exist, значит, m, у которого выполняется вот это вот все. И, наконец, есть implies, то есть есть implication между тем, что из а является животным, то есть a is an animal, и есть implication между вот этим и вот этой логической конструкцией. И вот с помощью, например, графов, либо в двухмерной плоскости, либо в трехмерном пространстве, в виртуальном гиперпространстве, мы можем визуализировать такие логические формулы высокого порядка с помощью графов, где вершиной графа может быть как конечный концепт или терм, так и любая связь в этом графе. Ну вот пример как раз мы обсуждали, который с Майклом Миллером, который как раз недавно опубликовал книжку про язык программирования «Премис», который, правда, сейчас доступен только для ограниченной новое бета-тестирование, желающим можно Майклу написать и спросить. Ну вот как-то антология с управлением спортсменом в маленьком мире спортивных, в маленьком мире Small Universe of Kitesurfing. как можно эту онтологию описать, и как в терминах этой онтологии можно описать любые процессы. При этом, опять-таки обращая внимание, есть проблема того, как кодировать время, то ли время является произвольным атрибутом или термом в предикате, или элементом языка. Вот в моей модели время должно выноситься отдельно, ну а вот у Майкла Миллера в премисе время – это вот просто один из аргументов утверждения. Ну и второй вопрос – как быть с вероятностью? То есть если мы хотим каждому утверждению присовокупить некоторую вероятность, то, как эту вероятность присовыкуплять. У Майя Клаймюллера, опять-таки, вероятность предполагается вписывать как один из атрибутов утверждения, а у Пиванга и Питера Хаммера в Нарсе есть отдельный грамматический элемент, который не является атрибутом предиката, а это отдельная аннотация, которая присовыкупляет true value к каждому конкретному предикату. Ну вот это видео, которое мы смотрели в прошлый раз, просто здесь немножечко лучше видно семантику, как выглядит язык программирования Дельта-0. В верхней части, насколько я понимаю, если я не прав, Евгений Евгеньевич, если он здесь может потом прокомментировать, В верхней части, насколько я понимаю, это текущая картина мира некоторой мыши, которая здесь сейчас бегает. И вот эта текущая картина мира представлена в виде соответствия некоторых целей и некоторым правилам, причем имеет место некоторая иерархия. целей и правил, применяемые при навигации агента. А в нижней половинке экрана справа написана некоторая просто последовательность событий, которые возникают в жизни некоторой мыши, ну и, соответственно, те решения, которые данная мышь принимает на основе тех правил, которые в данный момент представляют ее картину мира. Ну и, наконец, подводя конец разговору о языках, одна из идей, которую я в своем проекте развиваю, это то, что язык предикатов или язык, который выражают некоторые графы, или гиперграфы, описывающие реальность и используемые для взаимодействия между различными компонентами или агентами интеллектуальной системы или между различными агентами, где каждый агент является интеллектуальной системой, этот язык можно использовать в том числе для взаимодействия между агентами и людьми. Эту идею первый раз я рассказывал на конференции ИНЛ в 2014 году, по-моему, и был очень приятно удивлен, когда эту же идею обозначил... Эта идея прозвучала сейчас... Игорь, поправьте меня. Идея о том, что в скором будущем у нас агенты и Агенты искусственного интеллекта и люди будут говорить между друг другом на некотором упрощенном языке. То есть идея заключается в том, что люди будут использовать более простой человеческий язык для того, чтобы взаимодействовать, чтобы быть более понятными агентам. Агенты будут использовать этот более простой язык для того, чтобы люди понимали их однозначно, ну и люди так привыкнут общаться с агентами искусственного интеллекта, что сами начнут между собой говорить на этом же самом языке. Идея здесь примерно следующая, что с одной стороны мы можем с точки зрения графов, триплетов и предикатов общаться с роботами, кодируя информацию либо в виде тортля, либо РДФа, либо в виде JSON-LD, но это все немножко недружелюбно для человека. То есть РДФ человек вообще читать не в состоянии, JSON-LD тоже тяжело. Тортл – ну, в общем, тоже, потому что тортли нужно иметь дело с ресурсинг-дидифайрами, которые там занимают полстроки и запомнить их невозможно. Соответственно, вариант использования языков семантической разметки, классических, это проблематично для человека. Использование же языков более человеческого уровня, типа английского языка или синтетического языка эсперанто, или даже лачбана, который был специально разработан как формальный математический язык, не имеющий никакой двусмысленности, не допускающий двесмысленности в выражениях, но при этом с точки зрения своей структуры и словаря является естественным языком. Все эти языки для искусственного интеллекта тяжелы. Это нужно очень много ресурсов для того, чтобы система искусственного интеллекта эти языки понимала. Поэтому есть гипотеза, что мы можем сделать определенный класс промежуточных языков, которые можно называть семантическими языками или контролируемыми языками. И понятие контролируемый язык, оно на самом деле используется уже в индустрии достаточно давно. Например, вот такие понятия, такие понятия, как Роджер, башня. И это вот термины из языка пилотов, которые общаются с авиадиспетчерами. И когда пилоты общаются, самолетов общаются с авиадиспетчерами, у них есть ограниченный словарь. и ограниченный набор грамматических конструкций, с помощью которых они выражают запросы и сообщения, связанные с навигацией, воздушной навигацией, воздушными пространствами, взлетами и посадками самолетов в аэропортах и их движения по курсу, где нужно передавать информацию быстро, и передача этой информации не должна предполагать некоторую бессмысленность. Ну и, собственно, вот этот вот язык агентов, которые мы продвигаем в своем проекте, как раз предполагает вот именно вот такой вот некоторый синтетический язык, который на самом деле позволяет просто осуществлять обмен информацией в виде подграфов. Причем предполагается, что если мы имеем подграф без переменных или со всеми вершинами в подграфе заполненными без переменных, то это декларативное утверждение, а если какие-то вершины в подграфе являются переменными, и не определены, то это вопрос. Вот, например, синим цветом здесь показаны стрелочки, которые определяют вопрос верхней части экрана. То есть мы спрашиваем, какова температура, реальная температура и целевая температура у термостата, расположенного на кухне. Вот, соответственно, когда мы хотим, если мы хотим разрешить этот запрос, мы должны найти, значит, тот термостат, который имеет связь, значит, тот объект, у которого есть связь типа inheritance к термостату, и у которого есть связь типа location к китчену, и после этого к кухне, и после того, как мы этот термостат идентифицировали, мы, так сказать, обращаемся к оставшимся двум связям, к типам связей, которые должны привести нас к его текущей температуре, целевой температуре и текущей температуре. А если мы хотим, и после того, как мы это нашли, а мы в данном случае это искусственный агент, который персонифицирует этот самый термостат, вот этот самый агент отвечает утверждением на том же самом языке, в той же самой структуре, в той же самой синтексе, в той же самой грамматике, но он добавляет вот к этому вопросительному графу, собственно, означенное значение вот тех вершин, которые соответствуют как самому устройству, так и тем переменным, которые мы хотели заполнить. Соответственно, вот это выражение, что у данного термостата на кухне температура 30 и целевая температура 25. Ну и, наконец, мы можем, если мы хотим что-то изменить, в этом агенте мы тоже даем подграф с заполненными, со всеми определенными беспеременных, со всеми разрешенными, со всеми заполненными вершинами. где воскресательный знак говорит о том, что это вот команда. То есть команда говорит, что вот свойство target temperature должно быть установлено в 20. И восприятие этой команды приводит к тому, что данный термостат, он подчиняется этой команде и изменяет свою температуру. Но с точки зрения графового представления, что интересно, выражение на этом языке просто можно рассматривать как дамп некоторого подграфа. То есть у нас есть одна вершина, у нас есть связь, есть другая вершина, есть связь, есть третья вершина. И если у нас существует некоторый словарь, или определение словаря, связывающего определенную семантические единицы или определенные отношения через отношения типа name со словами в каком-либо словаре, допустим, в русском словаре или в английском словаре, то просто определением словаря для определенного типа связей и вершин, мы можем получать версии данного языка для того или иного пространства имен, либо для английского пространства имен, чтобы общаться с англоязычными пользователями, либо для русскоязычного пространства имен, чтобы общаться с русскоязычными пользователями. Ну и, собственно, наконец, вот здесь показана форма BNF для грамматики данного языка. Ну и, как я уже сказал, идея, высказанная по поводу использования промежуточного человека-машинного языка, была высказана как на EI Journey в прошлом году, значит, так и на Open Talks EI в этом году. Вот я вот вылетела из головы, так сказать, фамилия человека. Я думаю, Игорь подскажет, кто эту идею высказал. Я просто сейчас... Я уже в чат написал. Да, да, да, да, да. Спасибо. Ну, собственно, вот все, что я хотел сегодня рассказать. И вопросы, пожалуйста. 

S05 [01:24:41]  : Так, отлично. Спасибо огромное, Антон. У нас сегодня там достаточно много вопросов в чате, ну хотя нет, не очень много, а скорее тут много комментариев разных. Ну там есть вопросы на понимание, я так понимаю, Бориса Новикова, что имеет в виду только универсальные естественные языки или математически специализированные тоже? 

S04 [01:25:08]  : Значит, вот я, может быть, это было сумбурно, но вот я сейчас тогда вернусь. Задача-минимум, которую мне хотелось решить в рамках вот этого двухсерийного доклада, это то, что мы хотим создать некоторый механизм, для взаимодействия между различными модулями в некоторой универсальной когнитивной архитектуре, чтобы эти модули могли между собой общаться некоторым унифицированным языком, а поскольку у каждого модуля может быть своя онтология, то есть онтология у модуля или у сегмента коры, или у некоторого когнитивного блока в когнитивной архитектуре, У каждого будет своя антология. То есть, допустим, модуль или, так сказать, сегмент, отвечающий за распознавание символов, у него будет антология представлена, допустим, там какими-то штрихами, какими-то точками, какими-то кружочками и буквами какого-либо алфавита. Это вот его Small Universe. Small Universe – модуля, отвечающая за сборку слов. вот и у этого, да, она будет состоять, наоборот, из букв какого-либо алфавита и словаря этого языка. Вот это его онтология, да. А онтология модуля, отвечающего за автоматизированное поведение, там, робота-спортсмена, она будет состоять вот из пуша, так сказать, right leg'а и left leg'а. Главное то, что у нас есть некоторое унифицированное понимание того, как мы описываем эти предикаты, то ли мы описываем сначала глагол, а потом список этих самых аргументов вот этого предикатного выражения, где тип соответствует глаголу, либо мы их описываем в схеме Turtle, где сначала идет субъект, потом глагол, потом объект. Это как бы программа минимум. При этом одна задача – это именно создание определения языка или протокола, с помощью которого могут общаться модули в этой универсальной когнитивной архитектуре. которая может быть такая, а может быть такая, а может быть такая. А вторая задача, это вот задача, которую мы периодически обсуждаем с Димой Салиховым, это создание некоторого фреймворка, который будет позволять этот самый универсальный IGI тестировать. Что вот на вход у нас должен попадать некоторый просто поток предикатов, А на выход мы тоже можем воспринимать некоторый поток предикатов. И в зависимости от того, какую задачу мы решаем, скажем так, модуль, интегрирующий некоторую задачу в данный фреймворк, он должен просто предоставлять возможность получить все предикаты некоторых действий, которые можно совершить в данной модели. и все предикаты, которые могут описывать текущее состояние мира данной модели. И, соответственно, вот если мы возьмем универсального агента, он должен иметь возможность просто получить список этих предикатов и начать просто взаимодействовать с моделью, и какая бы модель ни была, он должен рано или поздно научиться с ней взаимодействовать. Да, еще третье, значит, самое, что мы получаем от модели с точки зрения ее состояния, что мы можем сказать модели с точки зрения на ее воздействие. Ну, в данном случае моделью я имею в виду некоторая среда, описывающая реальный мир, да, в среде, то есть вместо слова модель использовать среда, потому что под моделью мы подразумеваем то, что внутри агента. То есть есть некоторое описание, эмуляция среды в нашем тестовом фреймворке, принимает предикаты на вход и генерирует предикаты на выходе. Ну и кроме того, мы еще должны в данной симуляции или модели вот этого мира определить некоторые атрибуты, которые являются целевыми функциями. Допустим, там счастье и там печаль. То есть, что является счастью и печалью, допустим, вот в данном конкретном контексте. Ну и третья задача, что, может быть, мы хотим на основе вот этого языка, языка предикатов, предназначенного для общения, межмодульного общения и общения с некоторыми тестовыми фреймворками, мы хотим построить как раз пресловутый универсальный язык, который как раз будет позволять общаться с агентами Вот примерно вот в таких вот терминах. Да, и как, значит, я вспомнил, Аркадий Сандлер из МТС вот как раз эту идею рассказывал на EI Journey в прошлом году и позапрошлом году и на EI Talks. Нет. 

S05 [01:30:10]  : И Сандлер, и Сандлер, и Крайнов это говорили. А Крайнов? Саша Крайнов из Яндекса. Показательно, что это говорят те, кто разрабатывает сам голосовых агентов, с которыми люди общаются и видят, что эти голосовые агенты не самые продвинутые. В ответ люди говорят, что скоро вы сами опустите свой уровень и будете с ними разговаривать на их уровне. Владимир Смолин на эту тему жестче написал, но это комментарии мы оставим. Здесь есть пара вопросов, не по очереди, но по смыслу правильных. Во-первых, как формируются новые предикаты? 

S04 [01:30:57]  : Сейчас, я прошу посмотреть пример как раз насчет языка. Как формируются новые предикаты? Давайте, сейчас я вернусь. Тут есть два ответа на этот вопрос. Простой ответ на этот вопрос. он вот такой вот что если мы возьмем вот такого вот агента то у него ему новые предикаты вроде как не нужны да потому что в этом мире Он не может ничего сделать, как надавить, двинуть рукоятку влево, или двинуть рукоятку вправо, или не делать ничего. У него всего есть три варианта. Влево, вправо, стоять на месте. 

S05 [01:31:51]  : Мы про джаи продолжаем говорить, Антон? Или про что-то другое? 

S04 [01:31:59]  : мы говорим про агента который находится в некоторой среде вот смотрите значит у нас с вами есть две руки и две ноги мы не можем придумать ничего нового кроме как конечного на самом деле набора движений этими руками этими ногами другое дело теперь так сказать следующий уровень да что мы из этих самых движений рук и ног можем строить какие-то комбинации. И мы можем… Вот комбинация, допустим, двух пальцев или последовательность перебирания двух ног может привести нас к построению некоторой высокоуровневой когнитивной программы, допустим, хождения, прямохождения. И в принципе, мы можем вот это вот более высокоуровневое явление, как прямохождение, последовательную перестановку двух ног, объединить в некоторый, так сказать, составной предикат или предикат более высокого уровня. и поместить его в какую-то свою модель. Более того, в какой-то момент мы в принципе можем его сделать какой-то высшей ценностью. Если посмотреть на эту схему, На самом деле, тут есть очень интересная стрелочка. Да, есть стрелочка. Вот компрессор. У нас компрессор на самом деле может взять какую-то информацию, полученную из модели, и не просто создать новую сущность, например, кост. Вот у нас есть новый предикат на самом деле. Мы знали, что можно любить, мы знали, что можно быть счастливыми. А вот мы вывели некоторые отношения между любовью и счастьем. его сформировать в результате какого-то жизненного опыта, мы можем его на самом деле еще и поместить в базовые ценности, если это позволяет нам чаще достигать других базовых ценностей. То есть, приведу пример, который я, по-моему, в прошлый раз приводил, что если мы в какой-то момент понимаем, что делая любовь, мы бываем всегда счастливыми, то на самом деле делать занятие любовью может стать для нас в какой-то момент самоцелью, и мы будем просто заниматься этой любовью для того, чтобы не думать, не принимать решения по этому поводу, и не тратить на это свой ресурс. Я ответил на вопрос? Или вопрос был про то, как возникают новые типы предикатов? 

S05 [01:34:42]  : Ну вопрос был про то, как возникают новые предикаты, ну и видимо подразумевается новый тип предикатов, потому что дальше сейчас уже следующий вопрос этого вытекает. 

S04 [01:34:49]  : Ну как новые типы, новые типы, наверное, могут возникать, когда, например, ну я тоже могу поспекулировать, значит, чтобы придумать пример, наверное, нужно потратиться какое-то время, если вот используя Вот, например, такой вот этот вот пенитивный аппарат, сейчас я скажу, значит, тут вот даже есть. Если мы понимаем, что, оперируя с различными совпадениями в рамках различных событий, у нас какие-то события очень часто возникают в связи с друг с другом, то мы можем сказать, что между ними, и они никогда не попадали раньше в этой связи, и они никогда не попадали в тех контекстах, с которыми мы привыкли иметь дело, то мы можем сказать, что вот это вот сочетание вот определенных событий определенного класса, это вот является ну какой-то новой вообще категорией, вот, и эту новую категорию мы можем записать, это будет на самом деле какая-то unlabeled категория, да, но в какой-то момент мы можем, значит, подумать и придумать ей какое-то имя. Ну, собственно, как возникает… То есть, на самом деле, это происходит в процессе человеческого языка. Да, ну вот паровоз. Вот пример паровоза. Вот мы придумали предикат «паровоз». У нас был предикат «вези», у нас был терм-пар, а мы объединили терм-пар с предикатом «везение». И паровезение – это определенный способ везения, который осуществляется с помощью пара. Ну, вот как-то так они возникают. 

S05 [01:36:36]  : Антон, но получается, что вы сильно все упрощаете. Есть несколько вопросов, которые в принципе... Можно я сфимулирую свой вопрос на примере? Я их сгруппирую. Сейчас, подождите, Владимир, сейчас. У Бориса Новикова был такой вопрос. Облако приводит к дождю всегда, всюду, когда? И Алексей Кабанов там пишет, что можно сказать в рамках антологии о субклассах, о перемежаемости вещей, дифференциации признаков различия вещей и, наконец, о не идентифицирующих вещих группах дифференциалов. То есть смысл весь, как я понимаю, вопроса в том, что вот такая модель, когда мы берем терм-пар и терм-везет и как бы соединяем и получаем паровоз, она сильно упрощает реальность. И как из этого выбраться дальше? Ну или вы видите из этого выход? Потому что вопросы, я так понимаю, про это. 

S04 [01:37:35]  : Смотрите, во-первых, редукционизм – это, с моей точки зрения, свойство человеческого мозга, то есть задача человеческого мозга – это максимально снизить энергопотребление. И свести всё к максимально простой картине мира, которая будет задействовать минимальное количество переменных и минимальное количество предикатов, она позволяет сэкономить глюкозу. То есть редукционизм – это, с моей точки зрения, нормально, и задача того компонента, который называется компрессор – это как раз этот самый редукционизм. Это один ответ на вопрос. Что в каких-то случаях ситуации могут быть неупрощаемы – это как раз к моему комментарию в начале лекции, что в общем случае, даже если мы имеем дело с символами, именованными символами, или если мы имеем дело с какими-то неименованными контекстами, то в общем случае мы можем иметь дело с векторами. То есть, у нас может быть на самом деле распределенное представление, полученное на основе именованных сущностей, и в том числе восприятие этих сущностей может быть вероятностным. И тогда переходя к тому, что сказал Алексей Кабанов, это, наверное, про формал-концепт-анализ. То есть ответ, что мы на самом деле работаем в некоторой разреженной вероятностной матрице объектов и атрибутов, грубо говоря, или событий и свойств этих событий, и в общем случае мы делаем векторный анализ на нечетких множествах, Хотя мы конечно стремимся максимально эту нечеткость убрать, стремимся все-таки максимально все означать и максимально убрать всякую неопределенность, чтобы проще было просто. Ну и насчет облака-дождя, насчет всегда или не всегда. Если есть желание, я могу показать, как проблема всегда или не всегда решается на уровне вероятностной логики в стиле PLN в Упенкоге или НАРС у Пиеванга на примере, который сейчас на слайде. Если надо, могу попробовать. 

S05 [01:40:01]  : Борис там явно совершенно хочет свой вопрос задать. Борис, давайте, задавайте. 

S03 [01:40:09]  : Сначала критерия, что мозг стремится снизить энергопотребление. Это, конечно, не так. Мозг стремится организовать деятельность организма, чтобы потом получать более приятный для него сигнал. И обычно это связано как раз с большим потреблением сахара, а не с меньшим. Это первое. Дальше насчет всегда и не всегда про облако и дождь. Например, это зависит от места. На одном склоне Альпы дождь облако приводит, ну не Альпы, а Кардельян, допустим. Облако часто приводит к дождю, а на противоположном почти никогда. Любая модель делается для класса задач, она не может быть универсальной. Вообще я в этой группе вижу, на мой взгляд, в корде ложное стремление к универсальности. Модели делаются для класса задач. чтобы они были полезны для решения некоторого класса задач в некоторых ситуациях. Не может быть модели, которая полезна для решения любой задачи в любой ситуации. Это отдало к вечному двигателю. 

S04 [01:41:30]  : Ну смотрите, вот в ответ на ваш вопрос по поводу, значит, этого, значит, если мы, тут есть как бы три варианта, да. Вариант первый, если нам нужен AGI, То есть если нам не нужна система, которая научится, грубо говоря, предсказывать дождь на другой стороне Альп, если мы ее перевезем через хребет, а если нам нужно просто заведомо две разные системы, одна из которых будет работать только на одной стороне Альп. а другая только на другой стороне. Ну, как бы все очень просто. Мы делаем две разных доменных онтологии, вот в тех терминах, про которые рассказывал Дмитрий Иванович, там на любом языке, так сказать, либо там на премис, либо там на дельта-нолисы, либо на атомист. Вот загружаем агента, того и другого агента этой онтологии, запрещаем им строить новые предикаты, если они вдруг столкнутся с чем-то необычным, даже если их перевезут через хребет. И все, они будут работать очень эффективно и не будут строить лишних гипотез. Если же мы хотим, чтобы после того, как агента перевезут на другой сторону через Хребет и он мог все-таки построить новую модель, существует в рамках того тех понятий, про которые я сегодня рассказывал, как минимум два варианта, как мы такую систему можем построить. То есть, либо вы можете просто на самом деле географическую точку описывать как одним из отдельных аргументов, предикатов. То есть, мы можем сказать, что Cloud, Rain и Food Это все входит в некоторый предикат более высокого уровня, где координата, где аргументами которого также являются широта и долгота. И, соответственно, вероятностная логика в одних широтах и долготах будет приводить к одним заключениям, а в других широтах и долготах она будет приводить к другим заключениям. Это вот один вариант. Либо, второй вариант, мы можем просто заранее предикаться, сконструировать таким образом, чтобы там широта и долгота, как и время года, например, фигурировали в качестве аргумента. 

S03 [01:43:53]  : Но вряд ли эта модель сможет предсказывать медицинские всякие вещи. Предсказание погоды и предсказание эффективности лекарства, наверное, должны быть разные. Потому что пренебрегать модель это всегда Когда мы чем-то пренебрегаем из практики, ограничиваем область применения модели. Чем больше пренебречь, зависит от класса задач. И не может быть модели, которая годится для всех классов задач. 

S04 [01:44:34]  : Смотрите, во-первых, модели, безусловно, не может быть, чтобы годилась для любых классов задач. Вопрос здесь в другом. Вопрос в том, является ли наша система AGI открытой. для того, чтобы в нее загружали новые онтологии. Приведу себе простой пример. Мы делаем систему, в которую загрузили только широту-долготу, влажность, давление и закрыли на ключ тот уровень архитектуры, который отвечает за Вот как раз за антологию. То есть фундейшн антология загружили, на базе ее загрузили доменную антологию, все, закрыли на замок. Дальше система может только предсказывать погоду. Если же мы предусматриваем на уровне расширяемости базы данных, на уровне мета-онтологии, которая позволяет формировать доменные онтологии, механизм расширения вот этой онтологии, текущей в системе, новыми понятиями, допустим, система может прочитать учебники по медицине, И на основе прочита полученных знаний о том, какие есть у человека органы, и какие есть химические соединения, и какие есть гены, которые могут быть активированы или инактивированы теми или иными химическими веществами. Столкнувшись с каким-то новым заболеванием, система может, так сказать, методом индукции-дедукции-абдукции, который есть в Inference Engine, построить новое знание. То есть у нас универсальность заключается в том, что у нас есть фундаментальная онтология, которая позволяет строить доменные онтологии в рамках жизненного цикла системы. И у нас есть inference engine, который позволяет делать логический вывод и принимать решения в условиях любой онтологии, основанной на данной фундаментальной онтологии. 

S05 [01:46:49]  : Сейчас, Борис, смотрите. Вы поднимаете, на мой взгляд, правильный вопрос. но он не совсем в обсуждении доклада Антона, мне кажется, не совсем. я хотел сказать что вот тут есть еще один там комментарий от Александра Соколова тоже примерно на эту же тему там что по поводу упрощения или там должен ли сам строить знания и как раз вот эту тему я думаю что надо будет пообсуждать 19 числа мы с Антоном там точно сформулировать тематику в принципе вот эти вот Антология, путь достижения Эйджай, как архитектура, например, я лично в этом сомневаюсь. Но про это надо поговорить. 

S03 [01:47:43]  : Я понимаю, что здесь Эйджай используется как аналог класса математических или компьютерных моделей. То есть все, что можно сделать при помощи математики и компьютера. Ну это же не один Эджай, это же не конкретная модель, а это класс модели, который очень широк. 

S05 [01:48:09]  : Борис, вы говорите правильные вещи, но она не связана с докладом Антона. Я как модератор говорю вам, что это не связано с докладом Антона. Доклад Антона был про другое. У Николы Робческого... Николай, слышите меня? Вас не слышно, у вас выключен микрофон. 

S00 [01:48:41]  : Да, не только был выключен, я почему-то на короткое время перестал всех слышать. Сейчас меня слышно? несколько комментариев вот какого типа во первых вот по поводу той схемы хранения информации или знаний график которым антон упоминал вот поскольку в нем нету разницы между предопределенной заранее разницы между разными типами сущностей, типа объект, класс, отношения, то эта схема позволяет хранить любую набор утверждений о любых сущностях. То есть она в каком-то смысле, так сказать, метафорически похожа на то, как в компьютерах бинарное представление данных позволяет хранить чьи числа и строки и всякие прочие структуры данных. Поэтому вот любой набор антологии любой, любую последовательность информации, представленной на любом языке, включая набор формализованных этих предикатов любого порядка они могут транслироваться вот внутрь этой системы и наоборот извлекаться оттуда и быть представленными в любом из упомянутых языков любым способом в любых предикатах и так далее поэтому есть смысл использовать этот подход в качестве самого низкоуровневого элемента в представлении знаний. Второй момент это то, что технически есть смысл отделить логические связи между сущностями, которые вот таким графом представляются. и некие набор значений, набор данных, то есть температуры, координаты, адреса, имена и так далее. То, что можно хранить традиционным образом в базах данных. И, наконец, есть третий тип данных, о которых Антон сегодня не упоминал, это последовательности. Они имеют фундаментальное отличие и присутствуют как-то так или иначе везде. Последовательность событий, которая имела место, и последовательность действий, которые разворачиваются в некие обобщенные действия, скажем. и это есть третий тип данных которые тоже нужно использовать и есть смысл держать отдельно не представляя ни в виде графа ни в виде баз данных ну вернее так в качестве хранилища базы данных можно использовать но надо иметь ввиду что это речь идет о последовательности неких символов, чем данных, чем величин. Кроме того, еще один такой момент. Вот принципиальная разница есть между системой, которая пытается учиться просто пассивно наблюдая за внешним миром и системой, которая заведомо проектируется и работает как система активная, то есть она не наблюдает пассивно мир, а она анализирует отклик на свои собственные действия. И это позволяет ответить на вопросы типа того, которые задавал Борис Новиков. Откуда берется новое? новые предикаты. То есть на самом деле берутся не предикаты знания, которые могут быть представлены предикатом, а могут быть представлены в виде графа, который формируется внутри системы без явного формулирования предикатов. И эти знания происходит из того что система помнит что происходило то есть что происходило какие действия в какой ситуации приводили к каким последствиям и эта информация и есть источник формирования новых данных новых семантических связей в графе и в конечном итоге может быть извлечена при необходимости оттуда в виде предикатов или других словесных высказываний. И насчет времени, вот если использовать тип информации о последовательностях, то время, в общем, как раз и присутствует так или иначе в этом случае. В простейшем варианте оно никак не оцифровано, а является просто элементом последовательности событий. При этом квантование времени, оно может быть включено неявно, без цифровых меток времени, ну типа произошло то-то, потом в качестве события прошел час, еще в качестве события, еще прошел час, а потом в качестве события там что-то еще случилось. поскольку, как правило, речь идет не о точных временных интервалах, которые иногда могут быть нужны, но в этом случае они могут включаться как значения атрибутов, которые хранятся отдельно. А хранить значения атрибутов удобно в схеме, когда хранятся тройки значений, то есть объект, атрибут и соответствующее значение, число там, строка там, и так далее, и так далее. При этом и объект, и атрибут, они являются сущностями графа. Ну вот у меня все пока. 

S04 [01:56:25]  : Николай, спасибо. Вы очень много всего сказали, по поводу всего хотелось мне ответить, но стеком вытеснялось, я не все запомнил. Что запомнил, я отвечу. бинарного представления. Тут ситуация примерно следующая. Еще в 1998 году, когда с Беном и Пей Вангом эту всю историю обсуждали, как правильно делать искусственный интеллект, как раз и была речь о том, что Пей говорил, что мы можем вообще все построить только на связях типа ИЗА. но вот как у вас собственно все это сделано но это очень неэффективно вот собственно что нам известно из революционных баз данных что мы можем сделать базу данных с высокой степенью нормализации и все будет очень гибко но очень медленно потому что мы замучаемся делать джойны Либо мы можем все разложить по таблицам, и все будет очень быстро, и джойнов будет мало, но если нам придется поменять структуру, нам придется делать очень большой и страшный рефакторинг, потому что все наши таблицы поплывут. И, соответственно, вот есть такой график, что чем выше у нас производительность значит с правой стороны тем меньше у нас гибкости чем они выше у нас гибкость и меньше у вас производительность вот это вот одна сторона вопроса вторая сторона вопроса это вот то что что называется что есть на рынке то есть если я разработчик и gi и я не хочу тратить десять лет или сколько-то пять лет на разработку вообще на коленке собственной базы данных на основе бинарных связей типа ИзА, типа той, которая у вас есть, а я хочу, грубо говоря, взять какое-то индустриальное решение, то вот я могу взять либо TripleStore какой-нибудь и к нему прикорячить временной индекс, если им нужно со временем работать, либо вот еще пара вариантов. Либо, ну и поскольку ни того ни другого нету, каждый, вот как на этом слайде, изобретает что-то свое. Вот, например, если кто был на мероприятии Сбербанка, на совместном, ну вот Сбербанк тоже сейчас пилит свою собственную графовую базу данных, потому что ни одно из существующих решений их по ряду показателей не устраивает. И, соответственно, Игорь, переходя практически к каким-то оргвыводам, глядя на это и на это, у меня есть гипотеза, что одним из основных направлений, в числе первых трех направлений деятельности гипотетического института будет попытку взять как минимум первые три строчки на этой таблице и, может быть, еще пару срочек откуда-нибудь еще, и, так сказать, просто посмотреть, а что вообще наиболее пригодно для того, чтобы что-то делать дальше вот по каждой колоночке. Значит, теперь по поводу хранения информации. Значит, в каких-то задачах мы действительно можем там числа и имена собственные вынести в отдельные таблицы. Но это не всегда так. Если нам нужно делать систему распознавания речи, или систему, так сказать, распознавания текстов, ну, наверное, имена, и если имена, так сказать, список имен открыт, и если имена могут возникать новые, так сказать, в каких-то новых языковых доменах, локалях, то, наверное, на этой таблице не обойтись, все-таки нам нужен какой-то механизм именно конструирования слов из букв, ну, так сказать, нужен все-таки немножко более низкий уровень, если мы делаем систему для ГОКа в НЛП. значит насчет времени то что вы сказали на самом деле про последовательности я не говорил по той причине что я про них говорил летом вот то есть вот то что здесь написано сценарий и здесь и процесс это вот как раз собственно способ хранения в явном виде временных последовательностей, но проблема в том, что с точки зрения индустриальной реализации, где у меня этот слайд, ничего такого нет. То есть у нас не существует стандартного решения для хранения временных последовательностей. у нас есть только стандартное решение для хранения либо SQL таблиц, либо Triple Stores. В какой-то системе ориентированы именно временные последовательности, в open-source и в публичном доступе нет. Хотя вот есть ряд систем, которые проприетарные, вроде как на это ориентированы. Я сейчас не помню название, но это можно поискать. Ну и последнее насчет Active Perception. Значит, я, наверное, последний раз нечетко на этом ориентировался. На самом деле, значит, вот мне запомнилось выступление, забыл, как звали человека тоже, значит, на Байке-2017 в Москве выступал товарищ из Дарпы. И он приводил такие данные, что если взять, допустим, померить активность различных нейронов в голове человека, который, например, что-нибудь смотрит, что-то разглядывает, то объем информации, которая идет по нейронам, так сказать, из коры к глазам, грубо говоря, Он, если не на порядок, то он в несколько раз превышает той информацией, которая идет от глаз к коре. То есть на самом деле, вот тезис был такой, что все наше восприятие, оно активно. То есть на самом деле, когда мы на что-то смотрим, то мы на самом деле пытаемся, мы генерим, непрерывно генерим гипотезы, которые позволяют нам это нечто увидеть с разных точек зрения в различных контекстах, вот примерно так, как про это говорил Алексей Рядозубов. И только ряд из этих гипотез, которые мы генерим в процессе восприятия, они подтверждаются тем, что на самом деле мы воспринимаем и то, что воспринимаем как реальность. Вот. Ну, поэтому, в общем, вот, собственно, вот то, что мы здесь обозначены фиолетовыми стрелочками, как expectation, или то, что вот здесь вот фиолетовая стрелочка означена как expectation, что мы, когда смотрим на символы на экране, мы, так сказать, в данном контексте хотим видеть китайские символы, а не какие-то там абстрактные рисунки. И если мы увидели то, что хотели, мы еще и даем положительное подкрепление, что да, мы построили правильную гипотезу того, что мы хотим увидеть, мы в итоге увидели то, что хотели увидеть в рамках этой гипотезы, и мы еще похвалили сами себя за то, что мы действительно поставили правильную гипотезу и то, что мы увидели то, что хотели в рамках этой гипотезы. поэтому вот как бы у нас активное восприятие безусловно работает все время. 

S05 [02:03:24]  : Антон у нас есть две поднятых руки времени уже восемь давайте мы Сергей Терехов и Владимир Шмолин. Сергей Терехов. 

S02 [02:03:38]  : Добрый день. Добрый вечер. Антон Гермич, спасибо огромное за ваш доклад. У меня вот такой вопрос. Или на самом деле вопрос общего плана. Слышно меня, да? Да, спасибо. Значит, вопрос вот такой. Вот вы начали, или может быть не начали, просто я в середине нахожусь с языка, с описания языкового, так сказать, вот какой-то метафоры для описания знаний. Вот люди, которые идут со стороны нейронаук в течение там многих стилетий, как вот я, например, да, они мыслят совершенно по-другому. Они идут от engine сначала. Они сначала думают о том, какой должен быть процессор, такой, чтобы он делал как можно меньше ошибок, чтобы он решал те задачи, которые нужны, а потом придумывать язык, который способен будет на этом процессе интерпретироваться. Вот, например, если взять какой-нибудь компилятор с языка обычного, например, C++, C++, этому компилятору, процессору, engine, ему не нужно, например, решать такую задачу, как похожесть или в каком-то смысле эквивалентность двух программ. Достаточно уметь скомпилировать каждую из них и исполнить. и тогда соответственно по такому пути можно идти, можно сдавать компилятор под язык. Но в случае, когда мы говорим о знаниях, Нам нужно постоянно решать эту проблему. Представьте себе, что у вас есть какой-то граф, который является предусловием для какого-то более высокого уровня. И есть другой граф. И принципиально вопрос, можно ли тот другой граф, который вообще-то логически не укладывается в первый, считать фактом того, что выполнены условия этого предиката и использовать правила более высокого уровня или нет. Вот engine настолько принципиально по-другому должен быть устроен, чтобы можно было использовать такие конструкции. И не возникает ли из этого необходимости начать прежде всего с engine, а уже потом думать языки? Или это просто традиция? Спасибо большое. 

S04 [02:05:34]  : С моей точки зрения, глядя на эту картинку, у нас Engine находится слева. А язык находится справа. И я, честно говоря, не могу сказать, что что-то с моей точки зрения важнее, чем другое. Более того, у меня есть гипотеза, я могу быть не прав, что когда разрабатывается какой-то процессор, то сначала разрабатывается система команд. 

S02 [02:06:10]  : Это один из путей инженерных, но есть и другой путь. Он идет как раз в то, о чем сегодня уже говорилось. Он идет от приложения. То есть, если, например, процессор должен уметь, вот, скажем, было облако, потом пошел дождь. Или было облако в прошлом месяце, а потом через два месяца пойдет дождь. Вот если процессор должен уметь эти ситуации различать и нужно, как бы, правильно с ними работать, то это уже не структура команд, а это архитектурные какие-то вещи, принципы какие-то. То есть, ну я не знаю, приоритетность какая-то внутри. То есть, я правильно, я понятно объясняю все, о чем вы говорите? 

S04 [02:06:47]  : Понятно, но, значит, я тоже из того, что вы сказали, я услышал следующее, что нам вначале нужно разработать онтологию предметной области, да, Значит, если вы не видели мой предыдущий доклад, вы можете посмотреть, там как раз вот этот слайд обсуждается, первая половина доклада, что если у нас есть некоторый мир, в данном случае это мир спортсмена, кайтера, а может быть система предсказания погоды. Вот мы для этой системы строим систему формальных понятий или систему тех параметров, которое все возможные состояния этой системы, все возможные действия, которые в отношении ее можно предпринять, да, так сказать, взять, допустим, зонтик, выходя на улицу там, или построить дамбу от наводнения. Вот. И уже в рамках вот этих вот терминов и возможных событий мы строим С одной стороны, мы строим вычислительную архитектуру, которая в состоянии предпринимать какие-то действия и строить какие-то комбинации этих действий, допустим, одновременно толкать ногу и тянуть руку. С одной стороны, а с другой стороны, мы разрабатываем некоторую систему команд для того, чтобы мы могли эту систему либо программировать вручную, либо извлекать из неё эту информацию, которую она научилась. Тут, что называется, возможно, существуют разные способы проектирования, то ли идти от антологии, то ли идти от языка. то ли идти от процессора. 

S02 [02:08:21]  : Я еще раз, извините, последний, третий раз. Я вот что хочу сказать. Дело в том, что системы логического и искусственного интеллекта, они работают уже очень давно. То есть они не работали в 60-х, потом они не работали в 70-х, потом они не работали в 80-х и так далее. Причины много раз обсуждались. Одна из них состоит в том, что как только граф начинает усложняться и эквивалентности между подграфами начинают, установление этой задачи становится сложной, то система рассыпается. Она не может работать в соседней смежной области. Из этого следует, что нужно что-то архитектурно, концептуально поменять для того, чтобы эта штука не делала вот этих вот ошибок таких. Иначе получится так. На языке графов или на языке метаграфов можно описать любую сущность и утверждать, что она отвечает знанию. Но когда вы берете engine и начинаете конкретно состыковывать несостыкуемые, плохо похожие на себя куски графа, то этот engine начинает делать ошибки. И вот если у нее архитектура изначально не ориентирована на компенсацию таких ошибок, то мы получим опять всю историю 60-х, 70-х, 80-х и так далее. 

S04 [02:09:30]  : все спасибо я уже третий комментарий больше я комментарий делать не буду ну смотрите значит то что вы сказали это у нас очень часто говорит юрий бобуров да поэтому точка зрения известно вот второе значит то что В 50-х, 70-х и 80-х годах у нас нейронные сети точно так же рассыпались. И перестали они рассыпаться лет 5-10 назад, когда появилось очень много быстродействия и очень много памяти. И очень много данных. И ошибки они делают тоже, и во многих случаях, когда они делают ошибки, мы не можем понять, почему эти ошибки сделали, и не можем даже исправить. Единственное, что мы можем сделать, то мы можем это просто каким-то образом взять и начать искать больше данных, которые, возможно, нам позволят этих ошибок избегать, то есть, это как бы старая дискуссия по поводу того, какие недостатки глубокого обучения, да, и какие, так сказать, есть возможные пути их решения, это, так сказать, отдельная, так сказать, история, вот. Ну, в общем, есть, если коротко, то есть гипотеза, что дальнейшие разработки в области архитектуры вероятностной логики, позволят, с одной стороны, не потерять те возможности, которые получены в результате достижений глубокого обучения, а с другой стороны, позволят сделать процесс извлечения знаний и передачи этих знаний, да, вот этот вот слайд, где он здесь есть, тоже был на прошлом докладе по transfer learning. То есть, позволят как объяснять и понимать при принятии решений глубокими нейронными сетями и в случае чего корректировать их ошибки, так и загружать их к тем исходными знаниями, которые можно загрузить в систему вместо того, чтобы тренировать ее на большом количестве данных, если эти правила и исходные знания уже известны. 

S05 [02:11:47]  : Все. Окей, отлично, спасибо. Давайте еще, там у Владимира Смолина были вопросы, на которые он говорит, что можно сейчас уже не отвечать, потому что времени много, но к 19 числу. Владимир, назвучьте, уж если вы... 

S01 [02:12:01]  : Ну, я постараюсь покороче, хотя все-таки минуты три займу. Вот по поводу новых предикатов и вообще новых знаний. Вы там пытаетесь рассказывать, что у вас система открыта для загрузки онтологии и может получать новые эти знания. и вот давайте на примере что значит вот скажем так что у меня будет три с половиной вопроса значит вот на примере который вам нравится это как вы осваиваете кайт там действительно красивые фотографии всем нравится тоже пол вопроса состоит в том сколько вы там сэкономили энергии там физической и мозговой пока его осваивали это как бы отдельно опять-таки можете не отвечать вот второй вопрос что вы там рассказываете как вас научил инструктор инструктора учил другой инструктор того инструктора еще научил инструктора но как вы понимаете 15 лет назад никто на кайте не катался и кто-то значит начал это делать первый и его никто не учил Вот как ваши онтологи пытаются объяснить вот то вот новое знание, которое появилось. Если вы там будете говорить про озарение, то как бы на этом наше обсуждение заканчивается. Это как бы первый вопрос. Если вы будете говорить про индукцию, дедукцию, абдукцию и прочие логические методы, то вы, к сожалению, они очень широкую и вольную трактовку, примерно как вы трактуете, как работают нейронные сети, Теоремы Тюринга и Гёделя, если вы в курсе, то и у Тюринга и Гёделя есть теоремы как о полноте, так и неполноте. То есть теоремы о полноте и того другого говорят о том, что любую конкретную ситуацию можно хорошо логически описать. А теоремы о неполноте говорят о том, что никакая логическая система не позволяет описать все. Как вы, значит, относитесь к этой ситуации по поводу получения новых знаний логическими методами? Вот это как бы второй вопрос. И третий вопрос состоит в следующем, что даже в вашем докладе присутствовало много различных систем описания. ну значит вы там сравниваете какая лучше какая хуже но в целом это очень большая проблема то есть самые тяжелые споры они терминологические то есть значит всякую ситуацию можно разбить там по-разному назвать ну там назвать разные термины это отдельное слово но соответственно можно там разбить на три класса на пять классов или на два класса и и то, и другое, и третье будет, значит, какие-то понятия, значит, там, когда появляются подклассы, еще подклассов, там, значит, вариантов разбирения становится больше, и, значит, есть ли у вас какой-то автоматизированный подход, как это решать, чтобы не нам, значит, с вами, с Игорем Пивоваровым сидеть, обсуждать, а какая система лучше, а все-таки наш сильный искусственный интеллект как-то может эти вопросы, ну, значит, трактовать. Ну, значит, я не жду от вас, что вы сегодня будете отвечать, потому что время позднее и вопросы серьезные, но в целом вот просто хотелось задать, чтобы вы как-то попытались дать на эти вопросы ответ, может быть там как-нибудь в следующем докладе или там 19-го числа, в общем все, что я хотел у вас спросить. 

S04 [02:15:00]  : Ну, я точно забуду эти вопросы, вот, но могу попробовать. 

S01 [02:15:03]  : Ну, запишите себе для памяти. Это, собственно, новизна. Ну, коротко, коротко. 

S04 [02:15:08]  : Дерево неполноты и терминология. Смотрите, значит, насчет третьего вопроса мне проще согласиться. Вот. И автоматизированного решения здесь я не думаю, что оно есть. В конце концов, даже люди пока что не придумали автоматизированного решения того, на каком языке говорить. У кого, так сказать, больше пассионарность, и больше кораблей, в военно-морском флоте, вот на том, на языке того народа мы, так сказать, как бы и говорим. То есть, метод естественных последствий. 

S01 [02:15:45]  : Но мы все-таки вот сейчас с вами не по-английски общаемся. 

S04 [02:15:48]  : Ну, это, так сказать, в частном случае. Слайды все на английском. Если мы на конференцию серьезную международную поедем, мы будем разговаривать на английском. Да, да, но вы правы, да, но каждый в итоге все равно говорит на своем языке. То есть, есть консенсус по поводу того, значит, в польской нотации писать это все или там, так сказать, или в какой-то другой, да, так сказать, сперва писать verb-object-subject или subject-verb-object. И какие скобки ставить – это уже дело вкуса. Здесь нужно, что называется, договариваться. Вот. А на первый вопрос, значит, я так понял, можно не отвечать. Я его, честно говоря, не понял. А на второй вопрос я как раз отвечал на прошлой лекции. Значит, если у вас нет инструктора, То есть, как с процессором. Если вы хотите научиться программировать на каком-то языке программирования, на каком-то процессоре, то есть два варианта. То есть, либо вам должны рассказать правила, какие инструкции за какими следуют. И когда нужно сделать поп, когда нужно сделать пуша, когда нужно сделать муль, и что нужно сделать для того, чтобы в итоге сложить число и получить его значение в регистре А, положив аргументы в регистре А и Б. Либо вам это расскажут и вы это перечитаете в книжке по ассемблеру, Либо вы просто разберете процессор по транзисторам и поймете, а как он на самом деле работает. То есть, либо методом, так сказать, разбора вы это изучите, либо вам это расскажут. Варианты два. Но в любом случае, саму программу… Я попробую уточнить свой вопрос. 

S05 [02:17:32]  : Владимир Владимирович, слушайте, ну сейчас уже давайте 

S01 [02:17:35]  : Нет, раз не стали отвечать на мой вопрос, я хотел бы его уточнить. 

S05 [02:17:39]  : Вы же сами сказали, что мы его оставим на поток. 

S01 [02:17:43]  : Вы бы мне совсем не отвечали, но поскольку отвечают не на мой вопрос, то я хотел бы его уточнить. 

S05 [02:17:47]  : Владимир Владимирович, давайте мы 19 числа про архитектуру отдельно поговорим. Друзья, но здесь надо просто, вы не только вопрос задаете, вы еще и, как это сказать, высказываете параллельно массу мнений, и мы на это просто тратим реальное время. Давайте, Антон, вы ответьте, как вы можете сейчас, сегодня, на этом поставим точку. 

S04 [02:18:09]  : Я отвечу, что если вопрос в том, откуда берется исходная программа, то исходная программа составляется на основе широких знаний данной предметной области. она опять-таки не создается методом путем ошибок, потому что озарение, как я соглашусь с Владимиром, озарения тут как раз в данном случае быть не может. вы не можете методом озарения научиться программировать на ассемблере. 

S05 [02:18:38]  : Владимир спрашивал про то, каким образом появляются те куски онтологии, которые ну как бы 

S04 [02:18:44]  : новые куски антологии, когда никто не катался на кайте. Нет, а как? Они появляются исходя из того, что у меня есть рука, у меня есть нога, и я могу тянуть за руку и за ногу и смотреть, какие будут эффекты. 

S05 [02:18:57]  : А в вашей модели? 

S04 [02:19:01]  : А ваши нагели у меня есть просто все. Я могу, у меня есть руки и ноги, потому что они у меня есть. Я могу их толкать и тянуть. У меня есть голова, которую я могу повернуть либо вправо, либо влево. У меня это вот все мои органы чувств и, так сказать, те экторы, которыми я могу воздействовать на окружающую реальность. Это как бы естественная моя онтология в данной маленькой вселенной. 

S05 [02:19:24]  : И, исходя из этой естественной онтологии, каким-то образом появляются новые элементы онтологии, связанные вот с этим доменом? 

S04 [02:19:35]  : Ну, здесь новые элементы онтологии появляются после того, как я построил вот эту вот программу, Я могу сказать, что вот каждый фрагмент вот этой вот программы, которая сейчас на экране, соответствует некоторому терму. Я могу их объединить. Я могу и сказать, вот первое делай раз, делай два, делай три, делай четыре. И вот это вот делай раз, делай два, делай три, делай четыре для меня становится символами. И когда я делаю это упражнение, я себя настраиваю сначала раз-два-три-четыре. И вот я делаю это движение по мысленным командам раз-два-три-четыре. Я не успею просто с такой скоростью проговорить все, что здесь написано. Я делаю мысленные команды раз-два-три-четыре. А спортсмен следующего уровня, не моего, У него вся эта последовательность, поскольку вся эта программа уже закачана в мозжечок, у него вся эта связка – это одно движение. И он строит когнитивную программу уже из таких трюков. вот он прыгает, его делает сначала одно, другое, третье, потом четвертое, и он уже строит более высокую программу, используя термы более высокого уровня. У него каждый вот такой вот трюк становится отдельным элементом онтологии, из которых он собирает уже, так сказать, там показательные выступления, программу показательных выступлений. 

S05 [02:21:05]  : Окей, давайте мы этот слайд просто в обсуждение возьмем следующий и продолжим потом, ну там может быть не прямо с него, но короче на нем еще Владимир все вопросы дозадаст, а вы еще сможете... Поскольку все равно отвечают не на мои вопросы, то я, честно говоря... Владимир, ну все, давайте на сегодня уже, Сори, ну просто времени уже вагон. Пол девятого по Москве, а у Антона вообще уже пол первого ночи, да. Так что давайте скажем Антону Геннадьевичу спасибо за доклад, по крайней мере вопросы были, они явно показывают, что аудитории интересно, хоть у нас маленькая аудитория, узык их рук, как говорится, но тем не менее. Предлагаю на этом на сегодня закончить. А 19 числа мы сделаем обсуждение как раз архитектуры, в том числе антологии и прочего, но мы еще тематику сформулируем чуть точнее с Антоном Андреевичем. 

S04 [02:22:01]  : всем спасибо спасибо спасибо 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
