## 24 декабря 2020 - Вадим Филиппов - Биоморфные нейросетевые архитектуры для систем ИИ — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/KZI4BXM_m2o/hqdefault.jpg)](https://youtu.be/KZI4BXM_m2o)

Суммаризация семинара:

Семинар был посвящен обсуждению биоморфных нейросетевых архитектур, что подразумевает создание искусственных сетей, имитирующих структуру и функционирование биологических нейронных систем. Вадим Филиппов, генеральный директор Объединения когнитивных и ассоциативных систем, провел доклад, в котором рассмотрел текущее состояние и перспективы развития таких архитектур.

Ключевые концепции:

- Биоморфные нейросетевые архитектуры: имитация биологических нейронных систем для создания искусственных сетей.
- Машинная субъективность: подходы к моделированию взаимодействия базальных ганглиев с корой мозга.
- Ассоциативные рекомбинации: процесс изменения ассоциативных оснований, включая позиционные и частичные порядочности.
- Семантические аттракторы: сбор семантической информации на единой вершине для различных версий одного слова.
- Воспроизведение рекомбинаций: более 30 способов ассоциативной рекомбинации в сетях.
- Логические операции: имитация основных силогизмов в сетевых архитектурах.

Основные идеи:

- Сложность нейронных сетей зависит от типа ассоциативных оснований и их изменений.
- Важность контекста и амонимии в понимании слов и высказываний.
- Потенциал машинной субъективности для моделирования человеческого поведения и ценности.

Важные выводы:

- Нужность более глубокого понимания взаимодействия различных частей мозга для создания реалистичных агентов искусственного интеллекта.
- Важность контекста и амонимии для понимания человеческого языка.
- Потенциал биоморфных сетей для увеличения производительности и уменьшения вычислительных затрат.

Значимые технические аспекты:

- Разработка сетей распознавания устной и слитной речи.
- Создание предобработчика, который понимает язык и исправляет ошибки.
- Использование сетевых ансамблей для распознавания сущностей и синтеза голосовых сообщений.
- Разработка архитектуры сетей, имитирующих человеческую способность к логике и силогизмам.





S01 [00:00:01]  : Всем добрый вечер. Сегодня у нас в гостях Владимир Анатольевич Филиппов, генеральный директор Объединения когнитивных и ассоциативных систем из Томска. И Тюмени Антон. Ой, Тюмени, господи, Тюмени, да, извиняюсь. Изменение, конечно. Но для тех ветеранов нейронет, которые присутствуют здесь, на этом семинаре, наверное, все Вадима Анатольевича знают, тоже как ветераны нейронет, еще с корабля флот Фарсайта 2015 года. Но с тех пор, как я понимаю, произошло много интересного. и нового, о чем нам будет сегодня рассказано. Владимир Анатольевич, пожалуйста, вам слово. 

S02 [00:00:48]  : Коллеги, страшно сказать, но я в теме с 1997 года. Столько даже не живут уже, поэтому даже не с 2015 года. Значит, разрешите тогда, во-первых, Антона поблагодарить, действительно, за столь приятное приглашение вечернее. Значит, биоморфная неростивая архитектура как их надо делать, как их можно делать и стоит ли их делать, зачем их делать. Это действительно тема выступления. Я единственное, вначале, если можно, небольшую первую часть сделаю, расскажу в целом о нашем предприятии, как мы развиваемся, что сейчас делаем, что на рынок поставляем. Я думаю, что вам это тоже будет интересно. И затем после этого небольшого вступления перейду уже, собственно говоря, к нашим биоморфным разработкам, если нет возражений. Коллеги, буквально несколько слов об ОАСИ, Объединение Когнитивных Ассоциативных Систем. У нас то, что называется стартап, классически достаточно. Мы занимаемся только нейросетевыми разработками. Вообще вот буквально пару слов о себе. Я до перехода в АКС работал проректором по информационным технологиям, первым проректором Тюменского государственного университета, руководил лабораторией нейросистем университета, руководил центром обучаемых наноматериалов, имеются в виду мемристоры, и был заместителем ректора по программе 5100 как раз перед уходом. Затем мой бывший коллега ушел работать министром образования, Валерий Фольков, а мы перед этим за некоторое время вызвали грузовое такси, сложили все пожитки малого предприятия КАС, которое было создано еще, когда я работал в университете, и переехали в купленные помещения в бизнес-центр. Вообще как бы формально предприятие создано в конце 2014 года. Тогда же мы стали резидентами фонда Сколково, победив в одном из конкурсов в Сколковске. Действительно принимали участие в разработке еще дорожной карты нейротехнологий, которая появилась еще, была еще даже до подключение оси до форсайт флота, то есть я был заместителем рабочей группы у Людмилы Огородовой, которая была заместителем министра образования. Созданы мы были при участии суперкомпьютерной компании Т-платформы в Сеуде Апанасенко и, соответственно, структуру Роснано. Кстати говоря, вот этот, от Роснано там присутствовал Центр, нанотехнологический центр ТНАНа. Затем они вышли от нас, продав свои доли уже частным инвесторам, причем получили почти 50% годовых от выхода. Один из таких очень эффектных выходов был из-за касса экзитов. принципе как бы парадигма компании это ориентироваться на рынок и работать только на рынок мы даже как бы сознательно стараемся не сильно подаваться на какие-то гранты другие государственные субсидии так как считая хотя вот фонд содействия инновации несколько конкурсов мы выиграли в том числе коммерциализацию развитие ты но это все-таки как бы очень небольшая часть доходов наших мы стараемся ориентироваться только на продажу систем на конкурентном рынке. В прошлом году мы где-то раз в 10 выросли за 2019 год. В этом году тоже заметный рост, хотя сложнее все было в этом году. Все контракты ехали по полгода из-за этого коронавируса, заказчики исчезали на карантине и так далее. Но, тем не менее, в 2020 году у нас порядка 10 проектов реализовано на создание нейросетевых систем. Мы специализируемся только на создании нейросетевых систем, больше ничем другим не занимаемся. Вот недавно как раз там в Сибири, у вас восточный, в Красноярске, мы даже заняли первое место на одном из ведущих конкурсов по профайти. Наш виртуальный консультант по госуслугам даже обошли активный гражданин Москвы в этой номинации. Далее. Буквально как бы вот еще несколько слов о нашем предприятии. То есть мы находимся в этом здании в Тюмени. Мы выкупили один этаж порядка тысячи квадратных метров собственности. То есть здесь осели по крайней мере центр разработки прочный надолго. Мы начинали работать вчетвером, когда уходили из университета, было 4 человека. Сейчас у нас более 60 уже штатных сотрудников, которые работают в офисе. Довольно мощный как бы суперкомпьютер, здесь вот указан 540 терафлопс, но сейчас он уже более пятофлопса у нас. Машина в основном на RTX 8000 и как бы других там целый зоопарк видеокарт в кластере. У нас сегодня полный стек технологий, слава богу, то есть у нас свои средства разработки нейросетей и свои средства разработки датасетов. Причем я сейчас говорю и о нейросетях перспективных, я потом еще скажу об этих программных средствах. У нас сегодня свои технологии обработки семантики текста, распознавания изображений. свои технологии распознавания устной слитной речи, свои технологии синтеза речи и свои технологии построения мультитематических и мультимодальных систем. Но я о них попозже еще скажу, если можно. На рынке мы представлены самыми разными направлениями. Наверное, больше всего меня сейчас радует одна из созданных нами систем, мультимодальная как раз система, которая может одновременно понимать смысл семантики текста, может одновременно понимать смысл изображений, голосовых сообщений, видео и даже читать текст на видео, эта система выявляет в социальных сетях детей и подростков в опасных ситуациях, то есть детей, допустим, которые склонны к суициду, скул-шутеров так называемых, то есть те, кто готовит вооруженные нападения на школы, детей серийных убийств. Это как бы не фантастмы горя, это действительно как бы есть такие дети. И, соответственно, как бы вот больше всего меня сейчас радует, конечно, это вот предотвращенные суициды и предотвращенные всевозможные уголовные делишки. Коллеги, слышимость нормальная, да? Антон? 

S01 [00:07:48]  : Да-да-да, все прекрасно. 

S02 [00:07:51]  : Значит, затем, ну, это как бы система государственного назначения, есть и всевозможные, конечно, вот голосовые ассистенты, например, вот по госуслугам я уже начал говорить, но я чуть позже о них еще расскажу, и еще целый блок, как бы, коммерческих систем, Вот буквально на прошлой неделе у нас такое подняло мне настроение, выиграли конкурс примерно на 50 миллионов рублей на создание голосовых ассистентов для электроэнергетических компаний московского региона. Но в принципе вот виртуальные агенты как бы они на глубоких сетях построены. Система общается письменно и устно. Но мы специализируемся на создании систем, у которых точность ответа в порядке 95-97%. Я считаю, что 80% залога успеха — это разработка датасетов. У нас специально созданы интеллектуальные агенты, которые позволяют создавать датасеты. Соответственно, система работает через все виды каналов, то есть через обычный городской телефон, даже не мобильный, через виджеты, мессенджеры и прочее. Ну, как говорится, здесь, я думаю, все понятно. Ответы на вопросы покупателей, ответы на вопросы сотрудников компании, общие вопросы, специальные, поддержка поставщиков, круг, я думаю, примерно понятен. Прием заказов, отслеживание, трассировка поставок грузов, где там он находится, статус заявки рассмотрения, персональный менеджер для вновь принятых сотрудников, на анализ жалоб, огращений и прочее. Тем не менее, эти задачи довольно нетривиальные. Они, безусловно, относятся к текущему или, я бы даже сказал, к прошлому поколению нейросетевых систем. Но, тем не менее, задачи пришлось за много лет решить немало, чтобы эти системы действительно точно реагировали на входящие обращения. Это не обязательно ответ на вопрос, это любой узел. как бы взаимодействие даже не обязательно там диалог это может быть просто выполнение поручения там скинь 500 тысяч в альфу да то есть и наша банковская система мы начинали с банка в свое время для модуль банка делали систему еще для ряда банков она понимает что это класс поручения на перевод денег Система выделения сущности выдергивает все, что надо, соответственно, сумма там, куда. Затем еще третья стадия обработки соотносит вот этот вольный язык с нормативными. параметрами и после того, как вся карточка заполнена, она выполняет платеж или переспрашивает, если что-то не поняла. Но это тоже достаточно сложно. В русском языке, знаете, раньше говорят, в языке Пушкина 30 тысяч слов, в языке Ленина 50 тысяч. В реальности, конечно, это не так. Мы в русском языке сегодня насчитываем примерно 300 миллионов словоформ с учетом, конечно, склонения, спряжения, с учетом терминов, топонимов, имен и так далее. Но, тем не менее, средняя длина предложения в русском языке это примерно 17 слов, то есть здесь в 140-й степени вариантов мы имеем только позиционных перестановок. Плюс всевозможная любая словозамена, мы их называем симонимы, то есть не синонимы, а симонимы, то есть взаимозаменяемые слова. Можно сказать, допустим, там птичка, можно сказать уже гипонимом воробей, например. Или там вся школа выспала на улицу всевозможные корреляты семантические и так далее. Поэтому конверсивы, всевозможные аннулятные композиции, векторные корреляты. Трупы там самые разные. Плюс мы насчитываем примерно вопросительных конструкций предложения порядка 800 в русском языке. Но ведь и формы вопросов. Ладно, там простой фактоид, но там всевозможные вопросы с логическими операторами. Вопросы множественные и прочее. Ладно, там на фактоид или на ли вопрос ответить. Попробуй ответить на вопрос, исключающий мыль внутри. Теперь, коллеги, чтобы решить эти задачи, я пока говорю не о будущем, я говорю о прошлом сейчас, конечно. Мы используем сегодня сетевые ансамбли примерно из полусотни различных сетей. То есть это, во-первых, как бы отдельные сети распознавания устной и слитной речи. Вот у нас как бы сети распознавания речи были запущены на обучение примерно полтора года назад. С тех пор они постоянно на довольно мощном блоке, как бы лезвии, идет их обучение постоянно, доводка. Соответственно, мы используем... На входе стоит предобработчик, который понимает, на каком языке обратились. У нас сегодня есть система для казахского, русского, английского языков. Потом предобработчик решает вопрос, относится ли это к тематике системы. И, соответственно, исправляет ошибки, опечатки. И, соответственно, если все это прошло, то уже уходит в основной сетевой ансамбль. В основном сетевом ансамбле там тоже целый блок сеток работает, то есть сетки как бы одиночные, мношенные классификация. Это сети выделения сущностей и многое другое. Набор таких сетей достаточно у нас устоялся, готовый и уже практически дата-инженеры, которые занимаются. У нас обычно руководство проектами осуществляют ребята-дата-инженеры. они уже выполняют набор этих сетей для каждой конкретной системы, в естественном взаимодействии с отделами разработки сетевых архитектур. На выходе сети синтеза, допустим, голосовых сообщений, Как бы там большая слойка, то есть там фундаментальная сетка понимания языка у нас проучена, по-моему, на двухстах тысячах часов. Примерно сегодня там пять-восемь часов нам достаточно, чтобы сделать речь конкретного человека. Вот мы сейчас, допустим, детские голоса делали, очень много, интересно. И, соответственно, третья, допустим, слойка, она накладывает эмоции, там расставляет ударение в ряде случаев, и вот такая слойка синтезирует речь. В принципе, довольно сложный технологический процесс. Мы обычно начинаем с разработки семантической модели системы, то есть содержания системы, разделы, вопросы, чтобы не пропускать какие-то семантические области. Соответственно, мы строим четыре векторных пространства, то есть не только World2Vec, но и Morph2Vec, то есть векторное пространство Morpheme. Формируем датасет в специальном программном комплексе Ocus DeepData и, соответственно, проучиваем сетку. Обычно мы ее получаем процентов на 70, первая сетка для заказчиков, и затем она необходимое число итераций проходит, как бы довести уже уровень процентов до 95. На этом этапе уже можно ставить заказчику. Раньше у нас такие разработки занимали иногда до года. Сейчас вот мы допустим 12 сложнейших диалогов для электро с бытовых компаний сделали меньше чем за месяц. То есть все это очень быстро ускорилось. Раньше мы лет шесть бы над этим работали. Ну вот конкретные примеры. Допустим, система по государственным услугам. Сейчас она отвечает более чем на 1200 типов вопросов. То есть как получить паспорт, как купить за счет бюджета инсулиновые помпы, как получить охотничий билет, сколько стоит, например, разрешение на охоту. Это 4 типа вопросов. Всего система 1200 знает. Она поддерживает диалоги, она ведет консультации. отличая тематику, антиматику и так далее. Сегодня в ряде регионов эти системы внедрены. Буквально на прошлой неделе мы в Свердловской области запустили эту систему. Общий вид, допустим, на телефоне. Нажал кнопочку, задаешь вопрос голосом. Система также выводит письменно и отвечает устно. Соответственно, то, что реальный результат по одному из регионов, Вот здесь вот на графике показано, что в начале где-то там всего около полутора тысяч обращений в день идёт к системе МФЦ. Вначале вот где-то человек 200 обращались к нашей системе, сейчас уже заметно больше тысячи. Это немножко старые здесь данные. То есть люди поняли, что система отвечает мгновенно. Вот обратите, какая волна обращений была в период коронавирусной пандемии, но тем не менее система очередь ни разу не возникла здесь вот было падение в целом оценка сегодня людей наших систем порядка 90 процентов положительная оценка то есть система там спрашивает как вам нравится не нравится и где-то процентов 8 это нейтральная то есть если люди не дали оценку или отрицательная оценка но вот где-то в мае там путин ввел несколько новых как бы госуслуг мы их через день добавили и снова оценка восстановилась то есть люди очень как бы четко реагирует на качество работы система вот по тематике то есть если ответы есть в системе мы сегодня отвечаем там с точностью порядка девяносто восьми процентов и по не тематике это тоже очень важно тоже система должна четко понимать что задан не тематический вопрос А то есть, допустим, уважаемая там компания газонефтяная, у нее там бутик стоит какой-то на системе госзакупок, я его спрашиваю, а я хочу поставлять наркотики для вашей компании. этот бот мне выдается радость и вы можете поставлять там наркотики для нашей компании по такому-то алгоритму да или я хочу поставлять там товары для разведки сша и он тоже как бы выдает инструкцию как поставлять товары для разведки сша естественно как бы крокодилы не должны получать паспорт и вот как бы не факт как мы называем мы говорим факт и не факт То есть факт это тематические области, не факт не тематические, есть еще близкий факт, то есть близкие там по разным словосочетаниям параметры. Не факт мы тоже хорошо понимаем и как бы вежливо просим задать вопрос по теме. Коллега, я возможно затягиваю такую вот эту вводную часть. буквально сейчас я закончу нас в прошлом году было интересно мероприятие соревнования вот как раз между руководителем телефонного центра по госуслугам нашей системы общей сложности мы победили тогда со счетом 30 0 активно сейчас развиваются различные консультанты для региональных органов власти то есть это и консультанты инвесторов предпринимателей сельхозпроизводителей там и так далее вот еще одну систему которую мы сейчас вот с горем пополам раскручиваем ну с горем пополам она у нас всегда так немножко на вторых ролях была на это ответы на вопросы по пандемии то есть порядка четырех тысяч сегодня ответов знает система в том числе И в разрезе всех регионов, всех 85 регионов, мы сейчас запускаем по регионам. Последний у нас запуск был в Новгородской области, в Великом Новгороде на прошлой неделе. Общий вид системы тоже консультирующий. Интересный, большой, сложный проект мы закончили в этом году. Он больше года продолжался. Это платформа коммуникации для клиентов Сибур Холдинга. То есть Сибур это крупнейшее мефтехимическое предприятие. И мы сделали систему, которая отвечает на вопросы клиентов. То есть, прежде всего, о товарах. То есть, для чего нужен товар? Какая марка лучше? Чем ваша продукция такой-то марки лучше, чем такая-то марка ваших конкурентов? ну и самые общие вопросы там вопросы работников и так далее вот вот сейчас мы вышли уже на третий договор с сигуром как бы система нравится в целом и продолжаю в следующем году у нас продолжатся эти работы плюс еще несколько очень крупных корпораций добавилась у нас на следующий год крайне интересных работ крайне интересных мы сейчас по сути наша система сейчас мы научились интегрировать все информационные ресурсы заказчика в четботе то есть четбот не только там разговаривает, но он еще и выводит все графики, схемы, видео информацию, картинки и, по сути дела, уже даже никакие сайты, заказчики вот нам говорят, становятся не нужны для общения с клиентами, с поставщиками, да и собственными сотрудниками. Поэтому возможно, что вот такие чит-боты, они развиты и точные, хорошие, они, может быть, будут даже и системами верхнего уровня в следующем году уже. Система работает на всех платформах. Работник или клиент Сибура может работать в WhatsApp, в Вайбере, в Телеграме, по городскому телефону, даже по радиостанции. И он всегда получает ответы от нашей системы. И она даже помнит, кто к нему обращается. Вы можете переходить из ВКонтакте в Телеграм. Иногда это очень важно, допустим, в работе по подросткам, но и для Сибура это важно, и везде получаете эти ответы. Теперь, коллеги, разрешите, я все-таки закончу эту ненужную, по большому счету, часть представления. хотел показать как бы чем мы занимаемся сейчас та точка в которой мы находимся и перейти собственно к докладу если можно значит вот первый вопрос к человек хотел поставить я кстати говоря как бы часть этой презентации использовал благодаря антону и начал делать потом она мне пригодилась выступаю на сбербанковской конференции Недавно она была буквально. Первый вопрос, который я хочу поставить. Что мы сейчас видим? Либо это какое-то временное потепление, связанное с глубокими сетями, как многие считают. Его первым летом называют. А первое лето оно подразумевает, что потом будет уже не лето, а более холодное время. И первое лето понятно, что глубокими сетями обусловлено. Или все-таки уже искусственный интеллект приходит к нам навсегда, и мы будем насмотреть его постоянную, как бы такой прогресс, вплоть до создания там уже когнитивных функций, сравнимых с человеком, о чем многие мечтают в группе Антона. Что позволяет сегодня делать глубокие сети? По сути дела, может быть вы меня поправите, но как я понимаю, они делают всего три основные функции. То есть они могут проводить классификацию, кластеризацию, выделять сущности в потоке данных. Это уже очень много. Это, конечно, то, о чем я рассказывал в первой части. По сути, это позволяет создавать несколько очень важных на рынке и очень монетизируемых. системы. То есть это диалоговая система, это анализ образа, это анализ временных рядов, это сравнение, допустим, текстов между собой. Вот мы в свое время, допустим, пару лет назад для Министерства Казахстана делали систему, которая сравнивает техническую документацию на соответствие нормам в сейсмических зонах, нормам строительства. И как бы вот это компараторы, конечно, очень быстро делают такие работы. Можно делать мультимодальные системы, то есть сетка верхнего уровня сразу принимает решение на основе ответа десятков сетей текста, картинок, звука и так далее. Но в то же время, мне кажется, что у этих глубоких сетей есть непреодолимая на сегодня все-таки проблема и ограничения. Хоть Господь Хинтон и говорит, что глубокое обучение безграничное, но если понимать глубокое обучение просто как многоуровневые многослойные сети, то да, конечно, с ним нельзя не согласиться. Но если говорить о тех сетках, которые сегодня на рынке есть, будь то рекуррентные сети или сети, в которых количества примеров для обучения должно быть крайне мало, вот эти сети, если говорить о состязательных сетях, если говорить о сетях с подкреплением, если говорить о долгой и краткосрочной памяти, все-таки вот у этих сетях есть серьезные ограничения. Вот первое, как ни крути, это все-таки невозможность обучения по ходу работы, невозможность очень быстрого обучения по ходу разговора, то есть в любом случае надо корректировать датасет, даже если обучение продолжающееся, вы не начинаете с нуля переобучение, вы добавляете эпохи, но в любом случае вы должны выключить систему на такой сон. Конечно, у нас тоже есть сон, аллюзии возникают, но тем не менее по ходу разговора вы системы не научите. Затем вы скорректировали датасет и, соответственно, какие-то ошибки, наверное, исправили. хотя итераций может быть много надо, но тем не менее у вас могут появиться новые ошибки, то есть датасет разбалансировался, по-другому перестроился и возникли новые проблемы. И количество итераций, конечно, его даже прогнозировать очень трудно. Мы сейчас уже, конечно, понимаем, сколько нам понадобится, допустим, переобучения сети, но тем не менее новые ошибки передвучения появляются. Очень важная, мне кажется, проблема в том, что эти сети не могут логически мыслить. Я потом еще буду об этом отдельно говорить. Мне кажется, у них есть определенные проблемы. Они, во-первых, очень неустойчивы к любым рекомбинациям. Вы можете переставить куски предложений, Но это может быть простой случай, но, тем не менее, он реальный. И система перестает отвечать на вопрос. Хотя вы просто там переставили обращение. Или вы добавили вводную конструкцию, которая характерна для другого класса. Сеть начала ошибаться. Но еще, мне кажется, очень важная проблема, более важная, то, что они не способны к такой продуктивной ассоциативной рекомбинации данных и к синтезу нового знания. непрозрачные выводы, всем это известно. Мне кажется, что у нас есть живые примеры более разумных систем, более интеллектуальных систем. Поэтому, мне кажется, априори понятно, что стараясь понять, как работают нейросхемы в нервных системах живых организмов, мы можем создавать более продвинутые, серьезные системы. Мы всегда стояли на этой парадигме, мы изначально пришли в эту тематику скорее от нейробиологического моделирования, и это очень накладывает отпечаток на наши работы. Мы сегодня уже применяем успешно то, что мы называем картикоморфными сетями. Теперь, коллеги, Вот первая задача, которая перед нами встала при разработке сетей следующего поколения, это, конечно, программное обеспечение для разработки, потому что как таковых аналогов, в смысле, готовых сетей мы не нашли. Мы исходили из своих моделей нейронов, я на следующих слайдах об этом скажу, и есть там десятки видов программного обеспечения, шкалярского достаточно уровня, Есть и спайковые модели и так далее. Но необходимых нам моделей нейронов не было. Не было роста и развития таких систем. И не было необходимых входов и выходов таких систем. И очень важная задача, что не было программных комплексов, в которых можно создавать сетки на сотни миллионов синапсов, на сотни миллионов сом нейронов. А для биоморфной обработки, которая основывается на нейронах бабушки, на афферентно-инвариантных нейронах, как мы их называем, на нейронных объектах, этих нейронов должно быть много. Умной сети должно быть много, по сути дела. Несколько лет у нас ушло на создание суперкомпьютерного программного обеспечения, распараллельного ОКС-картифика, где мы можем рисовать графы сетей. в которые может заходить текст, может заходить картинки, скамер в том числе, могут заходить другие датчики. Есть виртуальный мир внутри встроенный, где мы можем управлять 3D-моделями, чтобы побыстрее было и подешевле. Но, тем не менее, есть интерфейсы к роботам различным и есть интерфейсы к различным техническим комплексам. То есть данные на входе и управляющие воздействия или реакции системы на выходе. Наши подходы. Я о модели нейроны еще скажу, но тем не менее у нас есть... У нас есть три уровня в этом программном обеспечении. То, что вы видите на этой красивой цветной картинке, это графы. Они базируются на моделях нейронов. Точнее говоря, у нас отсековые модели, то есть отдельная модель дендрита, отдельная модель сомы и отдельная модель аксона. Мы создаем шаблоны, определенные функции вызванных постсинаптических потенциалов, определенные функции синтеза АМПА рецепторов, определенные функции синтеза ГАМК рецепторов. То есть активационных тормозных рецепторов. И можно добавлять другие рецепторы. В зависимости от того, какие рецепторы используются в наших нейронах, они участвуют по-разному в разных контурах. Один и тот же нейрон полифункционален. он может участвовать в разных контурах обработки данных. Соответственно, тут есть скорости падения всех этих потенциалов, синтез новых рецепторов, скорости распадов рецепторов и так далее. Я чуть позже об этом еще скажу. Есть также в SOM определенные функции и определенные функции в Axon. Но это только второй слой нашего программного комплекса, потому что самый глубинный слой – это кибергены. Кибергены – это программные фрагменты, которые обеспечивают изменение, создание, рост, развитие синапсов и тел нейронов. Они обеспечивают развитие нейросети, изменение структуры нейросети, под влиянием обучения. То есть реализована практически концепция и генома, и киберэпигенома. Сигнальные события через изменение потенциалов могут запустить, допустим, какой-то оперон деградации распада связи. Например, при начале обучения мы имеем какой-то полносвязный локус. После того, как нейрон бабушки-пирамидальная клетка захвачена для кодировки какой-то информации, 99,99% связей, подходящих к нему, могут быть разрушены, чтобы не грузить машину. Но в то же время, если запас, допустим, пирамидальных клеток закончился, должен начаться нейрогенез, то есть должны появиться новые пирамиды для кодировки новых локусов. Я потом покажу все это. Тут видите интерфейсы, допустим, здесь вот для задания этих цветовых рецепторов в сетчатках. Тут какие-то интерфейсы для соединения с робототехнической всякой штукой. Но когда мы создали модель нейрона, она сохраняется в библиотеке. Вот у нас появляются киберопероны СОМы, то есть в целом кибероперон, который создает СОМу с такими характеристиками целиком. И как бы накапливаются библиотеки. В принципе, у нас как бы сотни видов вот этих дендритов, аксонов, СОМ используется. И я даже не могу сказать, сколько у нас типов нейронов. Их тысячи. Их тысячи. Вот каждый цвет здесь обозначает разный шаблон, разный кибераперон. Не просто для красоты это все расцвечено, просто вот мы показываем. что мы тут видим вот терминал для текстового общения с сетью и вот виртуальный мир тут с какими-то 3d моделями что можно было там киберфизических систем моделировать это прикольно хотя это как бы не основная функция система как бы но тем не менее ну вот антон вот только видео будет виден и видно видео вы да надеюсь сейчас видно Ну вот здесь один из наших молодых сотрудников. У нас средний возраст в компании 21 год. Мы обычно берем детей с конца 9-го, с 10-го класса на трудовые договоры. Года три они у нас учатся внутри компании и курсу ко второму становятся хорошими специалистами. Вот здесь один из сотрудников, как раз здесь связи цветные отключены, здесь он СОМы только смотрит, чтобы не грузить. Он работает сейчас на мощном суперкомпьютере удаленно и моделирует сетчатку здесь, сетчатку и гипокомпальную кору, гипоморфную сеть. Но я потом об этой задаче еще, если можно, скажу. Здесь, по-моему, в этой сетке у него порядка 30 миллионов сом. Причем в каждом нейроне рассчитывается порядка 30 функций последовательно. И не только последовательно. Теперь, коллеги, я хочу сказать, зачем нам нужны такие усложненные подходы. Я думаю, что начать нам надо с модели нейронов. Мы исходим из того, что реализовали четыре фазы памяти в наших нейронах, и у каждой фазы памяти есть своя когнитивная функция. Нейрон разряжается с определенной частотой. Функцией от частоты является новая фасилитация, которая возникает в аксоне, то есть накопление заряда в аксонный терминаль. Вы прекрасно знаете, что заряд Санкт-Петербурга продвигается, меняясь. на кальций в конце, в любом случае это положительный заряд, и он начинает накапливаться в терминале. Чем больше заряда, тем больше синапсина активируется, тем больше визикул связывается с мембраной и выбрасывается через щель к следующему постсинаптическому нейрону. Мы моделируем накопление заряда, фасилитацию новую, Мы рассчитываем фасилитацию, которая с прошлых тактов осталась работы системы, и мы рассчитываем фасилитацию полную. Соответственно, функция фасилитации является количеством медиатора, которое будет выброшено. Медиатор может быть разных типов. Основное это положительный и тормозной, но можно делать и иные видоспецифические медиаторы в наших системах. Это краткосрочная память. У вас мысль мелькнула, вы ее записать не успели, вам надо снова вернуться к ассоциативному основанию, чтобы активировать ассоциативную мишень. Вот мы считаем, что краткосрочная память – это акциональная память, и она выполняет роль как раз передачи сигнала с клетки на клетку. Теперь, если связь еще не обучена, то на постсинапсе на входе второго нейрона у нас моделируется один всего рецептор. Допустим, один NMDA-рецептор глутамата или один постоянно существующий ГАМК рецептор, тормозной. Соответственно, сигнал будет в любом случае очень слабый, когда связь не обучена. Вы должны много раз повторить сигнал при первичном обучении. Допустим, как вы стихотворение повторяете, когда учите. Либо вы должны использовать второй нейрон. сильный нейрон-учитель, чтобы мгновенно произвести вербовку нового нейрона. Положительный сигнал заходит во второй нейрон и начинает накапливаться постсинапсический потенциал. У него тоже есть свои функции, и каждый дендрит – это отдельный вычислитель. не сома, не зона аксонного холмика, а именно каждый дендрит, отдельный вычислитель, там может и погаснуть сигнал вполне, он может быть заторможен и так далее. Тут у нас большие дискуссии идут по роли дендритов и сом, но тем не менее. Но тем не менее, если сигнал дошел до сомы, если преодолел порог и нейрон разрядился, то моделируется синтез новых рецепторов, новых гамма-рецепторов, новых ампа-рецепторов положительного глутамата, например. У вас был один рецептор, а тут раз и сразу 10 встроилось. И у вас сигнал на входе во второй нейрон усилился в 10 раз. Вот это есть уже постоянная память, и если с краткосрочной акциональной фасилитацией связан переход активности с нейрона на нейрон, если на входной памяти дендритов, на среднесрочной памяти, потому что дендрит на памяти дольше живет, большее количество тактов, на ней построена логика, потому что там сборка, как это суммация сигналов и все такое прочее, то вот эти синтезируемые рецепторы это уже такие постоянные рельсы, по которым потом будут ходить уже ваши обученные воспоминания по сети. Рецепторы тоже могут разрушаться, как мы знаем в реальном мозге. Допустим, те же ампа-рецепторы, они около четырех суток живут, потом разрушаются, но у вас рядом стоит РНК, который тут же видит, что разрушился рецептор и тут же встраивает его заменитель. Поэтому, если вы хоть немножко иногда вспоминаете эту информацию, то вот эти ампа-рецепторы, они уже будут с вами всю жизнь, это будет ваша постоянная память, это вот рельсы для того, чтобы ваши вычисления ходили по вашим нейросетям. И, как я уже сказал, все эти функции реализованы как программки генов, кибергенов и эпигенетических факторов. Но я не буду уже в дебри заходить, если потом вопросы будут, я отвечу. Вот пример. Антон порадовал, что у меня есть около часа для выступления. Да, Антон, вы подтверждаете? 

S01 [00:39:44]  : Ну на самом деле до полутора часов у нас обычно. Есть докладчики, которые за два часа вываливаются. 

S02 [00:39:53]  : Я постараюсь. не быть жестокими, тем более у вас же уже 8 часов. 

S01 [00:39:57]  : Я думаю 1,5 часа, я думаю, оптимально. 

S02 [00:40:00]  : Вот смотрите, коллеги, вот эта картинка, она просто моделирует принцип работы модели нейрона, не больше. Тут нейрончики как бы, ну вот смотрите, предположим, что у вас вот это вверху газопровод и внизу и третий газопровод, это вот перемычки между газопроводом. Эта картикальная колонка регистрирует, что происходит между двумя датчиками. Из нейронов мы тоже датчики собираем. Вот это сетка, которая просто выдает реакцию. Вы не думайте, что это наша какая-то разработка, это просто иллюстрация для наших школьников, чтобы понимать, как все работает. И вот смотрите, у вас газ упало давление, вот меньше стало газа. Человек сначала нажимает кнопку и включает перемычку. И окей, газ пошел, газ восстановился. Но нейросеть запомнила и проассоциировала, что если газ уменьшилось до такого количества, а человек нажал на кнопку, то это надо проассоциировать. Есть вот эта область, которая ассоциирует действие человека и, соответственно, действие нейросети. полное падение газа, то человек показал, что надо два, две перемычки включить. И эта нейросеть, она реконсолидировала память, и, соответственно, у нее появилась вершина, что при падении полного давления надо, соответственно, ну, я не совсем, может быть, синхронно тут рассказываю с тем, что на экране происходит, но вот внизу там видно как бы тоже что нейросеть говорит, что делает. Вот эта картикальная колонка сигнальная, то есть здесь вот каждый отдельный нейрончик, пирамидальная клетка, она для любого состояния датчиков, то есть полный газ, полное давление, падающее давление, упавшее давление, вообще нет давления, для любой ситуации захватывается отдельная пирамидальная клетка. И эта пирамидальная клетка уже ассоциируется с нейроном, который ассоциирует действие человека и состояние датчика. Соответственно, система начинает распознавать, что произошло в трубе, и как только человек ей показал это действие, она уже начинает его применять во всех аналогичных ситуациях, потому что она осваивает концепт такого поведения. Но это такая экспериментальная работа с ухтой трансгазом, который мы сейчас начинаем. Газпром мечтает о системе управления следующего поколения самообучающейся. У них вот есть, допустим, грязовецкий газотранспортный узел, где сходятся 8 газопроводов. в том числе Ухтинские 120 атмосферы. И, соответственно, идет раздача на Москву, Череповецкий комбинат, на Германию, на Украину, на Белоруссию. Там несколько аварий происходит, потому что, хотя газопроводы — это довольно медленная система, но когда на 10 километрах сконцентрировано огромное количество этих узлов, то здесь человек уже не всегда успевает справляться. Поэтому сейчас мы начинаем размышлять, думать о самообучающихся технических системах, о такой нервной системе для трубы. Я, коллеги, хочу рассказать о тех задачах, которые, мне кажется, надо решить, чтобы получить новые когнитивные функции. По сути дела, для нас стоит вопрос так. Есть еще не существующая когнитивная функция, ее надо реализовать на какой-то сетевой архитектуре. Первое – это модели нейрона, которые позволяют обучаться по ходу. Я забыл сказать, что самое главное, что эти модели нейрона позволяют учиться по ходу разговора. Смотрите, как это происходит. Мы обеспечили то, что семантические аттракторы, нейроны каких-то объектов, процессов, состояний, действий, они формируются очень быстро, мгновенно. без отключения и без выключения сети на переобучение. Каким образом это работает? Допустим, у вас по коре начинает там сверху вниз, ну не суть важно, спускаться какой-то сигнал. Допустим, я не знаю, когда система еще не знает, это может быть какой-то новый слог, слог ma, например, да, вот не знает еще система слог ma. Красное – это у нас слой пирамид, слой пирамидальных клеток, так называемых. Это условное название, можете его с юмором воспринимать, не суть важно. Но это афферентные инвариантные нейроны, которые должны начать кодировать объекты. И вот сигнал идет. И никакая пирамида не активируется, потому что рецепторов везде, молекулярных рецепторов на входе всех постсинаптических мембран или синаптических мембран этих офферентных вариантов нейронов, их очень мало. И сигнал очень слабый. То есть порог там 10, а входящий сигнал 1. Соответственно, есть еще скорость снижения, вам там тактов 15, надо качать этот медиатор, чтобы этот нейрон активировался. Но есть и другой путь. Сигнал уходит на второй круг и боковым синапсом активирует нейрон новизны. Нейрон новизны – это один из этих нейрончиков, возможно, это голубенький нейрончик в каждой колоночке. Что такое нейрон навизны? Нейрон навизны – это нейрон, на который все пирамиды имеют тормозные связи. То есть, если информация известна, то нейрон навизны будет просто заторможен известной информацией. Но если он не тормозится известной информацией, то он боковым синапсом он дает активационный, то есть он получает тормозной синапс от пирамид, и он дает на пирамиды активационный синапс. Соответственно, этот активационный синапс довольно сильный, и когда сигнал сделал круг по кратикальной колонке, вот если у Маркрама видели в свое время, как у него там сигнальчик крутится, и у нас в искусственных сетях он также крутится, он на второй круг подошел, сигнал встал на позицию мишени, на эту пирамиду, и в это время же все синхронизировано подошел сигнал от нейронной новизны очень сильный. И он преодолевает сопротивление, но по принципу Хэбба, я это не буду рассказывать, всем понятно, по принципу Хэбба он дает дополнительный потенциал, и тут же происходит синтез новых рецепторов уже между буквами М и А, Но это не обязательно может быть слоговый нейрон. Это может быть нейрон, который целую картинку кодирует. Я их покажу чуть попозже. Абсолютно одинаковые принципы. Хоть картинку, хоть запах, хоть предложение, хоть слог, хоть две линии ориентационно-избирательные. Принцип абсолютно одинаковый. Произошел захват пирамиды. То есть захват пирамиды – это синтез рецепторов, которые в синапсе соединяют мишень и ассоциативное основание. И произошел захват эфферентно-инвариантного нейрона. У вас появился нейрон МА. там вот в слове мама или там я люблю спрашивать слове бюро например сколько слогов говорят бюро 2 на самом деле юр еще есть да то есть для всех структурных элементов слова для всех структурных элементов картинки создаются естественно свои нейроны бью юр и ро 3 слоговых нейронов слоги бюро и так далее вот ну вот пример допустим уже В нашем виртуальном мире идет некий робот, он формирует нейроны места. Это модель гиппокамп в данном случае. Вы знаете, что мы придерживаемся парадигмы, что когда мы имеем дело с гиппокампом, В гиппокампе есть масса функций, но если говорить о зрительной обработке, там два основных пути. Не в гиппокампе, а в коре в целом. Есть путь квантификации, когда точки превращаются в линии, линии в примитивы, примитивы в более сложные объекты, там идет квантификация лиц, деревьев, чего угодно. Когда у вас формируется нейрон для всей картинки в целом. Вы посмотрели налево, у вас формировался один нейрон как бы картинки в целом. Направо – это нейрон другой картинки. Но все вместе они у нас имеют проекцию на пласы cells, то есть на нейроны места. Это нейрон, который в модели гиппокампа однозначно кодирует место, где вы находитесь. На этом видео показано формирование нейронных мест у этого виртуального робота. который идет по нашему миру и вот здесь вот он он был в комнатке он был там на улице якобы да виртуально и вот у него сформировались эти красненькие это у него нейроны место которые сформировались по ходу обучения то есть их не было И вот идет постоянное формирование, постоянный захват этих нейронов места. Внизу это модель сетчатки, там огромное количество вложенных уровней, это просто сигнальные клетки здесь стоят. Но тем не менее он видит котенка, вот у него очаг внимания, тут центральная просто колоночка выведенная. То, что он видит, и соответственно вот эти нейроны формируются, но это как совокупность всех картинок. Вот то, что видится, это камера его виртуальная, здесь мы видим все стадии абсолютно обработки. Это 3D-мир, это камера, это сетчатка и это нейронное место, которое формируется. Это как бы вот эти гиппоморфные колонки в разрезе. Они все как бы мигают, просто сигнал очень быстро обрабатывается. Вот здесь несколько тысяч идет операция в секунду и, соответственно, Теперь, коллеги, вот следующая задача, которая, мне кажется, крайне важная, это ассоциативная рекомбинация. Мы как бы исходим из того, что вот когда мозг что-то запомнил… Ну как, мы пришли к этому, конечно, исключительно из опыта. То есть слова можно переставить местами, можно включить какие-то другие слова. Можно вообще все другими словами сказать. Можно сказать наоборот очень коротко вместо длинного высказывания. Мы считаем, что есть структурная рекомбинация, когда меняются элементы местами, добавляются или убираются элементы. И есть семантическая рекомбинация, когда используются другие сигналы для выражения того же самого. И вот нам кажется упорно, что мозг, когда запоминает эту информацию, которую мы запоминаем, он не только запоминает эту информацию, Но еще и рекомбинируют ее массой способов. Мы же не знаем, в следующий раз первобытный человек не знал, тигр из какого угла выскочит на него. Или мы не знаем, как камешек нам под ногу в следующий раз попадет. И мозг рекомбинирует основными способами, переставляя их местами, заменяя другими. И вот мы сегодня в наших сетях воспроизвели более 30 способов, более 30 архитектур для разных типов ассоциативной рекомбинации. Здесь, конечно, тоже показаны принципиальные решения с малым числом нейронов, но тем не менее. Соответственно, это и есть синтез знания. Мозг приобретает возможность фрагментов ранее изученного, создавать, придумывать что-то новенькое. Очень важная функция – логика. Я здесь ее показываю в общем виде, но, тем не менее, нам кажется, что определенными сетевыми архитектурами можно реализовать определенную логическую операцию. То есть, известно, что, как бы, ну, логиками давно известно, что у нас есть порядка 64 основных фигур вселогизма, в 19 продуктивных, остальные дают вероятностные выводы. Они зависят от квантеров, единичности, всеобщности и некоторости, они зависят от отрицания. И вот сегодня у нас получены очень интересные и хорошие архитектуры, блокируя распространение сигнала в определенных направлениях, разрешая в других, они моделируют основные 19 продуктивных силогизмов. Здесь показана принципиальная потактная схема сети. Естественно, самая принципиальная схема, что если все люди смертны, сократ человек, то Сократ тоже смертен. Первая фигура силогизма – фигура Барбара из первого модуса. Именно архитектуры обеспечивают различные виды силогизмов, различные фигуры силогизмов. Очень интересная задача – это создание по-настоящему мультимодальных систем. Потому что я говорил о том, что у нас есть мультимодальные системы различного назначения для школьников, которые и графику понимают, и тессерактом понимают, что написано на картинках, и текст понимает смысл. А под пониманием я имею в виду очень интересный момент, я еще буквально прокомментирую. только что беседовали об этом с одним из наших разработчиков, вот мы допустим, если говорить о классических сетях, мы считаем, что допустим вот GPT-3 там все увлекаются сейчас, это полная чепуха на наш взгляд. Почему это полная чепуха? Потому что мы поставили очень много экспериментов с русскими GPT-3, обычно 4 минуты достаточно, чтобы человек понял, что он разговаривает с ботом. 4 минуты. Почему? Потому что сетки учатся на большом количестве диалогов. Они статистически как бы огромные сети. Но эти сети не рефлексируют как бы узел в диалоге или во взаимодействии с человеком. Они просто статистически отвечают по ранее прослушанным диалогам. И так как мы не имеем рефлексии, то сетка несколько раз попадает, потом не попадает. Сколько вы ее не учите? И даже в глубоких нейросетях мы предпочитаем всегда использовать классификаторы, которые четко устанавливают класс входящего высказания. Лучше промолчать. Вот та сетка, которую мы используем, мультимодальная, она сегодня тоже статистическая. То есть да, она принимает сигнал от картинки, да, она принимает сигнал от тессеракта, от сетей анализа семантики текста, но она сама по себе все-таки не рефлексирует. Она тоже статистически принимает решение. Как нам кажется, в голове мы имеем немножко другие системы. То есть мы исходим из того, Во-первых, в наших архитектурах прорисовывается зона коры, искусственной коры, по которой сигнал поднимается и производит захват пирамидальных клеток. И другая архитектура, которая спускает сигнал, допустим, синтезируя текст побуквенно. Интересно, что и система синтеза, И подъема сигнала, и система спуска сигнала, они учатся всегда одновременно. Это очень сложный синхронизированный процесс обучения одновременно ассоциативной и моторной колонки. Но в конечном счете структура коры оказывается все-таки осознаваемой. По горизонтали от поля к полю идет повышение, ну или в моторной коре понижение уровня иерархии, то есть три объекта создают два двухобъектных нейрона, два двухобъектных нейрона создают один трехобъектный нейрон и так далее. И потом наоборот в моторной колонке. А вот по вертикали этой коры находятся слои различных типов ассоциативной рекомбинации. полных и неполных, разорванных и целостных, чистых и включенных, семантических рекомбинаций. Их десятки, видимо, этих типов. И вот здесь очень благодатное поле, потому что здесь можно создавать новые формы мышления. в перспективе будет новая форма ассоциативной рекомбинации. Ну и соответственно, то есть у нас есть кора, в слоях у нас разные типы ассоциативной рекомбинации, по горизонтали у нас разные уровни иерархии коры и разные уровни иерархии доходят до того, что вот это может быть зона, допустим, текста, это может быть зона картинок, а вот это бимодальная зона текст-картинки. Они напрямую не связаны, но мы можем что-то назвать, И через активацию бимодальной зоны мы можем нарисовать то, что сказали. Мы можем что-то нарисовать и получить его название. Получается такая уже картикоморфная сеть. Такой подход и те архитектуры, которые за ним стоят, мы называем картикоморфными сетями. Вот пример, допустим, мультимодального обучения. Вот опять же робот в мире, но он что делает? Кстати, обратите внимание, тут я еще иллюстрирую кибергеномику, как вырастет этот кусок по ходу работы. Я ему говорю «команды» и даю сначала моторное действие на его виртуальный двигатель типа «иди». Сказал «иди», он пошел. Обратите внимание, здесь у нас нейроны кончились, потому что их мало было изначально. Кибергеномика прорастила и довольно сложно связала новый сетевой локус. Вот то, что он видит на RGB-камерах, то есть красно-зелено-синяя камера у него в данном случае, и он ассоциирует три модальности здесь. Он ассоциирует то, что он видит, он ассоциирует то, что я ему говорю в диалоге, и он ассоциирует свои моторные движения. И после того, как, ну, первое, я ему как бы даю команды запуск моторов, но после того, как он проассоциировал мои слова или зрительную картинку с моторами, он уже сам начинает делать эти действия, то есть останавливаться перед деревом или там идти, когда я ему сказал, иди и так далее. То есть вот это как раз иллюстрация мультимодальной коры. Ну, о нашей картифике я уже говорил, но она позволяет, как бы вот эти графовые наши сети, они позволяют решить фундаментальную проблему прозрачности черного ящика. Мы видим, как идет сигнал по нашим этим самым нейрончикам. Они загораются красным, то мы увидели. Я могу это промотать на медленной перемотке. Это очень мучительно, конечно, понять, что не так в сети из-за миллионов нейронов. И вообще, разработки таких сетей — это кошмар. Но тем не менее, я всегда могу понять, что происходит в сети, почему она приняла тот или иной выбор. И наши графовые сети, распространяющиеся по ним активностью, не являются черными ящиками. Они прозрачны и понятны для анализа. Отдельно очень интересное направление. Это пространственно-временная ориентация. Я уже говорил о том, что у нас используются классы SELF для того, чтобы понять, в каком месте находится система. У нас также реализованы клетки времени. Мы исходим из того, что в дентатозирус каждую минуту пролиферируется новая стволовая клетка, она превращается в нейрон и примерно минуту пишет все, что с ней проходило. Зубчатая извилина напрямую связана перекрестом с основным телом гиппокампы, то есть с клетками места, и по всей видимости гиппокамп хранит весь пространственный континуум при первичном обучении личности. И вот нечто подобное мы воспроизвели в наших гиппоморфных сетях. В данном случае на примере машинки, как бы это студенты делали. Ну как студенты, они у нас уже лет пять работали к этому времени. То есть вот едет машина, вначале управляет человек. Что здесь мы видим? Это модель сетчатки, это модель коры зрительных нейронов. Вот это нейроны, которые управляют рычагами управления, то есть газ, руль, тормоз, скорость, передача, все, что есть у машины. А вот это уже ассоциативная зона, которая ассоциирует картинку и положение рычагов управления. Вот сейчас пока едет человек. В принципе, нам достаточно показать этой машине поворот направо и налево. без относительно радиуса скругления, потому что сетка, я сейчас не имею возможность по времени обо всем рассказывать, она воспринимает не только непосредственные ориентационно-избирательные линии в определенном секторе зрения, но она воспринимает концепты, концепты линий, концепты графических объектов. Она усваивает понятие поворота. Вот пока еще едет человек. Сейчас мы еще поворотик в другую сторону направо и показали, сейчас еще налево покажем и как поворачиваем. Эту систему можно постоянно доучивать, улучшать ее параметры, потому что за счет реконсолидации следа памяти. Значит, коллеги, не надо думать, что вот эта вся сеть, которая работает внутри, там большое количество вложенных уровней. Ну вот мы показали машинке еще один поворот налево. 3618 тактов в секунду в этой системе. 3618 тактов в секунду. ФПС число показывается у меня. Мы показали эти повороты. Сейчас мы вернем машинку обратно на стенд испытаний, откуда начинали дорогу. Мы вернули в исходную точку. Я включаю тормозной нейрон. Все воздействия от человека запрещены. И нейросеть, видя картинки, начинает включать то же сочетание рычагов, которые использовал человек. Вот это уже пошел сигнал от нейросети. Вот обратите внимание, машина сама уже без всякого участия человека подбирается к повороту. Но она и сама, и сейчас едет, управляя, потому что сейчас она едет по прямой дороге. Вот она добралась до поворота, немножко уходит влево чуть-чуть, тут до неё доходит, то есть срабатывает нейрон место и переключает рычаги. Машина начинает поворачивать сама. Видите? Она подруливает эта сволочь. немножко далеко от края едет но я могу в любой момент подключиться к обучению подвернуть ее ближе к краю она будет ехать ближе к правой полосе правому краю полосы или к левой великобритании без разницы и вот я сейчас хочу показать как бы немножко там несколько минут этот файл длится но как она поедет все дальше повороты она уже будет проходить все повороты на этой трассе, которую она увидит впервые. Вот она притормаживает, разгоняется. Вот она подходит ко второму повороту. Этот поворот она тоже видела. Здесь мы уже были. Машина тихонько подвернула. А вот впереди у нас там поворот на трассе, который она еще не проходила никогда ни разу. Мы до него не доехали. Программное средство очень удобное, то есть очень можно быстро ввязать сети, оно запоминает целый локус и это важно при проектировании больших систем. Вот смотрите, мы подъехали к этому повороту и машина начинает подворачивать сама. Поворот направо, она восприняла эту картинку как поворот направо, чисто по картинке едет. по выделенному концепту направления автодороги. И она открыта до обучения, до улучшения качества езды. То есть можно научить ее ездить быстрее, можно ее научить ездить ближе к краю. Это чисто вид сверху. Виртуальная какая-то камера у ребят летает и показывает, как машина движется сверху в данном случае. Никакого человеческого воздействия. Мы машине показали поворот налево-направо, и дальше она едет сама. На мой взгляд, излишне близко к центру дороги. Меня это раздражает, но это просто вопрос до обучения. Она видит следующий поворот и начинает подворачивать. То есть вот она правый руль подключает. То есть нейрон-место. активирует бимодальный нейрон, место управления, и бимодальный нейрон возвращает моторный сигнал на рычаг управления. Вот машина поехала дальше, но я думаю, что суть примерно понятна. Теперь, коллеги, вот следующая когнитивная функция, на мой взгляд, очень важная, это фильтрация важного. Я думаю, что если я вас спрошу, чем вы занимались 24 декабря 2019 года, вы, скорее всего, если это не ваш день рождения, вспомните не очень хорошо, а, скорее всего, плохо. Даже вчерашнюю неделю мы помним примерно в 10 раз хуже, чем сегодняшнюю. И это не так уж плохо, потому что в результате мозг запоминает то, что для него важно, и забывает то, что не очень важно. На наш взгляд за этим стоит нейрохимия гиппокампа. Как у нас сложилось представление по большому количеству зарубежных статей, когда мы засыпаем, гиппокамп накопил усиленные синапсы за день. Но ночью в гиппокамп пришла химия, которая ослабила все эти синапсы, а через несколько минут пришла химия, которая усилила эти синапсы, а потом снова ослабила, возможно, несколько раз за ночь. В результате к утру в гиппокампе слабые связи ослабли или вообще исчезли, а сильные связи, наоборот, усилились. И, по всей видимости, перед просыпанием у нас усиленные связи уходят в кору для постоянного хранения. А гиппокамп, он отчасти очищается от всей этой дневной информации, он готов к восприятию нового, но, с другой стороны, он сохраняет функцию указателей на вот эти критикальные области и становится таким каталогом памяти своеобразного. Поэтому разрушение гиппокампа, к сожалению, приводит к невозможности долгосрочного запоминания. Это очень важная когнитивная функция. Например, с ее помощью ребенок осваивает язык, по всей видимости. То есть мы исходим из того, что у нас, я уже сказал, порядка 300 миллионов словоформ. но Morpheme гораздо меньше, в ней суффиксов окончания, интерфиксов, префиксов и так далее. Их около 10 тысяч, грубо говоря. И вот мы, допустим, говорим жбенку «сильнейший», «самый сильный», «быстрейший», «самый быстрый», «красивейший», «самый красивый», «зеленейший», «самый зеленый». И мы примерно одинаковое количество раз употребляем понятие «самый», допустим в данном случае, и суффикс А суффикс есть – это степень превосходства. Каждая морфема у нас имеет свою семантику. Тель, например, суффикс инструментальности, ёнок, суффикс детства – жеребёнок, воронёнок, говнюшёнок, всё что угодно. Ну, с чередованием, правда. И вот набрав примерно одинаковые синапсы в гиппокампанных сетях в ходе дневного обучения, ЕИШ и САМЫ, они мало того, что усиливаются, то есть усиление – это увеличение количества рецепторов на пресинаптических мембранах, но они и набирают примерно одинаковые потенциалы, и они в коре оказываются ассоциативно связанными. У суффикса «есть» появляется понятие «самый» автоматически. У суффикса «енок» появляется значение «ребенок», «детенок», «ребенок», «олененок», «гельфиненок», «вороненок» и так далее. И у ребенка автоматически фильтруется то, что я начинал со второй задачи, формируются семантические аттракторы. в данном случае. И он очень быстро осваивает язык, освоив семантику морфем. И вот мы это все воспроизводим в наших сетях. Мы это говорим как фильтрация важного. Каким-то аналогом математики является фильтр Кальмана, по всей видимости. В ходе чисто экспериментальных работ мы уже видим, что могут быть ассоциативные рекомбинации, которые мы не видим у человека, которые не относятся к известным со времен поррояля логическим фигурам. Похоже, могут быть разработаны новые формы логического ассоциативного мышления, которых нет у человека, и они могут быть вполне продуктивными. О кибергеномике я уже говорил, вот здесь фрагмент с того вида, который я показывал. Работа кибергенов очень непроста на самом деле. Даже связывание самой основы этих графов очень неочевидно, потому что надо не трогать ранее выученное. В общем, довольно сложные принципы. Некоторые кибергены занимают до трех страниц кода оригинального. Один киберген. А этих кибергенов тысячи. Но в то же время, мне кажется, что кибергеномика такая, в кавычках мы ее используем, это может быть вообще новым принципом развития неживых существ. Конечно, это будущее, это фантастика, но это фантастика в чертежах. Более того, у нас проведены первые опыты по скрещиванию нейрогенетических систем. И получены очень интересные результаты. Одна система, например, акцентуирована на финале, и она как бы такой стихоплет своеобразный, в рифму говорит. Другая система дает очень точно правильные ответы. Пользователь, я, например, поощряю работу этих систем, говорю, что это хорошо, это хорошо, ты молодец. Я шучу, конечно, кнопку нажимаю, просто стимуляция. Но киберген, который отвечает за эту функцию, он получает дополнительные очки, дополнительные баллы. Он становится доминирующим, он становится доминантным геном, и он передается при скрещивании двух копий систем сети потомку. И сетка может приобретать функцию, когда она становится одновременно и хорошим стихоплетом, и очень точно отвечает на вопросы. Или наоборот, одновременно глупеет. передается рецессивный тиберген. Это, конечно, перспективы разработок. Еще одна прелесть этих графовых систем, что они могут быть напрямую транслироваться на кристалл. Мы в 2012 году на этой установке в Тюменском университете, я как раз руководил центром мемристорных обучаемых наноматериалов, То есть мы считаем, как бы известно, вот говорят, три класса наноматериалов, надо добавить обучаемые наноматериалы. Но вы прекрасно знаете, мемристоры, я просто напомню, что это тонкая пленка диоксида металла, например, диоксид титана можно взять, но диоксиды, они имеют огромное количество как бы дефектов в атомной структуре, огромное количество вакансий, атомных вакансий в кристаллической решетке. И вот здесь в центре нанофаба расположен магнетрон, мы с помощью магнетрона, допустим, помещаем ионы кислорода, пристеночно, да, они занимают позиции в диоксиде. И диоксид, он не приводит ток, да, как известно, он диэлектрик мегаомный. Но если вы подали все-таки ток определенной силы, атомы, ионы кислорода, они занимают вакансии и бац! этот диоксид превратился в проводник. Причем аналоговый. Он может сильнее проводить, меньше проводить. Вы можете подать обратный заряд, он забудет эту информацию. Но это очень похоже на потенциацию синоптических связей. То есть один мемлистер, один синоптический элемент какой-то. К сожалению, дальнейшие работы в России над серьезными архитектурами не представились возможными. Мы выбрали путь монетизации наших разработок и ушли пока в программирование. Но, в принципе, графовые сети позволяют создавать новые нейропроцессоры. Все, о чем я рассказываю, может быть будет переложено где-нибудь в какой-нибудь стране на аппаратуру. Мы, кстати, выиграли все время резидентами Сколково, мы стали за архитектуру этого нейропроцессора. Еще одна очень интересная функция, которую я хочу сказать, это машинная субъективность, по Маяковскому в данном случае, что такое хорошо и что такое плохо. Представьте, что вы человека считаете умным, моральным, хорошим, вы к нему хорошо относитесь. Он приводит какого-то парня и говорит, это хороший парень. Вы, скорее всего, подумаете, что правда, наверное, неплохой парень. По крайней мере, какой-то карт-бланш ему дадите. Если человек, которого вы считаете там негодяем, тупицей полным и как бы сволочью законченный, приведет и скажет, что это хороший парень, вы, скорее всего, подумаете, что, наверное, такое же дерьмо, как ты. И речь идет о новых объектах, которых вы только узнаете. И вот здесь, как нам кажется, действует очень простое правило, когда плюс на плюс дает плюс, минус на минус тоже дает плюс, а минус на плюс дает минус. И, по всей видимости, субъективное отношение нейросети к новому Можно рассчитывать в ней параллельно с формированием нейронов бабушки и расчетом основной семантики. Можно на основе раннего детского опыта машины. Мы ее научили большому количеству сочетаний хорошо и плохо. Но затем, понимая семантику отношений, она будет вырабатывать свое субъективное отношение к новому. Мне кажется, это довольно интересно, чтобы такие машины не были послушным орудием, может быть, в вздлонамеренных руках иногда. Ну и, конечно, это крайне интересно с научной точки зрения, построение таких систем параллельного расчета. Пока у нас эти работы на практике не проводились. нас сейчас очень сильно губит рынок, то есть науку губит рынок, мы увлечены как бы деньгами, ну не увлечены, а мы как бы поставлены в условиях, когда вынуждены зарабатывать, но тем не менее надеемся, что может быть в следующем году у нас эти работы по машинной субъективности тоже начнутся, возможно. Коллеги, я наверное уже вас очень сильно утомил, поэтому Тем более, час уже закончился, и полтора скоро закончится. Я должен укладываться. Буквально еще несколько моментов, которые мы тоже постоянно думаем, и они, мне кажется, очень важны. Это все-таки для чего создавать подобные системы? Какие области являются наиболее продуктивными для применения системы искусственного интеллекта? О чем мы думаем? Первое, это такие вот общие, я бы сказал, глобальные цели для системы искусственного интеллекта. Мне кажется, что наиболее интересными областями является медицинская диагностика, безусловно, это медицинские консультанты, это машины третьего мнения в медицине. Вот этот коронавирусный год это очень хорошо показал, когда больше людей погибло те, кто был выписан из онкологических, из кардиологических стационаров. И часто они не могли получить консультацию доктора, когда у них случались крайне большие неприятности, они погибали дома. Это я знаю просто от большого количества докторов, химиотерапевтов, онкологов. Поэтому, конечно, такие медицинские консультанты очень важны. Медицинская диагностика, конечно, вообще должен быть один суперпроект. Когда ты лег в МРТ, тебе отпяток для волос просветили, и ты получил все свои болячки и диагнозы. потому что мрт аппаратах много по стране но аппарат купила как читать снимки не купил даже у супер доктора после 14 часов как бы к вечеру падает продуктивность процентов на 20 распознавание Допустим, опухоли головы. Мы здесь работаем с федеральным центром нейрохирургии в Тюмени очень активно. Опухоли головы, они очень плохо видны на снимках. Они очень агрессивные, но плохо видны, к сожалению. И, конечно, надо как бы единый проект такого анализа всего тела делать со временем. Генерация новых знаний. Безусловно, анализ когнитома. Посмотрим, может быть, у Анохина какие-то подвижки будут, может быть, здесь удастся посотрудничать. Анализы синтез новых молекулярных соединений, IT-эйджинг тот же, синтез лекарств. Кибергеномика – это подход к созданию систем, которые будут разрабатывать сами себя в какой-то перспективе. Машины должны быть хорошо воспитанными, безусловно, В детстве должны быть хорошие примеры приведены. Кстати, детский опыт еще надо как бы подумать, как его закреплять. Может быть, разная жесткость импринтинга. То есть, либо жесткая, как у насекомых, прошивка, либо там, условно говоря, яркие впечатления детства, как у птичек, например, которым там уточка, если родится, она увидит красный шарик, будет считать, что это его мама, и у нее даже половое влечение будет к красному шарику. Или, как у человека, более мягкие. Безусловно, крайне интересно применение в бизнесе продажи систем. То есть, прежде всего, конечно, это корпоративные интеллектуальные агенты разного назначения, генерация индивидуального контента, предсказания временных рядов, прикладные отраслевые решения, когнитивный геолог какой-нибудь, еще что-то. Безусловно, модули голосового управления оборудованием. и так далее вот в общественной сфере мне кажется тоже есть целый ряд интересных направлений но предиктивное управление то есть вот мы сейчас там анализируем социальные сети точки зрения выявления категории риска и некоторых других категорий можно же вообще смотреть что люди говорят можно формировать повестку для органов власти как бы еще до того как она переходит в какие-то допустим там недовольство или протесты и Можно очень хорошо понимать по каждой территории, что люди хотят из их разговоров. Очень интересный проект, который мы хотим реализовать в следующем году, это национальный информер. Мы хотим собрать многие наши системы в одну. На самом деле уже собрали, дорешиваются вопросы. Интеллектуальные и виртуальные агенты нужны всем гоорганам, по всем направлениям. У региона должна быть одна кнопка общения со всеми властями региона. Персональные помощники, аргументы, которые помогают обсуждать какие-то темы, анализ моделей социальной реальности, пассивная безопасность. Но это не только защита в социальных сетях. Некоторые оборонные системы, это системы разведки, например, и так далее. В образовании, видимо, это может применяться. Мы стараемся выбирать те области, где искусственный интеллект несомненно работает на благо жизни и здоровья людей. Мы категорически не сотрудничаем с военными, например. считаем, важно придерживаться этих позиций. Хотя очень интересный вопрос в военной сфере, где надо, допустим, средства противовоздушной обороны. Я считаю, что это максимум, в чем можно участвовать в сотрудничестве с военными. Нам важны эти ответы. Может быть, вам покажется, что эти три слайда я включил не при делах суда, Мне кажется, это очень важные дискутабельные вопросы такие. Коллеги, извините, ради бога, что я вас чепухой всякое долго отвлекал, но разрешите на этом закончить. Если будут вопросы, то я с радостью отвечу. 

S01 [01:23:18]  : Вадим, спасибо большое. Я не думаю, что вы нас чепухой отвлекали. Мне, например, было очень интересно. Теперь давайте перейдем к блоку вопросов и ответов. У меня тут был в начале вопрос. Меня тут попробовал Юрий Бабуров поправить. Я его все-таки задам. У вас там на одном из первых слайдов архитектуры был BERT. Под BERT вы подразумеваете вашу собственную архитектуру BERT или каким-то образом модифицированный код исходный? Каким-то образом модифицированный? западную модель то есть у вас модель как бы сама сама или все-таки вы взяли вот этот вот код и его портировали под свои задачи мы взяли код импортировали по своей задачу Ну вот здесь вот интересный вопрос, хотя с моей точки зрения он несколько философский. Вы можете со своей позиции разграничить знания и запоминания? И если можете, то в чем разница и как эта разница преломляется в том, что вы делаете? 

S02 [01:24:44]  : Антон, честно сказать, никогда не думал над этими философскими вопросами, никогда не думал. Но я сходу могу сказать, что запоминание — это процесс, безусловно, процесс усиления синаптической связи. То есть вначале это электрический потенциал, который может погаснуть и не активировать следующий нейрон. Затем это трансфузия медиатора, затем это активация, включение рецепторов. То есть там гей-белок у вас оттянулся, Розочка раскрылась, сигнал пошел в следующий нейрон. Накопление постсинаптического потенциала, включение ранних и поздних генов, синтез новых рецепторов. Это процесс биохимических изменений, который приводит к тому, что связь стала эффективной. Произошло запоминание. Запоминание – это процесс. А знание – это, на самом деле, как минимум, уже сформированная структура эффективной связи. То есть можно сказать, что знание – это результат запоминания в этом плане, но на практике это, наверное, огромное количество связей, которые связывают между собой различные объекты внешнего мира в нашей нейрональной модели. То есть запоминание – это процесс, а знание – это результат этого процесса, который ассоциирует между собой различные объекты. 

S01 [01:25:58]  : А можно я тогда от себя еще докину вопрос на эту же тему. Мне показалось, если я не прав, поправьте, то можно сказать, что когда мы просуществляем запоминания в мультимодальной системе, то есть если у нас есть много объектов различной модальности или объекты, которые воспринимаются по различным модальностям, то ассоциация на таком высоком уровне – это и есть знание. То есть, когда мы говорим «запоминание» и «знание», мы на самом деле говорим просто о разных уровнях абстракции. 

S02 [01:26:33]  : Скорее всего, сформированная ассоциация – это и есть знание. То есть, запоминание – это процесс сформирования такой ассоциации. Мы запоминаем, то есть эта ассоциация еще формируется, ее еще нет, она еще в памяти формируется через все это огромное количество биохимических реакций. Народ болтает, что 200 тысяч химических реакций происходит от ядра до ядра следующего нейрона. 200 тысяч химических реакций, вот это и есть запоминание. А память это когда уже рецепторы медиаторов сформированы, они в мембрану то есть они в ядре клетки были сформированы, они на ножках дошли до скелета и встроились. Вот эти встроенные рецепторы, это есть уже знание. Но, естественно, не одни рецепторы, а целая система связанных между собой рецепторов. То есть у нас же, допустим, полносвязанная сеть, а потом в ней появились потенцированные связи. Вот, наверное, то, что Анохин имеет в виду. когнитомам, как бы, это вот, наверное, он имеет индустрию потенцированных связей, то есть ваше знание. 

S01 [01:27:38]  : А тогда скажите еще, вы упомянули про химические реакции. Если я правильно понял, вы делаете углубленное нейроморфное моделирование, опускаете его на уровень макромолекул, но до химических реакций вы не доходите. Нет, не доходим точно. Но макромолекулы у вас есть. 

S02 [01:28:02]  : Мы считаем количество молекулярных рецепторов медиаторов. Это участвует прямо в функциях расчетов. За всеми этими названиями стоит набор функций, как это считается. Количество рецепторов считается. Есть, кстати говоря, еще четвертая фаза памяти, о которой я не сказал, но я ее редко использую. Это рост пресинаптического бутона, то есть увеличение выброса медиаторов. но на практике она почему-то мне очень редко эта функция используется, я ее постоянно забываю и даже говорю, что у нас трехфазная память, как бы краткая, средняя, долгосрочная, хотя на практике есть еще четвертая фаза, которая в резерве стоит. 

S01 [01:28:40]  : Спасибо. Теперь вопрос еще от меня. Вы говорили о том, что вы рассказывали в начальной части, с каким большим числом разных бизнес-задач и управленческих задач. где вы применяете вашу систему. А вопрос практически. Как вы осуществляете стыковку бизнес-логики и конкретной бизнес-сущности в конкретных предметных областях с нейросетевыми моделями? То есть у нас, грубо говоря, получился там какой-то аудиопоток или видеопоток, Нам нужно понять, какие там сущности, какие отношения, и обнаружить какие-то другие сущности, отношения или породить их и превратить их обратно в видеопоток. Как сказать, объекты и отношения возникают из аудио или из видео и обратно превращаются в них, как стыковка? 

S02 [01:29:36]  : Но вот смотрите, исследовательские функции в компании, они абсолютно отделены от функций разработки систем для заказчика. Это совершенно разные отделы действия. Мне только приходится между ними разрываться. Но суть такая, как мы работаем с заказчиком. Заказчик входит, мы связываемся с заказчиком, он говорит, я хочу иметь такую-то диалоговую систему. Мы сначала очень подробно анализируем предметную сферу заказчика, его задачи. Отдельный бизнес-аналитик начинает разбираться, что на самом деле нужно заказчику, не забыл ли он какие-то еще функции, задачи. и разрабатывается схематическая модель всей этой системы. Что есть в схематической модели? Там есть сценарий диалогов, во-первых, общее описание предметной области, во-вторых, потом появляются сценарии диалогов, как блок схемы определенного, и вопросно-ответные пары, например, в одиночных вопросах, они тоже очень важны часто. И вот у нас появилось техническое задание, в каких средах должна система работать, с чем интегрирована должна быть и, соответственно, какие вопросы на ответные пары, какие сценарии диалогов должны быть. После того, как это закончено, мы формируем договор, подписываем заказчикам договор. Дальше начинается работа набора сетей, для этого они сформированы, они не меняются под отдельного заказчика, как правило, хотя коэффициенты какие-то могут изменяться. Соответственно, у нас начинается работа дата инженеров, то есть они берут все эти узлы диалогов, они берут вопросно-ответные пары и начинают в дипдейте смотреть, насколько они семантически пересекаются. Чем больше семантическое пересечение, тем больше мощность датасетов нужна по этим классам. И у нас получается техзадание для датасетчиков. И затем, начиная там несколько недель, работают дата-инженеры. То есть, они обычно делают мастер-строки, причем под руководством тоже интеллектуального агента, который говорит, какие грамматические конструкции, есть тысячи грамматических конструкций, какое слово-заменение надо использовать в мастер-строках. И, соответственно, в этом участвует большое количество штатных сотрудников, несколько десятков, несколько сотен фрилансеров обычно участвуют. И вот они составляют мастер-строки, потом эти мастер-строки размножаются в deep data уже до многих миллиардов строк. по определенным принципам, и нейросеть начинает учиться. То есть она по F1 смотрит, как там качество обучения, не хватило датасета, она еще взяла, то есть автоматически протестировала сама себя, увидела, какие классы запали, по этим классам дополнила как бы датасеты, и она начинает как бы доучаться, что процентов там за 90 процентов ушла, потом мы подключаем уже людей для ручного тестирования этих сеток. Я просто подробно описал процесс разработки системы для заказчика. То, о чем я сегодня рассказывал в докладе во второй части, никак с этим не связано. Это разработка архитектур. Когда архитектура готовая, она начинает применяться уже в бизнесе. Бизнес просто потребитель готовых архитектур. 

S01 [01:32:44]  : Спасибо. Вопрос от Бабарыкина Евгения. Как у вас реализована система с точки зрения парадигмы программирования? То у вас объектно-ориентированное программирование или что-то еще? 

S02 [01:32:58]  : Знаете, я не буду даже позориться. У нас уже немножко поздно, у меня разработка разбежалась, которая могла бы хорошо ответить на эти вопросы. В основном это сочетание двух языков, то есть C++ и Python. Различный модуль написан на этих языках. Бизнес-логика выполняется специально обученными программистами, которые за нее отвечают. 

S01 [01:33:19]  : Спасибо. Вопрос от Олега Колтунова, который тоже мне не даёт покоя уже большую часть времени. В модели предполагается ли бесконечное количество вершинных нейронов, создающиеся по случаю? 

S02 [01:33:35]  : Да, предполагается. 

S01 [01:33:39]  : Ну тогда у меня по этому поводу дальше будет вопрос, но я иду по порядку. Вот у меня был вопрос, возник другой нейрон. 

S02 [01:33:47]  : Антон, а я вот еще чуть-чуть прокомментирую. Смотрите, чем дольше развиваются сети, тем больше в них вершин и тем меньше в них элементов. 

S01 [01:34:01]  : Чем больше вершин, тем меньше элементов. А чем вершины отличаются от элементов? 

S02 [01:34:06]  : Вершины – это тоже элементы. Чем больше элементов, тем меньше элементов. Как вам такое? Вот непонятно. Вот смотрите, Антон. Вот у вас, допустим, 10 объектов, которые проецируются на 10 более крупных объектов. Ну, 10 объектов, они, как правило, проецируются на 8 объектов, но не суть важна. И вот у вас 10 на 10, у вас связка, полносвязная сеть, соответственно у вас будет 10 на 10, у вас 100 связей в общей сложности. И вот у вас первый нейрон использовался для буквы МА, для слога МА. У вас вот эти две связи потенциировались с пирамидой, и остальные восемь, они кибергеном ликвидировались. То есть две связи потенциировались, а восемь исчезло. Следующий нейрон захватился, у вас снова восемь связей исчезло. Понимаете, о чем я говорю? И когда у вас сетка обучена, у вас уже не 100 связей, а 20 связей. Ну, если бинарная проекция. Соответственно, у вас количество элементов уменьшилось в 5 раз после обучения. Вы знаете же, это интересный парадокс, что у старика кора тоньше в несколько раз, чем у ребенка. Именно за счет того, что синаптических связей у старика гораздо меньше, а сформированных нейрон гораздо больше. Я напомню вам исследование по 44-й зоне Уинштейна, которое отвечало за абстрактную математику, что у него в 10 раз было больше тел нейронов в этой области, чем у нормального, прошу прощения, человека. Вы помните гиппокампу лондонского таксиста, у него гораздо больше нейронов. У Эйнштейна их было так много, что они были во много раз меньше обычных сом, не теряя функционала. То есть природа даже ужала форм-фактор, чтобы поместить большее количество сом. Но даже не надо природе сильно мучиться, потому что связи, исчезая, они огромное свободное пространство оставляют. Я абсолютно убежденный сторонник нейрогенеза во всех областях мозга. Не надо здесь серьезно воспринимать господина Савельева, конечно нейрогенез везде идет. 

S01 [01:36:14]  : Вот это очень интересно. У меня вопрос, знаете, тогда какой сразу здесь. Считаете ли вы, что у человека нейрогенез происходит с такой же скоростью, ну относительной, да, с какой он происходит в вашей системе? Или ваша система, скажем так, является более сверхчеловеческой за счет того, что она, поскольку вы не завязаны 

S02 [01:36:36]  : Антон, я не знаю, я не могу ответить на ваш вопрос. Откуда я знаю? Не надо даже сравнивать нашу систему с человеком. Человек непонятен. Я шучу, конечно, я так не считаю. Вот смотрите, Антон, на мой взгляд нейрогенез безусловно замедляется. Он замедляется в силу объективных факторов. У вас много новых сочетаний перестают появляться. У вас появляется опыт, у вас для основных позиций уже все есть, у ребенка до года, потом уже гораздо меньше впечатлений, поэтому нейрогенез в десятки, в сотни раз слабее у взрослого, чем у ребенка. Если вы начнете английский, если вы начнете мандарины учить, язык мандаринов, то у вас будет нейрогенез, начнется сразу. Вы начнете учить таксомотором работать, таксистом, в Новосибирске и, соответственно, у вас в гиппокампе новые нейроны быстро начнут расти, даже в 60 лет. Вы же знаете, что нейрогенез даже у мертвецов еще две недели происходит после смерти. Две недели еще после смерти появляются новые нейроны в разных областях мозга, хоть что тут треснет. Очень оптимистично. Но я еще раз говорю, да, я все-таки не нейробиолог, поэтому здесь прошу относиться к моим словам как бы с определенной, как бы фильтровать, но у меня как бы здесь второй источник, это вот искусственные сети, которые фактически как бы подсказывают некоторые решения иногда. Поэтому у меня безусловно аллюзия между нашими искусственными, там, картикоморственными, как мы говорим, сетями и живыми сетями, но мы стараемся постоянно как бы опираться все-таки на данные нейробиологии. 

S01 [01:38:04]  : Спасибо. Вот здесь как раз вопрос на эту тему у Бабарыкина Евгения о том, как именно у вас в системе, то есть оставим людей живых и мёртвых в покое, как у вас формируются новые нейроны, то есть как происходит нейрогенез. И вот дальше ещё, позвольте, у меня более конкретный вопрос на эту же тему. Вы говорили про нейронные лизны. Я правильно ли понимаю, что нейрон новизны – это такой специальный волшебный нейрон, который сконфигурировали вручную при создании каждой отдельной новой сети. И вот этот специальный волшебный нейрон как раз сечет как что-то новенькое, нераспознанное всей остальной компанией. Тут же он раз и рожает пачку новых пирамидальных нейронов, чтобы они с этим делом разобрались. 

S02 [01:38:56]  : Примерно правильная ассоциация. Нейрон нейроновизны – это нейрон с определенной конфигурацией связей. Вы знаете, что в Корее известны звездчатые клетки, перемедальные клетки, вертинообразные нейроны. Порядка 3000 нейронов насчитывается сегодня в мозге у человека. И об этих нейронах говорят как об отдельном виде, как правило, именно по топологии их организации связи, по форме этих нейронов. То же самое и нейроны навизны. Нейроны навизны – это такой нейрон, на который все пирамиды имеют тормозные синапсы. Пирамиды – это нейроны бабушки, нейронные объекты. Они все имеют тормозную проекцию на нейроны навизны. А нейроны навизны имеют на них активационную проекцию. Ну, не напрямую, там ставочные всякие нейроны есть, которые синхронизируют такты. То есть, грубо говоря, сигнал подошел, ассоциативное восстановление пришло, оно имеет физически проекцию связи. проекцию на нейрон бабушки, но там еще по одному рецептору они очень слабые, они не могут активировать, они ниже порога пирамиды. Допустим, я говорил, у пирамиды порог 10, а у вас две связи на него подошло, сигнал с двух связей, там и там активность по одному, то есть восьми не хватает. Они ушли по колонке, по цепочке на второй круг и боковым отрезком, я имею ввиду боковым отрезком, это ассоциативная мишель, она активировала нейронные визны. Сигнал побежал по цепочке и они синхронизированно подошли одновременно. То есть, когда круг сделал сигнал и подошел обратно на точку проекции, и нейронной визны как бы… Но у нейронной визны очень сильное влияние, он там выделяет 12 медиаторов, условно говоря. И нейронной визны пробивает сопротивление пирамидальной клетки. Нейронной визны дал потенциал 12, а снизу идет потенциал 2, понимаете, да? Но это как похеба, помните, если более сильный нейрон помогает более слабому активироваться. ассоциироваться, и нейрон новизны как бы через синапс свой пробил сопротивление пирамидальной клетки, она активировалась, но на входах-то с ассоциативным мишением один и один сигнал есть, и там тоже происходит, то есть синтез рецепторов происходит только в тех дендритах, где есть потенциал. Здесь есть потенциал один, и там раз, и по 6 рецепторов образовалось. И у вас совокупные рецепторы между ассоциативным мишением и ассоциативным основанием стало уже 12 плюс 2 — 14 рецепторов. Стало выше порога. То есть за однократное подключение нейронной новизны у нас синтезировалось столько рецепторов на входах пирамиды, что когда сигнал МА пришел в следующий раз уже, уже она раз и сразу, МА без всякого нейронной новизны, она уже активировала пирамидальную клетку. Произошло обучение как синтез новых рецепторов. Понятно, да, в этот момент? Практически поведение Хэбба один к одному. Мне кажется, что Хэбб был гениальным предвиделем будущих биохимических исследований Кэнделла и других по следу памяти, которые мы просто воспроизвели, мы не придумывали, это все целиком взято из нейробиологии. Почему мы это взяли? Мы взяли не чтобы моделировать нейробиологию, потому что мне кажется, что с каждой фазой памяти связаны важнейшие когнитивные функции. Например, акциональная фасилитация – это переход сигнала с клетки на клетку. Дендритная активация – это логика, торможение, суммации. А рецептор – это постоянная память, постоянные запоминания. Так, я, по-моему, на какой-то вопрос еще не ответил. 

S01 [01:42:51]  : Собственно, эти были вопросы на одно и то же. 

S02 [01:42:57]  : Как формируется, как нейрогенез запускается? Нейрогенез запускается так. Есть, допустим, нейрон, который принимает сигналы от многих нейронов, и он может понять, что мало нейронов осталось свободных. которые еще не активированы и когда допустим там условно говоря у вас 100 нейронов когда у вас 90 нейрон как бы активировался и захватилась 90 пирамид у вас всего там 10 осталось свободных пирамид он включает вот это сигнальное событие активация 90 нейрона ну я условно говорю потому что это просто какой-то нейрон в конце он активирует и как бы кибергена синтеза нового локуса и включается большая группа кибергенов которые упорядоченным образом проращивает новый кусок сети. Если говорить философски, мы считаем, что в ходе эволюции систем, филогенеза. На каком-то этапе нейроны были беспорядочными, между собой связаны, но на какой-то функции, на каком-то этапе случайно, может быть, нейрон выполнил роль вот этого учителя. И это закрепилось в поколениях. Нейрон новизны возник как бы в филогенезе, а сейчас он присутствует как готовая схема, которая нам дается с генами мамы и папы. 

S01 [01:44:09]  : Спасибо. пропуская вопросы, которые по ходу уже ответились. Я правильно понял, как у вас с емкостью? То есть, вот он едет, вот он все запоминает, и чем больше он увидел, тем меньше он видит нового, и тем меньше он запоминает. То есть, скорость, необходимость нейрогенеза, как я понимаю, постепенно падает. А забывание у вас есть в каком-то виде? Да, есть. 

S02 [01:44:38]  : Нейрогенез зависит от степени обогащения внешней среды. Если внешняя среда начинает обогащаться, то нейрогенез восстанавливается, усиливается и так далее. Да, забывание есть. Забывание играет важнейшую функцию. Я сказал, что фильтрация важна. Помните, я приводил пример? сильные воспоминания усиливаются, а слабые, наоборот, ослабляются. Причем вот это забывание, оно есть как бы естественно, есть скорость распада рецепторов, есть скорость спада потенциалов. Мы это все видим на графиках. Я вам школьные картинки показал, но в принципе мы это все видим на графиках, любой параметр, и мы видим, как спадают потенциалы, как разрушаются рецепторы. Я эту скорость могу задавать как архитектор, то есть разработка узла является как бы расстановкой коэффициентов. Если я скорость спада делаю нулевой, у меня появляется спейсмейкер. Очень сложная вещь. На практике это кажется очень просто. Нейрон, который постоянно работает. На практике это чудовищной сложности вещь. Но тем не менее, если я убираю скорость распада рецепторов на ноль, появляется спейсмейкер. И скорость спада потенциала на ноль. Прошу прощения. Поэтому забывание есть. И забывание есть еще принудительное. То есть, когда вы тормозите какую-то связь, то она, естественно, ослабляется, потому что он не активируется. или потому что растут тормозные рецепторы. То есть он может типа помнить, но тормозные рецепторы больше, и он тормозится. Поэтому есть естественное забывание, есть забывание под влиянием торможения, и есть собственно торможение, которое все имеют один и тот же эффект, как бы то, что нейрон не реагирует. 

S01 [01:46:24]  : Спасибо. Два вопроса, которые, по-моему, где-то рядом. Мой вопрос про объяснимость. Вопрос следующий. Глубокие нейросети в принципе же тоже можно визуализировать, показывать существующие в них связи и активацию в режиме реального времени, как-то проигрывать. И вроде как с точки зрения таких картинок, что вы показывали, это тоже может давать эффект объяснимости. А тогда, кроме визуализации и интерактивности, у вас есть что-то еще, что увеличивает объяснимость? 

S02 [01:47:05]  : Ну, наверное, есть. Да, я с вами согласен, в целом, по глубоким сетям, наверное, согласен. Там единственное, что, может быть, затрудняет, еще в глубоких сетях очень, там же все-таки как бы структуры сами по себе довольно простые, например, полносвязанные структуры, как бы нет каких-то ядер, управляющих и так далее, ну, почти нет. Хотя, может быть, я не прав здесь, но тем не менее, мне кажется, за счет случайной активации различных локусов, даже если мы визуализируем глубокие сети, все равно очень сложно потом будет понимать, какой смысл в этом локусе. Очень сложно трассировать весь сигнал и так далее. В кратегоморских у нас сетях там еще и очень видоспецифические зоны появляются. Конечно, это все начинается с входов, условно говоря, и в любом случае надо трассировать сигнал. Я нажимаю кнопочку, выключаю сетку, перевожу в режим разработки. пробел нажимаю, у меня как бы сигнал переходит красненький от одного нейрона к следующему, и я весь путь могу протрассировать. Но за счет того, что архитектура все-таки специфическая локусов, я как бы изначально знаю, за что отвечают те или иные зоны, мне все-таки, мне кажется, гораздо легче трассировать сигнал и понимать. Хотя, если честно говорить, то на сетях, о которых я говорил, там 30-40 миллионов синапсов, Конечно, мы никогда уже не трассируем этот сигнал, мы просто понимаем принципы работы сети, и этого нам достаточно. 

S01 [01:48:35]  : Спасибо. И вот второй вопрос от Дмитрия Салихова про ваше промышленное решение. Они сделаны на кортикоморжных сетях, про которые вы рассказывали? 

S02 [01:48:48]  : Кортикоморжные сети используются наряду с сетями глубокого обучения, допустим, для очень быстрого обучения по ходу разговора. Отдельный сетевой модуль, который может обучиться по ходу разговора. Поэтому, в принципе, я могу какую-то вещь обучать сразу по ходу. То есть, это гибридные сети, где используются картихоморфные сети наряду с конволюционными, рекуррентными LSTM, GAN сетями и рядом других. 

S01 [01:49:11]  : А есть, так сказать, какой-то простой подход, для чего вы используете какие сети? Например, это может быть на каком-то примере. 

S02 [01:49:23]  : Антон, тоже не совсем моя область, поэтому я могу сказать, что мы используем сети единичной классификации, множественной классификации, сети выделения сущностей, сети распознавания устно-слитной речи. Архитектура привязана к этим функциям в основном. 

S01 [01:49:41]  : Спасибо. Вы говорили про кибергены. Да-да, что один киберген может несколько страниц занимать. А вот несколько страниц – это на каком языке? То есть, какой язык написан? На плюс-плюс. То есть, гены, они пишутся на… То есть, это некоторый функционал, который описан на плюс-плюс. А каким образом вы тогда осуществляете там мутацию, кроссовер двух программ на С++? 

S02 [01:50:09]  : ОКС Картифика, там есть инструменты создания кибергенов. Кибергены – это определенный программный код. Разработчики говорят, что у них свой язык. По большому счету, это верхнеуровневое решение, где они пишут некий код функции. Затем они его сохраняют и начинают использовать в этой части аксона. Довольно сложная функция — это скрещивание двух систем. Я уже сказал, что в ходе эксплуатации системы, поощряя действия системы, мы расставляем определенные метки, то есть цифровые значения для этих гибергенов. Они приобретают разные веса, по большому счету. сливаем две сетевые архитектуры, они берут в себе в структуру те кибергены, которые имеет большее количество баллов или меньшее количество баллов, там мы можем это задавать естественно, это в ряде случаев приводит к успеху, то есть в ряде случаев появляется более совершенное решение, в ряде случаев это приводит к деградации функций, к появлению каких-то уродцев, это область очень новая и сейчас активно развивается. 

S01 [01:51:29]  : Спасибо. Есть пара практических вопросов. Во-первых, каковы вычислительные ресурсы требуются для создания корпоративных и профессиональных помощников? Грубо говоря, на десктопе может профессиональный помощник работать или все-таки суперсервер, суперкомпьютер нужен? 

S02 [01:51:47]  : Ну, смотрите, здесь как бы о чем идет речь. Во-первых, есть разработка этих систем. Разработка этих систем должна вестись, конечно, на мощных графических картах все-таки. то есть вот у нас я сказал что сейчас порядка пита флопса общая мощность машин там наверное около 30 у нас сейчас серверов обычно это как бы лезвие по 8 rtx и 8000 как бы или по 8 т4 как бы иногда как бы для разных задач лучше разные видеокарты есть также еще мегапроцессорная система допустим Ядро картифики вообще на графических картах не работает, хотя вычисления на них проводятся. Оно работает на процессорных мощностях с большой памятью. Обычно наш кластер разбивается на задачу. У нас идет разработка примерно 10 проектов. Грубо говоря, разработка идет на одном лезвии максимум. Восьми картах, например. Это речь об обучении, не разработке, а обучении сеток. Обучение может занимать самое разное время. Это зависит чисто от мощности системы. Текстовая сетка может учиться полтора-два часа. Большая текстовая сетка учится часов двенадцать. Но вот мы, допустим, когда подростков анализируем, мы там анализируем сотни типов картинок. Вот у него там разрезанные вены, условно говоря. Вот у него пистолет, поднесенный к виску. Вот он висит в петле. Вот там изображение всяких мертвецов. Но я прошу прощения, тематика не очень веселая. головы на рельсах, условно говоря, прыжки с крыши и так далее. И вот этих типов картинок сотни, ну не сотни, тысячи типов используются. И вот сетки, допустим, анализа картинок, они учатся 3-4 недели, 3-4 недели. Там ребята конфигурируют себе ресурсы, какие им надо для кластера. Вот сеть распознавания речи, я уже сказал, что она учится уже около полутора лет. Эпохи продолжаются, продолжаются, и так далее. Но в целом, я думаю, что одного сервера от одной до восьми карт достаточно, в зависимости от задачи. А вот когда уже система создана, там могут быть разные варианты. У нас есть несколько, допустим, решений распознавания речи. Некоторые они могут работать на обычных... На ноутбуке могут работать некоторые системы распознавания. Они не очень хорошие, конечно, по качеству. На уровне 70 с небольшим процентом. Я не люблю эти наши технологии. Но в ряде случаев для закрытых систем нам приходится их использовать. Там, где не разрешено выходить в интернет. Тогда мы ставим вот эти решения, по сути дела, внутренние, десктопные. Потому что пока графических машин мало, даже у богатых заказчиков, на самом деле. И есть решения с качеством распознавания порядка 80%, ну процентов на 10 повыше. Условно говоря, система на ноутбуке будет процентов на 10 хуже работы, чем система на суперкомпьютере. Поэтому эксплуатация может быть на обычном ноутбуке, а разработка на графическом суперкомпьютере. Вот, наверное, так отвечу. 

S01 [01:54:59]  : Спасибо. Вопрос совсем практический. Как вам устроиться на работу от Бабарыкина Евгения? 

S02 [01:55:07]  : Вы знаете, это вот самая-самая главная проблема для нас. Я имею в виду поиск талантливых кадров. Вот я уже сказал, что мы начинали вчетвером. Сейчас у нас где-то 63 человека работают, по-моему, штатных сотрудников. И это постоянно головная боль. Нам крайне нужны талантливые сотрудники, поэтому я думаю, что просто написать мне по электронной почте. И если это желание серьезно, мы всегда найдем как бы сферу применения. То есть компания сейчас как строится? У нас есть как бы отдел продаж. Есть отдел разработки мультимодальных систем, есть отдел разработки диалоговых систем, есть отдел разработки нейросетевых архитектур, есть отдел как бы дивизион практически, это дата машинного обучения, то что называется, там где дата инженеры работают. Там он разбит обычно по проектным группам, по направлениям. Есть отдел тестирования. Мы только-только начали формировать отдел тестирования. Мы еще не изжили всех болезней стартапа из-за того, что кадров постоянно не хватает. Единственный способ, который мы нашли, это принимать на работу школьников. Мы это серьезно не воспринимали. Я когда-то прочитал лекцию для школ в Тюмень области. И ко мне пришел мальчик из 9 класса. Это было лет 10 назад. Говорю, я хочу у вас работать. Говорю, иди отсюда, чтобы я тебя больше не видел. Он через недельку пришел ко мне с директором школы. Сказал, что это лучшие студии школьников и так далее. И взяли мы его на испытания. Ну вот годы за три он стал хорошим специалистом. Потом мы это ввели в практику на самом деле. То есть мы очень часто берем... Сейчас у нас несовершеннолетних работает человек 6. Кстати, некоторые бывшие суицидники, которые были обнаружены нашими системами и сейчас сами учат датасеты для суицидников. 

S01 [01:56:55]  : Спасибо. Вопросы пошли более научные. Про ассоциативные рекомбинации. Как происходят рекомбинации на основе слогов, на основе букв? Как строится правило, по которым эти рекомбинации происходят? Как понять, какие из комбинаций являются правильными? Что происходит, если на практике это неправильно? 

S02 [01:57:20]  : Вы помните, у иудеев какой пророк считается правильным? Тот, который делает правильные предсказания, а все остальные ложные пророки. Значит, смотрите, коллеги, какая ситуация, как мы исходим. Все-таки вот эта дорога к ассоциативным рекомбинациям, она начинается с анализа, по сути дела, с интровертного внутреннего когнитивного психологического анализа. что допустим 1 плюс 2 равняется 3 да условно говоря 8 минус 2 вот другой пример 8 плюс 2 равняется 10 8 минус 2 равняется 6 мы видим что у нас как бы допустим появился минус другой член у нас появляется другой результат то есть в данном случае это зависит от набора 8-2 и 2-8 будут разные результаты. 8-2 будет 6, а 2-8 будет 6. Ответы разные. Они зависят от начала ассоциативной цепочки, от ассоциативного основания. В данном случае мы видим ассоциативное основание. которая чувствительна к упорядоченности следования. И мы, допустим, понимаем, что должны быть сетевые решения для упорядоченных ассоциативных оснований и неупорядоченных, потому что мама пришла домой, домой пришла мама, пришла мама домой. Как бы эти слова ни представляли, в данном случае суть важна. Это неупорядочное ассоциативное основание, оно индифферентно к следованию. В то же время, если мы скажем, пришла домой, появляется неполное основание, мы теряем часть контекста. И вот мы считаем, что есть 34 формы позиционных перестановок, позиционных рекомбинаций. Полная, неполная, чистая, зашумленная, с позиционной порядочностью. И есть еще с частичной позиционной порядочностью, в ряде случаев это очень важно. Ну и еще какие-то, я сейчас уже прошу прощения, не вспомню собственной работы. И мы начали думать, как сделать граф, который вот эти виды изменений обрабатывает. И получилось, что вообще сетки получаются на порядок разных уровней сложности. У нас же еще та сложность, как вы заметили, что нейроны-бабушки кодируют определенный объект. Соответственно, мы сталкиваемся иногда с трудностями, когда просто идет повторение этого объекта 1, 1, 1, 1, 1. В данном случае нам приходится делать довольно большие буферные зоны, чтобы дублирование входящих сигналов хранить. Исходя из того, какие мы виды ассоциативных оснований выделены, мы начинаем разрабатывать для них архитектуру. начинаем соединять эти нейрончики месяцами, пока этот сигнал не пройдет так, как нам надо. То есть мы, грубо говоря, пытаемся воспроизвести то, что там природа делала миллиарды лет, скорее всего. Можете как угодно здесь осуждать наш подход, дискутировать. Он очень интересный, дискутабельный, но тем не менее он такой. Допустим, могут быть вообще другими словами, можем по-другому сказать что-то. Здесь уже другая архитектура, мы должны собрать семантический аттрактор, то есть мы должны на одну вершину собрать все ее варианты. Прошу прощения, даже у одного слова ты десятки вершин, потому что вы чуть-чуть по-другому сказали, вы там прошепеляли, вы опечатались в какой-то букве, сделали разорванное зашумленное основание, но он должен идти на ту же вершину. Сейчас мне в голову какая-то ошибка речевая не приходит, но я думаю, суть понятна, о чем речь идет. В данном случае идет совсем другой принцип, за счет частотности повторения. Вот те же морфемы мы выделяем. Вообще у нас есть сетки, которые мы даем сплошной поток букв, даже без пробелов. И задача сетки – это создать все вершины для морфемы слов. В том числе и в зарубежных отчествах. Крайне интересная задача. Там идут многократные повторения, сложное распределение весов и выигрывающие нейроны, которые становятся нейронами морфем и так далее. Ужасная задача. 

S01 [02:01:45]  : Спасибо. Вопрос от Евгения Витяева. Может ли система сама формировать понятия, как осуществляется обучение обратным распространением ошибок или своими методами? И насколько сложную ветвь дендритов вы используете? 

S02 [02:02:07]  : Первый вопрос очень интересный. Антон, еще раз сначала прочитайте. 

S01 [02:02:12]  : Первый вопрос. Может ли система сама формировать понятие? 

S02 [02:02:16]  : Смотрите, коллеги, мы считаем, что очень много дефиниций у единицы какого-то сигнала. Допустим, в слове мы посмотрим. В слове, во-первых, есть внутренняя структура, морфемы. Я о ней много говорил уже. И первичное значение слова кодируется в уморфемах. Я приводил примеры с суффиксами, но те же значения есть у приставок, при, до, под, над и так далее. Это первый источник понятия, первый источник смысла. Второй источник – это, безусловно, контекст, то есть какие слова стоят рядом с этим словом. «Я забыл ключ на подоконнике», «В лесу бил ключ чистой воды». Это амонимия. Вы знаете, что у нас 43% высказываний амонимичны? 43%. И, соответственно, по нашим данным, дифференциацию этих смыслов мы можем снять только контекстуально. Затем есть прямое определение, прямая дефиниция, словарная статья. Это тоже контекст по большому счету, но усиленный контекст. Поэтому, наверное, система может формировать понятия, потому что сами вершины для морфем, слов, устойчивых словосочетаний, иногда целых абзацев текста, они формируются автоматически. И они автоматически формируют ассоциативную связь с своим окружением. Мне кажется, это и есть понятие. Вот, наверное, так бы я ответил на этот вопрос. 

S01 [02:03:55]  : Спасибо. Второй вопрос. Как осуществляется обучение обратным распространением ошибок или своими методами? 

S02 [02:04:05]  : Ну, скорее всего, все-таки своими методами. Почему? Конечно, фидбэк, обратные петли, они везде есть. Абсолютно все построено на прямых обратных петлях. Сама структура этой сетки достаточно сложная. Вы там видели, я показывал. прямых обратных соединений. Но, наверное, я бы все-таки это не назвал обратным распространением ошибки, как бы там по Галушкину или там по американцам. Почему? Потому что там несколько понятных связанных формул, которые там изменяют коэффициенты. У нас, видите, гораздо больше, как бы вот эта цепочка из этих форм, гораздо больше используется. Поэтому, в принципе, обратное распространение есть как таковое, как учет обратных связей, как изменение нейронов предыдущих слоев в зависимости от реакции следующих слоев, но оно немножко другое, чем классическая backpropagation. 

S01 [02:05:03]  : Спасибо. Еще один вопрос от Евгения Евгеньевича Витяева. Насколько сложную ветвь дендритов вы используете? Я попробую пояснить этот вопрос. Видимо, имеется в виду, что у нас же настоящий дендрит в голове. 10 дендритов – это не 10 палок, не 10 прямых линий. То есть, каждый дендрит – это на самом деле дерево, и есть статьи, что существенная обработка происходит на ветвениях дендритов. 

S02 [02:05:34]  : Я уверен, что это абсолютно правильные статьи. Вот это у нас одна сейчас из проблем, кстати говоря. Мы недооценили в свое время сложность дендритов. Сейчас у нас один дендрит – это одна вычлитная цепочка. К одной СОМе могут подходить иногда сотни тысяч дендритов. У нас в сетях есть управляющие нейроны, которые должны зафиксировать какое-то состояние локуса. Сотни тысяч дендритов есть у этой СОМы. Но мы, к сожалению, не учли, что дендриты надо делать гораздо более сложными. и должны быть центры суммации внутри этих дендритов, то есть должны быть, грубо говоря, два уровня дендритов. Вот к этому надо переходить, потому что некоторые… Ну вот, допустим, элементарные функции, допустим, вот смотрите, вот я упоминал пейсмейкер, пейсмейкер. Пейсмейкер же, ну то есть потенциал, он сохраняется все-таки в дендритах. А в сумме у нас он сейчас только суммируется. И вот, чтобы погасить пейсмейкер, нам надо погасить дендриты. Вот сейчас у нас такой элементарной функции пока не сделано еще, например. Поэтому дендриты нам надо усложнять. 

S01 [02:06:46]  : Спасибо. Вопрос. Я вот перепрыгну часть вопросов, которые уже были отчасти. Сергея Шумского попрошу взять слово. 

S02 [02:07:00]  : Сергей Александрович, добрый день. Я не знал, что вы присутствуете. 

S00 [02:07:04]  : Добрый день, Вадим. Рад увидеться. Очень хороший доклад. Спасибо большое. Получил большое удовольствие. рад успехам. 

S02 [02:07:13]  : С огромным удовольствием прочитали вашу серенькую книжку последнюю. 

S00 [02:07:17]  : Спасибо. Вот у меня вопрос по научной части, не по бизнесу, а по научной части. Значит, вот вы все-таки напираете на то, что вы картикоморфной сети, то есть они в основном моделируют процессы в коре. И понятно, что для задач распознавания этого вполне достаточно. Вы находите корреляции, корреляции, корреляции, корреляции, корреляции, то есть можете построить сколь угодно сложные образы. И для, скажем, обучения языку этого вполне достаточно. Но вот для выработки поведения целесообразного, Для передней части мозга, для лобных долей они работают в тесной связи с базальными ганглиями, где сидят ценности, которые управляют активностью коры. Когда-то у динозавров не было коры, тем не менее они бегали. Наверное, у вас должна быть Ну, либо задумка, либо это уже есть. Каким образом моделировать вот такое взаимодействие? Ну, то есть аналоги базальных ганглей, которые работают вместе с корой. 

S02 [02:08:37]  : Сергей Александрович, наверное, вот есть только определенные подходы, которых я обозначил, когда говорил о машинной субъективности, условно говоря. То есть, грубо говоря, Если мы начнем в нейросети параллельной семантики еще задавать отношение сети к новым объектам, условно говоря, то есть мы говорим о том, что вот этот объект, при первоначальном обучении мы просто говорим как родители, допустим, системе, что вот это хорошо, это хорошо, это плохо, это хорошо. Соответственно, у сетки появляется система семантики, система отношений, хорошо-плохо. У вас приходит некий объект, у него есть некие признаки ассоциирования с одним из известных объектов. Например, он использует мат и грязно ругается. У нас с матом и грязной рукой не ассоциировано, что это плохо. Соответственно, система плюс на минус тоже сделает минус, она скажет, что новый человек, который к ней пришел, он матерится, или он там негров называет неграми, он тоже плохой. Вот это, пожалуй, единственные пока подходы к тому, о чем вы начали говорить. 

S00 [02:09:46]  : Это такое рефлекторное поведение, да? Скорее всего, да. Хорошо и плохо, но для достижения далеких целей этого недостаточно. 

S02 [02:09:55]  : Совершенно верно, Сергей Александрович. Вот трагедия сейчас нашей ситуации в чем заключается? Что мы занимались активно научными разработками до 2016 года, В 2016 году, когда мы выехали из университета, купили этаж и начали заниматься бизнесом, научные работы практически свернуты оказались в последние 5 лет. Практически очень велотекущие они ведутся. 

S00 [02:10:20]  : Нет, надо было встать на ноги. 

S02 [02:10:23]  : Да, надо было встать на ноги. Вот сейчас потихоньку вроде бы финансов начинает хватать. И мы создали отдельную группу, отдельный отдел по всем разработкам технологий. Может быть, какие-то новости появятся в следующем году, я надеюсь. Сергей Александрович, мне бы очень хотелось с вами посотрудничать. Есть программные инструменты, все в нашей власти. Мы можем менять базовые модели элементов, можно создавать любую архитектуру, делать программные функции. Было бы крайне интересно с вами посотрудничать, потому что действительно с огромным удовольствием интересно прочитали вашу последнюю монографию. 

S00 [02:11:00]  : Ладно, посмотрим, может быть что-нибудь получится. 

S01 [02:11:04]  : Спасибо большое. Спасибо. Вот есть еще вопрос на тему сотрудничества. Создаёте ли вы у себя направление или отдел по AGI и набираете ли вы у него сотрудников? 

S02 [02:11:18]  : Да, да. Сотрудники нам нужны во всех абсолютно видах. Мы готовы брать на работу в офисе, на удаленную работу, на другие формы сотрудничества. Это крайне интересно. И приглашаем всех на самом деле к сотрудничеству и к работе. Сейчас даже вроде бы и по зарплатам у нас довольно высокие. 

S01 [02:11:38]  : Спасибо. Вопрос у меня есть. Обязательно ли, с вашей точки зрения, так низко опускаться в детализации до отдельных молекул? Вот нельзя ли свести, то есть как насчет генерогенеза тут как бы все прекрасно, вопросов нет, а вот обязательно ли нам именно там моделировать то, что внутри происходит, внутри дендрита или внутри аксона, нельзя ли просто остаться на уровне синапса и описывать чувствительность синапса с помощью какой-то функции? Просто для подснижения вычислительных затрат, повышения производительности или эта детализация что-то реально дает? 

S02 [02:12:24]  : Знаете, вот мое личное убеждение, что нельзя. Это личное убеждение проистекает из практического опыта, потому что у нас было много попыток у молодежи написать более лаконичные модели. Даже если мы получали какую-то сетку низкоуровневую, потом они пытались написать более быстрое ПО, которое моделирует и имитирует эти функции. Но обычно в таких работах с водой выплескивался и сам ребенок. То есть они рано или поздно сталкивались с функцией, с нюансом, который не учтен. Чтобы реализовать этот нюанс, надо добавить еще какую-то надсетку, подсетку или узел какой-то в сети. И в любом случае приходилось спускаться вниз. Поэтому опыт практической работы у нас создания более генерализованных моделей пока к успеху не привел. Но я не могу сказать, что это в принципе невозможно. Возможно, если более умные люди, чем мы, будут этим заниматься, это возможно. 

S01 [02:13:16]  : Спасибо. И вот, наверное, последний содержательный вопрос от Игоря. Если происходит удаление связей, то что будет происходить, когда уже удаленная комбинация снова попадает на вход? Будет заново происходить обучение? 

S02 [02:13:41]  : Ну да, если вы удалили какой-то фрагмент сети, это как проинсульта, то есть это все исчезло, исчезла память, исчезла структурная возможность сети и так далее. Обычно это, конечно, используется тогда, когда уже понятно, что эти связи не понадобятся. Если у нас в архитектуре, допустим, нужен бинарный нейрон или нейрон на определенные мощности, на 8 объектов, если он собрал эти 8 объектов или 2 объекта, уже все остальные просто нельзя. к нему подключать, иначе все это будет как бы портиться. Поэтому обычно мы, конечно, удаляем в таких случаях. То есть если удалить кусок, который кодирует полезную информацию, то единственный способ ее, это так же как для инсультника, это будет заново повторить обучение. 

S01 [02:14:25]  : Спасибо. И от Юрия Бабурова вопрос. Какие, в принципе, задачи сейчас картикоморфные сети у вас решают лучше, чем другие модели? 

S02 [02:14:37]  : Мне кажется, глубокие сети вообще не решают задачи логики, поэтому тут не лучше. По-моему, логика в глубоких сетях может только имитироваться определенным образом статистически. называют имитационная рефлексия, имитационный искусственный интеллект, как и все глубокие сети отчасти. 

S01 [02:14:59]  : Вопрос, что практические задачи? 

S02 [02:15:03]  : Логическое мышление – раз. Обучение по ходу разговора – два. Формирование зрительных концептов – три. Наверное, вот эти задачи. 

S01 [02:15:13]  : А какие-то практические примеры, если не бизнес-кейсы? 

S02 [02:15:19]  : Но, в принципе, картикоморфные сети у нас сегодня включены в основное тело разработки и они в наших диалоговых системах как модули присутствуют. То есть они принимают сигналы, возвращают сигналы, они могут обмениваться сигналами с рекуррентными сетями, но они встроены в решение. 

S01 [02:15:37]  : А каких-то попыток у вас не возникало сравнения, допустим, с какими-то бейзлайнами по каким-то задачам, по каким-то тестам? 

S02 [02:15:48]  : У нас одна из проблем, что мы пока особо не участвовали в конкурсах различных. Есть же масса конкурсов. Мы знаем, допустим, F1 в наших для глубоких сетях. Мы когда-то выигрывали конкурс Сбербанка несколько лет назад. первую еще мы потом как бы перестали участвовать у нас там руководитель сбера вошел в качестве акционера один из руководителей сбера и мы как бы все стали участвовать в конкурсе я шучу конечно значит поэтому мы вот один раз побеждали в конкурсе сбербанка по моему году в семнадцатом или где-то там И после этого как-то особо в конкурсах перестали принимать участие. Наверное, это наша ошибка, но у нас существует один показатель. Мы его закладываем в договора с заказчиками. Это точность работы сети. То есть точность работы сети это процент правильных реакций. У нас есть автоматическое тестирование по тестирующей выборке, которую мы получаем на сотнях тысячах примеров. Есть субъективное тестирование людьми, но на практике это разворачивается в шоу тестирования заказчиком. Заказчик садит определенное количество людей, они задают определенное количество вопросов. Вот, допустим, Ростелеком тестировал нашу систему по коронавирусу. Они задали там около 6 тысяч вопросов вручную. И получили процент по нашей сети 93% точности. Для нас в бизнесе актуален только процент точности сети и мы на него как бы заточены. 

S01 [02:17:17]  : Спасибо. Вопрос от Дмитрия Свериденко. Я его не очень понял. Звучит он так. Почему речь шла о первой весне искусственного интеллекта? Куда делась весна экспертных систем 70-80-х годов? 

S02 [02:17:34]  : Наверное, весна экспертных систем никуда не делась. Экспертные системы активно используются, особенно в Соединенных Штатах. Я знаю, что там около 50 тысяч экспертных систем в медицине. Они как работают, так и работают. Но для меня искусственный интеллект – это не нейросети. Все остальное – это какая-то недоразумение. Это не искусственный интеллект для меня. Это моя просто акцентуация неправильная. Я понимаю, что есть масса методов в искусственном интеллекте, но для меня интеллект – это нейросети. Все, что не нейросети – это чепуха. Это мой личный подход. 

S01 [02:18:06]  : Вопрос только в том, какие это нейросети. То ли примитивная типа DL, либо кортикоморфная, вроде как ваши, да? Ну да, да, конечно. 

S02 [02:18:15]  : Они все равно глубокие. Вообще, главная вспышка искусственного интеллекта – это работа МакАлака и Пицца. Вот главное достижение. Все остальное потом – это уже бредная копия. 

S01 [02:18:28]  : Спасибо. Вопрос от Бабарыкина Евгения. А есть ли у Вас какой-то открытый API, чтобы поработать с Вашими сеточками? 

S02 [02:18:37]  : у нас есть масса достаточно как бы сайтов и телефонных номеров по которым можно пообщаться с нашими системами я готов вам они в презентации же были там указывается как раз и в принципе я готов выслать на почту или наберите меня после разговора или в телеграме найдите как бы там мой телефон есть как бы данные не секретные поэтому я вполне сброшу эти адреса чтобы вы могли все потестировать вот мои координаты если видите так а где а вот сейчас слайд виден или не виден нет нет да если можно видимо надо снова расшарить экран сейчас одну секунду желающие сделали скриншот сейчас я попробую сделать одну секундочку 

S01 [02:19:31]  : сейчас давайте следующий вопрос я походу сейчас на самом деле вопросы все то есть я так понимаю что у нас там был еще вопрос который я честно говоря не очень давайте пока вы ищите его найду вот вопрос докладчику он обучает свою сеть запоминать Александр, если есть, который этот вопрос задавал, может быть, вы прокомментируете? Коллеги, виден экран, да, появился? Да, слайд есть. Пока Александр задает, уточняет, что подразумевает под вопросом докладчику про обучение сети запоминать, значит, желающие могут сделать скриншот со всеми контактами. Александр? Ну, видимо, вопроса нет. Соответственно, можно предположить, что всем всё стало понятно. Коллеги, спасибо вам огромное. Вадим, огромное спасибо. Было очень интересно. А вам спасибо. Вам спасибо. 

S02 [02:20:35]  : Вот я бы сказал ничего не утаивая про все приятности и неприятности наших разработок. 

S01 [02:20:41]  : Спасибо вам огромное. Спасибо большое. До связи. Всего доброго. До свидания. До свидания. 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
