## 20 октября - Язык MeTTa как основа платформы OpenCog Hyperon для AGI - Алексей Потапов — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/F5tjiuwccNc/hqdefault.jpg)](https://youtu.be/F5tjiuwccNc)

Суммаризация семинара:

На семинаре был представлен доклад Алексея Потапова, посвященный языку мета для OpenCog Hyperon. Доклад заключал в себе обзор развития проектов Singularity.net, в частности, создание языка для программирования AGI. Алексей начал свое выступление с объяснения, почему он хотел глубоко проникнуть в тему, объясняя ее отношение к AGI. Он поделился своим опытом работы в группе компьютерного зрения и аутсорсеров, где имел опыт применения принципа минимальной длины описания для улучшения прикладных методов.

Алексей также упомянул свою работу над книгой, посвященной применению принципа минимальной длины описания в компьютерном зрении и машинном обучении. В ходе доклада были затронуты вопросы минимальизации длины описания и ее связи с предсказуемостью и неожиданностью в системах. Алексей подчеркнул, что минимизация длины описания не всегда прямо связана с минимизацией самой длины описания, учитывая наличие шума и сложности моделей.

В ходе семинара обсуждались также вопросы специализации программ и метавычислений, а также идеи, связанные с универсальной индукцией и специализацией универсального движка вывода. Алексей отметил, что человеческий мозг способен работать параллельно с адаптивным резонансом Гроссберга и что такой подход может быть применим и в вычислительных процессах.

Кроме того, были затронуты темы вероятностного программирования, включая вероятностные зависимые типы и вероятностные зависимые типы в стиле non-well-founded, а также рекурсивные схемы и метавычисления. Алексей поделился, что его работа в этом направлении еще экспериментальна и требует дальнейших исследований, особенно в вопросах эффективности.

В конце доклада Алексей отвечал на вопросы, касающиеся прогресса в области логических схем и сравнения с классикой 80-х, а также о том, как его язык мета отличается от Prolog. Он упомянул о сложностях интеграции Prolog с большой базой знаний, о нейросимвольной интеграции и о возможностях создания самомодифицируемых программ на его языке.

В заключение Алексей отметил, что его язык мета позволяет делать вещи, недоступные на Prolog, такие как интеграция с большими базами знаний и нейросимвольная интеграция. Он также сообщил о том, что его синтаксис может быть заменен на другой, если это будет предпочтительно сообществом разработчиков.







S03 [00:00:06]  : Коллеги, всем доброго вечера. У нас сегодня в гостях снова Алексей Потапов, которого мы долго сюда звали. Примерно год назад Алексей рассказывал нам более раннюю стадию разработки компании Singularity на проекты Singularity.net по созданию языка для программирования AGI. Ну а сегодня, как я понимаю, будет уже более продвинутая версия. Проработки этой темы. Алексей, пожалуйста, вам слово. 

S01 [00:00:37]  : Здравствуйте всем. Насчет долга звали. К сожалению, время семинара все время выпадало на наши регулярные созвоны. Сейчас это немного изменилось, поэтому появилось больше возможностей. Но, собственно, есть, в принципе, что рассказать. да, сегодня мой доклад посвящен языку мета для OpenCoke Hyperion. я бы, наверное, начал там без слайдов. может, это будет не очень Весело, но все-таки хотелось бы очень пояснить, как я докатился до такой жизни, что в рамках AGI стал реализовывать такую штуку, потому что если я просто начну, как есть, рассказывать про язык, наверное, будет не вполне понятно подоплек, к чему все это, какое отношение это вообще имеет к AGI. потому что к иджа это имеет сильное, но далекое отношение. соответственно, если совсем издалека начинать, то в конце 90-х я стал общаться на тему иджа и сначала в российском сообществе. Параллельно с этим я работал в группе компьютерного зрения аутсорсеров, и мы на практике решали различные задачи в то время классическими методами компьютерного зрения, немного машинного обучения. и так далее. я заметил, что такая штука как минимальное длинное описание очень полезна для того, чтобы улучшать прикладные методы компьютерного зрения. вместо разных регуляризации, тогда регуляризация Тихонова, например, была популярна, можно все красиво представлять в теоретико-информационных терминах, там в каких-то методах это была просто взаимная информация, в каких-то это минимизация размеров графов, но в итоге все сводилось к тому, что есть некий общий критерий, которые реально можно использовать для улучшения кучи методов. я про это даже целую книжку написал, чисто про применение принципа минимальной длины описания в разных узких методах и компьютерного зрения, и машинного обучения. речи НЛП. ну и по ходу дела, пока я все это изучал и применял, наткнулся на индукцию по Соломонову. и она как бы раскрывала глаза на то, почему узкие методы остаются узкими. но в то же время она оставалась напрямую неприменимой на практике, потому что предполагало перебор в алгоритмически полном пространстве, было непонятно как это делать. я, соответственно, какое-то время пытался перебросить мостик между прикладным использованием принципа минимальной длины описания и универсальными методами типа индукции по Соломонову, там еще в части модели IJI AXE хуторовская тоже присутствовала, которая была еще менее практичной, чем универсальная индукция. придумал принцип репрезентационной минимальной длины описания, который говорил, что в принципе узкие методы отличаются опорной машиной, которая, соответственно, является не тюринг полный для узких методов, и за счет этого позволяет оптимизировать этот универсальный критерий. эффективно, и это в принципе красиво позволяло все свести узкие методы к какой-то единой схеме, но все равно не решало проблему, откуда эти представления берутся, как их выучивать и так далее. как-то вот по ходу дела я наткнулся на вероятностное программирование и понял, что в рамках вероятностного программирования люди переизобретают универсальную индукцию фактически. то есть развитие вероятностных методов само по себе тоже достаточно интересное, там все какие-то махнатые годы 20 века начиналось с наивного байоса, потом люди начали использовать байосовские сети, потом там не параметрические байосовские модели, которые делали следующий шаг в сторону общности, ну и, наконец, изобрели вероятностное программирование, в котором, соответственно, модели уже ничем не ограничены, кроме как вычислимостью. вот, соответственно, я с чисто таких математического анализа универсальных методов переключился на вероятностное программирование, потому что, в принципе, это было то же самое, только можно было не формулы писать, а программы, и смотреть, как это все работает на практике. ну и несколько лет достаточно активно занимался вероятностным программированием. там оно где-то в 2014 году приобрело локальную некую популярность благодаря проекту DARPA о том, что вероятностное программирование может стать новым словом в машинном обучении. Был всплеск работы на эту тему, и люди, в принципе, тоже пытались решить проблему эффективности вероятностного программирования. Но там все, к сожалению, сводилось к тому, что вот у нас есть простое сэмплирование, дальше мы его пытаемся как-то понемножку соптимизировать. сначала это просто слепое сэмплирование из априорных распределений случайных величин, потом мы над ними делаем вычисления, еще что-нибудь там сэмплируем. долгое время большинство движков вероятностного программирования сводились просто к MCMC, Monte Carlo Markov Chains, в частности MH Metropolis Gastings вариант, который вместо того, чтобы там сэмплировать каждый раз из априорного распределения, брал последний сэмпл и считал условное распределение относительно него. ну и в принципе это так чуть поэффективнее работало, но оно там часто в модах застревает и не всегда хорошо покрывает пространство вероятностей. но и главное, что оно при этом все равно имеет проблему найти первый сэмпл, то есть проблема поиска первого сэмпла, который удовлетворит всем условиям, которые накладываются в процессе сэмплирование вероятностной программой, там эта проблема ни в какую не решается. был ряд работ, которые пытались строить граф из программы с узлами, со случайными переменами и анализировать этот граф, но это как-то не приводило к каким-то универсально применимым результатом. какие-то методы... я, наверное, сейчас уже не покрою всех вариантов того, что предлагалось. в группе вудса в оксфорде. вот они там именно пытались в каком-то параметризованном виде представить семплеры сами. ну и там тоже люди занимались тем, что пытались выучивать семплеры. но у них, на мой вкус, все равно достаточно ограниченное представление для семплеров было. выучивать семплеры это, конечно, поинтереснее, но это как раз вот уже в сторону выучивания представлений по теоретико-информационному критерию, например. но оно тоже натыкается на ту проблему, что перебирать программы очень-очень тяжело, и перебирать программы посредством генеративных вероятностных моделей, выраженных в самих вероятностных программах, ничуть не легче, потому что как бы все равно проблема поиска не решается, а проблема поиска там исследовалась в искусственном интеллекте в рамках других методов, это и генетическое, вероятно, генетическое эволюционное программирование, вот, и ряд других. соответственно, я сам рассматривал некоторые варианты оптимизации вероятностного программирования тоже там. ну, кстати, параллельно там были идеи о том, что вот у нас есть генеративная модель, задающая распределение вероятности, и мы можем там параллельно, например, нейросетку выучить, которая будет предсказывать по заданному набору данных значение вероятностных переменных случайных, то есть фактически к генеративной модели добавлялась дискриминантная ну и тоже это в каком-то виде работало. и в принципе, если так посмотреть, то в этом есть определенный смысл, потому что Ну, человеческий мозг тоже умеет и в одну сторону обработку информации делать, от данных к значениям переменных и в другую. И зачастую эти процессы в стиле адаптивного резонанса Гроссберга, они работают совместно. параллельно с этим мой коллега Виталий Кудабахшев как-то съездил разок на конференцию PGI и предложил, ну в смысле он там не раз был, но вот первый раз когда он там был с докладом, он обратил внимание на концепцию метавычислений и на то, что специализация программ, то есть построение эффективных проекций и функций на фиксированные значения переменных, является тоже как бы достаточно актуальным с точки зрения hai, потому что действительно, если мы посмотрим на вычислительные процессы, то если человек первый раз, допустим, собирает кубика рубика или играет первый раз в шахматы, он будет просто перебирать все возможные варианты, неэффективным образом, но общим. и, соответственно, постепенно будет происходить, кто-то скажет, обучение, кто-то скажет специализация универсального движка вывода. соответственно, мы можем сказать, что человек, получившись играть в шахматы, фактически строит специализированную проекцию своего общего решателя на конкретную игру. и вот эта идея в принципе ложится также и на универсальную индукцию. это в принципе какой-то такой кусочек мозаики, которого мне не хватало когда я думал о принципе репрезентационной минимальной длины описания, потому что фактически нас интересует не только опорная машина, представление информации, но и процесс вывода. и вот этот фактически узкие методы машинного обучения, это не просто методы, которые там работают на каком-то ограниченном представлении, а это еще и результат специализации универсальной индукции на соответствующее представление. то есть сам процесс вывода является проекцией общего метода вывода на соответствующее представление. конечно от этого не сильно легче, в том плане что процесс специализации там люди как-то его осилили для компиляторов, когда мы берем два языка программирования и специализируем специализатор. в результате получаем компилятор, эти пресловутые проекции Футамура Турчина и так далее. но там не происходит, при этом ничего не делается с экспоненциальной сложностью. то есть там происходит фактически линейная оптимизация, она-то может быть в 100 раз ускореннее, но оно все равно не зависит от размерности задачи, а тут у нас в общем-то вопрос о том, как из NP-полной, а то и алгоритмически неразрешимой задачи синтезировать программу, которая будет работать за линейное или хотя бы полиномиальное время. поэтому это тоже как индукция по Соломонову, не столько решение проблемы, сколько указание на эту проблему, что с ней делать и так далее. вот в общем я пытался в рамках вероятностного программирования в явном виде заниматься специализацией движка вероятностного программирования в ну и нейросетки и обучаемые генетические алгоритмы. это в принципе приводило к некоторым интересным результатам, но все равно чего-то не хватало. вот как бы если мы посмотрим на анализ программ там работает какой-то фиксированный алгоритм. если мы посмотрим на какие-нибудь там мета-эвристические методы оптимизации, там тоже работает фиксированный алгоритм. каждый раз, когда мы решаем какую-то практическую задачу, мы все-таки достаточно сильно меняем наш inference, наши методы вывода под эту практическую задачу. и то же самое относится к представлениям. фактически проблема метавычислений, она не решается как бы в замкнутом виде, то есть вообще тут можно еще вспомнить Шейна Лэгга с его диссертации, которую он у Хуттера писал, где он говорил о том, что существует вычислимого универсального предиктора. на самом деле там, кто знает Ирая, например, от Скурала, он по этому поводу достаточно радикально высказывается. насчет этих результатов, их осмысленности и значимости. Но на самом деле суть в том, что если мы сначала зафиксируем предиктор, вычислимый, то мы всегда сможем к нему подобрать такой вход, на котором он не сможет работать. И в принципе из теории метавычислений там тоже много подобных результатов. То есть у нас получается, что не может в принципе быть какого-то фиксированного движка для универсального или общего искусственного интеллекта. Он должен расширяться. когда мне говорят, что вот мы сделали интерпретатор вероятностного программирования, который там будет решать все проблемы, это неправда. он не будет никогда, фиксированный интерпретатор, решать все проблемы. то есть он может решать какой-то класс проблем, но если он не будет расширяться, не будет способен к этому, то там всегда найдутся какие-то проблемы, на которых он будет работать плохо. То есть работать плохо – это само по себе не фатально, потому что люди тоже поначалу достаточно сильно тупят при решении разных задач. но все-таки способны расширять границы своих возможностей, то есть не обладают фатальным ограничением. Так же, как слабые методы обладают фатальным ограничением, например, машинного обучения тем, что их представление информации в принципе ограничено, также и любой движок вывода, фиксированный, он либо будет очень медленным, либо будет обладать такими же фатальными ограничениями, то есть будет принципиально не способен к каким-то решениям, решениям каких-то задач. соответственно, тогда акцент уже смещается с конкретных представлений информации, с конкретных алгоритмов вывода на то, что же это такое мы там хотим расширять. если смотреть на конкретные практические примеры применения того же вероятностного программирования, то оказывается, что то, что мы хотим расширять, это знания. это не обязательно эксплицитные знания, в принципе, это может быть какая-то другая форма памяти, но в итоге оказывается, что когда люди занимались анализом программ для вероятностного программирования, для повышения эффективности вывода в вероятностном программировании, они пытались придумать фиксированный алгоритм для этого, а это не получалось сделать, потому что нельзя это сделать в принципе. но можно хотя бы попытаться не закладывать принципиального ограничения на то, чему такая система сможет быть способна. и в общем получилось так, что вот если мы берем там какой-то вероятностный язык программирования, вот у нас есть программка какая-то и есть, ну я не знаю, там представляет кто-нибудь какую-нибудь вероятностную программу или нет, есть там какие-то условия, которые мы накладываем на результаты, ну да, я верю, что Виктор представляет вероятностные программы, вопрос насколько это характерно для всей аудитории, но неважно. там есть моменты сэмплирования, есть условия, которые мы впоследствии накладываем, и мы зачастую хотим эти условия распространять назад. если мы это просто пытаемся делать каким-то фиксированным захардкоженным алгоритмом в движке вероятностного программирования, понятно, то это не работает. а что будет работать? получается, на самом деле, что вот этот анализ вероятностных программ фактически выполняется неким дедуктивным движком. свою очередь. То есть мы смотрим на те ограничения, которые мы накладываем и понимаем, например, что такую-то переменную нам сэмплировать бессмысленно. Как мы это понимаем? За счет уже более такого дедуктивного вывода, ну фактически на основе знаний. Тут, конечно, сильный вопрос в области, на которой мы это рассматриваем. потому что если это будет обработка изображений, начиная с пиксельного уровня, там, конечно, каких-то явных символиных знаний не будет. но если мы возьмем книжку Тененбаума по Чорчу, вероятностные модели познания, то там большинство примеров из области когнитивистики, то есть через вероятностные программы моделируются достаточно высокоуровневые когнитивные процессы. И там в явном виде те случайные переменные, с которыми идет работа, они связаны с эксплицитными символьными знаниями. но, опять же, тут можно обсуждать, насколько вообще хорошо заниматься символьным искусственным интеллектом или не очень, или все надо в нейросетки пытаться запихнуть. сейчас проблематику символьной против нейросетевого я не буду обсуждать, иначе мы очень далеко уйдем. но в целом я как бы в своем пути занятий hi пришел фактически к тому, что нужно представление знаний, то есть фактически у нас вместо просто какого-то фиксированного движка индуктивного вывода там сэмплирования или еще чего-то там генетических алгоритмов внутри интерпретатора языка вероятностного программирования Должна быть система представления знаний. То есть эта система должна быть способна расширяться. Мы не просто там написали какую-то программку и выкинули. Это должна быть именно система, которая способно свой интерпретатор расширять этими знаниями, за счет вывода над этими знаниями получать все большую способность к более эффективному решению разных задач. понятно, что это можно программировать ручками. в идеале хотелось бы, чтобы система сама делала вывод таких программ, эвристик или еще чего-то нужного для вывода в разных доменах. Но это уже как бы следующий вопрос. И вот таким образом я фактически пришел к тому, что когнитивные архитектуры все-таки нужны. То есть долгие годы я, честно говоря, достаточно как сказать, ну не то чтобы негативно, а без особого интереса относился к когнитивным архитектурам, потому что зачастую, когда там на них смотришь, люди говорят, вот Нужно память, внимание, то да сё. Нужны такие блоки. Мы сделаем эти блоки, и всё станет хорошо. Какова суть этих блоков? все гораздо более мутно и туманно. и зачастую они из слабых методов клепаются. и, соответственно, если мы из слабых компонентов попытаемся собрать когнитивную архитектуру, но она эйджаем не будет. поскольку люди в когнитивных архитектурах чаще, по крайней мере, в то время рассуждали о том, какие должны быть блоки, вместо того чтобы решать проблемы универсальной индукции, например, или там еще чего. вот это было как-то не очень интересно. но в целом нужна когнитивная архитектура с точки зрения того, чтобы это была именно система, которая могла бы развиваться, пополняться знаниями и незнаниями, как это называть... и так далее. и в какой-то момент я пересекся с Беном Герцелем, ну в смысле мы с ним часто пересекались на конференциях HI, но в какой-то момент мы вдруг как-то законтачили с точки зрения того, что возникло какое-то взаимопонимание. Я вдруг обнаружил, что там тот, ну у него система OpenCock, наверное, все знают, вот тот OpenCock, который реализован, это на самом деле не совсем то, что он бы хотел видеть. то есть это реализовано то как получилось это текущий момент времени того что успели сделать и как бы его видение того, что надо делать, оказалось на удивление близким к моему, хотя мы пришли к этому достаточно с противоположных, что ли, концов, то есть он исходно больше... не, ну хотя тоже вопрос, потому что он еще в девяностые годы говорил там про алгоритмические критерии для роста паттернов и так далее. но впоследствии он скорее больше к логицизму тяготел. в итоге возникли какие-то планы о том, чтобы попробовать вероятностное программирование скрестить с OpenCog. ну тогда единственной версией существовавшей opencog. в opencog были очень любопытные компоненты типа универсальной, унифицированной машины правил, которая в общем-то весьма подходила как раз для того, чтобы делать некие рассуждения, вывод над какими-то выражениями, что ли. opencog работает над метаграфом. если мы говорим, например, о том, что мы представляем, например, вероятностную программу в форме какого-то графа, то opencog позволяет делать трансформации этого графа, то есть трансформации самой программы на основе произвольных правил, которые также задаются в форме графов и могут синтезироваться или задаваться вручную. это уже следующие вопросы. в итоге мы начали работать с OpenCog. до вероятностного программирования мы не вполне дошли. но мы начали активно заниматься нейросимвольной интеграцией в рамках OpenCog, то есть вот эти вот узлы метаграфа Grounded в нейросетках, эти нейросетки там могли комбинироваться на лету в процессе резонинга, то есть такие модульные нейронные сети, которые синтезировались не ad-hoc алгоритмами, а вот с помощью резонинга над какими-то знаниями. ну и пытались участвовать в разных применениях OpenCog, но в конце концов поняли, что все-таки текущая версия OpenCog оказалась в некоторых аспектах неудобной. вот опять же я не буду глубоко вдаваться в мотивацию перехода от OpenCog классического к разработке своей версии там это был до сих пор непонятно насколько правильный или сомнительный шаг мы это обсуждали с Беном и коллегами года два понемножку склонился к тому, что да, надо все-таки разрабатывать чистую версию и пытаться в нее заложить как бы новое понимание проблематики AGI в частности. к этому еще примешались разные идеи из современного компьютер-сайенс и математики. идея того, что общем, в теории зависимых типов там propositions, они как бы в виде типов формируются, а доказательство некоторого утверждения делается через предъявление экземпляра этого типа, то есть это такой изоморфизм Кари Горварда, в таком очень конкретном материализованном, что ли, виде. То есть когда мы говорим, что у нас между логикой и программами есть прямая связь. ну ладно, это тоже может далеко нас завести, если мы это начнем обсуждать. в общем, хотелось соединить все и вся. это немножко увело от начальной мотивации с вероятностным программированием, но при этом получилось довольно интересно с концептуальной точки зрения. ну и, наверное, я я могу показать какую-нибудь презентацию, слайды, которые мы делали еще до начала реализации. или сразу перейти к практическим примерам. 

S03 [00:33:29]  : Алексей, мне кажется, имело бы смысл показать презентацию, что хотели, чтобы было, что хотели, а потом, что получили. Мне кажется, было бы интересно. 

S01 [00:33:41]  : ну давайте, у нас в принципе времени довольно много, я думаю, что мы все успеем. так, ну вот презентация 2020 года, которая была на openpoc.com. это были предварительные мысли об opencock гиперони, тогда еще названия мета не было, использовалось название, ну фактически языка программирования из opencock classic он назывался и называется atomis. ну и помимо определенных прикладных соображений, почему мы хотели перейти к разработке opencog гиперона, я бы показал какие-то идеи, которые лежали в самой основе мета. OpenCog, там в самом ядре OpenCog находится Atomspace, это гиперграфовая база данных фактически, ну или база знаний, такое гиперграфовое хранилище, над которым может выполняться паттерн-матчинг. И мы, соответственно, в принципе, можем делать запросы к Atomspace, ну, как сами по себе, то есть они достаточно юзабельны, как есть, без каких-то дополнительных оберток и методов над ними. соответственно, в какой-нибудь традиционной графовой базе данных у нас будут запросы, и аналогичные запросы мы можем формировать в katam space. за тем исключением, что классические графовые базы будут в основном содержать триплеты, связи между парами узлов тогда, как в опенкоговском паттерн-матчинге у нас работают в принципе отношения произвольной арности, ну и прочей хитрости. то есть там модель как бы немножко другая. фактически я не знаю насколько это релевантно и интересно, но фактически графы складываются из триплетов. то есть это узел, ребро, узел. если же мы вместо простых триплетов или бинарных предикатов будем использовать произвольные деревья, то у нас аналогом связи в таком графе будет не просто какая-то дуга, описываемая там меткой над собой соединяющая два узла, у нас связью будет дерево, и это будет гиперграф. И что интересно, когда мы пишем просто какую-то программу, мы как раз пишем набор выражений, представляющих собой произвольные деревья, и вместо того, чтобы программу рассматривать как лес таких деревьев, мы можем каждое выражение рассматривать как некоторую металинку в метаграфе. но это на самом деле вопрос представления. эти представления эквивалентны. просто если мы не хотим делать дедубликацию узлов и связей, не хотим делать эффективное индексирование и квирирование программ, выражений в программах, то мы можем вполне работать просто со списком деревьев, вместо того, чтобы считать, что это метаграф. то есть в метаграфии, ну в этом нет, на мой взгляд, какой-то философии. но просто так получается, что если мы хотим все и вся проиндексировать в нашей программе, то удобно сказать, что это метаграф, и мы именно в нем ищем какие-то выражения по заданным паттернам. то есть первый момент это квирирование графа или метаграфа знаний. Второй момент – это рассуждения. В OpenCog Classic рассуждения выполнялись с помощью унифицированного движка правил. Эти правила задавались как… тогда это называлось bind-линки. они соответственно содержат паттерн для паттерн матчинга и некоторый результат трансформации с матчиного паттерна. это оказывается весьма удобно в том плане, что мы можем на этой основе моделировать произвольные рассуждения, мы можем задавать правила в очень отхок виде, то есть вот правило рассуждения о том, что кто-то лягушка может быть задано в таком квази-императивном ключе, и при этом мы будем там способны через фактически, что делает унифицированный движок правил, он просто чейнит запросы к атомспейсу. ну там не совсем просто. нил, который его разрабатывал, отталкивался от идеи реализовать бенновские probabilistic logic networks, поэтому они там работают с вероятностями, не помню как они там, не четкие вероятности, не точные вероятности, там либо пары значений, либо вообще четверки, которые там для common sense reasoning представляются достаточно удобными. поэтому unified rule engine – это не просто чейнинг, там есть дополнительные хитрости, но в целом можно сказать, что в основе рассуждений лежит соединение последовательных запросов к метаграфу знаний. мы следующим шагом вместо каких-то базовых правил делаем просто запись в базе знаний, а уже правилом является общее правило, например, логики, скажем, транзитивность импликации или там modus ponens или еще что-нибудь. то есть у нас в декларативном виде уже хранится знание о том, что Если кто-то квакает и ест мышей, то он лягушка, а если он лягушка, то он зеленый. А уже общим правилом будет правило логики, и мы в принципе вольны тут писать абсолютно любые правила, какие захотим. То есть мы не привязаны к конкретной форме логики, гипотетически. при этом мы пытались унифицированную машину правил применять, может быть, не совсем предназначенным для этого образом. например, есть правила дифференцирования. мы хотим сделать символное дифференцирование на движке opencog. Оказывается, что это не очень удобно, что вроде как это не совсем ризнинг, хотя вроде как это просто переписывание символьных выражений. То, чем занимается тот же самый байнтлинг. то есть он, по идее, должен из одного выражения порождать другое выражение. и в принципе унифицированная машина правил должна быть способна чейнить эти правила для того, чтобы перейти от начального выражения или там запроса к конечному выражению, но на практике вот как раз те особенности unified rule engine, о которых достаточно сложно без очень конкретных примеров и без знания разговаривать, они слегка помешали этому. чтобы в принципе такое работало, приходилось задавать правила дифференцирования гораздо более декларативным путем. то есть мы не просто говорим, что преобразуй нам производную от x плюс y в производную от x плюс производную от y. нет, мы должны сказать, что если у нас градиент одной переменной редюсится в некоторое выражение и градиент другой переменной редюсится в другое выражение, тогда градиент от их суммы будет редюситься в сумму этих выражений. получается задом наперед. Но если мы возьмем функциональное программирование и посмотрим на это дело, то чем это отличается от равенства в функциональном программировании? Фактически ничем. Вот одна из проблем OpenCock Classic, его языка Atomis и Unified Tool Engine, в том, что там такие функциональные вещи задавать крайне неудобно. было очень сложно контролировать процесс вывода, но фактически то, что делает функциональное программирование, оно также берет и переписывает некоторые выражения. И это очень похоже на рулы в Unified Rule Engine. Возникает идея объединить, вообще говоря, квалифицирование баз знаний, ризнинг и функциональное программирование заодно, потому что оно бывает очень удобно. То есть фактически функциональное представление и логическое представление, они взаимозаменяемы, то есть меняется только слегка форма. Одно выглядит чуть более, может быть даже не императивным, а именно функциональным, а другое чуть более декларативным, хотя в общем-то если читать одно задом наперед, получается как раз абсолютно то же самое, что и другое. тут было много чего еще интересного. там есть такая штука как ConstraintLogicProgramming и какие-то другие мы вещи также рассматривали. вероятностное программирование в принципе тоже сюда относилось. вот он там query on a church, то есть мы задаем некоторые случайные переменные и накладываем условия. и в принципе по форме это опять же по форме не сильно отличается от query в Atomisia, OpenCock Classic. То есть мы тоже задаем переменные. Единственное, мы не говорим, откуда они сэмплируются. Мы просто говорим, что может быть у них там тип какой-то есть. И дальше мы задаем саму query. Только понятное дело, что такого форма квыря к метаграфу знаний не даст нужного результата, если у нас в этом метаграфе в явном виде не прописано, что там какое-то конкретное число больше, чем там троечка. но вот то, что форма квырей в функциональном вероятностном программировании и в атомизия практически идентичная, это тоже наталкивало на мысль, что это все можно попытаться унифицировать. Ну там было еще много разных соображений про типы, про и про теорию категорий. и на самом деле от теории категорий мы там скорее перешли к гомотопической теории типов, потому что фактически все что мы задаем это преимущественно равенство между какими-то термами. я думаю что, во-первых, я сам уже не до конца помню, что в этой презентации там два года назад было. хотя тут были опять же тоже интересные вещи, например, связанные с тем, что мы там какие-то рекурсивные процессы можем декомпозировать и представлять, например, в виде катаморфизма. то есть мы декларативно описываем нечто, а дальше на этом работает единый рекурсивный процесс. если мы этот единый рекурсивный процесс берем и там разбиваем на маленькие шажочки, мы в принципе вот этот вот чейнинг unified rule engine, например, мы можем уже представить тоже в каком-то более формализованном виде и рассуждать о нем как о какой-то математической сущности. мы это отчасти сделали, но практически соображения превалировали над математикой в этом ключе, но вот эта идея того, что мы разбиваем все на отдельные запросы к базе знаний и фактически можем представлять чейнинг этих запросов как катамархизм над алгеброй, заданный собственно в знаниях. это в принципе тоже сыграло какую-то роль. у Бена там отдельные соображения были насчет того, чтобы различные рекурсивные схемы применять к метаграфу и там вплоть до футуморфизмов реализовать, это будет достаточно для большинства когнитивных алгоритмов. но и вообще у него там были дополнительные идеи про то, что большинство когнитивных алгоритмов, с которыми мы работаем, в принципе может сводиться к чему-то вроде динамического программирования, но это тоже отдельная тема. я сегодня хочу просто поговорить о базе. то есть того, что мы в результате создали, ну и то, что находится, продолжает быть в процессе создания. А до VGI мы сами не дошли до работ скажем так ajrnd на этой базе, только подбираемся к ним. поэтому все эти более продвинутые вещи, наверное, сейчас преждевременно пытаться описать. поэтому я сейчас перейду собственно к описанию мета. это язык программирования. условно говоря когнитивного, который пришел взамен атомиза из OpenPOC Classic. И вот если в Atomisia, если в OpenCock Classic паттерн-матчер фактически все больше вбирал в себя функции интерпретатора вместо того, чтобы просто делать статический паттерн-матчинг, то в Meta у нас эти две вещи разделены. То есть статический паттерн-матчинг – это маленькая, но она далеко не всегда маленькая по времени, но все-таки ограниченная нерекурсивная операция, которая представляет собой что-то вроде одного шага в когнитивном процессе, можно сказать элемент когнитивного цикла. Хотя опять же про В моем первом очень ограниченном прототипе я даже предусматривал возможности были примеры того что программа меняет сама меняет ход своего вычисления то есть как бы после какого-то шага вот этого когнитивного цикла там меняется Например, порядок выражений для последующей эвалуации. Сейчас такие возможности гипотетически предусмотрены, но пока такие примеры не реализованы. идея того, что запрос к паттерн-матчингу не должен вовлекать полноценную интерпретацию программ, а является именно является и именно какой-то ограниченной базовой операцией, а все остальное переносится в интерпретатор, который в каком-то смысле сливается с unified land. вот это различие оно остается в текущей версии. давайте я покажу вживую, как это работает. сейчас, наверное, надо увеличить шрифт. синтаксис Это дело как бы десятое. Многим хотелось Haskell-подобный синтаксис, но из-за того, что у нас не хватает ресурсов на разработку, фактически сейчас разработкой занимается только один человек, вот вопросы синтаксиса мы задвинули на задний план и использовали самый простой lisp подобный синтаксис. это не потому что мы фанаты скобочек, а потому что так было проще. итак, в мете все основано на символах. ну почти все. то есть мы можем писать в принципе любые символьные выражения и они будут валидными. то есть если мы напишем что-то такое, то как бы программа интерпретатор это воспримет нормально. это просто будет запись в нашем графе знаний. мы можем там добавить набор каких-нибудь выражений. их можно трактовать как мета-связи в метаграфе знаний. можно трактовать просто как выражения, которые там линейно хранятся, но сейчас идет разработка распределенного атомспейса, распределенного метаграфа, где все это, например, если у нас встречается там какой-то одинаковый кусочек, он будет дедублицироваться, все эти выражения, имеющие общие части, будут сливаться и образовать именно метаграф, который можно эффективно квалифицировать, но для понимания меты про метаграфы говорить вовсе не обязательно. Соответственно, мы можем посылать запрос к нашей базе знаний. Если мы это делаем в одном и том же файлике, то мы обращаемся к именно контенту этого atom space через self. вот у нас есть match, который запускает pattern matching. переменные у нас в данном синтексисе обозначаются через доллары. и в данном случае match работает На первый взгляд очень похоже на bind-линки OpenCock Classic. Мы можем, например, сделать запрос, какие у нас есть в нашем этом спейсе выражения, у которых некая переменная некий символ или некий атом объединен с лифт-2 или лифт-3 вот в такую структуру. в данном случае восклицательный знак заставляет интерпретатора вместо того, чтобы добавлять это выражение в atom-space, непосредственно его выполнять и в качестве результата мы получаем, что x у нас может быть или лиф 1 или может быть лиф 0, лиф 1. то есть он эти две структуры находит. не знаю, там можно какой-нибудь другой пример взять. преобразованием. это просто символы. это все на текущий момент просто символы. символы могут быть почти любыми. символные выражения это деревья и И мы можем их курировать. В отличие от BindLink OpenCock Classic, который ищет под выражение тоже, нам оказалось удобнее для языка программирования, чтобы матч искал выражение только верхнего уровня. потому что нас не интересует, например, если что-то встречается внутри какого-то другого определения, зачем нам это извлекать. То есть в некоторых приложениях, там паттерн майнинга, например, над какими-то большими графами знаний, например, я не знаю, там генной онтологии, такое может быть полезно, но это можно реализовать с помощью там отдельного матча или какой-то опции для него. ну в данном случае вот он будет преобразовывать whoiswhat в whoiswhat. в результате мы получим SamTheFrog, TomTheCat и SophiaTheRobot. то есть на таком уровне это в принципе работает как bind link в OpenCog. мы можем там, например, это опять же сугубо кастомный символ, то есть это не имеющийся у нас символ. например, сделать такой запрос. то есть, грубо говоря, мы хотим сказать, что выражение greenSem, например, равно true. в данном случае на текущий момент это просто символиные выражения. опять же, мы хотим узнать кто в действительности зеленый. в действительности зеленым оказывается сам. том у нас известно что не зеленый, он при этом белый. если мы добавим побольше переменных, у нас получится, что Sam в действительности зеленый, а Tom в действительности белый. Давайте пойдем дальше. Соответственно, в OpenCog Classic у нас было все завернуто в типы линков. Вот EvaluationLink, PredicateNode, ListLink, ConceptNode и так далее. И во внутреннем представлении это не были такие выражения, то есть это был в действительности один нод, но типа концепт нод. С точки зрения машины читаемости это неважно, с точки зрения человека читаемости это чересчур многословно. И главное, что не очень понятно, зачем эти типы линков нужны. в таком виде. вот если мы пишем это в виде отдельного выражения, почему бы нам в виде этого выражения с ним и не работать. мета такие выражения вполне себе нормально воспринимает. и если мы, например, запустим тоже очень выглядящий, похожий на OpenCock Classic запрос, там evaluation link, predicate node, там list link. И кто же у нас ест мух, то мы выясним, что Sam и Fritz, потому что у нас есть запись, что Sam ест мух и Fritz ест мух. а то есть мета в принципе позволяет писать выражение в виде соответствующем Atomizu OpenCock Classic и в очень похожем виде делать query к ним, хотя под капотом оно будет работать немножко по-другому с точки зрения того, что это будут в действительности не типы узлов и связей, а будут просто самостоятельные символы. это так, не столь существенно. что сильно отличается в мете от opencock classic, это то, что у нас паттерн матчинг, он на самом деле не совсем паттерн матчинг, он сразу уже почти что унификация. то есть мы можем унифицировать query с выражениями в базе знаний, которые также содержат переменные. В OpenCog Classic Этим занимался Unified Rule Engine, и там была специально отдельно написанная процедура унификации. Хотя Unified Rule Engine и опирался на pattern matching, но для унификации приходилось докоживать отдельно. И при том, что Unified Rule Engine не является компонентом ядра OpenPOC, это получается, что немножко неудобно, потому что это вносит сильное ограничение на то, что можно сделать с атомизмом без unified flow engine. то есть в данном случае мы можем написать выражение, что implies frog x green x, например, и спросить что будет имплицировать факт того, что Sam является фрогом, и мы получим, что результаты будут greenSam и itFliesSam, то есть он Sam взял из query и забайндил его с этим x, и вот он, соответственно, забандил с этими выражениями, подставив туда x и получил вполне себе результат. то есть у нас такой двухсторонний паттерн-матчинг, он работает по умолчанию. и на мой вкус это весьма удобно. можем там задать какое-то свое правило, что если у нас green sam это следствие некоторого предиката P над некоторым x, то есть если некое неизвестное выражение имплицирует Green's M, то, наверное, x может быть P, то есть наличие следствия из некоторой посылки дает нам возможность предположить, что посылка сама по себе верна. то есть мы получаем, что не исключено, что сам лягушка, потому что он зеленый. но это так, просто пример двухстороннего паттерн-матчинга. У нас, как и в OpenCog классике, можно составлять более сложный запрос. Перечень условий можно через запятую, например, задавать. Мы можем спросить, что если frog x в данном случае отключим, то результатом будет пустое множество. если мы в базу знаний добавим, что сам это лягушка, то мы можем выяснить, что если фактически это modus ponens, но только в таком очень частном виде. если x это frog, и если то, что frog x имплает некий y, то y мы можем вернуть. получаем, что Сэм зеленый, Сэм ест мув, потому что мы знаем, что Сэм это лягушка, при этом мы тут не задаем, что мы именно про Сэма спрашиваем. Идем дальше. Что с этим можно делать? Можно увидеть, что если мы такие запросы, скажем, выразим как равенство и будем их квырировать по очереди, вот в такой форме. допустим, у нас есть некоторое выражение, и мы спрашиваем, а чему может быть равно это выражение. мы запускаем. мы запускаем и получаем, что это выражение может быть равно вот такому выражению. дальше мы берем и говорим, а чему будет тогда равно такое выражение. Запускаем и получаем, что оно будет равно x. Но если мы дальше запустим такое выражение, мы получим уже пустое множество, то есть x – это конечный результат редукции выражений согласно этим правилам. ну это там правила комбинаторной логики в данном случае это не так важно и теперь мы видим что на самом деле если у нас есть набор равенства заданных например в функциональном стиле то чтобы получить функциональное программирование достаточно просто счинить запросы к базе знаний, в которой сохранятся эти равенства. то есть фактически процесс выполнения программы – это чейнинг запросов, чему равно данное выражение. мы можем, например, арифметику пиана также допустим, добавление нуля дает само число, а если мы добавляем x к следующему числу, это все равно, что добавить данное число к x. если мы, скажем, напишем matchSelf, чему равно addSzSz 1 плюс 1, чему оно равно, мы увидим, что оно равно... это то же самое, что добавить двоечку к нулю. теперь мы ручками пока подставляем это сюда, и в итоге в качестве ответа получаем двоечку, то есть 1 плюс 1 равно 2. то есть такие банальные примеры функционального программирования они через подобный паттерн матчинг работают. поэтому почему бы нам просто не взять и этот чейнинг не заложить в интерпретатор. вот это равно уже имеет специальную интерпретацию в интерпретаторе и вместо того чтобы писать матч что-то мы просто можем так давайте комбинаторная логика не всем удовлетворяема давайте добавим единичку к единичке. еще раз. вот и мы получим двоечку автоматически. то есть фактически интерпретатор просто запускает, формирует query на равенство выражения и чейнит эти query до тех пор, пока не получит нередуцируемое выражение. то есть он работает сугубо через pattern matching, и через этот pattern matching мы получаем выполнение какого-то функционального кода. но мы получаем не просто функциональное программирование, мы получаем гораздо больше. давайте лучше сразу побольше примерчик возьмем какой-нибудь допустим, в стиле opencock classic почти что, мы зададим сугубо символьные выражения о том, что, например, Платон это философ. я не знаю, почему в opencock classic есть такой пример. но тут это просто калька с этого примера показать, что он работает. вот у нас там есть evaluation какие-то, есть импликация о том, что если кто-то философ и там, я не знаю, он любит бороться, то он человек. вот, а если он человек, то он смертный. и мы можем задать функцию, как равенство, которая будет вызывать, например, матч. то, что в принципе в обычном функциональном программировании сделать нельзя, потому что у вас нет доступа к тому паттерн матчингу, который там присутствует, и этот паттерн матчинг принципе, не такой мощный. ну вот, например, мы там берем и выводим, что Платон смертен за счет каких-то правил того, что у нас там. если не знаю стоит ли это вообще подробно разбирать. наверное сложновато. давайте лучше вопрос что-то более понятное посмотрим. опять же это просто какие-то символы и мы задаем равенство что end от true true это true. дальше мы говорим что истинность у фрога равна end от истинности того что он квакает и ест муху. мы можем сказать что фриц квакает, фриц ест мух, а истинность грина вычисляется через истинность фрога. тогда если мы спросим грин фриц, например, то он нам вернет true. в данном случае, если мы спросим зеленен ли SM, он в принципе не вернет фолса, потому что у нас нету равенств для конъюнкции для других аргументов, он просто недоредуцирует выражение. Он скажет, что истинность того, что Сэм зеленен, определяется тем, квакает ли он и есть ли он лягушку этих мух. Что, в принципе, тоже достаточно интересно, потому что это реализует фактически нетотальные функции. и при этом позволяет получать какие-то выражения, с которыми дальше можно при желании что-то другое делать. но не это в общем-то главное. главное то, что мы можем делать вычисления с переменными. например ну это такой небольшой хак мы задаем некую функцию if true then then если мы вычислим попытаемся вычислить функцию if true green x x мы получим фрица. в обычном функциональном программировании вы, конечно же, такого не достигнете. то есть это может выглядеть эзотерично, как это работает, но если понимать, что под капотом это чейнинг паттерн матчинга, то интерпретатор просто создает query вида. какому-то результату. но он фактически там вызывает матч, селф и так далее. вот этот квыря паттерн матчится против этой. соответственно он дальше говорит, что значит green x надо унифицировать с true. дальше он смотрит. на такой запрос равно green x какому-то r, и r оказывается frog x. дальше он создает запрос frog x равно r. frog x это вот такой запрос. Соответственно, дальше он создает вот такой запрос. Он дальше видит, что end будет чему-то равен. Он просто паттерн матчит с этим выражением и говорит, что чтобы оно сматчилось, нужно чтобы r матчилось с true. croc-x, crocs-x, it-flies-x тоже матчилось. Чтобы это произошло, x должен быть фрицем. в итоге он просто здесь в данном случае возвращает фрица в качестве x. то есть концепция вроде как мы просто на первый взгляд реализовали странным образом функциональное программирование, но на самом деле мы получили гораздо больше, потому что мы работаем с унификацией, ну не совсем унификация, двухсторонним паттерн матчингом. что с этим можно делать дальше? дальше у нас в принципе есть недетерминизм. почему он у нас появляется? потому что как мы видели в самых первых наших примерах, как только мы запускаем pattern matching, он нам возвращает много результатов. опять же тут никакой мистики, просто pattern matching возвращает все результаты, которые находят. поэтому если мы вызываем color в данном случае, то мы получаем альтернативные результаты, потому что фактически интерпретатора создает вырю такого вида и r матчится с green, yellow и red. но дальше с этим можно делать много всего веселого. 

S03 [01:20:51]  : Алексей, пока пауза, я сделаю комментарий. Тут у нас уже полет продолжается почти полтора часа. Нам бы хорошо в течение 15 минут максимум 20-30 закруглиться, чтобы осталось время на вопросы. 

S01 [01:21:08]  : Ну, 20-30, я думаю, закруглимся. давайте просто какой-нибудь примерчик напишем. простенький. в результате у нас получится просто набор результатов и, в принципе, при желании это можно уже использовать в сторону вероятностного программирования, если очень захочется. но можно использовать любым другим образом. то есть мы можем детерминированные функции вызывать на недетерминированных аргументах из коробки. давайте это в принципе штука достаточно специфичная. лучше пойдем дальше. дальше у нас есть типы. пытались вообще-то достичь мощности примерно равной зависимым типам. давайте пока про типа тоже пропустим. собственно, что в классическом opencog было не очень хорошо, это то, как он работал с grounded атомами. то есть они там представлялись какими-то такими не очень полноправными сущностями, причина была как раз в том, что паттерн matching все инкапсулировал в себя и фактически там вот, например, в классическом OpenCog был специальный number node. он вводился в ядре OpenCog, и PatternMatcher его знал и выполнял над ним специальные операции. когда мы хотели там расширить какими-то другими, это можно назвать Foreign объекты или Grounded атомы или еще что-то, те же нейросетки, например, то приходилось делать достаточно неудобные вещи. вот в мете у нас grounded символы, они являются полноправными объектами, и мы их можем произвольным образом вставлять в наше выражение. их можно добавлять свободно или в питоне, если мы пользуемся питоновским враппером над метой, или в Rust, или в C++. На текущий момент API для других языков у нас нет. Но grounded символы, они как бы их сразу удобно ввести, вводить с типами. на самом деле это было не совсем сразу сделано, потому что если мы, например, хотим вызвать операцию сложения, которая опять же тоже является ground операцией над неподходящими типами, то мы не должны этого пытаться делать. либо это будет уже какой-то exception на стороне языка имплементации этих grounded символов, либо мы должны это дело обрабатывать как-то раньше. до этого просто exception кидались и интерпретатор их игнорировал. сейчас у нас для grounded символов есть типы и мы можем там например, посмотреть какой тип у единички. в данном случае это просто number. поскольку это из питона, мы не разделяем типы для int, float и так далее. если мы хотим посмотреть s, то это будет string. если мы плюсик посмотрим, то это будет функция из number number в number. соответственно типы также можно задавать и для кастомных символов. и, например, классическая арифметика пиана будет подразумевать то, что там z это, например, NAT, s это функция из NAT в NAT и добавление это, опять же, функция из NAT в NAT в NAT. это позволяет type-чекать соответствующее выражение. если в языках с зависимыми типами на типах делается большой акцент, потому что это используется в proof assistant, например, то в нашем случае вообще говоря типы это именно ограничения на форму выражений, которые мы можем задавать. если мы попытаемся при наличии типов выполнить плохо типизированное выражение, то оно просто вернет пустой результат. с точки зрения интерпретатора пустой результат, если он как-то специально не обрабатывается, это прерывание процесса интерпретации. если мы попытаемся это завернуть в какую-нибудь еще конструкцию, то тут не видно, но она просто не будет вызываться. если при этом выражение хорошо типизированное, оно хорошо типизированное, но у нас для него равенств нет, то оно не будет возвращать пустого результата, будет оцениваться в данном случае в себя, потому что мы для него не задали равенств. если мы эти равенства соответственно введем, выражение уже будет оцениваться в конечный результат. в этом смысле видно, что в мете нет разницы между конструктором типа и функцией. вот пока мы не задали равенство для add, он не редуцировался, то есть выступал фактически как конструктор. Вот для S не указано равенство, поэтому S выступает как конструктор. Это выражение просто не редуцируется. Когда у нас нет каких-то эквивалентностей для каких-то выражений, то они остаются как есть. соответственно, это базовые типы. если я успею, я чуть-чуть попозже поговорю про зависимые типы. Вот пример с использованием внешней базы данных. То есть мы можем подгрузить в некоторый атом-спейс какое-то другое содержание. В данном случае у нас Здесь какая-то информация о цветах, планетах и чем-то еще. Соответственно, мы можем делать запросы из нашего Atomspace, текущего, к другому Atomspace при желании. и, соответственно, он будет возвращать соответствующие результаты. то есть это в принципе не очень сильно отличается от импорта. в обычном языке программирования, но здесь акцент делается именно на том, что это вообще говоря может быть по-другому реализованный AtomSpace, например распределенный AtomSpace, к которому мы делаем запросы отсюда, здесь их обрабатываем и чейним там со следующими запросами к этому AtomSpace или к другому. вот через равенство мы также, ну это достаточно уже расширенные примеры, через равенство мы также можем имплементить, например, формулы для расчета truth values для каких-то правил. в opencog classic есть unified rule engine, где можно задавать какие-то правила и конкретная форма правил это probabilistic logic networks, которые напоминают нечеткую логику, но на самом деле являются гораздо более развитым и продвинутым вариантом. но по-простому говоря, там, допустим, можно заимплементировать нечеткую логику. но вот в OpenCock классике Формулы для расчета значений истинности не писались на атомизе, их приходилось писать, например, на схеме или на каком-то другом языке, для которого был API. атомиза. вот здесь мы формулы для расчета трусвелюсов можем писать на самой мете, но это по-простому говоря работает. то есть мы можем там, имея значение истинности для каких-то импликаций, это все кастомные символы, это не есть какая-то каноническая имплементация там вероятностных логических сетей это просто пример показывающий что мы взяли от балды как-то ввели какие-то символы и там ввели правила для пересчета вероятности этих символов и все это в принципе работает то есть мы там можем узнать что если мы посчитаем truth value для green fritz, то он будет обладать какими-то значениями, которые будут являться комбинацией truth values для этой импликации, truth values для каких-то конкретных фактов из базы знаний и так далее. ну давайте все-таки более подробно про имплементацию алгебраических, обобщенных алгебраических типов в мете. то есть мы можем... давайте перенесем примерчик. скажем мы можем ввести тип айза и ввести для него там два конструктора и соответственно мы можем написать можем написать например left 5 и у него будет тип айзенамбе там left string там это будет AZString, то есть у нас есть конструктор, который переводит тип T в тип AZT. Опять же, наверное, на это времени нет, но что очень интересно, это type checking тоже выражается через pattern matching. то есть если мы берем какие-нибудь зависимые типы, то то что там происходит с type checking это просто chaining запросов в pattern matching просто для типов. в мете это в интерпретаторе реализовано тоже через pattern matching, хотя там есть некоторые аспекты. соответственно, мы также можем задать тип пару, тоже классический алгебраический тип и комбинировать их как хотим. например, пара от left 5 и s будет иметь тип пара is a number string. соответственно, мы также можем вплоть до параметризованных типов типа список задавать и скажем Если у нас список будет хорошо типизированный, в данном случае, в отличие от обычных схемовских списков, если мы говорим, что список имеет такой тип, то а должно быть одним и тем же. Соответственно, если мы попытаемся кончить элементы одного типа у нас там будет list number получаться. а если мы попытаемся что-то такое сделать, то тип этого выражения будет пустым, то есть выражение плохо типизировано. они у нас не просто обычные типа, они у нас то есть мы можем типы никакие не вводить, и тогда у нас будет в данном случае тип undefined, number undefined, string undefined. потому что для этих символов тип неопределен и, соответственно, если функция, например, принимает неопределенный тип, она всеядна, она может брать элементы любого типа и это будет все type-чекаться. мы можем задавать функции более высоких порядков. например, очень забавная имплементация функции коррирования, которая может для кого-то открыть глаза насчет того, что это вообще такое. опять же, поскольку у нас это просто паттерн-матчинг над равенствами в основе лежит, то мы при определении функции вообще можем писать слева все что угодно. то есть это не обязательно функциональный символ и аргументы. это может быть применение функции к чему-то и еще к чему-то. поэтому коррированная Карированный плюсик – это функция, которая принимает на вход число и возвращает функцию из числа в число. при этом если мы просто запустим каль с плюсиком, то ничего не произойдет, оно не будет редуцировано, потому что для такой формы выражения нету равенства. и на самом деле практически ни один язык программирования не выполняет никаких частичных вычислений. они просто оставляют там какую-нибудь лямбу, которая будет ожидать следующие аргументы. в данном случае даже лямбу вызывать не нужно. мы просто оставляем это выражение нередуцированным. и мы можем его там передавать куда угодно и так далее. это будет просто вот такое нередуцированное выражение. при этом тип у него будет соответствующим, то есть карированный плюсик, примененный к единичке, будет иметь тип функции из number в number. А вот уж когда мы к нему еще раз его применим к чему-нибудь, вот только после этого оно и будет выполнено. и все это делается опять же за счет вот такого вот паттерн матчинга над равенствами. потому что вот это вот выражение вот с этим выражением уже сматчится. и после этого будет проэволуирована правая часть с подстановками. в равной мере там можно делать разные интересные вещи. 

S03 [01:40:22]  : Алексей, на всякий случай нам бы минут за 5-10 уже закруглиться. А у вас было написано с 18 до 20-30 или… Ну, смотрите, значит, у нас два с половиной часа, то есть нам бы хотя бы полчасика еще на вопросы оставить, если получится. 

S01 [01:40:41]  : ну хорошо, давайте я вот немножко еще про зависимые типы порассказываю. и на этом, наверное, тогда на сегодня будет достаточно. классический пример зависимых типов. это тип век, который сохраняет в себе длину списка. это у нас, в общем-то, работает. если мы спросим, например, какой тип у этого списка, он скажет, что у него тип век number двоечка в арифметике Piano. то есть эти конструкторы типов на вход могут принимать какие-то дополнительные аргументы и преобразовывать их в соответствующие выражения в типах. Соответственно, если мы посмотрим на функцию drop, которая будет у нас удалять один элемент списка. Точнее говоря, не удалять, а возвращать список более короткий. то мы тоже можем задать тип этой функции как век от sx в просто x. и, соответственно, если мы попытаемся узнать какой тип у выражения drop cons 1 nil, то тип у него будет список длины 0. если же мы просто попытаемся узнать, что будет, если задропать nil, то получится пустое множество, потому что это выражение плохо типизировано, потому что тип nil не паттерн-матчится с этим выражением типа. так что у нас через паттерн-матчинг вот такие классические зависимые типы вполне себе реализуются. есть, правда, некоторые тонкости, поскольку у нас нет разницы между конструкторами и типами, мы не можем проверять тотальность, поэтому мы не можем в таком виде предъявлять тотальную функцию как доказательство утверждения записанного в типе этой функции. но это уже отдельный вопрос. в типах мы также можем записывать какие-то а-ля рассуждения. например опять же классическое использование зависимых типов для подтверждения каких-то утверждений например мы можем сказать, что humans are mortal имеет тип human t, mortal t, то есть вроде как это функция. Но если этот тип читать как логическое утверждение, то это импликация. И мы, соответственно, там можем, например, иметь в зависимых типах инстансы того, что human сократ и human plate, ну ладно, давайте с сократом, например, то выражение humans are mortal, socrates is human будет иметь тип mortal secrets, то есть вот это выражение фактически является доказательством того, что Сократ смертен. Мне, честно говоря, все-таки зависимые типы как способ записи логических выражений кажутся не вполне удобными, но забавно, что в мете мы специально зависимые типы не то чтобы вводили, мы просто сделали типы как ограничения на форму выражений и просто добавили pattern matching также с переменными поверх этих типов. И оказывается, что этого достаточно, чтобы получить нечто очень похожее на зависимые типы. Но использовать ли их, например, для записи каких-то логических утверждений – это уже, наверное, вопрос вкуса. Если говорить про базовую часть языка, то, наверное, я какие-то основные вещи рассказал. Тут, конечно, можно поговорить про нейросимвольную интеграцию и так далее, но на это времени уже нет. И этот вопрос, опять же, для детального его анализа у нас пока руки не дошли. Но в целом никто не мешает заимбедить нейросетку как Grounded Atom в программу, использовать ее в каких-то манипуляциях, сами эти манипуляции это будут символьные выражения, которые там будут определяться равенствами или как-то косвенно генериться на лету. а под капотом это будут уже grounded операции, если это все будет выполняться и гипотетически. также в стиле модульных сетей можно будет делать какие-то рассуждения, в результате которых будут формироваться какие-то конкретные нейросетки, будет выполняться их inference и с обратным распространением ошибки все это будет обучаться. но мы пока просто не воспроизвели на текущий момент эти эксперименты в метте, они у нас в классическом OpenCode были. а так, конечно, много дальнейших планов и на inference control, то есть мы можем использовать недетерминизм из коробки, чтобы там решать какие-то задачки, но это будет, понятное дело, неэффективно, то есть по форме все красиво, мы там можем, например, задачу о сумме под множеств записать просто как недетерминированную программку и там из коробки будем получать решение, но поскольку экспоненциальный перебор никто не отменял. все это будет работать медленно и тут уже гораздо интереснее делать какой-то inference control. главное, что у нас в принципе есть возможности для того, чтобы делать какой-то reasoning. можно это делать на типах или просто на символиных выражениях и комбинировать это с процессом чейнинга equality queries, который эмулирует функциональное, в том числе и недетерминированное функциональное поведение. поэтому в принципе есть надежда на то, что удастся срастить ризнинг и функциональное вероятностное программирование. у нас есть коллеги, которые с лета подключились к использованию метты, ну не то чтобы использованию, но экспериментированию с ней. они делают какие-то гораздо более сложные вещи, чем мы сами успеваем. И достаточно занятно. Один из коллег реализовал вероятностные зависимые типы, причем в non-well-founded стиле, то есть там типа как бы quasi-бесконечности есть в типах. И у него это даже в большей степени работает. Или тот же Нил переносит PLN из OpenCock Classic, причем, как он говорит, в более правильной имплементации, чем там, потому что здесь это возможно. там есть еще коллега Адам, который рекурсивные схемы на мете реализовывает и тоже получается. но пока все-таки я просто хочу сказать, что это экспериментальная версия и у нас сильно не хватает времени на вопросы эффективности, поэтому такие скрипты работают очень быстро, но если делать что-то посложнее, то скорость может разочаровать. Но в остальном это уже какая-то часть вполне себе рабочая. Я не успел показать, сложно было бы показать какие-то сложные примеры. но они в принципе есть, то есть можно делать на этой основе достаточно интересные вещи. ну давайте, да, ставим время на вопросы. 

S03 [01:50:51]  : Спасибо большое за подробный доклад с запасом глубины, в которую мы еще не погрузились. У нас по вопросам есть два вопроса от Сергея Терехова и много вопросов от нас с Борисом Новиковым. Поэтому, учитывая то, что осталось чуть больше, чем полчаса, Давайте мы сначала дадим возможность Борису задать его два вопроса, а потом то, что останется нам с Борисом. Сергей, мне зачитать или вы сами сформулируете? 

S02 [01:51:27]  : пока сергей ты оговорился смысле ты хотел сначала борису дать или не вам а давайте вот ваши два вопроса у меня на самом деле как бы вопрос такой вот крупный один вы немножко не могли бы подняться вот на деталями особенностями разработки вот новых языков там и так далее так далее и покрупнее немножко нам рассказать о том какой же все-таки сейчас есть прогресс и в области вообще применения вот таких логических схем в сравнении с классикой 80-х, конца 70-х, 80-х годов экспертных систем пятого поколения машины. Японская программа была очень популярна в свое время. Что из того, что тогда не получилось, сейчас уже вот получилось и видно, что можно сделать. Если вот тут какой-то шаг ощутимый, Кроме роста производительности ЭВМ и возможности использования переборных методов, которые раньше были запрещены, просто потому что машины не позволяли. Ну и мелкий вопрос, если там будет время, если не будет, не отвечайте. как дифференцировать сквозь логические выражения, если вы хотите смешивать какие-то операции непрерывные а-ля нейронные, которые там в каком-то смысле проградиенты, а логические они соответственно логические. ну вот можно ли там как-то корректно аккуратно дифференцировать и как с этим делать. но второй вопрос не важен, а вот первый важен. спасибо. 

S01 [01:52:48]  : ну да вопрос на самом деле интересный потому что я в процессе разработки сам периодически думаю вот а все-таки что же в итоге мы получим от этого вот зачем все это дает ли это какие-то возможности или еще что-то. Но повторюсь, данная разработка идет от не только каких-то теоретических соображений, но и практических нужд. не знаю с чем сравнивать то есть вот есть например пролог да пролог штука хорошая и там иногда коллеги тоже спрашивают а вот почему не пролог например потому что, я не знаю, в прологе не получается делать многих вещей, которые бы нам хотелось делать. 

S02 [01:53:57]  : Алексей, смотрите еще выше, пожалуйста. допустим, все, что вы хотели замыслить логически сделать, вы сделали на каком-то языке, пусть это будет пролог, и вы мужественно прорвались через мистические какие-то дебри программирования и запрограммировали такие, что хотели. это тяжело, а на вашем языке легко. но, допустим, вы реализовали и там, и там. но не в этом же были ограничения логических систем искусственного интеллекта. не в том, что программисту трудно. то, что программист устал, ему тяжело работать, тяжелый труд программиста. не в этом проблем это было. 

S01 [01:54:32]  : но я не совсем согласен, потому что так-то можно сказать, что да, можно все что угодно запрограммировать на ассемблере, вот поэтому в принципе нам языки программирования не нужны вот нам нужно развивать там теорию общего искусственного интеллекта и как я в принципе сказал вот работа над непосредственно этим языком она нас несколько как сказать затормозила в части именно AGI исследований. И у Бена это было одно из сомнений главных, стоит ли начинать заниматься такой разработкой и отложить какие-то исследования на потом. То есть понятно, что сам по себе этот язык не решает никаких проблем AGI. Но для нас он является все-таки некоторой платформой для того, чтобы впоследствии эти проблемы решать быстрее. Приведу аналогию. Библиотеки глубокого обучения, они по сути дела ничего не делают. вот в концептуальном смысле да они там позволяют вам выполнять автоматическое дифференцирование вместо того чтобы уравнение ручками выводить ну и плюс там обсчитывают все на гпу но никто не мешал там использовать CUDA библиотеки, и раньше их так и использовали, но вот когда появился именно инструментарий, который легко позволяет проводить эксперименты, не тратя там по несколько часов в день на выведение формул, И потом несколько дней на программирование всего этого ручками просто позволяет взять и за пять минут проверить новую архитектуру нейросеток. сразу появилось большое комьюнити. 

S02 [01:56:51]  : Алексей, но вы до техники уходите. я хочу... давайте по-другому задам вопрос. вы считаете, что прогресс в искусственном интеллекте связан именно с таким логическим программированием? то есть нужно сделать вот это логическое программирование и вот таким вот совершенно изощренно красивым эффективным очень удобным быстрым и так далее и вот тогда мы создадим искусственный интеллект это ваш ответ или или вы говорили вы не рассказывайте про вы не рассказывайте мне про технические средства расскажите о принципе то есть вы хотите, вы считаете, что для того, чтобы создать систему искусственного интеллекта, сильного там или вообще какого-нибудь приличного, нужно развивать логические вот эти вот программы, делать их, так сказать, более удобными, массово экспериментированными, большему числу людей доступными, более эффективными и так далее. И это есть путь. Вот это правильно я понимаю ваш посыл? 

S01 [01:57:48]  : Ну, смотрите, здесь речь все-таки не совсем про логику. Можно сказать, что это речь про то, чтобы была и символная часть тоже. И мы понимаем нейросимвольные интеграции. И мы не исключаем того, что дальше мы будем строить нейросетевые эмбеддинги метаграфов программ и делать какой-то дифференцируемый вывод на них. этого мы тоже не исключаем, но тот факт, что символная часть важна, да, я считаю, что это так. то есть я не говорю, что вот должно быть все логическое и символьное. нет, оно должно быть гибридным. 

S02 [01:58:45]  : я в целом... ну хорошо, а нейронное плюс символьное достаточно? 

S01 [01:58:54]  : субсимвольная плюс символьная достаточно. вопрос что за этим стоит? то есть вот как бы универсальная индукция она символьная или нет? 

S02 [01:59:08]  : Все, ну понятно. В общем, мы уже уходим. Спасибо, Алексей. 

S03 [01:59:11]  : Спасибо, Сергей. Прежде чем мы с Борисом завладеем вниманием Алексея, хотелось бы все-таки уточнить ваш вопрос. Алексей, точнее, ответ Алексея на вопрос Сергея. Алексей, вот вы сказали, что Ваш язык позволяет делать вещи, которые вы не могли или вам было сложно делать на прологе. А вы можете акцентировать или перечислить какие-то принципиальные вещи, которые ваш язык позволяет делать лучше или по сравнению с прологом? 

S01 [01:59:45]  : Прямо такой анализ мы не проводили, но, например, та же нейросимвольная интеграция. то есть я встречал проблог, вероятностную версию пролога, и я встречал эксперименты по интеграции этой версии с нейросетками, но это было на уровне того что мы посчитали логическую вероятность того что мы на мнисте распознали 3 плюс 6 и это 9. вот мы учли что как бы знаем, что ответ 9 и вот это похоже на троечку, а это на шестерочку, а не на четверочку и пятерочку. вот как бы такие вещи. делаются в лед, даже OpenCock классики после наших дополнений с Ground объектами, а здесь это должно делаться еще проще и удобнее. на прологе такое просто практически не делается. То же самое относится, например, к интеграции пролога с большими базами знаний. Вот есть там BioAtomSpace, который объединяет многие миллионы высказываний, записей в базе знаний о протеинах, генах и так далее. Как в прологе сделать ризнинг над этим? Я не знаю. Может, я не очень хорошо знаю Prolog, может быть, там какой-нибудь гуру Prolog скажет, что это ерунда, но мы не можем загрузить такой объем информации в программу на Prolog и делать над ней резонинг. То же самое касается многих метавещей. Например, я не знаю, опять же, может быть, я Prolog плохо знаю, я не знаю, как написать на Prolog самомодифицируемую программу. Здесь это легко. То есть мы просто переписываем записи в текущем атомспейсе. И мы можем начинать делать массу интересных вещей при этом. Ну это так, сходу что приходит, там самомодификация, работа с большими базами знаний и нейросимвольная интеграция. Мне кажется, этого уже достаточно. В части именно реализации логического вывода, ну здесь, наверное, больших плюсов нет, но опять же вот возможность из самой программы менять поведение интерпретатора. 

S03 [02:02:46]  : Спасибо, Алексей, спасибо. Дальше у нас много вопросов с Борисом. Они о многом перекликаются, поэтому я их буду так задавать пачками. Одна пачка вопросов. Вы говорите нейросимвольная интеграция. Если я правильно понял, как минимум нейросимвольная интеграция возможно в парадигме, что называется, встраивания некоторых внешних ораколов нейросимвольных, фу, сабсимвольных в том числе. Это может быть просто какая-то программа алгоритмическая, внешние устройства, это может быть сабсимвольный оракол, какая-то нейросетка. И все это встраивается в символьную сеть. В дополнение к этому, возможно ли какой-то перенос знаний в рамках предлагаемой OpenCock архитектуры нейросименной интеграции перенос сабсимвольных знаний, выученных вот этим встроенным ораклом, или grounded schema node, или как у вас это называется в мете, в символьное представление. И наоборот, загрузка некоторых символьных моделей в сабсимвольные модели. Есть такого характера нейросимвельная интеграция? 

S01 [02:04:07]  : В OpenCog Classic это назывались GroundedSchemaNode и GroundedPredicateNode, но мы добавили GroundedObjectNode, чтобы именно враппать торчевские классы, определяющие архитектуру нейросетей, вызывать методы типа forward и backward пассов. Это уже была наша добавка, хотя поначалу мы использовали grounded predicate ноды, например, для того чтобы там сказать, что яблоко красное на картинке. Касательно основной части вопроса, тут есть разные опции. То есть, как я говорил, самое простое и работающее там из коробки это просто комбинировать нейросетки на лету в зависимости от каких-то символьных рассуждений, единой информации. И это понятный, но может быть не самый продвинутый вариант. как бы формирование новых символов мы пока не делали и пока мы до таких экспериментов не дошли вот но между этими двумя вариантами есть промежуточные то есть за что мне нравятся вероятностные модели за то что они как раз нативно позволяют объединять модели разной природы. то есть, скажем, генеративная вероятностная модель может быть нейросеткой, может быть байосовской сетью, и нет принципиальной проблемы, по крайней мере, если у нас эта байосовская сетка задана априорно. чтобы она накладывала условия на вероятности случайных переменных, из которых производится сэмплирование в нейросетевой генеративной модели. но опять же, если речь идет про выучивание новых представлений, это гораздо сложнее. но опять же, в рамках вероятностного программирования, полноценного, когда у нас не бсовская сетка с фиксированной структурой, а вероятностная программа, которая позволяет как минимум не параметризованного BIOS генерить, а как максимум генерировать любые другие программы с латентными переменными за которыми могут стоять и нейросетки или которые будут кондишенить вывод в нейросетках. В принципе это возможно. Другой вопрос, как это сделать на практике применимым достаточно эффективным образом. Но еще раз повторюсь, до таких экспериментов мы не дошли. есть и другие пути нейросимвольной интеграции, как бы идти с другой стороны. опять же, когда я активно занимался вероятностным программированием, я встречал работы по нейросетевому имбедингу вероятностных языков программирования. вот и можно соответственно но опять же это вот к предыдущему недообсужденному вопросу можно делать дифференцируемое представление программ в частности метаграфов в нейросетевом эмбеддинге. но опять же мы не делали этого. насколько это в мете можно сделать, но на текущий момент вот именно для этого, мне кажется, функциональности такой из коробки нет. но идеи такие есть. 

S03 [02:08:34]  : Спасибо. Следующий блок вопросов от нас с Борисом. Вот вы упомянули паттерн matching and reasoning. У вас вероятности на ветках дерева, мы же говорим про то, что у нас граф в общем случае может быть вероятностный, если я правильно понимаю в этом спейсе, так вот вероятности на вот ветках и узлах этого дерева, они учитываются при сопоставлении деревьев или нет? Или это же просто аналог SQL-запросов, томитографов, просто вот у вас свой язык для построения структурированных запросов, отличный от SQL, там GraphQL и так далее. Сейчас еще следующий вопрос из этой же вязанки. Борис спрашивает, какие логики используются в рассуждениях? Только формальная? Знания детерминированные или вероятностные? Знания в базах данных, которые вы используете, формальные или эмпирические? 

S01 [02:09:32]  : Ну, многовато вопросов, но ладно, попробую. похоже на SQL query, за тем исключением, что это все-таки не просто query к... как это сказать... полностью определенным записям, а они могут содержать переменную с двух сторон. плюс еще и форма query немножко разная, но в остальном на этом уровне это не сильно отличается. если мы хотим добавить к этому вероятности, вероятности добавляются эксплицитно. то есть вот если в opencock классике truth values были по умолчанию привязаны к атомам и всегда у них были, у нас атом не содержит внутри себя в обязательном порядке true values и в pattern matching они напрямую при этом не учитываются. в смысле они не могут учитываться, их там просто нет. и на текущий момент мы значение истинности представляем не как Вэлью, в Вакунгок классике, там было четкое разделение между атомами и вэлью, потому что атомы индексировались, по ним можно было делать поиск, а в Value они привязывались к атомам, их по отдельности искать было нельзя, но они были более легковесные и так далее. У Linus по этому поводу была большая концепция, но иногда было непонятно, чем там отличается Grounded Atom от Value. нет трузвелюсов, которые живут внутри атомов. И мы не требуем от паттерн матчинга. Кстати, в атомспейсе классическом там тоже паттерн матчинг не работал с трузвелюсами напрямую. Но мы можем в явном виде добавлять трузвелюсы как атомы, и составлять query, учитывающие эти значения истинности в явном виде. это может быть не слишком эффективно. сейчас обсуждается вопрос о том, что если у нас есть особо гигантский atom space и у него есть вот в старом opencog там помимо truth values были еще и attention values. и если мы там хотим реализовать что-то типа economic attention networks, то мы с этими attention values тоже должны работать и желательно на уровне запросов к паттерн-матчингу, чтобы он там нам не вываливал миллионы записей, а там концентрировался на том, на чем аллоцировано внимание. по тем или иным соображениям. На текущий момент, как я сказал, мы можем это в явном виде представлять через выражения символьные с атомами. Атомы могут быть заграунженные. 

S00 [02:13:17]  : Это можно представлять в этом виде. Что это? 

S01 [02:13:24]  : Значение истинности. 

S00 [02:13:26]  : Оно 0 единицы или между нулем и единицей? 

S01 [02:13:30]  : Какое угодно. Хотите 0 единицы, хотите между нулем и единицей, хотите четырехкомпонентные значения. 

S00 [02:13:39]  : В одном случае есть транзитивность, в другом случае нет транзитивности. Как вы с этим работаете? 

S01 [02:13:47]  : Это не мы работаем. А кто? это работает тот, кто разрабатывает соответствующую систему на мете, то есть если вы хотите работать с многокомпонентными значениями истинности, вы их задаете и вводите для них те правила, которые хотите. 

S00 [02:14:10]  : то есть все вручную, из данных ничего не выводится. Все символы и все соотношения между символами заводятся вручную или выводятся из анализа данных. 

S01 [02:14:27]  : Если вы напишете программу, которая анализирует данные и на этой основе задает новые символы, то будет делаться автоматически. 

S00 [02:14:39]  : Мне, в принципе, очень нравится идея соединения формальных модельных зданий и имперических, но есть одна большая проблема. Рассмотрим на примере свободного падения тел, экспериментов по свободному падению тел. Если вы введете данные падения в воздухе, то у вас получатся выводы Аристотеля, а не Галилея. то есть тяжелые тела падают быстрее. А в модели надо пренебречь сопротивлением воздуха и тогда они будут падать одновременно, а дальше вводить поправки и так далее. И любая модель строится на том, что мы на предположении можно пренебречь. То есть выделяются существенные признаки, а несущественные отбрасываются. А в любых эмпирических данных присутствуют все признаки. Как вот быть с этим? Это главная, на мой взгляд, философская проблема соединения модельных формальных знаний и знаний эмпирических. то то же самое Сюрвендер знали и знали через нейросетью. Как вы предполагаете с этим бороться? 

S01 [02:16:01]  : Ну, во-первых, пример с Галилеем Аристотелем – это вопрос именно представления моделей реальности. И никто не мешает вам иметь модели разной степени приближенности? 

S00 [02:16:22]  : Нет. Одна модель – эмпирическая. тяжелые тела падают быстрее. Если вы будете бросать тяжелые и легкие тела и замерять время, то вы увидите, что тяжелые тела падают быстрее. И это нейросеть вам однозначно выдастся. Из эмпирических данных. А то, что Галилей сказал, а вот если предположить отсутствие сопротивления воздуху, то будут падать одинаково, И эта нейросеть вам не выдаст. А как это совместить, непонятно. Это было бы очень хорошо совмещать, но непонятно как. 

S01 [02:17:01]  : Нейросети не должны выводить... Ну смотрите, как бы это не имеет прямого отношения к МЕТТИ, потому что это всего лишь язык программирования. для AGI-исследований, а не сама модель AGI. То есть идея была именно в том, чтобы обеспечить возможность реализации разных подходов и моделей, но при этом в каком-то объединенном фреймворке. 

S00 [02:17:38]  : Этот язык поможет объединить обобщение нейросети с формальными знаниями из абстрактных моделей. 

S01 [02:17:47]  : Этот язык поможет быстрее это программировать и проверять разные варианты. Но еще раз, я не вижу в вашем примере какой-то большой проблемы. Если вы хотите предсказывать скорость падения тел в определенных условиях и делать это быстро, то, пожалуйста, тренируйте нейросет. Конечно, если вы хотите на практике в конкретных условиях это предсказывать. Вот если вы хотите узнать, как получить модель Галилея, но это невозможно сделать без, скажем так, большой картины мира. То есть у вас в явном виде как раз символьно должны присутствовать знания о том, что есть воздух, что у воздуха есть сопротивление. Эти знания эмпирические, поэтому я не вижу здесь противоречия эмпирики и, скажем так, теории, потому что вся теория, она все равно строится из эмпирики. Вот просто вопрос, какой фрагмент... 

S03 [02:19:16]  : Да. Хорошо. Борис, давайте... Борис, давайте все-таки... Спасибо за вопрос, за дискуссию. Хотелось бы все-таки продвинуться еще по тем вопросам, которые остались. Значит, чтобы надолго нам не вываливаться за регламент. Значит, Алексей, я правильно понимаю, что вот то, что Бен с коллегами пишет про Пьеллен в чистом виде, оно метты не предполагается. То есть, если кто-то хочет реализовать PLN в рамках OpenCog, то он должен его реализовывать поверх метты с учетом некоторого расширения модели данных, предполагающего тут те или иные формы truth values. Да, абсолютно верно. 

S01 [02:20:04]  : Спасибо. Я бы все-таки по предыдущему вопросу хотел бы дать комментарий, что это действительно проблема. но, скажем, эта проблема в рамках индукции по Соломонову решается. то есть для меня это не является фундаментальной философской проблемой, это скорее проблема вычислительная. как эффективно это сделать? 

S00 [02:20:33]  : Если говорить по Расселу, мне известно. По Соломонову пока не смотрел. А в чем разница? 

S01 [02:20:42]  : По Соломонову вы строите алгоритмическую модель мира и эта модель будет соответствовать. Ладно, если вы говорите про то, что Человек может сделать какое-то предположение, которое вообще никак эмпирически не наблюдалось, но это сомнительно. 

S00 [02:21:11]  : Эмпирически Солнце вращается вокруг Земли, а человек сделал предположение, что Земля вращается вокруг Солнца. 

S01 [02:21:20]  : Это как раз очень легко укладывается в универсальную индукцию, потому что программа, описывающая движение планеты вокруг Солнца по эллипсам, гораздо более компактна, чем модели... Мне понравился Тихобраги, а Коперник... и не Коперник, а не Коперник, извините, а Теплер. 

S00 [02:21:45]  : А у Коперника были предсказания хуже, чем у Птолемея. И поэтому они были отброшены еще древними грехами. 

S03 [02:21:55]  : Хорошо, Алексей. 

S01 [02:21:57]  : Ну ладно, да. 

S03 [02:22:00]  : Да, вот все-таки хотелось бы двинуться дальше по вопросам, но вот здесь хотелось бы в сторону еще тоже отскочить. Смотрите, вы вначале говорили, сейчас это снова проскочило, что хочется максимизировать описание, длину описания. Минимизировать. Да, извиняюсь, минимизировать длину описания. А как вы отнесетесь к тому, что минимизация длины описания – это как бы одна часть построения модели с максимальной предсказующей силой, минимизирующей неожиданность или максимизирующей предсказуемой вероятность Это вторая часть, и это одно и то же. Можно ли сказать, что это все одно и то же, или одно является частным случаем другого, или это две разные вещи, между которыми должен быть баланс, и где истинная интеллектуальная система должна каким-то образом этот баланс учитывать. 

S01 [02:23:04]  : одно и то же. говорить про минимизацию длинное описание это не точно, потому что у вас там присутствует ансамбль моделей, совместимых с наблюдаемыми данными. совместимых, то есть эта модель может включать и шум, поэтому как бы тут не стоит это воспринимать утрированно, как там какую-то одну простенькую формулу, которая там идеально все описывает, а в наблюдениях шум, поэтому они не соответствуют. нет, как бы программа для машины тюринга вполне может там содержать сжатый до выбеливания, условно говоря, массив шумов, который будет прибавлять к модели и минимизации этих шумов, она будет балансироваться со сложностью самой программы, хотя эти шумы будут частью этой программы, если мы их так сильно разделять не будем. вот и как бы ну там есть вопросы к критерию сложности на самом деле абсолютно неважно длина это программы или какой-то любой другой способ задать априорные вероятности в пространстве программ согласованным образом. но в целом, если вы там зададите сложность по Колмогорову или сложность по Левину или еще что-то, то такая модель будет как раз обладать оптимальным с точностью до удачности выбора опорной машины предсказанием, а удачность выбора опорной машины не очень сильно сказывается на качестве самого подхода с точки зрения того, что там смещение будет достаточно небольшое по количеству требуемых дополнительных данных. 

S03 [02:25:08]  : Ну и можно тогда сказать, что в итоге всё всё равно выводится из того, про что говорит товарищ Фристон про свободную энергию, что на самом деле не свободная энергия, а степень неопределённости? 

S01 [02:25:22]  : Свободная энергия это опять же критерий. Критерия там давно решена. педалировать эту тему на текущий момент мне кажется ну как-то примитивно вот проблема с пространством моделей представления она тоже в общем-то решена в том смысле что понятно что если мы это пространство сильно заужаем мы получаем слабые методы вот поэтому ну называйте там энтропии свободной энергии но правильнее все-таки с учетом того в каком пространстве нужно работать правильнее говорить все-таки про какие-то алгоритмическую и вычислительную сложность. 

S03 [02:26:14]  : Хорошо, спасибо. Борис, давайте мы все-таки двинемся по... Я чуть-чуть не договорил. 

S01 [02:26:20]  : Проблема, собственно, заключается в том, как эффективно вывод осуществлять при этом. Над этой проблемой надо думать. 

S03 [02:26:30]  : Спасибо. Значит, у нас дальше вот хотелось бы практический вопрос еще несколько задать. А, еще, кстати, насчет языка. Вот с учетом того, что вы сказали, можно считать, что мета это является языком вероятностного программирования или это все-таки скорее язык для построения языков вероятностного программирования? Скорее второе. Спасибо. Теперь вопрос тоже от нас с Борисом. Допустим, у нас есть лягушки. Мы можем вообще на языке мета, в том виде, в котором он сейчас есть, без дополнительных настроек, сказать, что есть лягушки зеленые и есть коричневые? И задав вопрос про цвет лягушек, узнать, что лягушки могут быть как зелеными, так и коричневыми. И второй вариант этого вопроса. А можем ли мы сказать, что лягушки обычно зеленые, но иногда коричневые? И опять-таки, в ответ на вопрос, какого цвета лягушки, обычно узнать, что они зеленые. А какие они редко, узнать, что они коричневые. А если мы просто спрашиваем, какие бывают лягушки, узнать, что они обычно зеленые, но иногда коричневые. Как здесь. 

S01 [02:27:40]  : Это легко делается, за тем исключением, что дальше есть вопрос о том, как описать в данном случае семантику слова «обычно» и «иногда». То есть, если вы просто это ведете как символы, то это будет работать. с точностью до того, что вы там сможете узнать, что обычные лягушки зеленые, иногда коричневые и так далее. но если вы хотите это как-то добавить какого-то смысла этим словам, то придется или вводить какую-то вероятностную интерпретацию этих слов, и это будет уже гораздо сложнее. 

S03 [02:28:30]  : Спасибо. Следующий вопрос совсем уже практически. Два вопроса. Если я правильно понял, вы используете Visual Studio от Microsoft и, как я понимаю, интерпретатор Rust, и интерпретация идет прямо в терминале. Правильно у вас? 

S01 [02:28:52]  : Visual Code я здесь использую только как редактор с подсветкой синтаксис и с возможностью запуска в терминале скриптов. Никакой дополнительной интеграции не сделано. кто-то может настроить аналогичным образом любую другую оболочку, редактор. 

S03 [02:29:22]  : То есть привязки к визжо, к микропродуктам и микрософт нет, можно в чем угодно делать? 

S01 [02:29:27]  : Нет, привязки нет, да, это просто... Понятно, спасибо. Удобный редактор, там NIL, по-моему, EMAX используют, если я не ошибаюсь. 

S03 [02:29:40]  : Хорошо. И насколько все, что вы показали вместе с Atomspace и PLN. Вопрос, который я спрашивал в начале. Про PLN понятно. Насколько текущая реализация доступна для практического использования сообществом разработчиков или это совсем близкие к сообществу люди, которых вы назвали, способные, имеют возможность этим всем заниматься? 

S01 [02:30:07]  : Ну, как я сказал, в принципе, доступно-то оно доступно, то есть все open source, оно несложно вроде как собирается и запускается, но с точки зрения практического использования может быть разочарование на текущий момент в производительности, плюс какие-то вещи могут претерпеть изменения, то есть мы пока не гарантируем обратной совместимости. 

S03 [02:30:44]  : Хорошо, спасибо. И тогда последние два вопроса от меня и от Бориса. Алексей, во-первых, по синтексису вы сказали, что сейчас используется вот этот листоподобный синтекс. Он сохранится или как раз есть вероятность, что синтексис в том числе претерпит какие-то изменения в дальнейшем? 

S01 [02:31:03]  : Это больше зависит от будущего сообщества разработчиков. Меня такой синтаксис в принципе устраивает. Меня большое количество скобочек не напрягает. Если кому-то очень захочется реализовать, например, Haskell подобный синтаксис, он посимпатичнее. если это будет хорошо качественно реализовано, может быть мы на него перейдем. исходно вообще была идея, что синтаксис это штука заменяемая. то есть у нас атомспейсы, имплементация атомспейсов может быть разная, а к каждому атомспейсу может быть привязан вообще свой синтаксис или даже несколько синтаксисов. но, с другой стороны, какой-то вариант по умолчанию, наверное, чаще всего и используется. поэтому, конечно, будет ли вариант по умолчанию в будущем изменен, пока сложно сказать. Спасибо. Такое у некоторых людей есть, у того же Бена. Хотя после того, как он привык к Лисповскому синтаксису, оно у него подуменьшилось. 

S03 [02:32:32]  : Понятно. По умолчанию это все синтакс-агностик. 

S01 [02:32:40]  : В целом, да. Но опять же, как я сказал, если все будут пользоваться каким-то вариантом по умолчанию, то, скорее всего, он и будет в основном использоваться. Если будет что-то меняться, то может возникнуть сумбур. 

S03 [02:33:00]  : Спасибо. Борис, ваш вопрос. Так, Борис, пожалуйста. 

S00 [02:33:08]  : У меня вопрос, небольшой комментарий. Если здесь идет вопрос о построении модели при помощи искусственного интеллекта, то, на мой взгляд, вопрос о качестве моделирования решен в прагматической философии. И конечным критерием качества модели является только критерий практики. И он не универсален, а для определенного круга задач. Модели делаются под круг задач. Одни модели физики, другие модели психологии, третьи в социологии и в политике. И всюду нужны разные модели из разных стран. И критерием качества модели является только практика. И в это включается не только качество отражения реальности, но и простота работы с моделью. Поэтому не нужно стремиться к максимальному отражению реальности. Тогда это не модель, а сама реальность. Нужно то, с чем мы сможем работать. Но, выходя на практику, получать приемлемые для нас результаты. Спасибо. 

S01 [02:34:30]  : ну я в целом согласен на самом деле вот в том же AXE большая концептуальная проблема заключается в том что предполагается что условно говоря вычислительные мощности AXE выше чем у вселенной хотя он является ее частью и эта проблема в принципе не решена Вот. 

S00 [02:34:59]  : Спасибо. 

S03 [02:35:03]  : А вот тогда у меня сразу вопрос, а, наверное, уже последний. По поводу ресурсов, которые требуются. У Пиванга весь резонинг, который у него там есть, он завязан на ресурсопотребление и ресурсосбережение. То есть, он управляется доступными ресурсами. То есть, вы упомянули, что был классик attention, которого сейчас нет. Соответственно, вопрос в ризнинге и паттерн-матчинге метты attention или ограничение ресурсов, оно как-то используется, или это будет сделано в последующем под капотом, или это нужно будет делать поверх с использованием каких-то дополнительных механизмов поверх меты? 

S01 [02:35:59]  : Я фактически ответил на практически такой же вопрос. То есть, на текущий момент этого нет. А будет ли нужно это делать под капотом или поверх меты, пока не до конца ясно. Опять же, практика критерий истины. И когда мы на практике до этого дойдем, мы увидим. Одна из исходных идей была в том, чтобы сделать какие-то минимальные шажочки в интерпретаторе, и атомарной операцией является статический pattern matching. в статическом паттерн-матчинге, когда матч-квырю делаем, там нет никакой рекурсии. Вот, но может быть таких маленьких шажочков для последующего контроля ресурсов поверх них может оказаться недостаточно, если мы будем работать с чудовищными по размеру атомспейсами. И тогда придется уже внутри матчинга делать вот этот вот контроль ресурсов. А вот… но я не знаю. но это надо будет уже на стороне паттерн матчера делать. но это опять же может быть конкретная имплементация конкретного атом спейса. я бы скорее так сказал. то есть это будет не на уровне спецификации метты, а на уровне спецификации конкретного гигантского Atomspace, может быть это будет тот же Distributed Atomspace, то есть у него, на его стороне это будет работать под капотом, а мета об этом может даже и не особо знать. Хотя, ну опять же, ну тут постоянная дилемма есть. С одной стороны, сказать, но не рефлексивности, но вот доступа ко всем внутренним операциям. А с другой стороны, да, проблема эффективности. Вот если мы попытаемся для этого гигантского атомспейса сделать возможность вытаскивать эти attention values или что бы там ни было в мета интерпретатор и анализировать их сознательно на уровне мета, но это опять же вот в принципе очень похоже на человека, что там мы не контролируем работу своей памяти, нам иногда это Нас иногда это напрягает, что мы не можем простую вещь запомнить, просто сказав себе «запомни это». Но с другой стороны, если бы у нас был произвольный контроль памяти, может быть, мы бы вообще думать не смогли из-за того, что не хватало бы вычислительных ресурсов на это. Не знаю. С одной стороны, может быть, есть какие-то решения, которые бы комбинировали эти вещи. Все это работает на стороне Atomspace, и программа на месте об этом не знает до тех пор, пока это совсем не начнет быть нужным для каких-то вещей. или высокоуровневый API. никто не мешает в таком Distributed Atom Space сделать какой-то API для контроля. это будет не в ядре мета прописано, а это будет в этом Distributed Atom Space как внешнем объекте этот API, который мы можем управлять. поэтому я на текущий момент ну вопрос конечно туманный потому что вот там пейванг закладывает это вот прямо в основу архитектуры вот но на текущий момент я бы сказал что Это должно реализовываться на уровне программы на мете, если речь идет о сознательном контроле над ресурсами. И это должно не выходить за рамки того, что сознательно контролируется. Мы там не авансируем ресурсы на обработку разных пикселей изображения. У нас есть ориентировочный рефлекс для этого. То есть это делается не одним механизмом. То, что сознательно делается, вот мое представление, оно должно делаться на мете, ну аналог этого. А то, что делается бессознательно, оно может делаться на стороне Atomspace, например. И это, соответственно, не прописывается в спецификации языка как такового. 

S03 [02:41:14]  : Спасибо, Алексей. Спасибо, Сергей. Я вижу, вы появились снова. Давайте последний вопрос. 

S02 [02:41:21]  : Я хотел просто услышать вот этот момент очень важный. Ведь если в основе лежит паттерн матчин, то есть именно вот это является паттерн матчин, то это же есть инструмент проверки любого утверждения. И, соответственно, вы не можете отказаться от каких-то проверок. потому что в этом случае разрушится сама логика функционирования, которая лежит в основе предположения. Когда программист пишет, он предполагает, что паттерн матч будет выполнен полностью. То есть его нельзя выполнить частично. Это суть этого самого. И вот подтвердите или опровергните. Если это так, Тогда это концептуальная проблема, потому что при росте объема знаний, при росте любых каких-то глубины вот этого обработки принципиально есть ограничения. И связаны они именно с самим глубинным принципом, именно паттерн-матч. Либо опровергните, скажите, что нет, это не важно, если мы будем паттерн-матч делать не полностью, приближенно, что-то пропускать, ничего страшного, в целом система будет работать, но она будет там как-то похуже работать, но эта деградация будет инкрементальна и управляемая человек. 

S01 [02:42:27]  : Ну смотрите, да, если мы говорим про написание именно программ, которые там полностью понятны самому разработчику, то разработчик, конечно же, ожидает точного паттерн-матчинга без каких-то пропусков. вот но да это входит в противоречие там видимо по крайней мере противоречие с идеей того что это и джай системы у нее там объем знаний растет и так далее вот но с той стороны опять же атом-спейс для вот той программы, которую пишет человек, может просто отличаться от атом-спейса, который будет использован для огромного хранилища. и в этой связи Бен именно с этой мотивацией очень много говорит про параконсистентные логики, например. То есть консистентность остается, но на каком-то масштабе. Если мы начинаем этот масштаб увеличивать, то консистентность понемножку теряется. 

S03 [02:43:40]  : Хорошо. Алексей, спасибо. Я думаю, мы можем потихонечку закругляться. В общем, я так понимаю, что работы еще много, вопросов много. И ответов предстоит дать много. И, я так понимаю, у нас есть хорошие шансы еще через год встретиться и узнать много нового, куда это все продвинулось. Да, Алексей? 

S01 [02:44:02]  : Ну, будем надеяться, да. Очень хочется, конечно, начать уже использовать в каких-то более AI экспериментах. Мы очень надеемся к этому перейти. Ну, может быть, перейдем. Мы сейчас начинаем пытаться это использовать в диалоговых агентах, в Майнкрафт агенте и так далее. Но это мелочи, более глубокие вещи типа inference control, реализация генетического программирования вместо прямого перебора всех недетерминированных выборов внутри программ. И так далее. Вот к таким бы вещам хотелось перейти. Да, здорово. 

S03 [02:45:06]  : Будем ждать с нетерпением. Коллеги, всем спасибо. Алексей, еще раз спасибо огромное вам, что пришли и потратили столько времени на рассказы и ответы на вопросы. Всем до свидания и в следующий четверг мы будем узнавать новое про импульсные нейронные сети как инструмент для создания AGI. До следующего четверга. Спасибо. До свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
