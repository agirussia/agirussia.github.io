## 26 мая - Выявление значимых последовательностей на неподкрепленном опыте - токенизация без учителя в интерпретируемой обработке естественного языка - Антон Колонин — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/c3f4w25BZSk/hqdefault.jpg)](https://youtu.be/c3f4w25BZSk)

Суммаризация семинара:

ТЕМА:
Семинар посвящён обсуждению методов токенизации естественного языка без учителя, а также влияния механизмов воспроизводства на сложность модели агента.

СУТЬ:
- Анализируются подходы к токенизации, включая использование Mutual Information и BERT для определения контекстуальных связей между словами.
- Обсуждается возможность использования Transition Freedom для структуризации потоков символов естественного языка и её потенциал в качестве начального приближения для self-reinforcement.
- Поднимается вопрос о том, как определить оптимальные пороги и метрики вероятности перехода, специфичные для каждого языка, и предполагается, что это может быть связано с генетическим отбором у людей.

ДЕТАЛИ:
- Рассматриваются методы unsupervised learning и unsupervised структуризации последовательности слов, включая возможность использования что-то лучше, чем mutual information.
- Обсуждаются детали алгоритма токенизации и его сравнение с эталонным алгоритмом, где используется автоматическое сравнение результатов двух способов разбиения символа.
- Поднимаются вопросы о том, как обучение связано с стиранием пленочки с числами, заклеенными пассивными программами, и предполагается, что это одно из объяснений механизма обучения.

РЕЗУЛЬТАТЫ:
- Получены результаты, показывающие успешность модели в соответствии разбиения символов, созданного алгоритмом, с ожидаемым поведением.
- Обсуждение методов и подходов к токенизации без учителя и их потенциальном применении для улучшения предсказательной модели языка.
- Подчёркивается важность понимания того, как язык учится и развивается, и как это может быть отражено в алгоритмах обработки естественного языка.

Семинар также затрагивает вопросы о том, как сделать сильные алгоритмы, сочетая аналитические модели и машинное обучение, и как начинать обучение с предварительно внедрённых знаний, а затем улучшать их на практике.






S04 [00:00:03]  : Так. И включаю показ экрана. Так. Где у меня экран? Так. Вот. И... Так. Наверное, надо сделать View Full Screen. Да, в общем, сначала будет длинное вступление, особенно поскольку у нас было принято решение сфокусировать усилия на тематике сообщества и делать так, чтобы доклады были все-таки привязаны к теме EGI, с одной стороны. С другой стороны, у нас в свое время был разговор о том, что периодически давать возможность участникам сообщества делать доклады по своей работе, может быть, даже напрямую не связанной с AGI. С третьей стороны, в какой-то момент назад, сколько-то времени назад, у меня возникло желание у нас в сообществе пересказать материалы семинара по интерпретированной обработке естественного языка на английском языке, который прошел в прошлом году в рамках конференции AGI. 2021. И, собственно, как раз изначально я планировал сегодня на русском языке предсказать то, что рассказывал Лайнас Вепстас и другие товарищи, включая себя. Но последние несколько месяцев у меня тут появились результаты, которыми очень захотелось поделиться. Поэтому сегодня я буду делиться своими новыми результатами. которые, на самом деле, с моей точки зрения, связаны будут с EGI. Про это я скажу отдельно. А если у кого-то возникнет… Если станет ясно в результате дискуссии или потом, может быть, организую голосование в сообществе, если возникнет желание всё-таки рассказать материалы этого семинара по интерпретируемой обработке естественного языка, это можно будет сделать отдельно. Мотивация этой работы состояла из двух частей. Во-первых, многие помнят, когда я рассказывал в прошлом году на OpenTalks.ai и у нас на семинаре я тоже рассказывал про попытки технологии так называемого глобального подкрепления для обучения на собственном опыте. который является общим случаем reinforcement learning, но разница между experiential learning и reinforcement learning, что experiential learning это более широкие понятия, которые подразумевают обучение вообще на любом собственном опыте, не обязательно там должен быть reinforcement. То есть, это может быть селфрейнспорт, это просто могут быть результаты наблюдения за некоторыми процессами и попытками выяснить некоторую закономерность. Как, например, меня в свое время, когда мне давали первый урок катания на серфе, мне инструктор сказал, что вообще, прежде чем выходить на воду с доской, нужно день-другой посидеть на берегу и просто посмотреть на волны. Владимир Смолин решил позвонить. Извиняюсь. Для того, чтобы проникнуться некоторыми регулярными… Понятно, инструктор по серфингу объяснял это не в терминах машинного обучения или искусственного интеллекта. Он это объяснял по-своему. Но я это понял так, что нужно пропитаться некоторыми регулярностями и закономерностями. Для того, чтобы когда ты находишься уже на воде и смотришь на эти же волны, уже сидя на доске, чтобы вообще некоторые законы волнообразования уже были у тебя в подсознании, и чтобы их не нужно было открывать. Чтобы там можно было сфокусироваться не на том, какая волна сменит другую через сколько, а на том, в какой момент нужно начинать грести, чтобы эту волну поймать. Ну и вот, собственно, одна из проблем, которую я понял в ходе тех экспериментов с пинг-понгом – это то, что, с одной стороны, если мы будем распространять подкрепление от момента подкрепления на последующее состояние и связанное с этими состояниями действия с какой-то функцией, функцией распространения. По сути, у нас идет некоторый аналог backpropagation как в нейронных сетях в алгоритмах типа Q-learning и Deep Q-learning, но только подкрепление распространяется не по слоям нейронной сети, а по тем состояниям, которые находятся ближе к моменту подкрепления. Когда мы многократно получаем одно и то же подкрепление на похожие последовательности состояния, постепенно вся цепочка правильных состояний с правильными действиями постепенно закрепляется. Но это требует очень больших объемов повторение одних и тех же ситуаций и никакого one-shot learning не получается. А если бы у нас была возможность сразу подкреплять правильную последовательность действий в парадигме one-shot learning, то мы бы быстрее гораздо обучались. И, собственно, вот этот эксперимент с шариком, который который я показывал в тех своих презентациях, он как раз показывает, что да, если у нас есть возможность четко каким-то образом фиксировать последовательность действия, например, от момента последнего подкрепления положительного или отрицательного, отсчитывать новую последовательность действий, и если она закончилась успехом, то подкреплять всю последовательность действий, а если закончилась неуспехом, возможно, наказывать за всю последовательность действий. Если есть возможность такой глобальный, так называемый глобальный фидбэк, выдавать все последовательности, то мы учимся гораздо быстрее. Проблема в том, что когда подкрепление становится... Две проблемы возникают в этой ситуации. Когда подкрепление становится отложенным, то нам очень трудно идентифицировать эту последовательность. То есть, мы не знаем, где у него начало, где у него конец. Вторая проблема, это вот тоже, когда этот эксперимент ставился, я напомню, что там эксперимент ставился как на функциональном пространстве, когда поведение мячика описывалось непосредственно положением объекта мячик и положением объекта ракетка. И в пространстве символьном происходило обучение. И второй вариант, обучение происходило в дискретном пространстве, когда у нас были только пикселы, мы работали только с пикселом. Вот и оказалось, что если мы будем мерить производительность в тактах или в эпохах, то скорость обучения примерно одна и та же, что на пикселах, что на этих самых на символиных объектах высокоуровневых. Но вычислительные затраты, естественно, совершенно несопоставимы. То есть, для того, чтобы пропахивать с подкреплениями и последовательности состояний данные на уровне пикселов, требуется гораздо больше вычислительных ресурсов. Соответственно, если мы будем обучение нормировать на ресурсопотребление, в соответствии с парадигмой AGI, AGI – это не только достижение целей, это еще и минимизация потребления ресурсов, то получается очень все плохо. Соответственно, Open AI GYM, вот эта задачка, она плохо масштабировалась. И поэтому возникает другая задача, что окей, давайте мы будем учиться глобальным фидбэкам на символьных объектах высокоуровневых, но давайте мы тогда еще сначала научимся эти высокоуровневые объекты строить, то есть давайте будем пытаться обнаруживать, что вот эта последовательность пикселов является ракеткой, а вот это вот облако пикселов является шариком, а вот последовательность движений ракетки вправо обозначает перемещение ракетки в правый угол, а вот эта последовательность дерганий влево обозначает последовательность ракетки в левый угол. И таким образом на уровне таких высокоуровневых концептов мы можем гораздо меньше тратить ресурсов вычислительных на обучение. Ну и когда я по поводу той работы, которую я буду дальше рассказывать, писал вот эту статью. Она уже опубликована в архиве. И пытался под эту статью подтянуть некоторые фундаментальные ссылки. Я с удивлением обнаружил, что в марте этого года, буквально пару месяцев назад, вышла статья товарища Шмидтхубера со товарищами. которая вот на самом деле там прямо в абстракте вот этой вот статьи написано примерно это же предложение. Вот, то есть, они его написали немножко другими словами, но смысл примерно та же самая. Вот можно эту статью открыть в медхубер и посмотреть. То есть, они как раз предлагают действительно для зарешения задач reinforcement learning учиться выявлять, они говорят в первую очередь на темпоральной абстракции, то есть, выявление некоторых временных закономерностей, пресловутых сценариев на основе процессов, про которые вот я на самом деле сколько-то семинаров назад говорил, вот они это делают. Это вот как бы одна мотивация, это чисто как бы агишная мотивация. или, если угодно, мотивация с точки зрения reinforcement learning. Вторая мотивация, она пришла из того, что у меня в какой-то момент возникло устойчивое желание продолжить проект по unsupervised language learning, который в свое время вели с Беном Герцелем, на котором потом кончились деньги. Идея заключается в том, что мы строим языковую модель не в рамках на построение некоторой большой модели параметров, где есть куча каких-то параметров, которые что-то обозначают в каком-то тензорном пространстве. И при подаче на вход чего-то они дают ожидаемое нечто – пресловутый черный ящик. А мы хотим строить языковые модели в интерпретируемом семантическом пространстве, если угодно, где под семантикой подразумеваются некоторые концепты, где каждый концепт соответствует либо какому-то слову, либо какому-то смыслу, либо какому-то конъюнктивному или дезинтивному множеству, либо какой-то последовательности. И все эти вершины в соответствующем графе. Во-первых, они являются… Ну, естественно, этот граф взвешенный. То есть, между этими всеми вершинами связи они являются нечеткими или вероятностными в зависимости от того, какую вычислительную модель мы положим в обсчет этих связей. Это с одной стороны. С другой стороны, у него примерно следующая структура. У него структура имеет четыре направленных графа. Это языковая модель. Здесь описывается как четыре направленных графа, где один граф. описывает нормальную последовательность произнесения или написания или появления слов или символов, несмотря на каком уровне мы это рассматриваем. Допустим, на нижнем уровне так называемых экземпляров или прецедентов. Мы имеем дело с последовательностями слов. Вот у нас есть предложение, где слова идут в какой-то последовательности. На верхнем уровне того, что модели называются глубокими шаблонами, У нас есть некоторые устойчивые, скажем так, паттерны речевые. Допустим, конструкция с переходящим глаголом – субъект, объект, предикат, объект. Дальше может быть конструкция с непереходящим глаголом. И некоторое количество конструкций, из которых, собственно, составляются речи, которые описывают правила, исчерпывающие множество таких конструкций, оперирующихся абстрактными терминами, грамматическими категориями, типа существительное предложение, прилагательное. Они, в общем, описывают модель языка с учетом того, что, конечно, есть исключения, но исключения тоже вписываются в эту языковую модель. Соответственно, один граф описывает движение слева направо как на нижнем уровне экземпляров, так и на верхнем уровне абстракций. Второй граф идет навстречу. То есть, если нам нужно по этим словам ходить с конца в голову, а нам чуть-чуть позже, собственно, в содержательной части этого доклада нам это, безусловно, понадобится. Это значит, что у нас идет встречная игра, соответственно, справа налево, если по-русски, по-английски или слева направо, если, к примеру, по-израильски. Ну и ещё есть два графика. Один идёт снизу вверх. То есть, от экземпляров мы идём то, что называется восходящие связи. И сверху вниз – это нисходящие связи. То есть, связи идут либо от экземпляров к вышестоящим категориям, либо от вышестоящих категориев, типа там глагол существительное. или какой-то смысл к нисходящим словам или концептам, либо категориям слов, либо различным синонимам, которые обозначают один и тот же смысл. На пути этих графов у нас есть логические операции. которые тоже являются нечеткими. То есть, я здесь условно обозначаю OR и END, но на самом деле эти операции все являются нечеткими, и на самом деле каждой стрелочке здесь может быть приписан некоторый вес. Таким образом, этот граф является взвешенным, и вся логика здесь является невероятностной. Какая она Фу, небинарная, да? Значит, небинарных логики есть как минимум три. Есть так называемая нечеткая логика, есть байзовская логика и есть аксиматическая логика Пейванга, да? То есть, если посмотреть математику, то вот как минимум три логики есть распространенных, которые позволяют обсчитывать некоторым образом, так сказать, обсчитывать веса на вот таких вот небинарных логических графах. Но как мы это делаем, это как бы следующая модель. Сейчас я пока что рассказывал про мотивацию. То есть в конечном итоге мы хотим строить вот такие вот модели. И чтобы эти модели строить, когда мы последний раз занимались этим на проекте у Бена Герцелю, у нас на входе этого пайплайна, который мы пытались конструировать для выявления паттернов в NLP. Да, кстати, в проекте Бена Герцеля подобный подход предполагалось строить на грамматике связей. В грамматике связи нет того, про что я говорил только что. Там нет вот этих вот в четырех графах, направленных справа налево, слева направо, сверху вниз и снизу вверх. Там есть пронятие грамматика связи или граммар, где все немножко проще. Но я про это повторяться не буду. Я про это рассказывал раньше. Если надо, это можно будет либо в вопросах-ответах, либо отдельный доклад сделать. Так или иначе, Прежде чем нам обнаруживать паттерны на уровне слов, нам нужно текст превратить в последовательность токена. И в том проекте, который мы делали в 2017-2019 году, у нас довольно много усилий ушло на то, чтобы написать некоторый самопальный токенизатор, который бы обеспечивал некоторую вменяемую последовательность слов. и символов пунктуации на входе, для того, чтобы она была пригодна для того, чтобы, собственно, на основе этих последовательностей токенизированных учить уже грамматические особенности. Вот там было очень много геморроя, то есть, примерно, наверное, три человека месяца на этот проект ушло. Ну и, в общем, не секрет, что практически все Берты и ЖПТ-3, они на входе используют обычно какие-то захардкоженные токенизаторы. Там есть разные способы токенизации на основе статистики, на основе морфологии, в основном используются на основе словарей, на основе правил. Наиболее хорошие токенизаторы комбинированы, они используют и морфологию, и словари слов, и символов, и пунктуации, и правила. Но с точки зрения проекта, который у нас назывался Unsupervised Language Learning, то есть мы собираемся заниматься Unsupervised LanguageLearning, то есть нам не нужны никакие правила, у нас нет никаких словарей. Вот мы получили в manuscript Войнича, Или к нам прилетели пришельцы, у которых на голове лампочки разноцветные. Они как-то этими лампочками мигают. И мы хотим понять, где в последовательности мигания этих лампочек находятся границы слов. Три раза зелёно-красная лампочка мигнула. Это что? Это было три разных слова? Или это было одно слово? Вот как это понять вообще? Ну и вот, собственно, вся последующая работа с точки зрения этой второй мотивации, она тоже посвящена тому, как нам некоторую последовательность чего-то нарезать на кусочки, предполагая, что это что-то является буквами или иероглифами в естественном языке. Это была мотивация. Дальше я пропущу ряд слайдов, про которые я буду рассказывать на семинаре у Бена Герцеля завтра, где буду разворачивать историю с точки зрения AGI. А сегодня мы поговорим больше про то, как это как эта задача решалась и что получилось с точки зрения собственно НЛП, а и с точки зрения именно разбиения последовательности символов на слова. Значит, какие проблемы мы пытались решить в этом свете? Первая проблема, повторяюсь, это то, что у нас нет явных последовательностей начала и конца, последовательных потоков некоторого опыта. при взаимодействии с окружающей средой для того, чтобы получить пресловутый фидбэк или реинфорсмент с задержкой. То есть нам нужно как-то эту последовательность разбить на куски. Ну и с точки зрения НЛП. Нам нужно избавиться от сложного, неприятного, ненадежного, дорогостоящего ручного процесса настройки токенизации. Тем более, если нам неизвестен заранее язык, который мы хотим учить, мы это в принципе не можем сделать. Вот, ну и, значит, вторая проблема, которую, да, это вот, кстати, третья еще проблема, которую хотелось решить, значит, мы, когда мы, собственно, вот даже при том, что мы в том проекте на уровне, где пытались выучить грамматику на уровне слов, даже с учетом того, что мы все токенизировали вручную, Нам не удалось построить достаточно качественные грамматики, при том, что если мы могли, например, построить много правильных деревьев грамматического разбора вручную, где связи грамматические между словами идентифицированы, и потом вот эти деревья подать построителю правил, то на основе множества правильно построенных деревьев грамматического разбора построитель правил строил правильные грамматические правила. Он строил правильную грамматическую модель, которая потом могла сама в соответствии с этими правилами тексты разбирать. Но это же тоже был читинг, потому что вы можете обучить модель на уже разобранных деревьях, но а деревья-то кто будет разбирать? Тут вы вручную токенизировали, тут вручную построили деревья, но дальше ты дурак сможешь модели построить, а как деревья-то строить? А деревья мы строили с помощью Mutual Information в соответствии с теорией MST парсера. Minimum Spanning Tree Parser на основе Mutual Information, про который я рассказывал. Это отдельная история, что это такое, если будут какие-то вопросы. И, кстати, мы использовали не только Mutual Information, мы использовали BERT, то есть мы тренировали модели на BERT. И мы получали, что это называлось «contextual information», то есть некоторые связи между слов в контексте тех предложений, в которых они встречаются, с использованием этого самого пресловутого «attention». То есть, attention обеспечивается контекстом данного предложения. И вот в контексте этого attention мы смотрим, какие слова друг другом как связаны. И, соответственно, слова, между которыми была больше mutual information, они связывались в первую очередь, потом к ним подвязывались другие, ну и в конечном итоге строилось некоторое дерево грамматического разбора, которое максимизировало взаимную информацию. в рамках всего графа грамматического разбора. Ну и вот на самом деле деревья получались очень длинного качества, мягко говоря. И поэтому походу еще была идея, да, и когда, а поскольку мы брались за задачу токенизации, известно было, что одним из способов ансупервайза токенизации является как раз Mutual Information. Да, и вот хотелось понять, а вообще можно ли что-то придумать для вот такого unsupervised learning, unsupervised структуризации последовательности, может ли быть придумано что-то лучше, чем mutual information. И же с ними conditional probabilities, потому что conditional probabilities – это тоже одна из основ моделирования как раз языков, токенизации без учителя. когда мы переходим к содержательной части то есть у нас есть проблема моделирования языков и она примерно одинаково может быть построена как для предсказания последовательности слов что такое моделирование языков и построение некоторой предсказательной модели которая может предсказывать слова. Соответственно, модель русскоязычная, если ей скормить слово «мама», вот, тогда она может предсказать скорее два варианта. Первый вариант «мама меня любит», а второй вариант «мама мыла раму». Я условно говорю, что если дать модели слово «мама», то вот либо «меня любит», либо «мыла раму». А если дать модели слово «мыло» – «мама мыла» – два слова, то тогда практически наверняка третьим словом будет предсказано «раму». «Мама мыла раму» – здесь уже «меня любит». Соответственно, моделирование в первом приближении осуществляется за счет условных вероятностей, или conditional probability. Соответственно, conditional probability того, что после мама мыла появится рама, то есть у нас что такое conditional probability, Мама мыла раму, вероятность мыла раму, деленная на вероятность мамы мыла. Соответственно, эта вероятность будет достаточно высокая. Ну и вот то же самое мы можем рассматривать в пространстве символов. То есть мы можем граф возможных переходов от слова в слово при формировании последовательности слов рассматривать. А можем рассматривать последовательности символов. Допустим, у нас есть некоторая последовательность символов P, потом буква T. Это из одной из цитируемых статей картинка. с которой мы собственно начинали эту работу, а потом могут быть либо точка, либо буковка S, либо пробел. Соответственно, мы знаем сколько раз у нас встречается P, мы знаем сколько раз у нас встречается T, Мы знаем, сколько раз у нас после Т и П бывают точки С и С пробел. И, соответственно, можем посчитать вероятности. Какая вероятность того, что после П будет Т, какая вероятность того, что после П и Т будет точка. Можем посчитать взаимные вероятности появления Т и точки вместе и П и Т вместе. И третья интересная вещь, которую мы можем посчитать, это так называемая transition freedom. Это тоже штука, которая в одной из цитируемых статей была предложена для токенизации без учителя. И она обозначает очень простую вещь – количество возможных переходов. То есть, например, если после T у нас есть три возможных состояния – точка, S и пробел, то transition freedom перехода от T направо является тройки. А если после P… Коллеги, можно там звук отключить, кто говорит, чтобы мне не переключаться в режим администратора и не мьютить, пожалуйста? У кого звук идёт, отключите с него. А вот если после P возможно 7 вариантов различных символов, то transition freedom после P с перехода с P направо будет 7. Соответственно, transition freedom с P направо будет 7, а с T направо будет 3. Точно так же Transition Freedom можно считать справа налево. То же самое можно считать условные вероятности справа налево и слева направо. Вот, и забегая вперед, значит, обнаружилось, что mutual information, то есть попытка токенизировать слова, используя mutual information по взаимной связи, ну, грубо говоря, если два слова встречаются вместе, мы их сразу склеиваем, да? И если, значит, какое-то другое слово встречается, значит, чаще, чем другие с соседним словом, мы его тоже склеиваем. Таким образом, максимально часто встречающиеся пары мы склеиваем до тех пор, что mutual information не становится ниже какого-то порога, и тогда склеивать перестаем. И смотрим, какие островки слов у нас склеились в данном предложении. И считаем, что как склеилось, так и правильно, и задача оптимизации решается чисто подбором порога. Но, в общем, оказалось, что это работает хуже всех вариантов, которые были перепробованы. То есть, Mutual Information здесь не работало точно так же, как оно у нас не работало при построении деревьев грамматического разбора на основе слов. на уровне слов. Conditional probability, которое тоже использовалось в обоих этих статьях по поводу unsupervised learning, оно работало чуть-чуть получше, но, как я покажу на следующих слайдах, в общем, проблемы были очевидны. То есть, conditional probability тоже не работал. А вот Transition Freedom, она стала работать на удивление хорошо. Правда, немножко не так, как она была описана в статье, но я про это чуть позже скажу. То есть, мы не сами придумали, мы взяли вот этот принцип из статьи, но несколько его модифицировали, потому что в модифицированном варианте получилось лучше. И вот что самое интересное, что вот этот вот Transition Freedom и его эффективность, она удивительно совпала с пресловутой концепцией товарища Фристона по поводу минимизации того, что он называет свободной энергией, вот как Игорь Пивоваров хорошо подметил, что речь идет не о свободной энергии, а о неопределенности. То есть, мозг занимается не снижением свободной энергии, а мозг занимается снижением неопределенности. И в этом смысле процесс образования слов предполагает, что мы собираем воедино, то есть если у нас есть последовательность символов, которые мы хотим побить на кусочки, то мы на самом деле бьем текст на такие кусочки, между которыми минимальная неопределенность возникает при переходе от одного символа к другому. То есть у нас получаются островки определенности, разделенные реками или проливами неопределенности. И вот это вот не получается, что если мы… Ну, давайте я дальше уже пойду, потому что это значит… Так, я вот что-то заговорился, поэтому я сразу, давайте, буду сюда перепрыгну. Буду показывать, как это работает. Вот если мы возьмем, к примеру, пресловитое, самое первое, что мы можем взять, самый простой подход для токенизации, мы можем просто считать вероятности символов. Вот, например, текст где столбики обозначают вероятности. Причем вероятности могут считаться в общем случае и слева, и справа, и налево, если речь идет о н-граммах. Если речь идет о юниграммах и простых вероятностях, это вот первая картинка, то вероятности справа налево и слева направо одинаковые. То есть каждый столбик означает в данном случае вероятность отдельно взятого символа. Вот вероятность пробела, вот вероятность пробела, вот вероятность двойной кавычки, вот вероятность запятой, вот вероятность буквы Т, вот вероятность буквы О. Это, по сути дела, частотности слов в корпусе. Вот и что мы видим невооруженным взглядом, что пробелы через тупую вероятность символов, они отбиваются очень хорошо. То есть, если бы у нас текст токенизировался по пробелам, то мы могли бы просто посчитать частоты символов, выделить самый высокочастотный символ, решить, что это у нас пробел. И, соответственно, как только самый высокочастотный символ появляется, мы в этом месте ставим разрыв. Но это не работает, потому что у нас есть знаки припинания, которые в этой ситуации будут приклеиваться к словам. У нас есть кавычки, которые тоже не будут отделяться. А в китайском языке, например, вообще пробелов нет. Поэтому надо двигаться дальше. Этот метод в общем случае не канает. Что мы еще можем брать? Кроме вероятности, мы можем еще считать производные. Кроме того, что у нас есть график вероятности справа налево и слева направо. Кстати, оранжевый столбик означает значение, если мы идем слева направо, а синий, если справа налево. Соответственно, мы по этим двум графикам можем считать вероятности и пытаться по вероятностям резать. Но вот по вероятностям просто не режется, потому что вот эти кавычки тут и знаки припинания не обрезаются, не выделяются. ну и можем еще отклонение посчитать да то есть грубо говоря мы можем взять посчитать среднее по всем вероятностям это средняя вычесть и вообще получится вот такая красота вот все пробелы значит очень хорошо отбиваются по отклонениям но значит так не работает дальше переходим к условной вероятности Для того, чтобы считать условные вероятности, надо уже брать н-граммы. То есть, например, вот здесь вот этот столбик означает, что если у нас есть вероятность M с последующим пробелом, Мы делим вероятность m с последующим пробелом на вероятность просто m отдельно взятого. Это означает, что если это значение высоко, то это говорит о том, что вероятность появления пробела после m достаточно высокая. Это можно делать на биграммах, это можно делать на тринграммах, это можно делать на граммах любого ранга N. И мы обнаруживаем, что с помощью этих самых условных вероятностей, как с помощью их вероятности в чистом виде на верхнем графике, так и с точки зрения, так и с помощью производных, этих условных вероятностей, так и, значит, если мы отклонения посчитаем, ну как-то, в общем, у нас начинает что-то получаться, вот, но все равно у нас, значит, кавычки не выделяются, запятые не выделяются, вот. Кроме того, некоторые слова у нас начинают распадаться. Вот буковка «Т», а между буковкой «Т» и буковкой «О» у нас, так сказать, разрезание слова на две половинки прошло. В общем, тоже оказалось, что это не канает. Здесь я просто иллюстративный пример привожу, а на больших прогонах это будет ясно на больших корпусах. И вот, например, что теперь дает Transition Freedom. И Transition Freedom на самом деле делает все очень хорошо. То есть Transition Freedom, который как раз считается просто как число переходов слева направо и справа налево вверху у нас идет. transition freedom в чистом виде. А здесь у нас идет отклонение от среднего deviation, transition freedom. И вот получается, что он отбивает как пробелы, так и все кавычки и знаки припинания, если мы правильным зададим порог. То есть, грубо говоря, если мы задаем порог 0,35, то у нас идеальная токенизация получается. Забегая вперед, скажу, что по-русскому мы вышли на токенизацию F-меры 1.0. Что интересно, что в той статье, в которой мы эту идею фридома использовали, ребята предполагали токенизацию на основе так называемых peak values. то есть что такое пиквэлью? пиквэлью это такая простая вещь что если вот у нас вот если мы посчитаем производную вот от этой функции то есть вот у нас есть transition freedom идет справа налево и слева направо если мы посчитаем производную функцию мы получим вот это вот а теперь если мы будем брать в каждой точке сумму положительного значения с плюсом в данной точке и вычитать сумму с мину-минусом в предыдущей точке, мы получим пиковость. То есть, пиковость означает, насколько, с одной стороны, произошло увеличение при подходе в данную точку, а и насколько было снижение при уходе с этой точки на следующем шаге. Соответственно, вот те значения, которые находятся именно на локальным устремлении, они получают в итоге максимум. За счет того, что мы берем плюс производный на одном шаге, минус производный на другом шаге. И вот эти ребята, в той статье, которая внизу указана, у них получилось, что эти пики дают нужные значения. Вот, у нас пики заработали только на китайском. На китайском действительно на пиках получилось чуть-чуть лучше. Так, еще раз прошу поставить на Mute у того, у кого телефон включен. Вот, а вот если брать Derivative, Derivative, то вот так это, да, Tradition Freedom Deviation, то есть у нас лучше всего получилось как раз на Transition Freedom Deviation. Вот здесь я показал F1-1.0. В качестве примера, естественно, мы попробовали Transition Freedom считать как на юниграммах, так и на биграммах, и на триграммах, в общем, до грамм размерности 7. Ну и, что удивительно, получилось лучше всего на униграмах, на китайском получилось лучше всего на биграмах. Вот пример. пример того подбора гиперпараметров. Корпуса. Теперь, собственно, на чем мы экспериментировали. Экспериментировали мы на китайском, на английском и на русском. Каким образом? Учились мы на больших корпусах примерно такого размера, причем на разных корпусах. На китайском один корпус был, новостной корпус 270 мегабайт, другой новостной корпус 8,5 гигабайтов. В английском мы брали аж порядка семи у нас получилось. Браун, Бутерберг Чилдрен, Бутенберг Адалт. Собственный корпус из соцсетей, из Твиттера и Реддита натащенный и разные комбинации этих корпусов. Для русского брали две половинки – тренировочную и полную корпуса Русейч. так называемого, где лежит корпус для тренировки того, к какому возрасту текст относится к детям или взрослым. Соответственно, 141 мегабайт и 825 мегабайтов. Вот эти разные корпуса использовались для тренировки. А для проверки качества токенизации использовался совсем другой корпус 100 предложений параллельного англо-читайского корпуса с дополнительным переводом на русский. То есть с помощью гугла и небольшой ручной корректировкой он получился параллельный тройной корпус английский. русский, китайский соответственно одни и те же тексты на разных языках. И они использовались для того, чтобы сравнивать результаты токенизации относительно вот этих моделей. Дальше, каким образом мы пытались токенизировать в промышленных масштабах. Mutual information мы сразу выкинули. первое мы пытались строить токенизировать на основе собственно вероятности либо условной вероятности на основе ее отклонений на основе ее производных значит а также на основе transition freedom значит тоже прямой обратный deviation ее отклонения прямой обратной и производной да ну еще тут я забыл указать peak values то есть для transition freedom также считались peak values Соответственно, одна попытка токенизации предполагала, что мы пытаемся делать токенизацию с помощью одной из этих выбранных метрик. Какие гиперпараметры были? Во-первых, какая комбинация N? То ли мы на униграммах работаем, то ли на биграммах работаем, то ли на триграммах, то ли комбинируем N различные размерности. Вот на каком пороге отсекаем. То есть, какой порог отсечения, в какой момент увеличение или уменьшение значения метрики означает, что в этом месте нужно разрыв ставить. Это Threshold for Segmentation. И еще один порог был – порог для сжатия модели. То есть, мы в какой-то момент обнаружили, что если модели разряжать, как это прунинг называется, то есть, если графовую модель вот этих переходов между символами поджимать, выкидывая связи с низкими частотами, то, в общем, качество либо не падает, либо улучшается до каких-то пределов. Ну и, наконец, оценку качества токенизации мы делали следующим образом. Мы брали для русского, это был простенький токенизатор, написанный вручную, что мы сперва бьем на пробелы, а потом по словарю отрезаем известные знаки пунктуации, так чтобы это без ошибок работало на маленьком тренировочном корпусе с учетом того, что в нем можно было встретить. А для китайского брался некий токенизатор под названием джиеба. Это гибридный токенизатор, который умеет делать всё, начиная от китайской морфологии, статистики, словари и закархоженное правило. Вот утверждается, что это лучший китайский токенизатор. Соответственно, с помощью референсного токенизатора того или другого мы разбивали каждое из этих 100 тренировочных текстов на последовательность, после чего этот же текст мы разбивали на токену с учетом натренированной нами модели переходов и этих гиперпараметров. После чего у нас получалось два множества токенов. Ну и мы считали просто Фмеру пресловутую по этим двум множествам. Ну и что у нас получилось? Давайте я перейду к выводам, а потом немножечко иллюстрации. Получилось, что Transition Freedom в любом случае, безусловно, лучше как взаимной информации, так и условных вероятностей для решения данной задачи. Второе. Для русского и английского нужны одни гиперпараметры, очень близкие. А для китайского совсем по-другому. Начиная с того, что для русского нужно Variance брать для токенизации, а для китайского нужно брать Pequelius. Третье. Для русского и английского f-мера показалось вообще удивительно хорошее. То есть для английского 0.96, для русского 1.0. Для китайского получилось хуже 0.71, 0.92. Как считать? Дело в том, что я к сожалению китайский не знаю, может кто-то знает китайские обсуждения. Может прокомментировать, но вот получилось так, что, допустим, мы берем, токенизируем, сравниваем свою токенизацию с референсной токенизацией, и видим, что некоторые символы побились на кусочки, в то время как референсная токенизация дает эти символы как один. Или наоборот, референсная токенизация дает два символа по отдельности, а у нас они слепились в одно слово. Но после того, как мы это забиваем в Google Translate, слова по отдельности, когда мы эти буквы забиваем по отдельности, и когда мы эти две геероглифы забиваем вместе в Google Translate, в общем, смысл совпадают. То есть, грубо говоря, если я беру два символа вместе, получается дурак, слово дурак, а если я беру символы по отдельности, это получается глупый и человек. Ну и смысл-то один и тот же. Глупый и человек – это дурак, это одно и то же. То есть, в этом смысле Есть такая гипотеза у меня, что в китайском токенизации она обладает некоторым свойством амонимии, то есть можешь как хочешь так и токенизируй, а смысл от этого не изменится. И поэтому я допускаю, что на самом деле у нас в китайском токенизации, если переходить на уровень смысла, то она лучше. И мысль как бы на будущее, что на самом деле Процесс токенизации и процесс грамматического разбора с точки зрения частей предложения и позиционных ролей грамматических – это на самом деле процесс неразрывный. И нельзя их разбивать на два этапа. Смысла в этом никакого нет. Дальше. Следующий вывод. Большой корпус совершенно не обязательно увеличивает качество. Вот, например, на английском у нас самые лучшие результаты получились на брауне. То есть, тренировка на самом маленьком браун-корпусе дала лучшие результаты. При этом подрезка моделей, когда мы выкидываем малознающие связи, которые, очевидно, являются мусором, в общем, обычно даёт некоторые статистические улучшения, если не сильно подрезать. Вот, значит, еще следующий интересный вывод получился, я его чуть позже поиллюстрирую, что если мы будем пытаться смотреть на проблему токенизации, unsupervised именно токенизации, как на решение задачи обнаружения лексикона, Получили, говорю, некоторую последовательность миганий лампочек от инопланетян. А теперь хотим построить словарь инопланетянского языка. Запустили нашу систему, записи всех этих миганий лампочек туда загрузили. Она нам дает словарь. Только без перевода. То есть, мы все слова знаем, а как переводится, не знаем. Если вот такую задачу решать, так вот выясняется, что задача решается просто прекрасно. На русском и на английском. То есть, на русском и на английском мы практически в лед получаем словарь вместе со справочником знаков припинания. Дальше который уже можно переводить или что-то там еще с ним делать. Спрашивать, а что означает это слово? Что означает это слово? Что означает этот знак припинания? или символ, или последовательность символов. Но они уже есть. В китайском получилось хуже. Я допускаю, что причины похожие тем, что я говорил, что можно так разбить, а можно так побить. Ну и последний эксперимент, который ставился, это брали из тех же самых 100 текстов, просто выкидывали все пробелы и смотрели, сломается бензопила о лом или нет. Бензопила, естественно, об лом ломалась, но ломалась достаточно понятно. То есть, ломалось достаточно понятно. То есть, грубо говоря, там, где оно неправильно резало слова, можно было понять, почему оно неправильно порезало. Потому что просто возникает некоторая двусмысленность в слитной речи. которая разрешается только за счет того, что мы пытаемся на лету построить некоторый оптимальный граф взаимодействия слов друг с другом, то есть мы не только на слова разбиваем, мы еще смысл между ними выделяем. Кроме того, в устной речи, хотя она и слитно идет, то у нас там все равно есть некоторая артикуляция, которой нет в тексте. И эта артикуляция позволяет структурировать поток этих звуков, которые мы производим. Там структуризация немножко по-другому идет. В общем, результаты получились вполне внятные. Ну и теперь еще немножечко картинок. Вот на английском языке я картинки показывал. Да, вот, кстати, собственно, как поиск осуществлялся. То есть, допустим, для английского примеры для Браун-корпуса. По вертикали идут различные наборы ранков грамм. То есть, если единичка, в этой строчке все делалось на униграммах. А вот здесь, вот во второй строчке, все делалось на биграммах. А если на первый, второй, вот раз, два, три – это все делалось, допустим, на средних значениях по униграммам, биграммам и триграммам. По всем трем посчитали справа налево и слева направо значения вот этих вот вероятностей или в данном случае transition freedom. И на пиковых значениях выше порога делаем отсечение. По горизонтальной оси здесь как раз пороги. То есть, что мы здесь видим, что по верхней картинке, грубо говоря, на биграммах на пороге 0,6, получается 0,92. А если мы возьмем среднее по униграммам и биграммам, то на порогах 0.4 и 0.5 получается 0.94. Это не сжатая модель. А если мы модель подрежем, то получается одна тысячная доля процента получается, подрежем на сотой доле процента, подрежем частотности связи, то мы получаем на пороге 0,4, на униграммах получаем сразу 0,99 f-меру. Это на английском. На китайском вот что у нас получилось с Transition Freedom. Причем на английском Transition Freedom именно Variance считался. Вот. А на китайском лучше Variance работало абсолютное значение Transition Freedom. Вот здесь вот 0.6 получается. А вот если взять пики, Peak Value пресловутый, то получается Peak Value у нас работает лучше всего на биграммах. Причем модель тоже нужно чуть-чуть поджать. Модель нужно чуть-чуть поджать. Тоже в данном случае на одна стотысячная процента. Подрезаем модель, берем биграммы, и на биграммах получается, начиная от нуля до 0,05, уже везде получается 0,7. То есть, если значение больше нуля, то там ставим разрыв. стоп это значит у меня на да вот на на 0,1 процента нужно было резать чтобы 0,71 получилось это на китайском ну вот пример того значит как по-разному на китайском работают например transition freedom и conditional probability вот вверху текст И вот видно, что Transition Freedom разрезал эту последовательность правильно. То есть все слова, если порог задать 0,15, все слова правильно разрезались. Вверху токенизация основная, наша нами полученная, а внизу токенизация реферсная. а вот внизу получается интересный эффект conditional probability conditional probability этого перехода она сделала как бы неправильную токенизацию то есть вот эти вот видите вот здесь вот три последние символы, четыре последние символы они все идут по отдельности в эталонной токенизации с помощью джеба токенизера А на основе условных вероятностей произошло слепление. Вот это последнее слепление, оно очевидно ошибочное, потому что это вот точка. То есть китайская точка такая, это точка. значит, и она явно не должна была слепиться с этим символом. А вот это вот слепление, это вот как раз, с моей точки зрения, некоторая амбициозность, потому что, ну, я не помню точно китайский перевод, это можно, значит, погуглить и вспомнить, но смысл примерно такая же, что если в Google Транслейте перевести вот этот вот символ, потом перевести вот этот вот символ независимо, то получится два слова, смысл которых на английский будет тоже тот же самый что если мы взять возьмем эти два символа и опять-таки переведем на китайском поэтому вот я не знаю может быть на китайском может быть можно по-разному токенизировать и так и так значит наконец итоги Итоги, да, почти итоги, я почти закончил. Первое, что мы получили для английского. Вот у нас токенизация на основе свободных переходов получилась 0.99. Если попытаться делать токенизацию в английском на основе лексикона, то здесь мы добавляем еще некоторый способ референса. То есть пытаемся делать токенизацию на основе известного словаря, тоже известный способ токенизации, и получаем то же самое. То есть получается, даже если мы не знаем язык, С помощью нашей тегнизации мы можем получать ровно те разбиения на символы, которые получаем при знании языка. Второй референс. Пытаемся проверить, насколько все символы, выявленные с помощью такой токенизации, оказываются в лексиконе. Смотрим то, что называется Precision. Lexicon Discovery Precision. И обнаруживается, что 0,99 слов действительно оказываются в лексиконе. Это для английского языка. Дальше. Так, эти две строчки пока пропущу. Следующие строчки в середине русские. В русском как разбиение на символы, так и обнаружение лексикона. Здесь f-мера, тут precision 1.0. При этом, если мы будем пытаться на основе лексикона разбирать тот же самый текст, получится еще даже хуже. Ну, потому что просто русский язык более богат и, видимо, или, можно сказать, просто как ляпуса такие, неподходящие лексикону. В общем, в русском языке получилось даже хуже. В китайском токенизация на основе нашего метода получилась 0.71, а вот если на основе лексикона китайского со словариком 0.83. Если мы будем пытаться решать задачу обнаружения лексикона, то почти все слова, которые мы выявили, несмотря на то, что они не совпали с эталонным разбиением, они оказались у нас в словаре. При том, если мы будем брать токенизацию с помощью эталонного токенизатора, с которым мы сравнивали и получали вот этот вот 0.71, то там будет 0.94. То есть получается, что эталонный токенизатор, он в общем тоже не бог и тоже не все слова правильно обнаруживает. И в этом смысле можно сказать, что мы очень близки. по лексикону, соответственно, если мы токенизируем, то получается как бы лучше, но не в разы. Ну и последнее, значит, тоже для справочки, если мы попытаемся убрать все пробелы, то получится все не очень хорошо. То есть, на основе лексикона, конечно, получается лучше, чем нашим способом. Как для русского получается без пробелов, нашим методом получается 0.42, а на основе лексикона 0.79. За счет неоднозначности получается 0.79, потому что непонятно, если без пробелов можно и так, и так. То есть, если не понимать смысла того, не подбирать смысл взаимосочетания гипотетических слов в последовательности символов, то это невозможно установить. ну и в русском порядке примерно те же самые значит на основе нашего метода без пробелов 0.26 на основе лексикона 0.72 ну и напоследок значит попытались понять насколько вот эти вот модели с переходов позволяют классифицировать символы вот ну и получилось что мы достаточно хорошо можем строить, кластеризовать знаки припинания, вроде как открывающиеся скобки и кавычки идут вместе, закрывающиеся скобки и кавычки идут вместе, при этом запятые после слов идут вместе со скобками и восклицательными знаками, точками и запятыми. Гласные вместе, согласные отдельно. При этом получилась удивительная вещь, что если мы берем русский корпус, то кластеризация англоязычных символов получается более правильной, чем если брать англоязычный корпус. То есть, с точки зрения знания смыслов этих символов, небольшого количества англоязычных слов, которые появляются в русских корпусах, они позволяют построить более правильную модель английского языка, чем английские тексты. Ну вот это просто другой способ визуализации тех же самых деревьев. Мы вот подобным образом пытались визуализировать деревья слов для того, чтобы строить антологические, грамматические модели языков в предыдущем проекте. Но здесь я попробовал визуализировать таким же образом деревья, как это как раз пресловутая иерархия из тех четырех про которые говорил только значит вертикальном направлении категоризация частей речи в данном случае таких как слова ну и последним значит напоследок заключение значит Имея вот такое качество Supervisor токенизации на основе Transition Freedom, как Recall, так и Precision получается достаточно хорошее в качестве некоторого начального приближения, то есть есть понимание того, что 1.0 где-то может оказаться что где-то может быть там на каких-то корпусах будет других корпусах будет не один ноль а существенно меньше но тем не менее мы же понимаем что в чистом виде язык никто не учит то есть мы все равно же мы же не ожидаем от того что если мы ребенка там посадим на 16 лет запертого в комнате с собранием сочинения мнению русской литературы, что он, вы и все учебники по школьной программе, что он через 16-17 лет выйдет из этой закрытой комнаты и сразу же поступит в ВУЗ. То есть, мы же все-таки, кроме как дидактического материала, даем некоторое наставление, обучение, подкрепление, ошибки исправляем в контрольных работах. Но как начальное приближение для последующего self-reinforcement использование такого метода на основе Transition Freedom для структуризации потоков как минимум, символов естественной языки окажется вполне приемлемым. Следующее, что оптимальные пороги и оптимальные способы метрики использования вероятности перехода, они специфичны для языка. Как их определять? Это интересный вопрос. Я могу предположить, что это вообще вопрос генетического отбора. У меня есть гипотеза, что, грубо говоря, один человек рождается с порогом 0.4, а другой человек рождается с порогом 0.5. И это как раз связано с тем, что условно говоря, что некоторые люди там лучше в 4 года осваивают человеческий язык, а некоторые в 3 года, а некоторые в 5 лет не могут освоить. Или есть люди, которые не знают ни одного правила, но у них есть вожденная грамотность, они всегда пишут без ошибок. А есть люди, которые там кандидаты и доктора наук, но они без ошибок писать не могут. Хотя люди прекрасные и умные. И вот есть такое ощущение, что вот некоторые гиперпараметры, которые связаны именно с восприятием речи у людей, они просто генетически предопределены. средние значения, возможно, появились в результате некоторого естественного отбора. Дальше. Кластеризация частей речи в пространстве вот этих вот переходов в графе состояния от символа к символу, она может дать некоторые подсказки в части с морфологией и структуры морфологического и пунктационного корпуса для то, что называется low-resource and domain-specific languages, то есть, хотим автоматизировать грамматические и синтаксические разборы по строению языковой модели для какого-нибудь языка племени Тумба-Юмба, ну, в принципе, можно вот с этого начинать, а дальше уже корректировать. Вот, значит, соответственно гибридизация вот этого подхода с подходом, основанным на лексиконе, тоже для этих же самых доменспецифических и low-resource языков может быть полезна. Вот, на основе того, что я рассказал, можно дальше лезть на уровень слов и пытаться понять, может быть, использование метрик на основе вот этой вот Transition Freedom окажется будет работать для Supervised Grammatical Structure Learning на уровне слов, может быть, будет работать лучше, чем на основе Mutual Information, что мы делали раньше. Следующее, возвращаясь к первой мотивации, что, может быть, если мы научимся с помощью вот такого метода нарезать любые последовательности любых состояний на кусочки, может быть, после этого мы сможем более правильно делать глобальный фидбэк. и категоризировать некоторые элементарные последовательности в укрупненные последовательности, вроде «доведи ракетку до правого угла» или «доведи ракетку до левого угла» и оперировать с некоторыми понятиями на более высоком уровне. Ну и интересная как бы еще идея, которая возникла по ходу, которую навеяло в ходе всей этой дискуссии, что вообще, если мы будем пытаться работать с историческими последовательностями, неважно причем то ли это последовательности там каких-то состояний рынка в случае финансового прогноза, о чем я буду как раз под более подробно говорить завтра на семинаре у Герцеля, вот, либо это последовательности символов, то мы, используя вот эту вот принцип самоподкрепления за правильность предсказания, мы можем, в принципе, использовать аппарат reinforcement learning для unsupervised learning на исторических данных. То есть, как может выглядеть такой подход. Вот у нас есть последовательность каких-то состояний символов, слов, состояния рынка, движения руки, ноги какого-нибудь робота. Вот мы идем по этой последовательности и пытаемся предсказать, что будет на следующем шаге. Если мы угадали, мы получили реинфорсмент нашей модели. Если не угадали, получили, может быть, отрицательный реинфорсмент. Тоже отдельный вопрос. Нужно ли только положительное подкрепление или отрицательное. То, что я рассказывал в прошлом году, напомню, говорит о том, что лучшее обучение в конечном итоге получается только если пряниками кормить, а кмутом не бить. То есть, если за ошибки не наказывать, то в конечном итоге обучение к достижению конечного результата обучение достижению конечного результата происходит быстрее. Но это уже лирика, детали. А смысл такой, что используя метод reinforcement learning через self-reinforcement за счет правильности текущей поведенческой модели, может позволять, собственно, unsupervised learning на любых исторических данных. Что, собственно, и хотелось рассказать. Спасибо за внимание. Вопросы. Вижу вопрос от Бориса Новикова. Так как же измеряется успешность модели алгоритма или результатов их применения? Смотрите, здесь успешность измеряется очень просто. Две успешности. Первая успешность – это соответствие того разбиения символов, которое сделал алгоритм. тому разбиению на символы, которое сделал захардкоженный алгоритм, который дает ожидаемое поведение. 

S00 [01:05:57]  : Есть эталонный алгоритм или сравнивается вручную? Слышь ты меня? 

S04 [01:06:05]  : Да, еще раз, смотрите, смотрите. Вот мы берем текст. То есть, вручную ничего не сравнивается, все сравнивается автоматически. Как это сравнивается? Есть 100 предложений. Для каждого предложения делается два способа токенизации, разбиения символа. Первый способ с помощью нашего алгоритма. Второй способ с помощью алгоритма, который известно, заведомо, что для данного языка он работает правильно. 

S00 [01:06:36]  : А что есть такие? Что? Разве есть такие, которые 100% правильно работают? 

S04 [01:06:43]  : Для данного набора текстов. Ну, давайте, да, я уточню, я уточню, смотрите. Значит, для русского и английского, да, для русского и английского я гарантирую, что он работает правильно, потому что после того, как я вот эти вот сто текстов разбил вот этим вот эталонными тогенизаторами для русского и английского, которые сам же написал, исходя из простого правила, что сперва разбиваем на пробелы, а потом отрезаем все знаки припинания, вот, для них я проверил, что все правильно. для китайского хуже, потому что китайского я не знаю. Я просто вот взял, прочитал, что джея ботокинайзер – это лучшее, что есть у прогрессивного китайского человечества. Поэтому я просто сравнивал с ним. Соответственно, вот у нас есть два разбиения. После чего мы сейчас посмотрим, насколько они соответствуют. Степень соответствия количественно определяется так называемый F-score или F1-score или F-measure. Как она считается, я ссылку в группу выкладывал, это можно посмотреть в интернете. 

S00 [01:07:43]  : Второй вопрос. Есть уже эталонный алгоритм, который дает идеальный результат. Каково прикладное значение всех прочих алгоритмов? 

S04 [01:07:55]  : Прикладное такое, что для тех языков, где такого алгоритма нету, для других языков, где такого алгоритма нету, попытаться сделать такой алгоритм. Грубо говоря, смотрите, как работает такой алгоритм. Один из таких алгоритмов. Берется словарь, и на основе словаря делается разбиение. Вот, но для того, чтобы сделать такой алгоритм, нужен словарь. Я утрирую, потому что там не только алгоритм, там еще и правила, но вот утрированно. Вот, а мы делаем что? Мы определяем словарь, после чего у нас автоматически получается алгоритм. То есть после того, как мы сделали словарь, мы дальше можем этот же грамматический разбор делать просто дешевле, да? То есть вот сейчас нам для того, чтобы сделать грамматический разбор с помощью этого метода нам надо построить дорогостоящую модель вот я кстати поясню дополнительную информацию вот на русском и на английском мы пробовали значит на рангах n грамм от единицы до 7 а на китайском у меня на компьютере 32 гигабайта Вот и допустим маленький корпус китайский, модель на маленьком китайском корпусе у меня до N равным 3 влезала в 32 гигабайта памяти, а на большом китайском корпусе модель с N равным 3 уже в память не влезала, только модель с N от 1 до 2. Вот, соответственно, ну вот, соответственно, ну это затраты, вот. Соответственно, мы один раз эти затраты сделали, получили словарь, дальше со словарём всё гораздо проще. 

S00 [01:09:38]  : Хорошо, тогда у меня гипотеза или совет. То есть русский, английский, они похожи по длине слов, типичные. А есть немецкий язык, где слова очень длинные. И будет ли там пробел чаще, чем какая-нибудь буква Эй, например? 

S04 [01:10:00]  : Хороший вопрос. Языков очень много. Можно всю жизнь заниматься этой задачей. 

S00 [01:10:09]  : Если вы говорите, что у вас универсальный алгоритм, который для лапочек годится, вот интересно проверить его на немецком языке. 

S04 [01:10:20]  : Спасибо. Не возражаю. 

S00 [01:10:24]  : Спасибо. 

S04 [01:10:28]  : Хорошо. Коллеги, еще вопросы? Владимир, неужели у вас не будет вопросов или конструктивной критики? Ну тогда, наверное, хорошо. Игорь? 

S05 [01:10:54]  : Я просто от себя откомментирую. Я лично просто из интереса послушал, потому что вся эта токенизация, мне кажется, немножко далековато, но любопытно. Особенно никаких вопросов нет. Интересно. Я себе и сделал заметки. Спасибо. 

S04 [01:11:12]  : Хорошо. Коллеги, тогда всем спасибо. И если других вопросов нет, комментариев, то тогда до новых встреч. 

S03 [01:11:20]  : Анатолий Георгиевич, можно, да? Да, конечно. Мне понравился в начале выступления красивый образ с волной. То есть наблюдение за волной, как и за средой. И потом человека на доске. уже оперирующего таким набором действий в соответствии уже с ощущениями в этой среде. А с языком в этой модели как-то получается, что закономерности находятся только в самом языке. Вот немножко мне не хватило языка, действующего в среде. 

S04 [01:12:07]  : Вот как-то… Смотрите, вопрос хороший. Я давайте пару комментариев сейчас сделаю. Олег Серебренников сейчас появился и удалился. Мы некоторое время с Олегом очень много обсуждали пресловутую задачу пинг-понга. вот и вот Олег очень правильно и очень хорошо настаивал на следующей позиции что когда человек Возьмем, к примеру, игру в пинг-понг, да? Пресловутый понг, допустим, от Ари, да, из этого самого опена Яйджима. Вот. Что в реальной жизни нет такого, что человек начинает просто заниматься тем, что обучение в пинг-понг происходит не через максимизацию тех вау, которое человек получает в результате выигранной партии. Пинг-понг так не учат. Во-первых, человек какое-то время вообще учится держать в руке ракетку. Он же привыкает. Как нужно сжимать ракетку? С одной стороны, чтобы руку с удорогой не свело, с другой стороны, чтобы ракетка из руки не выводилась. Потом он понимает, как нужно двигать рукой, с какой скоростью, для того, чтобы попасть по мячику. Потом он должен понимать, строить некоторую модель движения мячика. Если по мячику ударили с левой стороны, он полетит в правую сторону, а если в правую, то в левую. То есть прежде чем человек на самом деле начнет пытаться не то что выигрывать партии в пинг-понг, а хотя бы не пропускать мячи, он должен массу всего научиться сделать в операционном пространстве, теннисного стола, мячика и ракетки. И всё это делается, и это всё не reinforcement learning. Это всё как раз пресловутый experiential learning, когда человек пытается строить предсказательные модели движения мячика, движения своего тела относительно стола, движения ракетки в зависимости от движения собственной руки, движения собственной руки в зависимости от напряжения собственных мускулов. И уже когда вот эти все элементарные скиллы выучены, человек начинает сначала отбивать мяч, а потом еще следовать правилам игры для того, чтобы в конечном итоге получить вау от зрителей от выигранной партии. Вот это вот один момент. Игорь, я вижу твою руку. Сейчас я первые два момента откомментирую. И когда мы с Олегом это говорили, как раз я вспомнил историю, которую рассказал в начале про то, как меня учили. серфингу, что нужно вначале пропитаться некоторыми закономерностями той операционной среды, в которой ты собираешься работать, а уже потом, собственно, переходить на реинфорсмент от этой среды. А уже потом, после того, как ты начал получать реинфорсмент от этой среды, научился вставать на волну и ехать по ней, после этого ты уже можешь выходить на третий уровень, когда ты будешь получать реинфорсмент а там от девочек или мальчиков, которые тебе аплодируют за то, что ты проехал дольше всех на самой большой волне. То есть реинфорсмент это как бы уже самое последнее дело. До этого идет в чистом виде селф-реинфорсмент, причем селф-реинфорсмент сначала от собственно, при удачных предсказаниях, а потом от взаимодействия со средой. Теперь насчет языка. Вот давайте я сейчас даже быстренько покажу слайд, через который вначале перепрыгну. Как раз эта история про Unsupervised Language Learning ко мне примерно на 50%. Я ей с детства, можно сказать, интересовался. Дополнительно, к интересу детства, мне большой толчок дали товарищи Бен Герц и Лилайна Свепстас. Как раз в семнадцатом году они меня в эту историю втянули. И там как раз та модель, которую они исследуют, используют, она не такая, как я рассказывал в начале. А там у них модель построена на грамматике связей. Вот, где грамматика связи предполагает следующее, что у нас есть некоторые объекты в пространстве, ну вот в конкретном случае языка, это слова, да, вот там this, do, I. И есть связи. И вот, например, устойчивые комбинации связей, допустим, если от WAN идет связь CS направо, CV направо и связь MVS налево, то вот этот вот кластер связей, он описывает то, что называется некоторую lexical entry или лексическую структуру, которая соответствует слову where, а также слово which. Соответственно, некоторая группа слов, которая соответствует определенной грамматической категории, она описывается в пространстве вот этих типов связей, некоторые позиции в этом векторном пространстве. Но дальше, в том докладе, который был на конференции OpenAGI в нашей секции Interpretable Language Learning. Linus как раз делал доклад, из которого я этот слайд взял. Эти слайды есть в онлайне, я могу конкретно на них скинуть ссылку. Там он как раз показывает, как этот подход, основанный на Условно-грамматики связей, которая, очевидно, должна как-то быть немножечко расширена от языка на некоторый формально выше уровень. Как она может быть использована для описания всего угодно, в том числе для того, что называется граундинг. grounding-text-learning, где он как раз показывает, что вот у нас любые, так сказать, события или последовательности каких-то действий, допустим, motor-sequence. Вот есть motor-sequence, которая состоит из элемента rise-elbow и turn wrist. Вот между ними есть связь, последовательность, что после того, как я поднял локоть, я должен повернуть запястье. Вот. И есть некоторый текст. It hurts when I do this. То есть, когда я делаю это, мне больно. То есть, здесь какая ситуация представлена. Приходит человек к врачу. Врач спрашивает, что у вас болит. Пациент с одной стороны поднимает локоть или предплечье. Поворачивает запястье и говорит доктору, когда я делаю так, это болит. Соответственно, происходит не только связывание вот этих вот слов в некоторую грамматическую структуру, но и связывание некоторых действий в некоторую последовательную цепочку, а также связывание вот этой последовательности физических действий с некоторой последовательностью произнесенных слов. Дальше понятно, что тут же может быть некоторая картинка, тут же может быть некоторый текст. Человек может написать это на бумажке, человек может произнести это вслух. Утверждается, что все эти связи описываются с помощью взаимной информации. Во что я сейчас плохо верю, у меня есть такое ощущение, что математика и формулы тут должны быть совсем другие, немножко другие. У меня есть гипотеза, что эти формулы как-то должны выходить как раз из пресловутой свободы переходов, но это нужно проверить. Но тем не менее, теоретически, вот этот подход он масштабируется на любые задачи, в том числе на мультимодальное обучение. 

S03 [01:20:25]  : Ну да, вот в этом примере немножко видно именно с физичностью локоть, движение. 

S04 [01:20:34]  : да здесь немножко это прослеживается да есть такие есть на самом деле статьи на интересное значит но там правда это статьи они основаны на нейросетевом подходе вот я к сожалению сейчас не вспомню ссылку но там как бы показывается там с одной стороны дается некоторая управляющая программа ну то есть там совсем по сути такой же пример то есть грубо говоря делается параллельный корпус С одной стороны у нас текст на английском языке, а с другой стороны у нас просто управляющая программа для робота, на некотором условном языке DSL, который управляет движениями робота. И задачу обучения робота, некоторые последовательности действий, она решается в чистом виде как задача машинного перевода. Вы можете перевести английский язык на русский, можете перевести английский язык на китайском, а можете перевести английский язык на программу, исполнимую сенсомоторами робота. Какая разница? 

S03 [01:21:41]  : Наверное, должна быть какая-то более обширная, более многофакторная. Как в примере вы говорили, в теннису учится человек, нужно много факторов учитывать. Здесь, наверное, тоже с языком должна быть как-то… 

S04 [01:21:57]  : Конечно, да. Я сразу скажу, хорошо, что вы про это сказали. Смотрите, такой простой подход, если вы обратили внимание, что лучше всего работает на N-граммах и никаких B-грамм не нужно. я прекрасно понимаю, где это не будет работать. Например, если мы посчитали, что transition freedom для точки, что вправо, что влево достаточно большое, что точка всегда отрывается от слов, это не будет работать, если мы пойдем в среду математических текстов. Потому что у нас в математических текстах Вот, у нас точки будут сплошь и рядом находиться внутри слов, внутри чисел с плавающей точкой. А если там у нас будет много интернет-ссылок, то опять-таки мы будем разбивать на куски интернет-ссылки. Соответственно, контекст, он безусловно важен. То есть то, что я сейчас рассказал, там результаты, несмотря на то, что они хорошие. Во-первых, они хорошие для конкретного корпуса, который взят из реальной жизни. То есть, это как бы уже имеет практическую ценность. А во-вторых, есть понимание, что это просто некоторое начальное приближение. И в конечном итоге, безусловно, даже вот если мы пытаемся построить вот такую вот интерпретируемую символную модель, где все описывается понятным графом с понятными сущностями на вершинах этого графа. В любом случае, там должна быть некоторым образом, каким это тоже интересный вопрос, но учитываться контекст. То есть, мы должны понимать, что вот у нас, если у точки transition freedom в окружении символов один, то в окружении цифровых цифр этот транзиссион Фрида может оказаться другой, но грубо говоря. 

S03 [01:23:53]  : Спасибо. 

S04 [01:23:54]  : Хорошо, Игорь. Спасибо, Илья. Игорь. 

S05 [01:23:59]  : Да, спасибо. Вот теперь у меня появился комментарий по поводу того, что ты сказал, по поводу этого пинг-понга. потом и это и удачно что здесь присутствует александр болдачев как раз мы сейчас эту тему можем немножко затронуть с одной стороны у меня ощущение что я может быть даже и был первым кто ну неважно там первым не первым в общем один из тех кто точно значит там говорил про то что в пинг-понг так вообще в любую игру конечно человек так не учится на реинфорсмент ленингах а мы все-таки там начинаем бежать ракетку и прочее. Но потом я эту мысль, честно говоря, сильно там додумал. И у меня есть на эту тему пара замечаний. Во-первых, игра Atari, пинг-понг, абсолютно отличается от настольного тенниса. но потому что это не в физическом мире в настольном физическом мире мы стоим у нас куча степеней свободы там 600 разных мышц которые за это отвечают там держу ракетку, мячик летит так, леток, мне нужна куча разных групп мышц. В игре Atari там ракетка движется всего в двух направлениях. Реально там в OpenAI Gym там шесть как бы движений, на самом деле их три, вверх-вниз и ничего не делать, но они там дублированы Поэтому их шесть. И в этом смысле это как бы другая история. Это первый момент. Все-таки это не реальная физическая игра, в которую нужно, например, учиться держать ракетку. Грубо говоря, в пинг-понге в Атаре ракетку мы держать уже умеем. Там это не надо учиться. Просто там нужно понять вверх-вниз двигаться. Но я бы сказал так, что Как я теперь думаю, мы, я, например, и многие другие люди, мы в пинг-понг или в другие игры учим играть алгоритмы не в том смысле, что они в пинг-понг конкретно должны научиться играть, а в том смысле, что когда я учу алгоритм на некоторой виртуальной игре, абстрактной, я предполагаю себе, что это будет алгоритм, который способен из потока данных вытаскивать закономерности заранее не зная как устроен этот поток данных и в этом плане ну как бы не сильно важно как эта игра устроена она устроена там так или и так и следующий тонкий момент который уже как бы подходит сейчас к вопросу, который мы с Балдачевым там начали обсуждать, я не успел там ответ очередной написать, но сейчас могу там что-то сказать, ну или потом напишу еще в чат, для всех в чате. Значит, можно ли от маленького reinforcement learning, от обучения простой модели, простым действием с reinforcement learning в конечном итоге перейти к какой-то более сложной модели? например, модели мышления, которые оперируют, например, словами или логическими цепочками. Это принципиальный момент, который в зависимости от этой развилки, от того, человек верит в это или не верит, он действует так или иначе. Я уверен, что можно. И могу сказать, что ребенок, безусловно, учится с детства. Вы должны понимать, реинфорсмент ленинг – это упрощение, это некоторая математическая модель того, что происходит в жизни, но вполне себе рабочая. В этом плане ребенок, например, учится, конечно, играть не в пинг-понг, но с самого детства самые простые вещи, когда он, грубо говоря, съел ложку каши, мама ему говорит, молодец. Хотя он ничего там, ну, гигантского-то особенно не сделал. Он делал то, что, в общем, необходимо для жизни. И мама его постоянно поддерживает. Мама, папа, кто-то говорит ему, что он делает очень простые действия, простые движения и за это получает некоторые поощрения, комплексные, иногда положительные, иногда отрицательные. И в этом плане, конечно, игра в теннис, как мы берем нулевой алгоритм и сразу бах, его выпустили на поле, допустим, физическое, играть в робота, который вообще ничего не умеет делать, а мы выпустили на физическое поле с ракеткой и хотим, чтобы он играл в теннис. Это некорректно поставленная задача. Но моя гипотеза и гипотеза многих других людей состоит в том, что если мы создаем наращиваемый алгоритм, постепенно наращиваемый алгоритм, который учится вытаскивать последовательности из, например, на базе reinforcement learning, то иерархически наращивая его возможности, мы можем перейти и к оперированию более серьезными существами. Есть еще тонкий вопрос, он состоит в том, можно ли от обучения алгоритма на reinforcement learning, на поощрениях, перейти к обучению с учителем, перейти к моментам, которые учитель показал, а алгоритм повторил. Это отдельная длинная тема. Я глобально считаю, что можно, нетривиально, но можно. ну не тривиально с точки зрения там текущего программирования, но в общем это и вопрос устройства этого подкрепления и вопрос устройства восприятие этого алгоритма, если он просто воспринимает картинку, это одно, если он другие. Человек же в этом смысле мультимодальный, как мы говорим. Я к тому, что на мой взгляд, когда мы учим алгоритмы простым играм, это безусловно И этот алгоритм научится играть. Это, безусловно, важный, необходимый, но недостаточный шаг на пути построения настоящего интеллекта. А вот идти от онтологии... я считаю ну недостаточно потому что не будет того самого граундинга я собственно в чате это писал но еще раз это могу сказать вот тот путь который антон там ты показывал сейчас в публикации где там согнул руку Согнул локоть, повернул запястье и там стало больно. И это как бы некоторая связка с физическим. Откуда эта физическая связка возьмется? Мы что, руками закладываем? Повернул локоть, повернул лодыжку. Откуда берется вот эти факты? Они откуда берутся? Если мы выращиваем алгоритм самых мелких действий, вернее так, просто с чернового восприятия, как ребенок учится, с расшифровки картинки и постепенно он вычленяет какие-то отдельные действия, он даже еще не знает, как они называются, а потом их связывают со словами, потому что у него идет там мультимодальный потоп, и его учит кто-то этим словам еще к тому же. И поэтому, собственно, мы с Шумским считаем, что робот надо будет воспитывать, чтобы вот это самое мышление в нем развить, создав правильный алгоритм, который способен воспитывать последовательность и создав правильную модальность ввода данных, недостаточно будет датасетом обходиться, нам нужно будет его воспитывать, то есть обучать, и в этом будет состоять отдельная задача. И это к нашему разговору, в частности с Александром Балдачевым, и сейчас к твоему комментарию. Извините, что долго. 

S04 [01:31:54]  : Игорь, короткий. Я, в принципе, не против. Я единственный комментарий сделаю, что даже если нам нужно будет роботов обучать, то после того, как мы обучим одного робота, мы потом можем сделать дамп и прошить миллион других. 

S00 [01:32:11]  : Это правда. 

S04 [01:32:14]  : Борис, пожалуйста. 

S00 [01:32:16]  : В развитии обсуждения у меня есть такая гипотеза, чтобы сделать сильные алгоритмы, надо сочетать использование аналитических моделей и машинного обучения. То есть в прошлый раз был семинар и говорили, что лучше случайного. Желательно начинать обучение не со случайного, а с результата аналитической модели, которая заносится предварительно, вручную. Например, человек, если покупает новый электроприбор, сначала читает инструкцию к нему. а потом уже учится пользоваться им на практике. И вот это вот, по-моему, стратегически то, что нужно реализовать, чтобы делать сильный интеллект, хотя бы специализированный. То есть сначала внести роботу или алгоритму какие-то предварительные знания, а потом уже, например, это улучшать. Или, скажем, на модели пинг-понга, игры вот на плоскости, но с двумя ракетками, можно по динамике высчитывать прямую, куда придет шарик, а потом Можно уже на практике вводить усложнение, что в зависимости от скорости ракетки он может закручиваться и уходить от прямой. И вот это уже трудно предсказать, это уже пытаться на машинном обучении улучшать. И вот такой подход мне представляется постепенного усложнения, Вот сочетание предварительных знаний и их улучшение на практике, вот наиболее перспективно. 

S05 [01:34:18]  : Все верно. Человек же тоже так же учится. Абсолютно верно. Я и говорю, что человек учится. 

S00 [01:34:23]  : Да, да, абсолютно верно. А у него ограничиваются либо только аналитические модели, либо только машинное обучение. Если мы научимся их сочетать, то тогда мы сможем делать лучше. 

S05 [01:34:38]  : Все верно, Борис. Просто тезис, который мы с Антоном до этого обсуждали, он такой, что для того, чтобы научиться воспринимать правильно аналитические модели, мы сперва должны сделать алгоритм машин-ленинга такой, который сможет дорасти до восприятия этих данных. а потом его правильным образом учить, и тогда это будет гладкий переход. В противном случае вам придется делать все время костыли и как бы совмещать эти две модели в каких-то пропорциях. Это всегда костыли. 

S00 [01:35:09]  : Я не согласен. 

S05 [01:35:11]  : А вы программист, Борис? Не, извините. Вы программировали когда-нибудь? 

S00 [01:35:16]  : Ну, немножко давно. 

S05 [01:35:17]  : Немножко. А я постоянно программирую. Поверьте мне, что это как бы прям серьезно. 

S00 [01:35:21]  : Ну, я видел. Вот модели, где это сочеталось. 

S04 [01:35:27]  : Ну хорошо. Маленькие комментарии, прежде чем Александру дать слово. На самом деле есть некоторые люди, которые инструкцию читают в последнюю очередь. Они сначала пытаются сделать так, чтобы заработало, а потом, когда не получилось, они спрашивают, мучают искокружающих. И только в крайнем случае читают инструкцию. Я, например, такой человек. Александр, пожалуйста. 

S00 [01:35:52]  : А я наоборот. Всегда читаю инструкцию сначала. 

S01 [01:35:57]  : Так, добрый день всем. Пошла такая пьянка, значит, ну давайте. Вот я, например, категорически возражаю Игорю по поводу того, что человек обучается не с нуля, а вот практически с нуля. Вот вы посмотрите, очень интересная штука. 

S05 [01:36:13]  : Еще раз, еще раз, еще раз тезис обоснуйте. Я сказал, что человек обучается не с нуля? 

S01 [01:36:18]  : Не с нуля. 

S05 [01:36:19]  : Еще раз. 

S01 [01:36:20]  : Да, не с нуля, я услышал. С нуля? Ну, конечно. С нуля. Хорошо. Тогда развиваю, тогда развиваю. Не возражаю. Значит, смотрите, очень интересная штука у нас получается с человеком. Это вот в каком-то тексте, который я писал Игорю, было, что в отличие от животных, он рождается голенький, голенький наголым. Абсолютно. То есть если копытные практически сразу же начинают ходить, огромное количество инстинктов заложено в животных. То есть тот же бобер, в нем уже заложено то, что он может построить плотину, ему не нужно обучаться этому. Птица строит гнездо тоже не обучаясь. То есть получается так, что животные рождаются с очень сильным предобучением. большим количеством инстинкта. И, скорее всего, с этим связано то, что они плохо обучаются, в дальнейшем поддаются дрессуре. То есть у них как бы уже этот каркас предзабученный существует. И это то же самое, как и в нейронках, если мы чему-то обучили, а переучить практически невозможно. 

S05 [01:37:28]  : александр можно я вот я вставлю одну копейку я говоря я знаю такую точку зрения я не буду сейчас опровергать ее но как биофизик знающий как бы как появляется организм как клетка развивается я совершенно не понимаю абсолютно никак не понимаю каким образом из клетки которая делится потом проходит дифференцировку из нее появляются все эти нейроны мозга в частности еще там миллиарды других клеток каким образом там возникает предобученная модель вот честно Я не вижу ни одного физически разумного и биологически разумного элемента для этого. 

S01 [01:38:09]  : Дойдет дело, как-нибудь попытаюсь объяснить, что я на этот счет думаю. То есть я считаю, что это возможно, поскольку мы это наблюдаем. И механизм там есть. ДНК имеет три миллиона пар. 

S05 [01:38:22]  : Три миллиона пар сильно изучены, как они работают. Процесс дифференцировки до сих пор очень мутный, неизвестен, действительно. Но, короче, в общем, эта точка зрения, честно говоря, Ну, она биологически известна, но как это происходит... Давайте не будем говорить о механизмах, потому что механизмы нам неизвестны, и не будем обсуждать. 

S01 [01:38:45]  : Мы обсуждаем факт, что человек рождается с минимальным количеством инстинкта, сосательный. И если его бросить, вот как он есть, голенький, он не выживет. Абсолютно не выживет. И еще он обучается 20 лет, в отличие от животных, которые обучаются либо за несколько дней существовать, либо в течение недели. И тут возникает очень интересная действительно история, что если мы вот забудем про механизмы, биологические механизмы забудем, а будем рассматривать как будто техническое устройство перед нами. У нас есть некие технические устройства, у которых есть какие-то параметры, мы их анализируем, что мы замечаем, что действительно животные предобучены, сильно предобучены. и пользуется практически готовыми программами в сечении всей жизни. Человек рождается с нулевым предобучением, и все, что он научается, он приобретает благодаря воспитанию и обучению родителей и социума в результате деятельности, в которую он включен. Даже с той же историей с ложкой. Это не та история, в которой он начинает тыкать и хватать все угодно, и когда он попал на ложку, ему говорят «О, молодец, спасибо». Нет, ему сразу дают ложку. То есть это не история об предобучении с подкреплением. Это обучение конкретное всегда. И тут возникает очень интересный момент, наверное, именно потому, что у человека каким-то образом эволюционно, и я могу сейчас гипотезу высказать, получилось так, что его мозг оказался непредобученным совершенно. Мы способны к обучению, очень сильному обучению. благодаря именно целенаправленному обучению, не случайному. То есть, скорее всего, получилось каким-то образом, что эволюционный отбор уговенит пошел в сторону именно уменьшения предобученности мозга. То есть, допустим, у нас рождаются хорошо предобученные организмы и плохо предобученные. Так вот плохо предобученный организм, он научается быстрее реагировать на изменения среды, скорее всего, на изменения социума, он становится самцом, альфа-самцом и свои гены продвигает. Таким образом получилось, что человек в конечном счете, гобиниды в конечном счете получились минимально предобученными и способными к обучению. И если теперь говорить, перебрасывать это на технические стороны, что мы должны как бы сделать? Допустим, мне бы поставили такое значение, как это реализовать? Вот у нас есть, мы обучили играм в Atari и, скажем, в теннис, прохождение там лабиринтов, еще какие-то игры, куча-куча игр. Так вот, нам нужно у этой сетки стереть. стереть конкретные знания, то есть, по сути, ведь все знания, которые были накоплены, вот инстинкты, они у человека стерты. Зачем-то, почему-то они стерты. Вот нужно стереть и оставить только саму предуготовленность к обучению. То есть, скорее всего, нужно иметь несколько уровней сеток, нижний какой-то уровень, который будет отслеживать сам процесс обучения, а верхние будут меняться и обучиваться разным-разным. Останется некий уровень совершенно непредобученный, но каким-то образом умеющийся учиться с учителем. И именно ваншот обучения. Вот дал ему ложку, он взял ложку. Здесь очень интересно. А, а вопрос, то, что говорил Борис с инструкциями. Ну, мы с новым инструкцией рождаемся. Это уже потом, когда научился читать, считать, тогда уже инструкция нужна. Обучение идет только в деятельности. Конкретная деятельность с конкретным обучением. Но предобучение биологическое, глобальное предобучение биологическое, которое потом нужно стереть, одно необходимое мы наблюдаем. Просто вот это эмпирический факт. 

S02 [01:42:52]  : Александр, не совсем точно. Вот есть там комментарий. 

S04 [01:42:55]  : Да, Сергей, пожалуйста. 

S02 [01:42:57]  : Я, коллеги, добрый вечер. Всем, извините, не слышал докладов. Тщательно все прослушиваю. Александр, вот дело в том, что все те же самые факты, о которых вы говорите, они могут объясниться и другим способом. У человека, на самом деле, очень много готовых программ. Просто они все в пассиве. Обучение состоит в том, что число уже записано, оно заклеено просто пленочкой. Нужно монеткой стереть пленочку и увидеть число. Иначе таким сложным действием невозможно обучиться. Ребенок очень быстро учится на самом деле. Это можно сделать только если у него инейт. Готовых программ много, но в отличие от птиц, у которых их мало, поэтому они быстрее проявляются, у ребенка их потенциально очень много. Они закодированы, как-то разрежены. И обучение первичное состоит только в том, что просто стираются монеткой те части, которые задействованы. Это одна из гипотез. Иначе с нуля эволюционно невозможно такие сложные действия сделать. 

S01 [01:43:57]  : А это не эволюционно. 

S02 [01:44:00]  : окей, хорошо. мы услышали главное друг друга, да? 

S01 [01:44:04]  : нет, есть две гипотезы. происхождение языка, значит, генетическое, то есть исходное, что у человека запрограммированный язык, а есть, что язык обучается. гипотеза Хромского. поэтому две красные гипотезы, и ни одна из них не имеет еще подтверждения. 

S02 [01:44:21]  : Ну, хорошо, но главное, что мы это услышали, что в принципе, вообще-то говоря, может быть и вот такой механизм. Спасибо, еще раз извините, что я не послушав доклад вмешиваюсь в обсуждение, но это только касается вот этого. 

S05 [01:44:32]  : Правильно, правильно, потому что это же тема как раз про... Но, Александр, из вашего тезиса можно еще вывести такой как бы подтезис, который вы в чате явно писали, а здесь он как бы сейчас ну так расквозил, что там вот это вот человек как бы очень сильно отличает неким скачком отличается от всего остального животного мира и типа вот это создает некоторую там чуть ли не какую мистическую значит там вещь и 

S01 [01:45:04]  : Мистике никакое. Я про мистику никогда не говорил. Это хорошо, что про мистику не говорили. Но скачок явный. Скажем так, он даже эмпирически явный. С одной стороны, по рождению мы видим кардинальное отличие рождения ребенка от рождения животного. 

S05 [01:45:21]  : Я вам про эти скачки расскажу сейчас голосом и потом в чат еще все-таки напишу. Но этим вопросом много занимались люди, которые занимаются эволюционной биологией. Есть там целая наука, которая изучает гены, то, как виды друг от друга образуются, то, как эти гены появляются, всякие мутации и прочее. И известна такая любопытная вещь, что, во-первых, Фундаментальная вещь такая, когда происходит некоторая удачная мутация или пара мутаций, неважно, чем больше мутаций одновременно, тем меньше статистически верно, но сейчас неважно, пусть даже одна очень удачная мутация, за счет которой некоторые особи получают достаточно сильные преимущества, допустим, в своем виде и начинают там дальше активно размножаться и так далее. то любопытный факт состоит в том, что она очень быстро, грубо говоря, съедает предыдущую мутацию, если так можно выразить сейчас, как бы это сказать. Конкуренция происходит всегда между самыми активными. И когда у вас, когда происходят удачные мутации, то следующие некоторые, как бы некоторые предыдущие виды, они достаточно быстро уходят из, как бы из наблюдения. Поэтому мы не видим некоторого непрерывного, постоянного изменчивости некоторой. Грубо говоря, от обезьяны к человеку нету вот всех промежуточных кусочков. Просто потому что, когда появились такие обезьяны, которые, они достаточно быстро эволюционировали, предыдущий свой вид они быстро истребили. Человек истребил всяких неандертальцев, он не трогал орангутангов, шимпанзе. Игорь, можно перейти? И в этом плане мы как бы с вами просто наблюдаем уже некоторую историю, которая выглядит как скачкообразная, а на самом деле она была вполне себе эволюционной, но просто вот эти кусочки, они быстро проходились. 

S01 [01:47:21]  : Игорь, проблема же не в объяснении, а проблема в смысле, как это происходило. А проблема в том, что вот есть один уровень, есть второй уровень. И между этими уровнями… Верно, верно. 

S05 [01:47:31]  : Но вы просто… Не как это было, а… Александр, мы с вами в чате обсуждали, и сейчас это тоже как бы в частности это идет, что можно ли перейти от простого обучения простых алгоритмов постепенно перейти к некоторому там мышление в вашем понимании там материирование некоторыми абстрактными сущностями то есть может ли этот интеллект постепенно вырасти я думаю что может если мы создадим правильную правильную как бы условно алгоритм в я очень упрощу, сейчас примитивно скажу, выявление закономерности, поиск, построение иерархии этих закономерностей, обобщение экстрагирования, то он постепенно этому выучится, но вы же утверждаете, что вот этот скачок устроен так, что, грубо говоря, вот есть зверушки, которые там очень простые, а есть вот человек, которые уже как бы сильно сложны, поэтому надо заниматься антологиями и прочим. А я говорю про следующее, что человек появлялся из этих зверушек, ну просто очень постепенно, но потом, допустим, в какой-то момент времени, когда человек понял, что, или там не понял, а появились особи, которые друг друга начали обучать, они начали прогрессировать настолько быстро и сожрали всех своих не обязательно сожрали, а, допустим, все, кто с ними не прогрессировал, быстрее вымерли. И они сильно оторвались от остальных, и они остались доминирующим видом. А те, кто не научился друг друга обучать, они как бы быстро погибли. И в этом смысле то, что мы наблюдаем, это результат постепенного, последовательного перехода, но движущийся не с одинаковой линейной скоростью, а движущийся как бы рывками, если хотите, рывками с точки зрения количества эволюции видов в единицу времени, потому что эволюции движутся вот такими вот... Давайте коротко отвечу, осталось несколько минут. 

S01 [01:49:26]  : Во-первых, отвечу на начало вашего разговора. Я эволюционной биологией занимался много, написал книгу, и эта книга является учебным посланием по эволюционной биологии в одном из биологических биофактов в университетах нас в России. Так что эта тема мне очень хорошо известна. Второе. Основное мое возражение было против вашего тезиса обучения вот этого зверюшки доинтеллектуальной особи, что это никогда невозможно произвести на одном предмете. То есть это нельзя себе представить, что вот у нас есть нечто, и вот мы это развиваем, развиваем, развиваем, и из этого получаем. То есть переход вот этот осуществляется через социум. То есть нам нужно, с одной стороны, иметь индивидуальное развитие некоторое, индивидуальный переход, а с другой стороны, строить социум. Без социума этого перехода невозможно реализовать. Согласен, согласен. И поэтому непосредственно, вот когда мы говорим, берем некий алгоритм, тут же обучение с подкреплением, развиваем, развиваем – ничего не получится. То есть это многоуровневая система, многоуровневая система, скорее всего многоагентная система. И вот так вот тупо это не сделать. И сегодняшний разговор у нас тоже в канале получился именно на близкую тему. Это нельзя сделать с методом тыка. нужно иметь теорию, нужно иметь представление, и особенно нужно иметь представление о различии. когда мы говорим это всего лишь постепенное изменение какое-то, вот мы сейчас начнем с маленького и дойдем, мы ничего не получим. 

S05 [01:51:07]  : вы просто немножечко, может быть я косноязычно говорю, когда я говорю про то, что мы учим алгоритм на маленьких вот играх, допустим типа таких, типа игр Atari, Речь идет о том, что мы создаем некоторый универсальный алгоритм, способный учиться на любых последовательностях. Никто не сказал, что дальше мы будем его миллион лет учить на играх Atari и он вдруг станет супергением. Этого никто не говорит. Утверждается следующее, что если мы создаем некоторый алгоритм универсальный, растущий, иерархический, который может учиться на любом входном потоке данных, то вот создав такой алгоритм, дальше встанет следующая задача – каким образом его учить правильно, чтобы он стал этим самым супергением. И опять, возвращаясь к предыдущему тезису, мы с Шумским уверены, ну Шумский уверен, а я, считайте, вместе с ним уверен, что нам такого супер интеллекта воспитывать придется. В этом смысле мы стопроцентно с вами согласны, но сейчас пока некого воспитывать. Возьмите сейчас любую нейросеть, что там воспитывать-то, господи, ну это же просто стахастический попугай, ну там воспитывать нечего. Поэтому никто не говорит, что маленькие тестовые среды используются как полигон для отработки модуля, если хотите. А дальше уже другой вопрос, как мы будем его учить. Это тонкий, интересный, чарующий вопрос. Так что я не говорил про то, что Изотарик все вырастет. Все. Спасибо. 

S03 [01:52:47]  : Можно один комментарий по поводу голенького ребеночка? В одной из книг, вот я прочитал, мне понравилась мысль, что чем более интеллектуальный человек, животный или человек, тем он рождается более беспомощным. То есть он как бы более недоношенным. и требуется процесс его рождения уже непосредственно в среде. Поэтому ребенок рожает занедоношенным, но уже после его физического рождения продолжается развитие, созревание когнитивных структур, которые требуют корректировки развития от самой среды. Поэтому, когда у ребенка есть исследованные и подтвержденные этапы развития, называется, в ГОЦКе я отмечал, до семи этапов, и ПИАЖЕ тоже, по-моему, до нескольких 12 этапов выделял, когда происходит именно перестройка. В зависимости от физического развития созревания отделов мозга происходит перестройка. А то, что животные рождаются уже готовыми к деятельности, Там, видимо, такая моя собственная версия, что происходит уже перенос опыта из взрослого особи в детеныша. То есть там уже получается зрелый мозг взрослого, он как-то формирует зрелый мозг детеныша. А у ребенка это получается, социальная среда формирует, после рождения уже формирует мозг ребенка. Вот такая версия была. 

S05 [01:55:01]  : Механизм непонятен, ну ладно. 

S03 [01:55:05]  : механизм там получается воспроизводство как раз вот то что вы говорили как от простой модели перейти к более сложной там самый ключевой момент это найти механизм воспроизводства чтобы сам агент не строил последовательность, не делал предугадывания последовательности, а чтобы он воспроизводил то, что он умеет. Процесс воспроизводства. И потом, когда уже в самом агенте есть механизм впитывания в себя этих воспроизводящих процессов, потом от простых воспроизводящих процессов чтобы он мог уметь воспринять в себе более усложняющиеся новые комбинации процессов воспроизводства. И таким образом может происходить усложнение модели. Спасибо. 

S05 [01:56:12]  : Сори, я вынужден убежать. У меня предложение, я убегаю, но жалко. Я потом, Сергей Александрович, послушаю в записи. Давай, Антон, мы на эту тему отдельный семинар замутим, потому что тема оказалась живая, она вообще прям врезалась. Ну, ты сформулируй повестку. Да, да, хорошо, я сформулирую, ладно. Спасибо. 

S04 [01:56:33]  : Хорошо. Пока, счастливо, Сергей. 

S02 [01:56:35]  : Может быть, мы тогда отдельный семинар этому посвятим, потому что я-то хотел... 

S04 [01:56:40]  : развеять оптимизм Игоря по поводу того что он думает что он делает какой-то модуль я предлагаю сделать краткую затравку на следующий семинар ваши вашими словами и продолжим когда соберемся Так, не слышно, Сергей. Вы тогда задайте тезис. 

S02 [01:57:02]  : Нет, нужен докладчик базовый. Семинар-то у нас ведь он же не вокруг темы, а вокруг человека. Поэтому, если кто-то захочет еще раз вспомнить о Республике Ленин, мы с удовольствием в конце поговорим и изучим. А так ведь сложно. Ну, то есть это опять многие вещи гипотетические. На самом деле Эльдар абсолютно многие вещи точно сказал. ключевое место, почему сегодняшние модели не являются кирпичиками. Я просто хочу напомнить, вот был такой замечательный математик, потрясающий, Джин Голуб, по линейной алгебре, специалист по матричной алгебре, вот ему принадлежат такие замечательные слова, что каждый раз, когда размерность матрицы, с которой мы работаем, увеличивается на порядок, то есть каждый раз, когда матрица увеличивается в 10 раз, Мы должны абсолютно полностью пересмотреть весь алгоритмический подход и весь арсенал, который мы используем. Все разложения, которые раньше работали на маленьких разделах, не работают другими. Собственные числа были далекие, стали близкие. Были ортогональности вычисляемые, теперь она не вычисляемая. Это абсолютно изменение принципиальное. И вопрос состоит в том, что когда мы работаем с маленькими модельками, Они будут большие, эти модельки, но они просто маленькие. Тут только подкрепление собирают. То, что из них взять с собой на следующую уровень, это ключевой вопрос. Ровно то, о чем говорит Эльдар. И это вопрос гораздо более важный, чем научить эту модель играть в этой конкретной анатомии. То есть, как закодировать этот опыт, который взять с собой на следующий уровень играть. Сейчас, к сожалению, Поскольку оппонента отсутствует, то давайте я на этом завершусь. 

S04 [01:58:45]  : Хорошо. Сергей, спасибо. Коллеги, всем спасибо за участие. Продолжим в следующий раз. 

S02 [01:58:53]  : До свидания. Всем до свидания. 

S04 [01:58:54]  : Спасибо. Спасибо. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
