## 19 мая - Поиск в пространстве стратегий: альтернатива обучению с подкреплением - Сергей Довгань — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/LHJH7WD8YV8/hqdefault.jpg)](https://youtu.be/LHJH7WD8YV8)

Суммаризация семинара:

Семинар касался вопросов и задач, связанных с искусственным интеллектом, машинным обучением и оптимизацией. Основная цель обсуждения заключалась в анализе и сравнении различных подходов к решению задач, в частности, задач оптимизации и задач, связанных с искусственным интеллектом.

В ходе семинара были затронуты такие темы, как методы машинного обучения и их применение в сложных задачах. Особое внимание было уделено обзору и сравнению различных алгоритмов и подходов, включая методы обучения с подкреплением, которые позволяют системам самостоятельно принимать решения на основе полученных наград.

Докладчики представили свои исследования и технологии, которые они разрабатывали, включая такие методы, как DQM и Transformer Decision, а также обсудили свои подходы, отличающиеся от традиционных методов обучения с подкреплением, таких как Dreamer.

Важным аспектом семинара стало обсуждение вопросов, связанных с определением и пониманием того, когда задача считается решенной. Были подняты вопросы о градуальных оценках стратегий и оценках только в виде 0 и 1, где проблема заключается в том, что в сколь-нибудь серьезных условиях решение может быть мера 0 в пространстве поиска.

Кроме того, обсуждалась проблема пространства поиска, которое может быть как дискретным, но большим, так и континуальным. В контексте рассмотрения были упомянуты картинки с экрана в виде вектора, а также эффективность некоторых систем обучения с подкреплением, таких как Dreamer.

В целом, семинар представил собой платформу для обсуждения и сравнения различных подходов и методов в области искусственного интеллекта и машинного обучения, с акцентом на задачи оптимизации и понимание того, когда задача решена.








S05 [00:00:00]  : Запись и трансляцию. 

S03 [00:00:11]  : Добрый вечер. Все готовы? Можно уже начинать? 

S05 [00:00:16]  : Да, Сергей, пожалуйста, мы уже в эфире. И сегодня Сергей Вдовгань расскажет про поиск пространства и стратегии как альтернативу обучению с подкреплением. Сергей, пожалуйста. 

S03 [00:00:31]  : Да, тут сразу же две такие ремарки. Первое – это то, что сама по себе технология не новая. и она достаточно даже старая, но при этом она работает по качеству соизмеримо с обучением с подкреплением. В каких-то местах лучше, в каких-то местах хуже, тем не менее она какая-то не очень известная что ли. Это момент номер раз. Момент номер два. По ходу разработки этой технологии ко мне присоединился человек из нашего чата, Никита Демерза, и значительную часть экспериментов провел он, поэтому ему за это большое спасибо. Итак, приступаем. Для начала я отталкиваюсь от определения такого – искусственный интеллект – это решатель задач, а общий искусственный интеллект – это решатель любых задач. Это определение, оно не значит, что ситуация вот в реальности именно такая и есть. Это просто слова, просто такая постановка задач у меня. В этом определении нет ни слова про похожесть на человека, про знание ничего не говорится, про владение речью, про обучение и так далее. Это не значит, что всех этих свойств не будет, но если они и будут, то скорее как побочный эффект. Смысл не в них. Как говорил Миямото Мусаси в книге «Пяти колец», подняв меч, думая о том, как поразить им противника, чего бы это ни стоило, атакуя и парируя, нанося удары и делая выпады, рази противника теми же движениями. Не отклоняйся от цели, думая о том, как атаковать и парировать, как наносить удары и делать выпады, ты не поразишь противника. Прежде всего думай о том, как каждым своим движением пройти через защиту противника и рассечь его плоть. Поэтому вот думать надо о том, как сделать систему, которая именно достигает цели. Моя мотивация заниматься AGI, Artificial General Intelligence, такая, Я знаю, насколько оптимально можно достигать целей. Просто посмотрите, например, на баллистические ракеты. Вот так выглядит оптимальность. Для меня AGI – это некоторая машина, которая настолько же эффективно достигает целей. Но ракета действует в заранее известной среде. Во-первых. Во-вторых, у неё не так много вариантов, каких целей можно достичь. А для AGI должны быть допустимыми задачи вроде такой. Сделай, чтобы у меня на столе был килограмм золота. То есть я сформулировал эту задачу и сразу после формулировки возникает вопрос, а как это вообще сделать? Это вообще возможно? А второй вопрос, я придумал пару вариантов, но они либо ненадежные, либо сложные, либо опасные. Еще при такой формулировке заметно следующее. Легко сказать, решена ли задача, и очень тяжело найти настоящее рабочее решение. И тяжело понять, какое решение наилучшее. Задача. Представьте себе, AGI сделан. И вот этот искусственный интеллект – это библиотека на Питоне. Библиотека должна принимать на вход какое-то описание задач и на выходе должна давать какое-то решение. Когда мы говорим, что она решает любые задачи, это немножко философский подход. Для программиста любая задача – это конкретный тип данных, то есть конкретный набор чисел. И решение задачи – это тоже конкретный тип данных, то есть там какая-то конкретная структура. И обычно подразумевается, что AGI – это такая система управления, либо это генератор системы управления. То есть это некоторая штука, которая находится в среде. Она принимает на вход данные и она выдает их на выходе. Например, это управляющая система некоторого робота, может быть, роботизированная машина, чего-нибудь такого. И в зависимости от задачи этот алгоритм управления будет разным. Обычно в искусственном интеллекте задачи формулируются в терминах результатов. То есть там, например, в обучении с выкреплением они формулируются в терминах результата. Для агента одни варианты будущего более предпочтительны, а другие менее. И агент пытается проложить траекторию так, чтобы пройти через хорошие ситуации и избежать плохих. И такой подход от желаемого результата позволяет описать широкий спектр задач, плюс такой подход математически осмысленный, математически корректный, и он опирается на проработанный язык теории оптимального управления. Если же не отталкиваться от функции полезности, то запросы к искусственному интеллекту начинают выглядеть как «сделай мне самый вкусный торт». Вот так, без уточнений, без контекста и дополнительных данных. То есть если искусственный интеллект умный, то он, конечно, что-то сделает. но вряд ли этот торт будет подходить именно вам. Таким образом, задачи формулируются в терминах результатов. Концептуально AGI – это не вычислитель, а система автоматического управления, либо построитель такой системы. Если удовлетворительный алгоритм известия, то AGI яблокса. Можно просто использовать этот алгоритм. Если неизвестно, как отличить хороший результат от плохого, то и нет способа поставить задачу. Возможно, язык результатов и функций полезности не единственно возможный, не единственно правильный, но это точный математически корректный язык, пригодный для широкого круга задач. Тут у него выстроена куча математики, поэтому тут не вижу смысла чего-то менять. Лучше научиться свои хотелки переводить в термины функций полезности. И еще важный момент, что Искусственный интеллект может быть более узким, либо более широким, многозадачным. Как показано на графике, на этих графиках, может быть так, что он решает одну задачу хорошо, а может быть так, что он решает много задач. И много задач лучше, чем случайное поведение. И во втором случае мы говорим, что это общий интеллект. Обычно так получается, что если система решает много задач, то она с ними справляется хуже, чем узкая система со своей одной задачей. Потому что сложно сделать универсальную систему, которая может все. В этом докладе я покажу подход к общему интеллекту, который не оригинален, но неожиданно хорош по сравнению с мейнстримом. Этот подход называется поиском в пространстве стратегий. и наиболее распространенный его вариант называется нейроэволюцией. Кроме того, я приведу доказательства, что эта технология является общим решателем задач и покажу, в каких условиях. Для начала поговорим о бейслайне, о том, какая технология сейчас доминирует в этой области в общем решении задач. Это обучение с подкреплением. Наибольших результатов добились системы, которые предсказывают, сколько награды среда выдаст в ответ на то или иное действие в перспективе нескольких тактов. Правило о том, за что даются награды – это и есть наша постановка задачи. Обучение с подкреплением очень универсально, но требует большого количества взаимодействий со средой, потому что сам подход требует того, чтобы агент, во-первых, выучил модель среды, во-вторых, выучил за что конкретно ему дают награды, поймал его за экономерность. Чем отличается наш подход, поиск в пространстве стратегий? Как он устроен? У нас есть некоторый контроллер, показан в верхнем правом углу. Здесь контроллер – это та самая система управления, и система управления зовут параметрически, то есть она описана некоторым геномом. Геном – это такой вектор, массив чисел. Контроллер взаимодействует со средой, то есть он выдает управляющие воздействия на какие-нибудь органы управления и принимает на вход наблюдения. Он взаимодействует со средой какое-то количество раз, какое-то количество кадров. И дальше мы оцениваем, насколько он хорошо провзаимодействовал. То есть мы, когда делаем эту среду модельную, тестовую, мы в нее встраиваем правила оценки результатов, то есть что для нас хорошо, что для нас плохо, за что начисляются очки, за что снимаются. И вот контроллер, агент взаимодействовал со средой, и на выходе у него некоторое одно число. Это и есть оценка качества стратегии, это число. То есть, допустим, у нас тестовая среда – это игра в Wario. и мы хотим, чтобы агент собирал как можно больше монеток. Вот мы даем ему тысячу кадров и вот на момент тысячного кадра сколько у него монеток будет – это и есть качество стратегии. А дальше мы подходим к некоторым способам Подбираем этот геном так, чтобы он решал задачу оптимально, так, чтобы качество было наилучшим. Подбираем в смысле решаем задачу статической оптимизации. Здесь на нижней картинке показан некоторый рельеф – это зависимость качества стратегии от генома. То есть геном – это супермногомерный вектор, здесь всего только два измерения. И одно измерение – это качество стратегии. Мы некоторым оптимизатором подбираем хорошую стратегию. Что именно за оптимизатор используется и в каком виде описывается этот геном, это будет говориться позже. Пока что небольшая демонстрация. Здесь у нас агент учится попадать в мишень ракетой. То есть он управляет ракетой. Здесь более-менее реалистичная симуляция физики. У ракеты есть какой-то запас топлива, можно с разной мощностью включать двигатель, можно по-разному подруливать этой ракетой. И вот это все единая стратегия для попадания во всякие разные цели. То есть это все один и тот же геном попадает в разные мишени. Тут же проработаю некоторые возражения. Тут же замечаем, что ракета не идеально попадает в цель. То есть это не оптимальная стратегия, но некоторая около оптимальная. И некоторые возражения, которые я слышал в связи с методом перебора стратегии в чате. Вот, например, существует возражение, что метод перебора и оптимизации не способен породить ничего нового, он лишь комбинирует из существующих элементов. Это не совсем так, потому что любое новое – это всегда комбинация из известных, старых элементов. Любая, сколь угодно, оригинальная книга состоит из букв, которые придуманы когда-то давно. Любой новый механизм состоит из атомов. И если научиться это замечать, то можно очень многие задачи сводить к задачам оптимизации. Еще есть возражение о том, что данный подход может найти только локальный оптимум, но не глобальный. В пределе это не так, потому что у нас может быть оптимизатор, который перебирает абсолютно все конфигурации. Но на практике за конечное время мы обычно можем найти только локальный оптимум. Но тут надо сравниваться с бейзлайном, тут надо сравниваться с другими методами. Другие методы не лучше. Они тоже находят за ограниченное время только локальный оптимум. Разве что может быть система какая-нибудь очень простая, в частных случаях можно найти глобальный оптимум, но это только в частных случаях. Еще существует возражение, что перебор стратегии попадает в комбинаторный взрыв и не сможет решить сколько-нибудь сложной задачи. С одной стороны, да, мы от этого не застрахованы. С другой стороны, опять же, надо сравниваться с альтернативами. Какие есть другие универсальные методы? Насколько они склонны к комбинаторному взрыву? Другие методы действительно есть. Это вот обучение с подкреплением. Оно в теории менее склонно к комбинаторному взрыву, но на практике вы гораздо быстрее получите какое-то хорошее решение через нейроэволюцию, через перебор стратегий, чем через обучение с подкреплением. Но в долгосрочной перспективе обучение с подкреплением скорее лучше, чем нейроэволюция. Потом, с третьей стороны, есть эвристики, которые заменяют полный перебор. и которые намного лучше борются с комбинаторными взрывами. Опять же, это не значит, что они их прям совсем устраняют, но они существенно упрощают ситуацию. Дальше. А вот если обучать стратегию, не любую мишень поражать ракетой, а просто поразить одну мишень. Вот сейчас посмотрите, это то, как типичная типичное оптимизационное решение задачи. Ракета летит как-то. Она точно попадает в цель. Она буквально делает то, что ей сказали. Ей сказали, попади в цель, найди способ. И она это делает. Она никакой оптимальной траектории не пользуется. Она не пытается оптимизировать затраты топлива, потому что ей этого не сказали. Она просто делает то, что ей сказали. Решения получаются ощутимо заметно нечеловеческими. И часто это означает, что искусственный интеллект ищет более широко и более нешаблонно, чем человек. У него меньше ограничений. Существуют разные способы записи стратегий и, соответственно, к записи подходят разные способы оптимизации. Здесь на слайде я показал разные способы записи стратегии, соединил их с подходящими к ним способом оптимизации. Как мы видим, полный перебор или случайный поиск годятся для любого случая, но это достаточно плохой способ оптимизации. В большинстве случаев самый быстрый способ оптимизации – это градиентный спуск. И он годится всего для двух способов записи стратегии. Это либо рукописный алгоритм, в котором встроены некоторые числопараметры, и эти числа подбирают рудивный спуск. Либо это нейросеть, и ее коэффициенты – это и есть вот этот геном. Плюс генетический алгоритм, он плюс-минус неплохо работает, он гораздо лучше работает, когда пространство поиска, функция полезности в зависимости от генома гладкая. То есть он лучше всего работает в тех условиях, когда применим градиентный спуск или хотя бы частично применим. Так, далее. Полнота. Разные способы записи стратегии дают разную полноту. Например, через линейную функцию невозможно выразить экспоненты. Через обычную нейросеть, у которой на входе только текущий кадр, нельзя реализовать стратегию с памятью. Через рекуррентную сеть можно реализовать, скажем, сортировку. Но если массив увеличивается, то придется сеть увеличивать, иначе не получится отсортировать. Через машину тьюринга можно выразить любую закономерность, потому что все, что происходит в компьютере, это доказуемо можно свести к машине тьюринга, и сам компьютер тоже можно. Это, конечно, чисто теоретическая концепция. Она может быть неудобна на практике, но в теории возможна. И это все идеализация, потому что бесконечных лент тьюринга не бывает, бесконечного числа шагов в алгоритме тоже не бывает. Это идеализация. Но достаточно хорошая, практичная. Разные способы поиска тоже дают разную полноту. Например, полный перебор случайных поисков они ищут в полном пространстве стратегии. И некоторые варианты генетического алгоритма порождают мутации так, чтобы они покрывали все пространство поиска, но фиксировались возле нулевых изменений. Например, с вероятностью 1% выдается мутация, которая покрывает все пространство поиска. С вероятностью 99% какая-нибудь маленькая мутация. Таким образом, алгоритм с одной стороны практичен, за неограниченное время он найдет глобальную оптику. Я слышал возражения, что метод перебора стратегии – это узкая система, то есть он решает лишь одну задачу, а не много. Обычно алгоритм перебора – это очень универсальная система, а вот получившаяся стратегия, она и правда узкая. Но это обычно, я позже покажу, может быть иначе. Кроме того, Я слышал от некоторых людей, что генетический алгоритм даже за неограниченное время не найдет некое решение, или что, например, случайный поиск не найдет некоторое решение, которое существует во множестве решений. Например, если у нас есть бесконечно много попыток, и мы пробуем случайные тексты, а затем проверяем, получилась ли хорошая книга, то мы все равно не создадим хорошую книгу. Ну, тут можно сказать, что текстов длиной с «Войну и мир» конечное количество, программ длиной с «Windows» конечное количество. Если мы смотрим такой теоретический вариант полного перебора, то вы со стопроцентной вероятностью найдете то, что нужно, если оно вообще в принципе существует в заданном вами множестве. Там проблемы скорее в другом. Во-первых, нужен хороший алгоритм для отделения зерен от плевел, иначе вместе с войной и миром у вас получится триллионы других плохих книг, и вы никак не сможете их разделить. Это во-первых. Во-вторых, бесконечный перебор – это все-таки упрощение, и на практике всегда применяется некоторый эвристический алгоритм, который начинает поиск с наиболее вероятных вариантов. И этот эвристический алгоритм в каких-то случаях может работать лучше, а в каких-то хуже. То есть, например, эволюция – это тот генетический алгоритм или градиентный спуск – это та самая эвристика. В каком пространстве поиска я сейчас действую? Это нейронная машина Тьюринга. То есть у нас существует некоторый контроллер – это нейросеть. У нее есть пишущие головы, читающие головы. Они подключены к внешней памяти. Это либо одномерная, либо многомерная лента. И, соответственно, эта штука даже не обязательно должна быть дифференцирована, потому что мы ее не через обратное распространение ошибки обучаем. но в моем случае она дифференцируема, то есть пишущие и читающие головы работают через схему внимания, там что-то вроде гауссовой функции, то есть если пишущая голова находится, допустим, на седьмой метке, то и там фокусировка размытая, записать в седьмую ячейку, произойдет также запись с меньшей силой в шестую, пятую ячейку, с еще меньшей силой в четвертую ячейку и так далее. В общем, по гауссу произойдет размытие. Некоторые говорят, что нельзя любой алгоритм представить в виде машины тьюинга. Но на самом деле есть известный факт – большинство языков программирования можно выразить друг через друга. Написать компилятор C++, например, на питоне. И все эти алгоритмы можно выразить через машину Тьюринга, соответственно, через нейронную машину Тьюринга тоже можно. И поэтому нейронные машины Тьюринга – это полное пространство поиска. для поиска в качестве оптимизатора я использую систему, которую когда-то давно разобрал Аббатур. это обертка над несколькими оптимизирующими алгоритмами. туда можно запихать сколько угодно каких угодно оптимизирующих алгоритмов. этот Аббатур обертка по очереди просто запускает эти оптимизаторы. оптимизаторы дают какой-то прирост. и сколько времени тратят на исполнение. Соответственно, каждому оптимизатору проставляется рейтинг за его последнее одно или несколько исполнений. Рейтинг зависит от прироста и штрафа за время. И берутся оптимизаторы пропорционально их рейтингу. В первую очередь, с наибольшей вероятностью, берутся те, у которых рейтинг выше. но те, у которых ниже, они тоже берутся просто с меньшей вероятностью. Это все нужно, потому что алгоритмы оптимизации, они склонны к тому, чтобы попадать в локальные оптимумы. То есть некоторые точки, в которых стратегия хорошая, но если делаем шаг вправо, шаг влево, то она становится хуже. У разных оптимизаторов шаг вправо, шаг влево делается по-разному, и поэтому Сложно сделать так, чтобы много разных оптимизирующих алгоритмов попали одновременно в один и тот же локальный оптимум. Если один застрял, то вылезет другой. Показываю следующую демонстрацию. Здесь агент обучается стрелять из пушки по самолетам. Вот пролетают самолеты, и он выучил вот такую стратегию, которая позволяет ему достаточно часто попадать в цель. Чтобы этот агент хорошо обучился достаточно быстро и не проводил очень уж массового полного перебора, я ему сделал такую функцию полезности. Ему дается одно очко за точное попадание в самолет и дается какая-то мизерная доля, что-то типа одной сотысячной очка за пролет снаряда вблизи самолета. Таким образом, мы ему сообщаем, на какой результат примерно нужно ориентироваться, на какой точно нужно ориентироваться, на какой нужно ориентироваться примерно. Далее. на играх Atari в OpenAI Gym. Я не буду приводить видео того, как выглядит обученная система для обучения с подкреплением. В каждой игре есть объективно оптимальная стратегия, либо некоторое небольшое их количество. Когда стратегия найдена, то обычно уже нельзя определить, что за оптимизатор ее породил. Гораздо важнее то, за сколько времени алгоритм сойдется к этому оптимуму и насколько хороши будут промежуточные результаты. Увидим то, что здесь это не какая-то прям идеальная оптимальная стратегия, это некоторый промежуточный результат, который лучше случайной стратегии, который ощутимо лучше случайной. И этот результат получен быстрее, чем в случае обучения с подкреплением. На вход агенту идет В данной ситуации векторизованная картинка с экрана, то есть экран нарезан сеткой с разной степенью дискретизации, то есть размерность экрана снижена в разное количество раз. Три разных варианта, что-то типа от 11 на 11 до 5 на 5. Каждая из этих картинок выложена в виде вектора, то есть, допустим, 5 на 5 – это 25 чисел. И эти числа подавались на вход в агенду. Как мы видим, каких-то результатов он достигает лучше случайно. Если сравнивать это с системами обучения с подкреплением, Сравниваем вначале по эффективности по данным. На играх Atari самая эффективная из систем обучения с подкреплением, самая эффективная из тех, что я проверял, это называется система Dreamer. Она выдала стратегию лучше случайной примерно на 100-тысячном кадре. Поиск в пространстве стратегии выдал стратегию лучше случайной на кадре номер 5000-10000. Тем не менее, перебор стратегии, не учитывая, как награды распределены внутри эпизода, пытается явным образом предсказывать кадры, потому в пределе он намного менее эффективен по данным. Высокая эффективность и высокая скорость получаются потому, что меньше необходимости моделировать среду. Часто стратегия относительно простая, но среду замоделировать тяжело. По числу параметров, безмодельные системы, модел-фри, например, кулернинг, которые предсказывают, сколько награды можно получить, если выполнить заданное действие в заданных условиях. Это двух-трехслойные нейросети со слоями размера порядка 128 нейронов. Системы Model-based, которые явно предсказывают будущие кадры, могут иметь довольно большой размер, измеримый с размером нейросетей для компьютерного зрения. А вот перебор стратегий может оперировать малыми сетями. Например, это может быть в моем случае 4 слоя по 80 нейронов, плюс блок памяти, это лента на 100 чисел одномерная. Простые задачи можно решить сетями из 2-3 слоев по 8-9 нейронов, такое тоже бывало. Дальше. Некоторые говорят, что нельзя обучить такую систему работать на сложном окружении. Вот я сделал игру в жанре стратегия. Синий – это наш искусственный интеллект, красный – это рукописный бот. Цель игры – контролировать как можно больше монеток, и награда идет за это. Сколько монеток собрано, плюс в случае, если один из игроков контролирует поле, то половина из монеток, оставшихся на поле, считается, что переходит ему. Как мы видим, перебор стратегии вполне справляется с такой задачей. Еще одно сложное окружение. Здесь задача про торговлю и производство. У агента есть изначально некоторое количество денег, плюс у него первый столбец – это у него есть склад и на нем какое-то количество разных товаров. Второй столбец – это цены на товары. И третий столбец – это какие производства агент развернул, какие фабрики построил. Он может строить фабрики на свои деньги, имея соответствующие инструменты. Кроме того, существует комиссия, то есть если агент покупает некоторые товары, а потом продает его, то продает он его на 20% дешевле, чем купил. И есть, чтобы совсем уж не было линейной простенькой симуляции, есть события случайные. Они появляются вероятностно, и события эти зависят от действий агента. То есть от того, что он продает, что он покупает. Это некоторые глобальные события. То есть, например, агент может своими действиями привести к какому-нибудь политическому кризису или к экологическому кризису. войну спровоцировать или что-нибудь еще, просто потому что он торгует каким-то определенным образом. Итого, запускаем агента, смотрим, что он делает. Первым делом он покупает машины, это все продал, потом покупает машины, велосипеды почему-то выглядят неразумно, потом бамс он купил, он купил кучу леса, и вызвал кризис леса. Смотрим, что после этого происходит. После этого цена на дерево начинает расти. И агент просто держит, ну он создал экологический кризис и дальше он просто удерживает древесину у себя. И фабрики он вообще строить не стал. В последний день симуляции он продал эту древесину и поднял себе кучу денег. Я еще думал, почему он вначале покупал и продавал машины и велосипеды. Это выглядело совершенно неразумным, непонятным. На самом деле, я потом посмотрел Он же не на одной симуляции обучался, он на многих симуляциях параллельно, на многих вот таких, просто с другими начальными условиями. Там покупка, продажа машин и велосипедов, они тоже триггерят некоторые события. И он пытался затриггерить эти события, но не смог, и поэтому переключился на другие. Идем дальше. Можно обучить агента решать несколько разных задач. Показано на диаграмме снизу, если мы делаем решатели задачи номер один и решатели задачи номер два, то сложно сделать так, чтобы агент делал и то, и другое. У него должна быть какая-то очень специальная конфигурация. Но здесь важна постановка задач. Если вы просто будете заставить агента изучить стратегию, которая хорошо работает и в тех, и в других условиях, то это будет сложная задача. будет некоторая унифицированная постановка задачи, то может так выйти, что агенту будет гораздо легче обучиться. То есть, например, и в том и в другом случае агент будет решать задачу обучения с подкреплением. То есть он будет получать какое-то количество очков в зависимости от того, на каком расстоянии синяя точка от краски, каждый ход. а это в симуляции левой, а вот в правой симуляции за каждую зеленую точку будет получать очки, а за каждую красную точку будет терять очки. В этом случае задача уже как-то решаемая становится. Ну и в любом случае обучить такую систему намного сложнее, чем специализированную. Качество работы будет обычно ниже, но это надольше. В связи с этим, Никита предложил следующую идею. Давайте сделаем модулирующие нейроны, которые позволят системе обучаться прямо внутри эпизода. Вот красный прямоугольничек – это нейрон. Вот он сумматор, вот она нелинейность. Следующий красный прямоугольничек – это тоже нейрон, просто другого слоя следующего. А между ними идет связь. И эта связь сделана как вентиль. То есть можно этот вентиль открыть пошире или закрыть посильнее. И модулятор, то есть это просто некоторое число берется с выхода какого-то из нейронов последнего, допустим, слоя, подает сигнал на накопитель. Накопитель – это просто инерционный элемент, то есть изначально число 0. Модулятор присылает туда десятку, там становится десятка. Модулятор присылает туда минус 5, там становится пятерка, ну 10 минус 5. Видите, соответственно, это какой-то коэффициент усиления у нас на связи. Идея в том, что мы таким образом можем менять веса прямо по ходу эпизода. Не все веса менять, а только некоторые. Таким образом получать что-то вроде обучения прямо посреди процесса. прямо внутри симуляции, внутри эпизода. И дальше показано, насколько это эффективно, как это работает. Нет, пока что проведено недостаточно экспериментов, чтобы сказать точно, но очень похоже на то, что это действительно работает, что вот вначале агент делает что-то бесполезное, а потом уже достаточно неплохо обучается и что-то начинает делать более осмысленно, уже начинает более эффективно стрелять по цели. Одна симуляция, другая симуляция, тот же самый агент с модулированными нейронами, Он тоже вначале действует как-то более случайно, а потом действует каким-то способом, который дает ему больше наград. Но здесь нужно проводить больше экспериментов. Это нужно серьезнее исследовать и особенно нужно это исследовать на множестве разных симуляций, на множестве разных задач. И переходим к следующему слайду. Пройдемся по достоинствам и недостаткам методов перебора стратегий. Достоинство подхода – то, что стратегия записывается очень компактно, она намного компактнее, чем нейросеть. в обучении с подкреплением. Потом подход очень устойчивый. По сравнению с обучением с подкреплением легко реализовать память, которая будет на всю жизнь. То есть в обучении с подкреплением у нас есть некоторое контекстное окно, которое мы учитываем при принятии решений. Здесь же у нас есть просто некоторый модуль памяти и начинается эпизод, туда в этот модуль памяти что-то пишется, что-то читается, агент, контроллер сам обучается работать с этим модулем памяти по ходу эволюции. Потом можно провести аугментацию сети, то есть строить в нее какой-нибудь рукописный навигатор, базу данных, симулятор физики, все что угодно, любой софт, можно сколько угодно, не дифференцируемый. Система научится этим пользоваться. Легко реализовать доказуемый общий решатель с любой формой постановки задач. Это концептуально легко, но вычислительная проблема все еще стоит, потому что это эволюция, она не очень быстрая и нужно будет подобрать. Недостатки подхода следующие. А, еще одно достоинство важное не сказал. Легко параллелиться. распараллеливаются лучше, чем обучение с подкреплением. Если у вас есть сервер на 100 ядер, вы намного лучше реализуете возможности вот этой нейроэволюции, чем возможности обучения с подкреплением. Недосадки подхода следующие. Постановка задачи – это только симуляционная среда. То есть нативно постановка задачи такая. Там уже можно внутри неё что-то другое сделать. Но это именно что внутри стимуляционной среды сделать некоторую стратегию, которая ведет себя как обучение с подкреплением. Но обучение с подкреплением сейчас дела не лучше. Она тоже работает практически всегда только симуляцией. Способы перебора. неэффективны по сравнению с аналитическим градиентным спуском, который используется в обычных нейросетях, но что компенсируется тем, что стратегии часто более просты, чем честные модели мира. Перспективы применения следующие. Во-первых, эта работа в среду, где симуляция возможна, Во-вторых, это обучение стратегии для нескольких симуляций, похожих на реальную задачу. То есть, например, нам нужно обучить стратегию для шагохода. Но мы точно не знаем, какой это будет шагоход. Кроме того, он будет в реальном мире, а мы не можем обучить прямо точно. У нас нет симуляции реального мира хорошей. Мы обучаем в симуляции модель, которая подходит для нескольких разных шагоходов. Она автоматом начинает подходить и для нашего реального шагохода, просто потому что наш реальный шагоход где-то оказался в множестве тех шагоходов, на которых мы обучаем стратегию. Кроме того, аугментированные сети для задач, где решение примерно известно в общих чертах. Например, у нас есть какая-нибудь задача, которая как-то связана с ориентированием в пространстве. Мы можем сделать отдельный слой, который будет картой, то есть двухмерный массив, в который контроллер сможет записывать и читать из него данные. К нему будет приделан какой-нибудь протокол взаимодействия полярные координаты. И это может быть более эффективно, чем обучение с подкреплением. И можно обучать reinforcement learning, обучение с подкреплением, прямо внутри нейросети. Ссылка на код приведена здесь, презентация доступна. Вот яндекс.диск и гид вашим услугам. Теперь я готов ответить на вопросы. Сейчас я открою чат, прочитаю. Так, что здесь называется стратегия? Стратегия это то, как устроен контроллер. Там внутри контроллера находится нейронная машина Тьюринга. Вот веса этой нейронной машины Тьюринга – это геном, а то, как эта нейронная машина Тьюринга откликается на внешний мир – это ее стратегия, это ее поведение. Просьба остановиться на проблеме локальных минимумов и комбинаторного взрыва при поиске стратегий. 

S05 [00:39:15]  : Это про то, что вы говорили в начале, то есть я думал, вы как раз прочитали мое сообщение в начале доклада, то есть вы в начале доклада говорили, что вот есть утверждение, что вот эти генетические алгоритмы плохо работают с локальными минимумами, на что вы сказали, что reinforcement learning тоже плохо с этим работает. С точки зрения возможности избегания глобальных минимумов, можете ли вы сопоставить то, про что вы говорите, с reinforcement learning? Это первый вопрос. Ну и второй вопрос, что если у нас достаточно большой геном, то нам нужна очень длительная эволюция, и нам нужно очень долго искать в этом пространстве стратегии. Соответственно, у нас случается экспоненциальный взрыв с перебором. И как с этим быть, тоже вопрос, с точки зрения сравнения с РЛ. 

S03 [00:40:08]  : Если сравнить с RL, то добиться результата лучше случайного через методы нейроэволюции проще. Добиться результата глобального оптимума через нейроэволюцию, по-моему, сложнее. Я просто не вкладывал туда столько ресурсов, сколько настоящие исследователи РЛ вкладывали в свои РЛ. Это момент номер раз. Момент номер два. Если геном большой, то нам не обязательно попадать в глобальный оптимум, нам можно попасть в хороший локальный. И хороших локальных очень много получается, потому что геном обычно избыточный. Обычно это не какая-то одна точка является хорошим оптимумом, а хороших точек много. Скорее у нас комбинаторный взрыв происходит тогда, когда стратегии нужно выполнить некоторый маловероятный набор действий, некоторую сложную цепочку действий, и вот эта цепочка награждается, а все остальные цепочки не награждаются. И в этом случае, да, у нас случаются проблемы комбинаторного взрыва, но обучение с подкреплением – ровно та же самая проблема. Оно же тоже пока не поймает вот эту первую награду, оно не пойдет, куда надо идти. В случае с обучением с подкреплением эту проблему можно решить так. использовать имитационное обучение, то есть не на тестовых задачках, конечно, а на задачках реальных, продакшене, например. Можно использовать имитационное обучение, чтобы человек показал системе, как может выглядеть успешное достижение цели. А система пусть дальше экспериментирует, ищет свои стратегии, отталкиваясь от этого. В нашем случае этот подход не получится, но мы можем мы можем записать взаимодействие между нашей стратегией и актуаторами таким образом, если мы знаем какую-то хорошую стратегию, но наш агент не может ее найти, мы можем записать взаимодействие между актуаторами и нашей стратегии таким образом, чтобы с высокой вероятностью попадать вот в эту выигрышную стратегию. То есть условно можем написать руками какую-нибудь одну простенькую стратегию, но которая приводит к какому-то результату хорошему и написать, а вот агент исправляет ошибки этой стратегии, то есть он накладывает на нее какие-то свои воздействия, он описывает отклонения от этой стратегии. Рукописная стратегия генерирует каждый раз какой-то вектор, а наш агент генерирует некоторый вектор, который прибавляется к вектору рукописной стратегии. Таким образом, получится, что мы стартуем с рукописной стратегии, но затем нейроэволюция ее как-то исправляет, дополняет, улучшает. 

S05 [00:43:22]  : Так, следующий вопрос. Вопросы на понятие, что называется, то есть сможете сформулировать четко, что называется стратегией и что такое цель и задача и является ли цель и задача у вас одним и тем синонимами или это все-таки разные вещи? 

S03 [00:43:41]  : Так, стратегия здесь называется схема реагирования агента, то есть его алгоритм поведения. если бы там внутри агента находится нейросеть, но эта нейросеть по сути эквивалентна некоторому программному коду, то есть вот этот программный код нейросеть здесь работает просто как некоторый язык программирования, который пригоден к эволюции. 

S05 [00:44:09]  : Извините, можно я все-таки перевью? Потому что все-таки нейросеть – это одно, а язык программирования – это совершенно другое. То есть, когда вы говорите, что нейросеть – это просто такой язык программирования, у меня, честно говоря, случается когнитивный диссонанс, потому что я не понимаю, что это значит. То есть, это нужно очень серьезно объяснить тогда. Как это нейросеть? Это вроде языка программирования. 

S03 [00:44:33]  : Язык программирования описывает то, как компьютер реагирует на входные данные. То есть, что он делает… Ну, не язык программирования, программа это описывает. А язык программирования – это способ конструирования таких программ. Нейросеть внутри моего агента – это такая же программа. Геном – это как… Подождите, Никита, секундочку. 

S05 [00:45:00]  : Давайте про геном. С геномом до генома мы еще доберемся. Все-таки программа на языке программирования – это программа, у которой есть ключевые слова, есть операторы, есть операнды. А нейросеть – это набор, это коэффициенты, которые связывают вершины некоторого воображаемого графа. Вот. Соответственно, вещи это совершенно разные. Соответственно, видимо... Вот вы поясните, пожалуйста, как у вас вдруг оказывается одно другим. Ну, у меня есть гипотеза на этот счет, на самом деле. То есть, я на самом деле где-то... Я понимаю, о чем вы говорите, но вот я боюсь, что не все понимают. 

S03 [00:45:40]  : Да, сейчас я постараюсь это почетче сформулировать. Когда мы пишем программу, вот программа – это строка такая, то есть это текстовый файл. Мы этот текстовый файл загоняем в компилятор, компилятор выдает на выходе бинарный файл. Этот бинарный файл – это схема реагирования компьютера условно на входные данные, то есть в него прилетают какие-то например, числа с клавиатурой, какие-то нажатия клавиш. В общем, какие-то идут биты через поток ввода и какие-то биты идут на выход. То есть вот у нас есть строка, это программа, например, сишный код – это тип данных строка, набор символов. мы их компилируем в некоторый исполняемый файл, дальше мы запускаем этот исполняемый файл и получается некоторая схема реагирования. 

S05 [00:46:42]  : Когда мы компилируем программу, мы получаем просто другую программу. У нас программа на языке C компилируется в программу на языке того или иного процессора, который в качестве таргета в компиляторе. У нас нейросеть в результате не появляется. Как набор инструкций, Неважно, инструкция или это в системе команд процессора, или в системе команд Java машина, или в интерпретаторе Python, неважно. Вот как это у вас становится... Алло. 

S03 [00:47:16]  : Да-да. Сейчас постараюсь объяснить. Вот есть, допустим, рассмотрим программу на машинном ходе. Это набор бит. Они как-то воздействуют на процессор. Они описывают то, как процессор будет реагировать на входные воздействия, с мышки, с клавиатуры, из файловой системы и так далее. То есть они описывают его поведение. У нас здесь нейросеть, нейронная машина тьюлинга – это некоторый аналог процессора, а геном – это некоторый аналог вот этого машинного кода. Я сейчас нормально объясню? 

S05 [00:48:09]  : Нет, то, что геном – это аналог машинного кода, я понимаю. Я просто не понимаю, где здесь нейросеть. Потому что нейросеть по определению – это просто множество коэффициентов, которые просто связывают некоторые входы на некоторых слоях с некоторыми выходами. 

S01 [00:48:31]  : Можно я попробую переформулировать вопрос? попробуйте вот с такой стороны. нейросеть это, конечно, тоже программа, потому что мы ее описываем программой и, соответственно, реализуем. но программа, описывающая нейросеть, отличается от вообще программы тем, что в программе, описанной нейросеть, есть вектор скрытых несомслезий, и мы меняем только его. а просто программа, там если мы что-то изменим, то она, скорее всего, сломается. Вот как вы обходите эту сложность? 

S03 [00:49:04]  : Можете нам пояснить? Я в данной ситуации рассматриваю нейросеть как что-то вроде PLIS – программируемая логическая интегральная схема. То есть это набор модулей и связи между ними. 

S01 [00:49:24]  : Вы как-то определяете, что есть какие-то связи, которые вы можете менять. То есть из вот этого плиса вы выделяете связи и это как-то некоторый аналог вашей нейросети. 

S03 [00:49:37]  : В нейросети есть связи и в этом плане нейросеть это что-то вроде такой интегральной микросхемы. То есть нейросеть, когда она исполняется на компьютере, она ведет себя как некая виртуальная микросхема. 

S01 [00:49:55]  : Я попробую объяснить разницу между нейросетью и интегральной микросхемой. Если в нейросети менять коэффициенты немножко, то она не сломается, а в интегральной микросхеме все по-другому. Если там что-то изменить немножко, она вообще перестанет работать. 

S03 [00:50:09]  : Да, именно поэтому и используется нейросеть. Она является некоторым аналогом интегральной микросхемы, но малое изменение ее не приводит к тому, что она ломается. Она ведет к малому изменению ее поведения. Здесь смысл в том, что геном – это программа для… Геном – это конфигурация этой микросхемы. Там есть функция assemble. Она прошивает эту микросхему, она проставляет веса в нейросеть. В этом случае нейросеть превращается в некоторую виртуальную микросхему. Есть очень большое количество микросхем, в которые она может превратиться таким образом. Этот набор, условно, микросхем, он полон по тьюрингу, то есть нейросеть может превратиться в нечто, что выдает такое же поведение, как некоторые программы. 

S05 [00:51:21]  : То есть, из того, что я услышал, я правильно понимаю, что под стратегией подразумевается просто некоторая программа. То есть, у нас разные версии генома приводят к порождению разных программ, и как раз вот каждая отдельная программа является стратегией. Это правильно? Да, все правильно. Но я все-таки хотел бы зафиксировать, что я не понимаю, почему это называется нейросетью, потому что в программе вы либо выводите сигнал в порт, либо вы его считываете из порта. где здесь нейросети, где здесь коэффициенты, которые, как сказал Владимир, могут меняться плюс-минус, я, честно говоря, пока так и не увидел. Я просто не понимаю, почему это нейросетью называется. Просто берёте Википедию, читаете определение нейросети и пытаетесь... Я просто предлагаю двинуться дальше, потому что мне кажется, просто вы некорректно используете термин нейросеть. 

S03 [00:52:18]  : Там есть некоторая программа, содержащая параметры. Это, скажем так, большая формула. 

S06 [00:52:32]  : Сергей, называйте ее не программой, а функцией. Это просто функция. Она из входов вычисляет выход. тогда все станет на свои места. нейросеть умеет вычислять по входам выходы и функция, написанная на любом языке, тоже умеет вычислять по своим входам выходы. вот в этом смысле вы имеете ввиду программу. это функция плюс у нее есть модуль памяти. функция с памятью, с изменением состояния. тогда все стало на свои места. 

S03 [00:53:01]  : просто архитектурно выглядит похоже на нейросеть. 

S05 [00:53:06]  : Хорошо. Сергей, спасибо, что помогли разобраться. По цели и задаче еще поясните. Цель и задача – это у вас одно и то же или разные? 

S03 [00:53:18]  : В данном контексте можно считать, что это одно и то же. Я в курсе, что часто люди с задачей называют более широкую вещь, чем цель. Поэтому я с самого начала оговорился, что мы работаем в таком ключе, что в некоторых ситуациях мы можем назвать одни результаты хорошими, другие результаты плохими. То есть у нас есть функция полезности. у нас система работает с некоторой функцией полезности, то есть это более строгий термин, а цель или задача я использовал как более простой нестрогий термин. Спасибо. Следующий вопрос. Но именно термин такой, как задачный подход, я, наверное, не слышал. 

S05 [00:54:24]  : У нас просто на семинаре неоднократно были доклады Евгения Генича Витяева и Дмитрия Ивановича Свериденко как раз по задачному подходу, который где-то близок к тому, что вы рассказываете. Дальше вопрос от Владимира Смолина. Когда вы говорите, что задача решена, подразумевается, что найдено одно частное решение, правильно? 

S03 [00:54:46]  : Да, найдена одна стратегия, которая приводит к достижению результата. 

S05 [00:54:51]  : А вопрос от Сергея. Насколько легко понять, что задача решена? Как вы решенность задачи определяете? 

S06 [00:55:00]  : Антон, можно сначала, вот прежде чем этот вопрос будет озвучен, все-таки у меня есть еще один вопрос. Конечно, давайте. Я так понимаю, все-таки вы модерируете, Игорь не модерирует, правильно? 

S05 [00:55:10]  : Видимо, получается так, да. 

S06 [00:55:12]  : Сергей, вот у меня сначала вопрос. Дело в том, что у нас там много очень таких вопросов в чате. Они там разного масштаба, разного плана и затрагивают разные, так сказать, варианты. А так получилось, что вы как-то не очень представились. Возможно, вы вот в чате, если вы с кем общались, уже люди вас знают, кто вы там и так далее. Но здесь присутствуют слушатели вашего доклада, которые не знают, где вы, где вы учились. Ну то есть, как-то немножко два слова скажите, есть ли это если это я корректный вопрос задаю, можете это сделать? потому что нам это это позволит нам понять на каком как бы нам направлении лучше двигаться. 

S03 [00:55:52]  : да, представиться, меня зовут Сергей Довгань, я учился в МГТУ имени Баумана на специальности системы автоматического управления. соответственно Сейчас работаю дата-сайентистом, аналитик по данным, и у меня достаточно большой бэкграунд по искусственному интеллекту, то есть по машинному обучению, по задачам оптимизации. Это очень много своих экспериментов, плюс например reinforcement learning делал в продакшене. то есть достаточно много промышленной разработки систем машинного обучения. 

S06 [00:56:39]  : понятно, спасибо. ну да, ну мы услышали. теперь вот так теперь собственно вот тот вопрос. я на самом деле просто этот тот же самый вопрос, который задавал Владимир, только я его немножко хочу как-то ну как бы так сказать повнятнее, что ли, сформулировать. Не повнятнее, а по-своему сформулировать для себя. Дело в том, что вы отметили такую вещь, говоря о любых задачах. Вы же хотите сделать какие-то любые задачи. Вы упомянули от части вскользь, что легко понять, что задача решена. Трудно найти решение, а вот то, что задача решена, понять легко. вообще-то это утверждение спорное, потому что в зависимости от того, что вы понимаете под классом задач, то есть какие задачи вы собираетесь рассматривать, проблема обнаружения того факта, что задача решена, во-первых, когда она решена. вот мне еще ждать этого решения или она вот она его найдет или она его уже закончила работу, она сказала я поискала не нашла или что-то еще. то есть Если вы сужаетесь до каких-то этапов отдельных, то есть вы рассматриваете такие задачи, в которых рассматриваются отдельные этапы, у которых абсолютно ясное начало и абсолютно ясный конец, этап заканчивается. И после этого у вас есть какой-то простой предикат, который проверяет, решена задача или нет. То есть вот если только такого класса задачи вы рассматриваете, тогда этого вопроса нет. Но тогда слово любые задачи не употребляете. Вот я к чему. потому что есть проблема остановки поиска, есть проблема... просто банально, если вы запускаете какую-то программу, то остановится она или нет, это же нерешенная задача. и что такое задача решена? вы рассматриваете такие классы задач, для которых наличие решения самоочевидно. вот вы такие задачи рассматриваете. 

S03 [00:58:35]  : Я работаю над искусственным интеллектом в смысле машины результатов. То есть это не просто решатель задач, где задача – это что-то абстрактное. Решатель задач, где задача – это некоторый результат, который можно… Скажем так. возможность легко проверить, это неотъемлемая часть задачи. это то, что мы сообщаем системе. 

S06 [00:59:06]  : это мы не говорим, какой критерий решенности. все понятно, то есть у вас есть критерий решенности, он конечный, обязательно проверяемый для любого решения, которое выдала машина. Машина выдала вам какую-то стратегию, и у вас есть заведомо гарантия того, что вы можете проверить, эта стратегия решила задачу или нет за конечное время. Вот это утверждение, которое вы делаете. Вы такие задачи рассматриваете, да? Да. Хорошо. Просто тогда слово «любые задачи» не употребляйте, а то у нас плывут мозги, как говорил Антон. 

S01 [00:59:37]  : Нет, есть еще и другая сторона вопроса, что вы не употребляете слово «интеллектуальное». Потому что все интеллектуальные задачи, если кто интересуется, как они решаются, они имеют такое понятие, как SOTA – State of the Art. То есть тот уровень, на котором сейчас эта интеллектуальная задача решается, этот уровень постоянно повышается. То есть там вопрос того, насколько решена задача, он, мягко говоря, творческий. И вот под ваше определение, где он, они не подходят. То есть все интеллектуальные задачи под ваше определение не подходят. Вот, собственно, в котором вывод легко делается из того, что вы рассказали. 

S03 [01:00:10]  : Сейчас я покажу слайд один ближе к началу. Для каждой задачи условно одно число – это то, насколько данная система ее решает. Худшее условное решение – это то, как делать случайное поведение. Есть просто две важных отметки. 

S01 [01:00:40]  : Это мы поняли, что тезис о том, что так сойдет – это критерий решения задачи. Но сказать о том, что при этом решена интеллектуальная задача, мягко говоря, не совсем обоснованно. 

S03 [01:00:54]  : Я не использую термин. 

S06 [01:00:56]  : Вы понимаете, вот пожалуйста, я вас очень прошу, будьте пожалуйста поточнее. Вот вы только что сказали, что вы применяете понятие задача решена и тут же одновременно употребляете следующее понятие, есть градуальная степень решенности задачи. То есть вообще говоря, вы каждое решение пробное, которое выдает ваша система, то есть каждая вот эта стратегия, да? может быть оценена числом градуальным не только в случае 0 1 не только решена или не решена а у вас есть количественная оценка любого такого решения так или нет конечно ничего себе конечно это это это это потрясающее сужение класса задач которые вы рассматриваете вы просто не подозреваете насколько сразу же вы удаляетесь от слов любые ну любые это бесконечно далекое слово от него к нему нельзя приблизиться понятно но когда вы говорите о том что я рассматриваю только такие стратегии которые после конечного акта их применения я смогу не только установить факт решения задачи но и количественно оценить степень решенности задачи вот вы просто это скажите, если это так, все, мы эту сторону силососку убираем. 

S03 [01:02:12]  : да, предполагается, что мы можем количественно оценить, насколько решена задача, и бинарная оценка просто частный случай такой ситуации. в случае с бинарной оценкой система будет работать плохо. чем лучше отградуирована шкала, чем она более гладкая, тем система работает лучше. 

S06 [01:02:36]  : Хорошо, до бинарной оценки мы еще дойдем. 

S05 [01:02:39]  : Сергей, можно я чуть-чуть вступлюсь, потому что, по-моему, то, что говорит Сергей, не противоречит тому, что сказал Владимир Смолин про State of the Art, потому что если мы возьмем тот же самый пинг-понг, который нам демонстрировал Игорь Пивоваров, ну там, естественно, партию можно либо проиграть, либо выиграть. Но, извините, количество процентов выигранных или проигранных партий при той или иной стратегии как раз и дает State of the Art. То есть, грубо говоря, если State of the Art 75% выигранных партий, то, значит, этот State of the Art бьет State of the Art с 85% выигранных партий. При том, что, естественно, одна отдельная партия может быть либо выиграна, либо проиграна. Давайте двинемся дальше, наверное, по комментарию. У меня вот есть вопрос. В чем возражение было? Понимаете? Это возражение... Все, это был комментарий. Мне в чем возражение? Давайте следующий вопрос. Как задается вектор генома? Откуда он вообще берется? Его должен составлять человек или подбирать другой ИИ? Я просто проведу пример, чтобы был понятен вопрос. Мы похожие вещи тоже делаем в своем проекте. У нас тоже есть эволюционный поиск. Но набор исходных генов задает вручную человек, исходя из своего какого-то знания. Как вы определяете исходный геном, который потом варьируется для того, чтобы получить пространство стратегий? Программируете ли вы его вручную или есть какой-то внешнеискусственный интеллект, который этот геном предопределяет? 

S03 [01:04:22]  : Геном задается случайным образом. а дальше происходит оптимизационный поиск, отталкиваясь от этого генома. 

S06 [01:04:33]  : Нет, не значение элементов генома, а сама структура генома. 

S03 [01:04:37]  : А, сама структура. В тех ситуациях, где решалась задача, вот, например, где она тут решалась, была некая фиксированная архитектура, То есть я описал, в каком порядке идут слои нейронов, какие типы слоев. Там было что-то типа четырех слоев по 80 нейронов плюс один блок памяти. То есть структура задается вручную, исходя из того, чтобы она была, во-первых, тьюринг-полной, во-вторых, Если мы полагаем, что у нас задача какая-то огромная и сложная, то, наверное, надо сделать побольше этого кино. Если мы предполагаем, что она маленькая и простая, то, наверное, надо сделать поменьше. Вот из таких только общих соображений. 

S06 [01:05:30]  : То есть правильно я понимаю, что есть базовая нейронная сеть, которая определена? есть параметры. эти параметры, их конечное число, их можно пересчитать. и вот в этом порядке пересчета они просто кладутся в виде отдельных чисел в вектор из генома, который называется геномом. правильно? да, все верно. все, спасибо. 

S05 [01:05:49]  : Спасибо. Вы в какой-то момент сказали, что получается нечеловеческое решение. Прям вспоминается Альфа-Го, которая в какой-то момент сделала нечеловеческий ход. Вы нечеловечность решения как-то можете формализовать? Почему это решение является нечеловеческим? 

S03 [01:06:08]  : Ну, некоторые решения выглядят такими, что человек бы, скорее всего, к нему не пришел. Это субъективная позиция. То есть вот, например, вот это решение, вот такое, для меня выглядит довольно-таки нечеловеческим. Кажется, что человек вел бы ракету иначе. Вот эти системы нейроэволюции достаточно часто сводятся к решениям, которые достигают цели, но при этом они достаточно сильно отличаются от того, как обычно делают люди. 

S05 [01:06:41]  : Ну а в данном случае пример, который вы показали, здесь может быть дело просто в том, что находится неоптимальное решение, которое неоптимальное, но является решением. 

S03 [01:06:50]  : Дело не в этом. Здесь оптимизируется не то, что оптимизируют люди. Здесь задача – достичь цели. Просто люди никогда не ставят задачу так. Люди всегда ставят задачу, что там еще нужно как-то по топливу оптимизировать, по чему-нибудь еще оптимизировать. А здесь этого всего нет. Здесь ставится задача, и она достигается с нуля, без использования прошлого опыта людей, поэтому оно получается не так, как обычно это делают люди. 

S00 [01:07:21]  : Какая из этих двух траекторий человеческая, а какая нечеловеческая? 

S03 [01:07:27]  : Ну вот это больше похоже на человеческие траектории, на то, как люди делают. 

S00 [01:07:31]  : Но это тоже... Как раз наоборот. Истребитель всегда заходит в хвост, а не идет в предполагаемую точку пересечения. И ракета заходит в хвост, если мы возьмем практику. 

S05 [01:07:49]  : Хорошо. Ладно, давайте мы военную тематику сейчас не будем развивать. Вопрос. Вы сказали, что Reinforcement Learning на долгосрочной перспективе лучше. Тогда, соответственно, в чем выигрыш в том, что вы предлагаете? 

S03 [01:08:07]  : Эта система лучше, во-первых, в краткосрочной перспективе. То есть я рассказывал, что чтобы обучить самый быстрый reinforcement learning, нам нужно 100 тысяч кадров, чтобы обучить его на то, чтобы он просто лучше случайности действовал. А чтобы такая система действовала лучше случайности, 10 тысяч кадров вполне достаточно обычно. Она даже в какой-то мере, какой-то локальный оптимум найдет скорее всего. То есть лучше в этом. Это первое. Второе, lifelong память. В reinforcement learning ее поставить не так-то просто. Ну и третье, то что вообще в целом можно архитектуры делать более разнообразными что ли. потому что в обычном reinforcement learning мы решаем задачу оптимального управления через задачу аппроксимации, а здесь мы ее решаем как бы напрямую, соответственно, мы гораздо меньше ограничены в архитектурах. И нам даже не обязательно, чтобы она была насквозь дифференцирована. 

S01 [01:09:17]  : Спасибо. Сергей, вот учитывая, что у вас лёгкость мысли необыкновенная, и она, конечно, радует, вот, но вот это значит сравнение, когда вы говорите, что вот там 100 тысяч, а у нас 5 тысяч, это замечательно, вот, но понимаете, какое дело, что насколько при этом совпадают условия, может быть, там, там, опустим экран, там с хорошим разрешением, там нужно 100 тысяч векторов, там по миллиону значений или там по 5 миллионов значений, а у вас там вектор 25 на 25, и конечно у вас быстрее. вот складывается именно такое впечатление, что вот за корректностью сравнения вы не особо следите, потому что зачем это тонкости, и так все хорошо работает. я прав или не прав? 

S03 [01:10:02]  : Когда я обучал систему reinforcement learning, которая тестовая, которая потребовала 100 тысяч кадров, это была самая быстрая из существовавших на тот момент системы reinforcement learning. Там на входе был вектор размера 25 или 16, сейчас точно не скажу. Короче, какой-то маленький вектор. какое-то двузначное число. Но перед этим изготавливался автоэнкодер для того, чтобы сжать картинку в этот вектор. Сколько кадров понадобилось на то, чтобы изготовить автоэнкодер, я сказать точно не могу, но вряд ли 100 тысяч кадров реально изготавливался автоэнкодер. 

S01 [01:10:56]  : То есть про 100 тысяч – это ваше предположение? 

S03 [01:11:00]  : Это я запустил систему Dreamer на некоторых задачах Atari. У меня получилось 100 тысяч. Потом я читал код этого Dreamer и видел, какого размера вектор входит в reinforcement learning. Плюс я понимаю вообще функционирование таких систем. Я не могу точно сказать, сколько из тех 100 тысяч использовалось на автоэнкодер, сколько использовалось на reinforcement learning, но я делал автоэнкодеры и в той задаче очень не похоже на то, чтобы надо было много кадров на тот автоэнкодер. 

S01 [01:11:42]  : То есть, это результат прочтения вами книг и их творческого осмысления, а не то, что вы сравнили одну модель с такими же условиями и другую модель на таких же условиях. Это было или не было? 

S03 [01:11:52]  : В идеально одинаковых я, правда, не сравнивал. 

S05 [01:11:58]  : Спасибо. Вопрос от Сергея про оценки стратегии. Сергей, он еще актуален? 

S06 [01:12:05]  : он чуть-чуть немножко актуален поскольку вот уважаемый докладчик он легко говорит о задачах в которых есть градуальные оценки любой стратегии и задачах в которых есть оценки только 0 1 0 1 да ну то есть это всегда 0 и если нашли решение то 1 просто проблема состоит в том, что в сколь-нибудь минимально серьезных условиях решение это всегда мера 0 в пространстве поиска. но у вас пространство поиска конечно, как я понимаю, вы все-таки работаете с какими-то дискретными величинами или оно у вас континуально? вот на этот вопрос вы сначала ответьте. у вас континуум возможных решений или дискретное, но большое число? 

S03 [01:12:52]  : меня континуум, но это континуум из чисел флот 16, они все-таки это не настоящие действительные числа, они все-таки это просто множество. 

S06 [01:13:04]  : но приближаетесь, вы хотите приблизиться к континууму, так и такая цель. началась континуум, тогда среди всех континуального множества, которое имеет известную мощность этого множества, количество траекторий, вернее количество этих самых стратегий, которые достигают результата, это мера ноль. и поэтому если у вас стратегия оценивается так, что попали единицы, не попали ноль, то вы ищите меру ноль. правильно? с понятием мера ноль знакомо вам? я употребляю это слово. лучше уточнить. но мира ноль это условно говоря вот вы стреляете из лука по мишени да и не знаете заранее в какую точку вы попадете но в какую-то точку вы попали в этом смысле события попадания в какую-то точку реализовалась но вероятность попасть в каждую наперед за одну точку равна нулю и вот в этом смысле вот это я имею ввиду что это вероятность так сказать равно нулю вероятность найти успешную стратегию равна нулю, если целевое множество имеет меру 0. вот я к чему. и тогда вы можете рассчитывать исключительно только на везение, на расположение звезд, на хорошее настроение или на что-то еще. но если вы ищете меру 0, у вас нет никакого содержательного способа найти решение, которое имеет вероятность 0. Вот это я просто хочу отметить. Либо вы изначально на самом деле говорите, что нет, у меня все-таки дискретное пространство поиска. Кроме того, под решением задачи я понимаю не меру нуля, а какой-то объем конечный обязательно. Это тоже тогда требования, тоже сужение класса задачи, обращаю ваше внимание. И только вот в этом случае, когда не нулевая вероятность нахождения решения, вы применяете вашу технологию. 

S03 [01:14:59]  : здесь вероятность нахождения решения не нулевая. Класс задач, которые мы решаем, это не те, где нужно попасть в одну конкретную точку в континууме. 

S06 [01:15:16]  : да и на самом деле вы сейчас рисуете вот то что на экране это как раз мировой вам нужно попасть в точку нужно попасть в точку как можно ближе смысл слушайте но вы но вы же у вас любил курс математического анализа в бауманском университете скажите пожалуйста, тогда у вас и эпсион какая-то оценка есть, есть какой-то там диапазон, есть что-то, что превращает вашу цель в нечто с мерой, которая отлична от нуля? 

S03 [01:15:49]  : Я полагаю, что в реальности не существует задач про попадание во что-то с мерой ноль. 

S06 [01:15:56]  : Любые задачи про попадание, они про попадание… В реальности существует очень много чего. Еще вопрос, существует ли самореальность. Мы сейчас говорим о той формальной системе, с которой вы работаете. Мы хотим услышать от вас те ограничения, математически услышать, не с точки зрения, что какая-то программа куда-то больше, сравнили одну программу с другой, микросхему какую-то там еще сравнили, что-то еще. Мы хотим услышать теоретические утверждения относительно того принципа, который вы предлагаете. Этот принцип опирается на какие-то предположения. Мы потихонечку по одному вытягиваем из вас эти предположения. Изначально у вас было любые, говорит слово. Потом мы это слово любые уменьшили, потом еще уменьшили. И вот это еще одно то место, где я хочу услышать от вас, как вы собираетесь рассматривать задачи, у которых множество решений мера ноль. Вы говорите, я таких задач рассматривать не буду. И дальше мотивация литературно-художественная о том, что в некоторой реальности вроде как таких задач нет. Вот эту литературно-художественную мотивацию мы давайте оставим, так сказать, философам и историкам. А как математике ответим на следующий вопрос. Тем самым ваша методология работает в условии, когда мера, то есть множество, мощность стратегии, которую вы находите, она не нулевая, конечная. вот это вот существенное требование к исходной задаче. 

S03 [01:17:19]  : Да, мы всегда ориентируемся на то, что есть некоторый объем стратегии, в который нам нужно попасть. 

S06 [01:17:28]  : Вот это и был вопрос, и вот мы услышали ответ. Замечательно, значит класс задач, который раньше назывался любыми, еще осузился. Услышали, дальше двигаемся. Спасибо. 

S05 [01:17:41]  : Спасибо. Вопрос. Вы можете сформулировать, чем заявленная в названии доклада стратегия отличается от генетических эволюционных алгоритмов, описанных в литературе? 

S03 [01:17:59]  : Генетические эволюционные алгоритмы – это оптимизатор. здесь соединён оптимизатор, контроллер и, кроме того, проведена некоторая работа над тем, чтобы поиск проводился в полном потюринге пространства, чтобы не было слепых пятен. 

S06 [01:18:20]  : Сергей, невозможно это слушать. Во-первых, у вас нет никакого контроллера, у вас чистый оптимизатор. у вас стратегия определяемая вектором задана и есть факт того, что вы сейчас задали конкретный экземпляр стратегии. после этого этот вектор стратегии вы оцениваете как результат и этот результат вы оптимизируете. это чистый оптимизатор. 

S03 [01:18:48]  : генетическим алгоритмом называется оптимизация. алгоритм называется оптимизатор без контроллера. 

S06 [01:19:03]  : Во-первых, это неправильно, это утверждение узкое, это генетическим алгоритмом в каком-то пакете так может называться кем-то из программистов, это я пойму. А вот что такое генетический алгоритм, это способ программирования, это очень широкая вещь. Но не это важно. Важно, что у вас нет контроллера. У вас же нет обратной связи. У вас нет никакого изменения стратегии на лету. Вы берете стратегию и ее оцениваете. Вы берете стратегию и ее оцениваете. Это функция. Эту функцию вы оптимизируете. Точка. Больше вы ничего не делаете. поэтому ваша задача, сейчас ту, которую вы нам рассказали, может быть у вас там внутри еще какая-то информация, которую вы нам не донесли, но та задача, которую вам рассказали, это задача оптимизации функции. 

S03 [01:19:56]  : согласен, это задача оптимизации функции. но далеко не любая задача оптимизации функции приводит к тому, что у нас появляется контроллер, который вот так вот решает какие-то динамические задачи. 

S06 [01:20:09]  : подождите, тогда расскажите про контроль, вы о нем не сказали ничего. 

S03 [01:20:15]  : Сейчас расскажу подробнее. Контроллер устроен вот следующим образом. Это некоторая нейросеть. Это нейронная машина Turing. 

S06 [01:20:28]  : Сергей, стойте. Мне от вас нужно только одно. Параметры этого контроллера на момент оценки стратегии заданы? параметры этого контроллера в тот момент, когда вы собираетесь ценить стратегию, уже заданы? 

S03 [01:20:44]  : у него есть параметры, которые заданы. 

S06 [01:20:50]  : они все заданы. все, после этого забываем. контроллер, вертолет, столб фонарный, роман. любыми словами можно назвать. неважно. у вас есть вектор параметров. Часть из этих параметров относится к хэдсам, часть к мэмори, часть к счёту и часть к контроллеру. Это вы так их раскрасили для себя, для удобства. Комментарии в программе, чтобы были понятны. Это вы их назвали просто литературно. Но по математической сути они являются частью одного вектора, который вы оцениваете. Итак, я возвращаюсь к своему исходному вопросу. У вас есть набор чисел, составляющий вектор, который вы назвали геномом. этот набор чисел вы применяете через вот ту программу, которую вы нарисовали сейчас на экране или через какую-то любую другую программу, это неважно, и после этого получаете оценку этого вектора. Задача вашего алгоритма изменять этот вектор так, чтобы улучшить оценку. То, что я назвал, называется задачей оптимизации. 

S03 [01:21:51]  : это задача оптимизации. 

S06 [01:21:55]  : поэтому берем ваши слайды аккуратно и переписываем оттуда убираем слова reinforcement learning, стратегия, там игровая ситуация, убираем слова агент, убираем там пушку, убираем траекторию и пишем один единственный слайд. слайд такой рассмотрим вектор параметров x Этому вектору параметров мы умеем приписывать оценку. Наша задача – изменить значение элементов вектора параметров так, чтобы улучшить эту оценку. Рассмотрим решение этой задачи генетическим алгоритмом. Все, один слайд. Правильно я понимаю или я все-таки что-то качественно ключевое упустил в этом рассмотрении? 

S03 [01:22:38]  : смысл же не в этом, не в самом генетическом алгоритме. смысл в том, что мы в результате решаем те задачи, которые решает reinforcement learning. 

S06 [01:22:47]  : все задачи сводятся к тому, что найден некий вектор чисел. этот вектор чисел может быть либо параметром нейросети, которая аппроксимирует функцию, или это решающая параметра решающего дерева, которая ее аппроксимирует, либо набор правил параметра, который его аппроксимирует, кодирует, либо что-то еще. но задача состоит в том, что нужно найти некий вектор параметров, который, будучи найден, определяет возможность достижения решения. Решение мы умеем оценивать. Оценку эту мы хотим улучшить. Поэтому все, что происходит здесь, это за исключением литературно-художественного антуража, который вокруг этого существует. Вот если его убрать, этот литературно-художественный антураж, то надо, конечно, скучно было бы слушать. Но вот если его убрать формально, то тогда то, что сейчас здесь происходит, то, что делается, это задача решения, задача оптимизации оценки какого-то вектора путем изменения значений параметров элементов этого вектора. хорошая, замечательная задача, очень фундаментальная математическая задача, но она называется задачей оптимизации. 

S03 [01:23:56]  : задача оптимизации это метод. 

S06 [01:23:59]  : задача оптимизации это класс. метод это у вас в классе питония функция называется метод. Метод, вот понимаете, метод. Хорошо, Сергей. Я на самом деле вопрос задал и комментарии высказал. Я думаю, что наша задача, так сказать, здесь истину, так сказать, и познать, а не там как-то и так далее. Но, Сергей, я просто обращаюсь к вашему вниманию, что, пожалуйста, услышите вот эти комментарии. может быть они вы не рассматривайте что они какие-то там жесткие что я хочу там как-то так сказать там как-то это самое нет это я просто для того чтобы вот это было кристальной смысле понимаете чтобы сутевая составляющая она не было покрыта шлейфом вот это вот шлейф этот он он мешает мы здесь про про суть они прошли Поэтому вот так. Замечательно, все хорошо. Если вы меня услышали, да, дальше вы можете на эту тему поразмышлять. Я на этом вот эту часть, так сказать, закончу. Спасибо большое. 

S05 [01:24:53]  : Спасибо, Сергей. Да, Владимир, давайте сейчас Борису Новикову дадим слово. 

S00 [01:25:02]  : Добрый вечер. Я прослушал доклад. Мне представляется, что он крайне нечеткий. Но я еще читал тезисы, и там я, как мне кажется, заметил одну важную и, возможно, полезную мысль в системе АБАТУР. Там предполагается в одной программе или в одном решении сравнивать различные методы поиска решений. и пытаться найти среди них наилучшее. Вот, возможно, эта идея является ключевой и наиболее продуктивной. Что если у нас есть сложные задачи и много разных классов методов решения, то попробовав как-то, а вот как это сложно, но, видимо, на каких-то упрощенных примерах, на малых размерностях или как-то еще, попробовав разные методы решения, выбрать из них новые лучи и потом применять его уже для большой задачи. Вот именно этот тезис мне представляется возможной перспективой. А все остальное, мне кажется, действительно, как говорилось раньше, литературно-художественное оформление. Спасибо. 

S05 [01:26:26]  : Борис, спасибо. Владимир, давайте еще дадим слово Владимиру Фролову, у которого есть вопрос. Владимир спрашивает. Корректная постановка задачи предполагает эффективное время, за которое надо получить оптимальное вознаграждение. Дилемма исследования и эксплуатации. В этой системе где это время сидит и на что влияет? 

S03 [01:26:58]  : Оценка стратегии предполагает, что мы запустили агента в тестовую среду на какое-то время, на какое-то количество кадров. Это количество кадров фиксировано, и у нас есть некоторый способ оценить, насколько нас устраивает то, как он справился с этой средой. Например, он в этой среде собирает очки каким-то образом. Дилемма исследования эксплуатации зашита внутри контроллера. То есть алгоритм оптимизации подбирает такую конфигурацию контроллера, который решает эту дилемму каким-то наилучшим в данном множестве сред способом. В большинстве случаев сейчас я обучал систему на вариациях одной и той же среды, то есть один контроллер подходил под достаточно малый диапазон сред, то там эта дилемма explore-exploit особо не стоит. Если будет более широкий диапазон сред, там будет тема исследований. 

S02 [01:28:09]  : Извините, пожалуйста, вопрос в том, что не конкретно про один выстрел, а за сколько выстрелов надо, так сказать, получить хороший результат, за 10 выстрелов, за 100 выстрелов, а не в масштабах одного выстрела. я так предполагаю, что если мы хотим очень хорошее решение получить, мы можем поэкспериментировать, несколько раз промазать, чтобы найти более-менее оптимальный алгоритм. вопрос, количество выстрелов, а не внутри одного выстрела, как мы будем себя вести? 

S03 [01:28:47]  : вы имеете ввиду вопрос в том, как оптимизационный алгоритм? 

S02 [01:28:55]  : Я говорю, что постановка задачи разная. Если мы хотим за 10 выстрелов найти что-то приемлемое или хотим за тысячу выстрелов найти более-менее хорошую систему. Разные задачи получаются. 

S03 [01:29:16]  : да, я согласен, разные. сейчас есть некоторая конфигурация оптимизатора. сам оптимизатор я не оптимизировал. сами настройки оптимизатора я мало менял. я его делал таким, чтобы где-нибудь за один 2-3 дня нашлось хорошее решение даже задач какой-нибудь уровня пресса. 

S02 [01:29:49]  : Такая поставка задач, что какое-то одно за 10 выстрелов, чтобы было оптимально и ничего не менялось? сейчас вы говорите о том что происходит внутри эпизода или о том что происходит внутри эволюции я говорю по количеству по количеству обучающих эпизодов то есть количество выстрелов если говорить пропушки количество обучающих эпизодов 

S03 [01:30:19]  : просто оптимизаторы сейчас сделаны вручную и настроены тоже вручную таким образом, чтобы там за пару дней оно сходилось. 

S06 [01:30:29]  : Сергей, вопрос не об этом. Смотрите, существуют две такие очень сильно отличающиеся между собой постановки задач. Первая постановка задачи. Задан фиксированный ресурс, например, 10 выстрелов. Задача так распорядиться этими 10 выстрелами, чтобы один из них был как можно более хорош. Это вот одна постановка задачи. Другая постановка задачи. Ресурс 10 не задан. Вас интересует обязательное нахождение решения задачи, сколько бы оно не потребовало времени. Ну, за исключением там какого-то бесконечного всем этом же ресурса. то есть если в течение трех дней, четырех дней вы решение не нашли, вы будете пятый день искать решение, шестой день искать решение, то есть вы в общем-то ресурсом не ограничены. Вот уважаемый коллега Владимир Фалау спрашивает, решаете ли вы задачу нахождение того, что можно при ограниченном количестве ресурсов, и тогда вам обязательно нужно решить дилемму exploration-exploitation, то есть вам нужно потратить сколько-то на поиск возможных решений, а сколько-то потратить на то, чтобы хотя бы одно из десяти-то все-таки было неплохим. это вот первая постановка задачи. а вторая постановка задачи она такая, стреляй пока не попадешь или пока там не наступит семинар, утром к семинарам надо готовить слайды, вот до утра семинары и будем стрелять. вот дай бог найдем это лучшее решение. это две очень сильно разные задачи. 

S03 [01:32:01]  : сейчас задача сформулирована так, что нужно находить решение там с горизонтом примерно в три дня. Реализация этого ограничения это настройки оптимизаторов. То есть, например, я мог бы сделать генетический алгоритм с поколением размером в тысячу, но там я этого не делаю, потому что иначе оно за три дня не сойдется. 

S06 [01:32:30]  : Если вы за три дня не решите задачу, что произойдет? Какая стоимость не решения задачи за три дня? Никакой? Если никакой, тогда вы можете и четвертый день выторговаться. Это вопрос неформализован. Ну вот все, тогда это задача второго типа, не первого. Владимир как раз спрашивает именно о том, что если решается задача первого типа, что всегда обычно решается в задачах управления ресурсами, в системах каких-то там создания каких-то промышленных вариантов, там всегда есть какой-то ресурс, конкретный зафиксированный ресурс, стоимость которого сильно возрастает, если вы выходите за порог. И вот при условии наличия вот этого фиксированного ресурса вам нужно распорядиться. сколько пробовать, а сколько, как говорится, в пользу класть себе. Вот тогда возникает вот эта дилемма. Если этой дилеммы фактически у вас нет, тогда вы, собственно, и ответили. 

S03 [01:33:22]  : Ресурсы находятся внутри симуляции, снаружи симуляции считаем ресурсов никаких. 

S06 [01:33:26]  : Все услышали, тогда этой проблемы у вас нет. 

S05 [01:33:30]  : Сергей, спасибо. Я по этому поводу тоже хочу несколько вещей сразу прояснить. Во-первых, вы сказали, что то, что вы делаете, оно применимо только в симуляционных средах. Вот, соответственно, ответ на вопрос Сергея, значит, что только в симуляционных средах. С другой стороны, вы сказали, что reinforcement learning, он тоже работает только в симуляционных средах, поэтому то НАТО. Но при этом вы сами сказали, что вы делали reinforcement learning production. Соответственно, вопрос такой, все-таки работает ли reinforcement learning не только в симуляционных средах, но и в продакшене? Это первый вопрос. Второй вопрос, что если reinforcement learning работает в продакшене, исходя из вашего опыта, как вы умудряетесь переводить reinforcement learning в продакшен при том, что в продакшене есть реальные ограничения по ресурсам, про которые Сергей сказал? Ну и третий вопрос тогда уж, если можно Reinforcement Learning перенести в продакшн, то можно ли вот описанную вами технологию перенести в продакшн? 

S03 [01:34:44]  : Так, во-первых, уравнение Reinforcement Learning пригодны для переноса в реальность. Но это не значит, что вот эти вот описанные задачи Atari, что их можно решить в real-time. а можно решить задачи reinforcement learning какие-нибудь более простые в реальном времени. И в продакшене я решал более простые задачи, то есть без двухмерных картинок. 

S05 [01:35:14]  : Вы можете привести пример, какие задачи вы решали в продакшене reinforcement learning? Да, могу. 

S03 [01:35:20]  : Я работал тогда в агрегаторе такси и систему выбирал ценовой коэффициент. а начальник задавал метрику, которую нужно оптимизировать. То есть, например, мы хотим максимальное количество поездок или мы хотим максимальное количество поездок, но при этом, чтобы конверсия, то есть вероятность пассажира уехать была 80% и выше. Или там еще какие-нибудь ограничения. То есть, допустим, изначальная метрика максимальное количество поездок. Такое постановки, да, оно работало. Плюс у нас огромный датасет был. То есть вот все эти задачи reinforcement learning, вот в тестах он их решает с нуля, без датасета. Он датасет этот весь собирает. Соответственно 100 тысяч точек, я говорил, это 100 тысяч точек датасета система собирает в реальном времени. А у меня эти 100 тысяч точек уже были. 

S05 [01:36:18]  : Спасибо. И тогда последний вопрос. Вы видите, как вашу технологию выводить в продакшен из симуляционной среды? 

S03 [01:36:27]  : В первую очередь я эту технологию хотел показать по следующей причине. Потому что я видел достаточно много разговоров на тему того, что вот эта система искусственного интеллекта не общая, а вот эта не общая. Я хотел показать, как выглядит что-то отдаленно похожее на общую систему. То есть вот эталон общей системы сейчас – это reinforcement learning. Но то, что я показываю, это тоже, ну она близко к тому, чтобы быть общей, в смысле, что вот она много задач решает, нет других таких. То есть есть reinforcement learning, но в общем-то это все, больше таких нет. Вот. Как выводить в продакшн? Во-первых, некоторые продакшн среды все-таки можно моделировать. Во-вторых, можно делать несколько симуляций, которые похожи на реальную задачу. То есть, допустим, мы реальную задачу не можем моделировать точно, но можем сделать несколько плохих симуляций. И вот можно пустить этого агента в несколько вот этих плохих симуляций. То есть, если он обучится работать в нескольких неточных, то и в нашем мире тоже научится, скорее всего. Во всяком случае, это можно проверить. И следующий вариант это сделать быстрый RL внутри сети обучить с помощью вот этих модулирующих нейронов. Быстрый в смысле, который не требует 100 тысяч кадров на то, чтобы обучиться. 

S05 [01:38:06]  : Спасибо. Владимир, вы что-то хотели еще спросить или прокомментировать? 

S01 [01:38:11]  : Да, я хотел на два вопроса. Значит, я первый задам, пока вы не будете думать, я задам второй. Первый состоит в том, что Reinforced Nutrient, как вы правильно говорите на своем слайде, это обучение с подкреплением. То есть, если я полностью согласен с тем, что рассказывал Сергей Александрович Терехов, о том, что мы оптимизируем некоторый вектор. Есть, собственно, обучение с подкреплением, а есть с учителем. Причем и подкрепление, и учитель – это не обязательно человек. Это может быть и среда выступать в роли этого учителя. Важно, что в обучении с учителем указывается, какой должен быть правильный вектор, а в обучении с подкреплением просто дается некоторая оценка – хорошо или плохо. Причем она может быть как градуальная, так и, собственно, бинарная. Это уже, кстати, тонкости задач. Вот вы нам рассказали, что у вас другое обучение, нейрофосфункционное. Если вспомнить из того, что я рассказал, если вы нам... вы укажете что-то третье, то есть есть, конечно, всякие разные стратегии обучения, соревновательные схемы, много чего-то другого, но, в принципе, исходя из векторов, вариантов два. Вы говорите, что у вас какой-то третий. Попробуйте сформулировать в чем, или все-таки честно скажите, Вы сравнивали с какой-то программой, которая названа рефорсным кленом, и вот у вас что-то удалось лучше. Причем вы не пробовали в одинаковых условиях. Смотрите, вам просто так кажется. Это первое. Второе, пока вы думаете ответ. Я хотел задать второй вопрос. Борис Новиков ошибочно говорит о том, что надо пробовать на простых примерах, и если получится хорошо, то давайте там, где получилось хорошо, распространим на большой пример. Это глубокое заблуждение, которое широко распространено в машинной линейке. То есть если у нас простая задача, то самый надежный способ ее решения, эффективный, это полный перебор. там мы точно найдем оптимальное решение и, соответственно, задачу решить. Но на большие задачи это никак не распространяется. И, соответственно, опыт решения маленьких задач без анализа их масштабируемости, он, соответственно, ни о чем не говорит. Вот были ли у вас какие-то попытки эту масштабируемость анализировать? 

S03 [01:40:18]  : Так, во-первых, насчет reinforcement learning. Существует задача reinforcement learning, то есть задача оптимального управления. ну оптимального наведения на сигнал награды. 

S01 [01:40:30]  : Вы еще раз говорите, вот простой термин, давайте не будем усложнять задачу. Стоит в том, что у нас есть вектор, который нужно найти оптимальное значение. Управление, выигры в го, что угодно. Но здесь есть вектор параметров, который мы оптимизируем. Есть два способа. Либо мы указываем, какой он должен быть, и учим систему, чтобы она именно такой выдавала на такой вход. либо мы говорим о том, что мы даем системе оценку, она уже сама находит, какой вектор обеспечивает лучшую оценку. у вас какой-то отличный от этого способ или нет? 

S06 [01:41:03]  : Сергей, я знаете, вот еще секундочку вам дам, пока вот вмешаюсь, извините, Володь, я добавить хочу. Дело в том, что вы используете понятие термина обучение с подкреплением в каком-то очень конкретном смысле, как способ решения некоторого уравнения, а-ля уравнения Билбана. И только вот это называете ресурсным тлюнем. Если это так, то тогда действительно вы от решения этого уравнения отличаетесь. Если же вы будете использовать понятие ресурсным тлюнем в общепринятом смысле, а именно как Владимир его формулирует, то есть как способ получения неточной оценки того, что вы хотите получить. То есть это любой алгоритм, абсолютно любой, не связанный ни с Беллманом, ни с рекурсией, ни с нейронной сетью аппроксимации кофактора, ну ничего вообще. Любой алгоритм, у которого на выходе, выход оценивается не точно, а только вот с точки зрения вот его как бы поощрения. Вот это называется реформа смыклёнка. то тогда вот справедливый вопрос Владимира. то есть определись пожалуйста с определением, что вы называете словом reinforcement learning. 

S03 [01:42:08]  : когда я говорю про то, что моя система отличается от reinforcement learning, я имею ввиду, что она отличается от атрицы Dreamer, Decision Transformer. 

S01 [01:42:20]  : То, что она отличается, мы поняли. Расскажите, что вы понимаете под Reinforcement Learning, потому что то, что мы понимаем, это совсем другое. 

S03 [01:42:31]  : Существует сота в Reinforcement Learning. То, что я описываю, оно отличается от соты. подхода к решению. 

S01 [01:42:40]  : бывает только в каждой конкретной задачи. в reinforcement learning нет соты как класса. то есть при разных задачах reinforcement learning дает разные соты. 

S06 [01:42:54]  : терминологические вопросы. вопрос исключительно в терминологии. 

S03 [01:43:01]  : терминологические. существуют две разные вещи. это одна вещь это задача reinforcement learning, то есть задача оптимизации награды, другая это то, какие методы сейчас наиболее эффективны в этом. 

S06 [01:43:17]  : Задачи и методы не смешивайте, пожалуйста. Существует, вы сказали, два разных типа задач. Владимир только что назвал эти два разных типа задач. Первый тип задач, когда точно известен ответ, который нужно найти. Второй тип задач, когда он известен не точно, а в нечеткой форме дополнительно вот в виде подкрепления. Вот формулировка, которая принята в области машинного обучения. Но есть еще третий там, это там без учителя, но оставим его пока. Но и там и смешанные какие-то. Но вот эти вот два базовых, они Владимир сформулировал. 

S01 [01:43:49]  : Я их не упоминаю, поэтому я просто как запятая, в скобочках, забыли о них. 

S06 [01:43:52]  : Вот эти два принципа Владимир их сформулировал абсолютно четко. первый из них называется обучение с учителем, второй называется обучение reinforcement learning с подкреплением, так называемым, с reinforcement условно говоря. теперь я так подозреваю, судя по вашему рассказу и по словам, которые вы говорите, например, вы употребляете такое понятие уравнения reinforcement learning. это нонсенс. reinforcement learning это направление. В нем есть одна из девятнадцатых подзадач, называется рекуррент на уровне Билмана, записанная в форме там какой-то. Вот если вы еще раз понимаете решение вот того рекуррентного уравнения, да еще теми способами, которые описаны сейчас популярной литературе на архиве, то тогда да, тогда вы отличаетесь от него. И вот если вы подтвердите просто это, скажите, да, я имею ввиду, что я не так решаю задачу, как решают ее вот эти DQN, как ее решают вот эти вот системы, которые вы там назвали тогда. 

S03 [01:45:04]  : Можно я сейчас все-таки скажу, меня не будут перебивать хотя бы минуту-две, пожалуйста. Есть задачи reinforcement learning, это задачи оптимизации награды. Есть методы reinforcement learning, то есть это конкретные Вот на гитхабе можно скачать эти конкретные методы. Это DQM, например. Это Transformer Decision, это Dreamer. То есть вот то, что хорошо себя зарекомендовало, то, что сейчас работает, то, что тянет на соту. Нет единой соты для всех сред, но несколько алгоритмов можно назвать вместе, совместно сотой. И если посмотреть на эти алгоритмы, то у них есть общее свойство. Они все содержат в себе FitPredict. Все. FitPredict – это то самое обучение с учителем. Мой подход не содержит. Он решает задачу reinforcement learning, но не содержит методов FitPredict ни в каком виде. То есть он не решает задачу подбора функций по точкам. он решает задачу оптимизации в более… ну, короче, он решает другую задачу оптимизации. Так что то, что у меня, оно отличается от подходов reinforcement learning, который условно SOTA, но задача в целом та же. 

S06 [01:46:28]  : Понятно. 

S04 [01:46:30]  : Хорошо. Коллеги, еще есть какие-то вопросы или комментарии? Если только комментарии. Давайте короткие комментарии. 

S01 [01:46:43]  : На самом деле пожелать конечно строгости изложения и формулировок особенно Сергею хочется. Но в целом слушать его было очень приятно. И там не знаю дотягивает ли он до Демиса Хасабиса. лёгкости работы с менеджерами, то есть вот нам ему тяжеловато было объяснить, но я думаю, с менеджерами хорошо общаться, и это, конечно, очень важное и положительное качество. Вот, но вот такой вот есть, допустим, Роман Душкин, вот он примерно как бы вот вполне его уровня может достичь. Защищаться ему, конечно, тоже будет тяжело, поскольку строгость у него страдает. Но я рад, что он с нами общается и буду рад с ним общаться в дальнейшем, поскольку сама его жизненная активность и готовность решать сложные задачи, конечно, меня восхищает. Вот, собственно, что я хотел сказать. 

S04 [01:47:36]  : Коллеги, еще есть какие-то комментарии? 

S05 [01:47:41]  : Нет. Ну, тогда спасибо Сергею за доклад. У меня на самом деле тоже. У меня где-то по некоторым моментам я хотелось бы поспорить с Владимиром, а может быть даже и с Сергеем. Но с одной стороны. А с другой стороны, Сергей, вам хотелось бы тоже сказать, расплывчатость и некоторых формулировок она тоже вот создавала определенную трудность восприятия хотя конечно радует то что у вас богатый опыт и вы действительно свою технологию опробуете на большом количестве практических примеров но вот вопрос здесь действительно только так это главный такой насколько это действительно то что вот практические примеры которые вы показываете не является ли это тем самым случаем когда хорошо работает и полный перебор А масштабирование на большие объемы и на сложные задачи – это как раз тот самый случай, который вы обозначили, что в долгосрочной перспективе reinforcement learning лучше. То есть если очень грубо говоря, если ваша технология работает только там, где работает глобальный полный перебор, а во всех остальных случаях работает reinforcement learning, то получается, что предложенная технология, она но не является какой-то реальной альтернативой. Вот над этим я предлагаю подумать и, может быть, там как-то это дело исследовать и, может быть, поставить некоторые эксперименты на предмет масштабируемости и сопоставления действительно с полным перебором. Ну а в остальном спасибо, спасибо за проделанную работу и живые демонстрации. 

S06 [01:49:31]  : Спасибо большое за доклад. 

S04 [01:49:32]  : И всем спасибо за обсуждение, коллеги. 

S05 [01:49:36]  : Сергей, удачи вам на работе и в ваших собственных исследованиях. Всего доброго. 

S06 [01:49:42]  : До свидания. Всем счастливо. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
