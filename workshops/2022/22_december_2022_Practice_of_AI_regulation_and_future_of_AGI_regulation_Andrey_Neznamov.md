## 22 декабря - Практика регулирования ИИ и будущее регулирования AGI - Андрей Незнамов (кандидат юридических наук, управляющий директор Центра регулирования искусственного интеллекта ПАО Сбербанк) — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/TWK29yLOhKs/hqdefault.jpg)](https://youtu.be/TWK29yLOhKs)

Суммаризация семинара:

В ходе семинара были затронуты следующие ключевые моменты:

1. Юридические исследования и последствия создания AGI: Участники обсудили необходимость разработки юридических рамок для регулирования ИИ, включая вопросы собственности и ответственности. Было подчеркнуто, что для понимания последствий действия ИИ необходимо включить в исследование работы юристов, а также специалистов из смежных отраслей.

2. Различные подходы к регулированию ИИ: В семинаре были рассмотрены различные подходы к регулированию ИИ в разных странах, в частности, различия между западными и восточными странами. Было отмечено, что в России, несмотря на декларации и попытки регулирования, существуют значительные пробелы в законодательстве, особенно в области этики ИИ.

3. Собственность и авторство в ИИ: В дискуссии были затронуты вопросы собственности и авторства в контексте ИИ. Участники обсудили, что продукция, созданная с помощью ИИ, может принадлежать владельцу технической системы, на которую установлена нейросеть, а не саму нейросеть. Также было подчеркнуто, что проверка результатов ИИ важнее, чем сам ИИ.

4. Роль и будущее AGI: В семинаре были рассмотрены вопросы, связанные с AGI, включая возможность использования ИИ на благо всего человечества, а также потенциальные риски и опасности, связанные с созданием сверхразума.

5. Вклад юристов и гуманитариев: Участники подчеркнули важность вклада юристов и гуманитариев в регулирование ИИ, поскольку они могут увидеть аспекты, которые не видны с технической стороны.

6. Перспективы и риски: В заключение семинара были затронуты вопросы перспектив и рисков, связанных с развитием ИИ и AGI, и возможность заглянуть за горизонт для понимания будущих вызовов.







S04 [00:00:04]  : Да, коллеги, всем здравствуйте. Мы сегодня приветствуем Андрея Незнамого у нас в гостях. Мы поговорим об этике, связанной и регулировании в области искусственного интеллекта. Последний раз мы говорили на эту тему с крупнейшим русскоязычным специалистом в этой области на территории США. с Романом Ямпольским в прошлом или позапрошлом году. А сегодня мы поговорим об этом с крупнейшим русскоязычным экспертом в области этики и регулирования искусственного интеллекта Андреем Незналовым, который скурирует эту тему в Сбербанке. Ну и мы приготовили ряд вопросов. Андрей, как вы предлагаете поступить? У вас там есть своя позиция, в какой вам удобно было бы по этим вопросам пройтись? Или мне пытаться? 

S03 [00:00:57]  : Может быть для удобства поддержания дискуссии просто вы можете их задавать. 

S04 [00:01:01]  : Да, давайте тогда в порядке интервью. Ну вот и на всякий случай напоминаю, регламент у нас такой, что через час нам бы хотелось нашего гостя отпустить, ну а потом желающие могут еще задержаться на дискуссию. Значит, первый вопрос, который мы обычно задаем всем в этом сообществе – как вы сами определяете AGI или сильный интеллект? И что это для вас? Можете прокомментировать. 

S03 [00:01:32]  : Ну, конечно, я много достаточно выступаю, поэтому вот это вот разделение между AGI и прикладным искусственным интеллектом — это базовое, про что я рассказываю всем, кому рассказываю в целом про регулирование новых технологий, зачем оно нужно и что это такое. И когда объясняешь людям на пальцах, что вот есть прикладной AI или слабый AI, и есть AGI, конечно, какое-то разделение у себя в голове проводишь. Мне очень тяжело с гуманитарным образованием вступать на тропу терминологии, что такое AI и что такое AGI. Поэтому я, во-первых, не претендую на то, что у меня есть там Какое-то универсальное определение, я сейчас скажу просто своими словами. И второе, это, наверное, касается всего моего интервью, я выражаю здесь свою позицию как ученого, который занимается исследованием темы регулирования и достаточно давно, а не то, чтобы это была официальная позиция Сбера. Хорошо, сразу это сверим, чтобы не было вопросов. Я обычно объясняю одним очень простым словом, что AGI – это сверхразум. Обычно это снимает вопрос и направляет людей на нужные ассоциации. Дальше можно уже уйти в детали. Большое количество литературы читал, где объяснялись разные подходы к тому, что такое AGI. Основная идея, как мне показалось, – это универсальный искусственный интеллект. который не является сильным только в одной какой-то сфере, а в значительной большей степени напоминает человеческий. Но мне кажется, что слово «сверхразум», используемое неоднократно во многих книгах, является достаточно метким. Хотя оно совершенно не точно, с технической точки зрения. Вот поэтому для меня AGI – это такое какое-то очень-очень сложное, далекое, абстрактное будущее, которое может быть, а может и нет. Это находится в руках не моих, а ваших, скорее, этот, сайентистов. Но дальше можно уходить в терминологические особенности. Должен ли он быть универсальный? Универсальный E там условно… когнитивно способный во всех сферах жизнедеятельности, должен ли он существенно превосходить человека и так далее. Но здесь, мне кажется, эта дискуссия, она бесконечно интересна, но настолько же она и бесконечно сложна. 

S04 [00:03:58]  : Спасибо. Ну и как вы относитесь к позиции, которую высказывают многие исследователи, а многие с ними достаточно аргументированно спорят, что об вот этом сверхразуме или AGI можно говорить как о некотором цивилизационном преемнике человечества, который понесет Геокод человеческой цивилизации к звездам. 

S03 [00:04:29]  : Ну, здесь во мне борются два начала. С одной стороны, моя профессия – это регулирование. А регулирование посвящено сложившимся общественным отношениям. И в этом плане любой юрист очень практичен. С этой точки зрения говорить про регулирование EGI мега сложно. Право мало того, что достаточно инертно, повторюсь, оно может оперировать с существующим предметом. Я думаю, что в данном случае, как юрист, я бы сказал, что это что-то не совсем реальное, неприменимое к сегодняшнему дню. Если пофантазировать, пофантазировать, посмотреть, какие-то прогнозы визионерские, то вполне возможно все может быть. Мне, знаете, нравится приводить такой пример. В конце XX века, по-моему, появился термин «киборгизация», и все, кто смотрел фильм «Робокоп», всегда ассоциируют киборгов, как правило, с этим полицейским. И если вдаться в определение киборга, то зависимость от определенного технического устройства, имплантируемого или нет, она является одним из таких доминирующих характеристик возможного этого определения. По-моему, нет легального определения у него нигде, но тем не менее. И в 21 веке с изобретением смартфонов мы в значительной степени, если вдуматься, стали такими зависимыми людьми. Мало кто сейчас ездит без навигаторов, не использует поиск, не пользуется видеосвязью, мессенджерами. И я сейчас не говорю про социально-психологические аспекты привязанности, очень много интересных исследований есть. Я говорю просто про нашу с вами обутинную жизнь, как она сложилась сама собой. Мы в значительной степени опираемся на смартфон и на другие достижения и информационного общества и технологического процесса. В этом плане мы отличаемся от людей, которые жили 100 лет назад. Мне кажется, да, отличаемся мы от людей, которые жили тысячу лет назад. Безусловно, отличаемся. Здесь можно говорить не только про то, что мы какой-то там приблудой прикольной пользуемся каждый день. Можно говорить еще и про то, что у нас мышление изменилось. И мышление, оно сегодня, ну согласитесь, существенно отличается от, значит, от той логики, от той парадигмы, в которой мыслили, например, сто лет назад. Я вот хотел, наверное, мне нужно было сразу, Антон, уточнить, у нас аудитория какая? Скорее у нас ученые, специалисты в этой сфере, или скорее, наоборот, такая общая? 

S04 [00:07:19]  : У нас, скорее, так. Специалисты в этой сфере, но специалисты с разной точки зрения. То есть у нас есть как дейто-сайентисты, И, собственно, люди, которые больше из науки и серьезной разработки именно про AGI, а не просто про Machine Learning. То есть у нас есть люди, для которых AI – это программа на питоне, а есть люди, для которых AI – это программа в PowerPoint. Знаете, эту шутку, наверное. Вот здесь есть и те, и те. Соответственно, можно на эту аудиторию ориентироваться. 

S03 [00:07:56]  : Это очень сложно, потому что для восключающих аудитории одно я насмешу своими определениями искусственного интеллекта, своими подходами, а другое, может быть, изъяснюсь не очень понятно. Окей. Ну, я попробую какой-то... золотой компромисс найти, но раз возник вопрос о том, является ли AGI или вообще в целом и некой следующей вехой, то я тогда, может быть, раз у нас аудитория в целом, мне кажется, более применительна к науке, чуть более глубоко копну, не на примере, там, телефончиков. Давайте вот вспомним, есть такой очень интересный рассказ, уверен, те из вас, кто занимается наукой, неоднократно его читал, про изменение способов мышления, вообще про изменение самой парадигмы мышления, например, в античной Греции. Было такое замечательное, даже не знаю, как это назвать, исследование или эссе от Петрова к Пентекантера в первом классе Европейской школы мысли. И в этой Пентекантере как раз описывается в этом, не знаю, как это назвать, исследование в небольшом, описывается как просто изобретение, ну если совсем по-простому, определенного способа мореплавания, определенного корабля изменило и сформировало мышление античного грека. Ну это такой предмет, который исследуют на очень глубоком уровне философы, в частности. Но вот применимы ли они сегодня? Ну конечно применимы. И повторюсь, мышление современного человека, я в этом глубоко убежден, сильно отличается от предыдущих поколений людей. Я могу судить практически, у меня есть трое маленьких детей, я вижу, как они рассуждают, в какой среде они существуют, я вижу общую среду, в которой работаем мы. Так получилось, что у меня очень много командировок по России, по стране, я имею возможность по странице поставить. Я вижу, что очень сильно у нас начинается стратегизироваться, стратегизироваться различные подходы к жизни, к мышлению. Почему я такой длинный сделал, такую длинную вводную? Потому что вопрос, будет ли AGI следующим поколением нас, он для меня может быть актуальным, если мы будем рассматривать пирамидку, шаг за шагом движение человека от какой-то точки к какой-то. То есть это не тот процесс, когда вот сегодня мы с вами Антон, а завтра AGI. Я думаю, что в какой-то момент тому поколению людей, которые будут жить максимально близко к этой точке, вообще не будет стоять этот вопрос как революционный. Он скажет, что больше и меньше, как говорится. Но в целом по себе, конечно, эти размышления, они достаточно всемерны, повторюсь, потому что я, к сожалению или к счастью, небольшой специалист именно в AGI. Мне, повторюсь, как юристу, безумно интересно смотреть попытки различных ученых предложить подходы к регулированию этой сферы, но я понимаю, что мы здесь мега-зависимы от тех, кто делает AGI, как они его определяют, какие методы они используют. Именно эти люди, как мне кажется, лучше всего видят, как этот AGI будет складываться и как он скажется на нашей жизни. Но гуманитарии все равно здесь нужны, потому что не все видно с технической стороны. 

S04 [00:11:03]  : Андрей, спасибо. Но тогда, собственно, с точки зрения того, что мы будем регулировать, почему и зачем, какие основные риски, вы видите, могут возникнуть здесь и сейчас или на следующем шаге в жизни наших детей в связи с развитием или с появлением того, что мы можем назвать AGI? Мы пытаемся заглянуть за некоторый горизонт, здесь может быть имеет смысл вообще задать вопрос, а вообще возможно ли это с вашей точки зрения, то есть может быть этот сверхразум возможен на столько далекой точке, что об этом не имеет смысла говорить сейчас. То есть, если это так, то тогда можно просто сменить немножко тему. А если вы допускаете, что это может возникнуть там, скажем, для наших детей или хотя бы для наших внуков, то, наверное, об этом уже надо думать. Соответственно, вот о чем мы будем говорить тогда, давайте определимся. 

S03 [00:12:06]  : Смотрите, юристы работают с рисками. Когда люди подписывают договор, они вроде договорились дружить, а юристы там уже прописывают раздел, посвященный разрешению споров и ответственности. Это что значит? Что мы прогнозируем различные варианты развития ситуации и придумываем определенные выходы из тех или иных спорных вопросов. С этой точки зрения вообще неважно, что я думаю как юрист. о том, будет ли создана AGI или нет, замечательными разработчиками AI. Я просто обязан исходить из того, что существует сценарий, когда он будет создан. Кроме того, мне кажется, юристы обязаны исходить из того, что существует сценарий, когда он будет создан в разной точке, в разных, ну, короче, там возникает много всяких развилок. Поэтому, конечно, да, с этой точки зрения мы в каком-то из сценариев исходим, что AGI может быть создан, значит, у него будут риски. Какие эти риски? Ну, очень интересно было читать мне книгу Ника Бострова, потому что на сегодняшний день именно эта работа до сих пор, кстати говоря, она, по-моему, Я даже не помню какого года, оно сильно больше 5 лет, 7. По-моему, первое издание, даже я не помню, 2010 или 2012. В общем, книга Ника Бострома про риски искусственного интеллекта, мне кажется, долгое время оставалась и до сих пор остается, наверное, ключевой работой. Неудивительно, что под ее влиянием в значительной степени находился Иван Маск, и находится, кажется, до сих пор, когда формулировал свои отношения к сильному искусственному интеллекту. И могу сказать, что мое впечатление от этой очень большой книги было в том, что Bostrom не видит сценариев, когда AGI будет для нас дружественным, видит в этом огромное количество риска. Ну, здесь, конечно, может быть, не все, как сказать, не все Не всё можно поддержать, ко многим вещам мы относимся критично, но в целом вот есть такая точка зрения, что AGI будет едва ли не последним изобретением человечества, даже книжка такая есть, с таким же названием тоже. И это, наверное, тот риск, с которым мы, прежде всего, должны работать. Люди боятся, люди видят, что ИДЖАИ может быть недорожественным нам. Значит, как юристы мы на эти риски тоже смотрим. Я не говорю, что они реальны, я говорю, что их люди выделяют. Существует риск того, что AGI будет непрозрачной, не будет контролироваться, будет очень превратно понимать задачи. Но я простыми словами говорю. Вот недавно попалась на глазах статья, где как раз объяснялось, что от неправильной постановки цели может произойти очень-очень много неприятных последствий. Там были разные варианты, как неправильно можно постановить. ставить цель неправильно понять. Это тоже все, безусловно, риски. Самые такие лайтовые исследователи говорят, что ну да, EJI будет нам помощником, но мы многие лишимся работы. Это риск. Безусловно, этот риск тоже выделяется. Такую карту рисков мы можем составить для AGI и, может быть, даже по ней посмотреть. Но я хочу сделать небольшой шаг в сторону и сказать, что мы, в общем-то, для любого явления, для любой новой технологии можем составить карту риска, и там будут и совсем черные риски, которые будут носить экзистенциальный характер. Поэтому мне очень важно, чтобы отношение к AGI не было таким, что это вот нечто эксклюзивное, все вокруг остальное, значит, оно вообще безрисковое и никаких проблем не несет. Вот нельзя выхватывать из контекста. Так же, как вот AGI, мне кажется, не может существовать вне информационного общества, так как и риски, которые он несет, они неотрывно связаны со всеми другими технологиями, которые, может быть, не атрибутивны непосредственно искусственному интеллекту. 

S04 [00:15:50]  : Спасибо. Ну, поскольку вы представляете, ну, может быть, вы здесь представляете свою позицию, точку зрения, но, тем не менее, вы, так сказать, связаны со Сбербанком, вот, непосредственно, и, значит, вопрос, который часто возникает, ну, не только в связи со Сбербанком, на Западе аналогичные вопросы задаются в связи с теми корпорациями, которые двигают эту тематику на Западе. Есть такая тема, что крупные компании, которые имеют ресурсы вроде Сбера у нас, Google, Facebook на западе, они имеют возможность вкладывать ресурсы в разработку, имеют вычислительные ресурсы. но они будут направлять эти ресурсы на благо своих акционеров, вместо того, чтобы направлять это на благо общества в частности и человечества вообще. Как вы смотрите на такую проблему или на такую точку зрения? 

S03 [00:16:55]  : Ну, конечно, мне не просто отвечать на этот вопрос, потому что я действительно сотрудник СБЕР, но поскольку сейчас я озвучиваю исключительно свою личную точку зрения, я могу, конечно, наверное, себе позволить личное свое отношение озвучить. Вы знаете, мне кажется, что такая постановка вопроса, она, конечно, может иметь место. В принципе, понятно, все открыты для дискуссий. Но для человека, который работал в больших компаниях или корпорациях, я думаю, ответить на этот вопрос несколько проще. Потому что такое некое благо акционеров или руководителей, Это достаточно эфемерная вещь. В больших компаниях работают десятки, сотни, в некоторых случаях, по-моему, и миллионы людей. И, скажем так, обслуживают они, ну, помогают. Общество взаимодействует с еще большим количеством людей. Поэтому говорить, что все это падает в карман какого-то одного капиталиста, простите, пожалуйста, это очень напоминает весьма упрощенное представление об экономических процессах. Весьма упрощенное. Поэтому я считаю, что вот такие вопросы, они очень хороши, например, для каких-то таких верхнеуровенных дискуссий, но на самом деле они некорректны в том плане, что нужно понимать экономические процессы, ту роль, в которой в них сейчас играют большие компании в мире, И с этой точки зрения, мне кажется, вопрос, что все упадет в карман руководителю Гугла, мне кажется, уже в значительной степени отпал. Мы понимаем, что личное богатство одного из учредителей, оно, во-первых, сейчас наносит эфемерный характер. Сколько твои акции торгуются на бирже, это значит, что у тебя эти деньги лежат в кармане. А второе, повторюсь, это достаточно упрощенное представление о том, как работают экономические процессы. В целом, мне кажется, что AGI, безусловно, может быть использован на благо, и не на благо одного человека, а на благо всего человечества. Так же, как он может быть использован, наверное, и не во благо. Это характерно для любого инструмента. которые создает человек. Это характерно вообще для поведения любого человека. 

S04 [00:19:11]  : Спасибо, Андрей. Теперь, раз уж мы поговорили о благе, давайте затронем еще более больную тему, особенно в нынешних реалиях. В течение ряда лет активно обсуждалась тема, что ученые, разработчики каким-то образом должны препятствовать милитаризации искусственного интеллекта, избежать появления автономных систем. как это, Little Autonomous Weapon System, то есть, которые самостоятельно принимают решения на поражение целей. Но вот сейчас такое ощущение в связи с международной повесткой, что про это все либо забыли, либо замолчали. И лучшие умы мира начинают потихонечку решать совсем другие задачи. Ну или, по крайней мере, То есть, каким образом вы видите сейчас ситуацию с точки зрения как политик, как юрист, и, может быть, даже поглядеть на нее с глазами разработчика в условиях, когда война становится все более и более компьютеризированной, и искусственный интеллект имеет очень хорошие шансы на применение для того, чтобы обеспечить преимущество той или другой стороны? 

S03 [00:20:33]  : Спасибо за вопрос, достаточно долго его озвучивали, но я уже сразу понял, что, к сожалению, не смогу на него ответить. Я совсем ничего не понимаю военным AI, вот правда совсем ничего не понимаю. Мне очень сложно прокомментировать применимость этой темы. Я знаю, как юрист, что на протяжении нескольких лет на ООН велась дискуссия о том, является ли, можно ли применить некоторых военных роботов, по-моему, я там тоже был, к так называемому неконвенциональному оружию, но я достаточно давно не слежу за тем, до чего они в итоге договорились, и дальнейших комментариев мне, правда, дать будет очень сложно, ну и к тому же, как вы понимаете, действительно сейчас ситуация достаточно... Мне будет сложно дать какую-то... 

S04 [00:21:15]  : Ну а вот тогда можно попытаться, если на мою гипотезу высказать, то есть в моё понимание, здесь я в своё время даже давал комментарии в этом, в РСМД, где мы с вами являемся экспертами, что в нынешних условиях Нужно, безусловно, решать вопросы о запрете автономных робототехнических систем, но решать это можно исключительно на основе каких-то более глобальных договоренностей о контроле над вооружениями вообще и над разрядкой напряженности в целом. То есть, это можно делать только, что называется во фреймворке, каких-то глобальных договоренностей о взаимной безопасности и уже на основе этого можно будет как-то юридически ставить вопросы о конкретных технологиях, вроде химического, ядерного и других видов вооружений. 

S03 [00:22:11]  : Я могу сказать, что в целом человечество придумало глобальные конвенции для того, чтобы установить некие общепринятые правила жизни. Наверное, в целом конвенция – это хорошо. Хороша ли конвенциональность в определении правил жизни? А вот этих летальных вооружений, правда, не знаю, возможно ли она. Я тоже не знаю. Для этого нужно понимать, как работает этот механизм. В остальном это будет просто с моей стороны, как эксперт по регулированию, любой комментарий будет некорректным. Поэтому я ничего не смог здесь добавить. 

S04 [00:22:42]  : Хорошо, спасибо. Тогда давайте поговорим о хорошем. Есть такая точка зрения, что для того, чтобы искусственный интеллект был более безопасным, нужно его максимально демократизировать. В частности, есть точка зрения, что нужно по максимуму обеспечить доступ ко всем разработкам, то есть обязательно все это должно быть в открытом коде, все модели должны быть открыты для публики и те компании, которые имеют средства на разработку этой технологии, они должны делать это максимально открытым, а не так, как это делает, к примеру, тот же самый Google, который закрывает доступ к своей семантической базе данных после того, как он ее несколько лет краудсорсил. Сбербанк, как я понимаю, публикует свои модели в открытом доступе, и это хорошо. Но вот как вы относитесь вообще к идее демократизации искусственного интеллекта? Это же идет за счет тех компаний, которые в него деньги вкладывают. 

S03 [00:23:44]  : Прошу прощения, есть разница между словом демократизация и, я его, честно говоря, впервые слышу, но, допустим, это какой-то коннотация есть, и открытостью кода и моделей. Это две разные вещи. И я могу вторую часть прокомментировать, потому что идея, что нужно код открывать и допускать к нему внешних консультантов или вообще выкладывать в паблик, Я неоднократно встречал, когда работал в сети Европы, когда мы помогали готовить позицию страны по документу в ЮНЕСКО и еще в наряде других международных форумов, мне часто этот вопрос задавали про открытость кода, и сейчас это вообще тренд, что нужно, давайте обсуждать, как будем выкладывать. И вы знаете, я, конечно, повторюсь, не делаю модели AI, и мне сложно прокомментировать то, что существуют площадки вроде GitHub, где все делятся открытым кодом, но я смотрю на AI как одну из технологий. Есть такое ощущение иногда, что мы смотрим на AI, определяем определенные требования, но давайте будем честны. Слово «демократизация» подразумевает, что мы все-таки достаточно трезво, одинаково относимся Мне кажется, ко всем технологиям новым. У нас что, в любой новой технологии мы требуем абсолютной прозрачности, вообще в любой технологии, вообще в любом аспекте нашей жизни мы требуем, например, чтобы для общества обязательно все выкладывалось, рассказывалось, показывалось и было доступно. Вот если с этой точки зрения посмотреть, не делать из ИАИ некую икону или суперопасное оружие, а посмотреть спокойно, объективно, в контексте разных других явлений, которые в нашей жизни происходят, неизбежно возникнет вопрос, почему именно к ИАИ мы предъявляем требования которых нет, например, в других сферах. Это первое замечание. Второе замечание. Хотел бы я, чтобы в открытый доступ был выложен код, например, тех продуктов, которые я использую в своем телефоне, приложений банков, объявлений. Хотел бы я, чтобы это сделали? Я бы подумал. Потому что код этот можно использовать в разных целях. Третье замечание. Для чего такое требование можно было бы ввести? Вот это было предметом одним из самых жарких дискуссий, в том числе среди Европы, в специальных группах. Для чего это делается? Самый частый ответ – ну, чтобы общество знало, что все безопасно. Мне хочется сказать, come on! Серьезно, если сейчас в паблик будут выложены чертежи какой-нибудь атомной станции, что общество от этого станет безопаснее? Да нифига! Вы простите, пожалуйста, за грубость, но общество скорее от этого станет менее безопасным, потому что специалистов, которые способны понять, что там написано, очень мало. Если сейчас мы будем заставлять всех несчастных разработчиков AI депонировать свой код в паблике, прямо заставлять это делать, ну, что мы добьемся? Люди будут в большинстве своем, я согласен с коллегами из Яндекса, люди просто будут говорить, мы не делаем AI, мы просто делаем компьютерную программу. У нас же нет требования выкладывать открытый код всех компьютерных программ, правда? Ну, нет такого требования. А к AI почему-то хотят предъявить. Вот эти замечания, конечно, они в сумме у меня складываются в том, что я идею вот этого выкладывания чего-то в открытый доступ, обязательного, пока не могу понять для себя. Но я догадываюсь, откуда найдёт. Она идёт, исходя из того, что люди очень боятся AI. Люди боятся AI не из-за того, что он страшный, не из-за того, что разработчики делают какие-то чудовищные вещи, а потому что само представление людей об AI – это уже как раз моя сфера. Предсамо представление людей об искусственном интеллекте складывается не из научных составляющих, оно складывается из массовой культуры. Вот слово «робот» появилось в книжке Карла Чапика, «AI», оно, конечно, слово не появилось из массовой культуры, но распространялось, формировало представление людей именно кинематографическая индустрия, книжки, комиксы, мультики, ну, понимаете, да, игры компьютерные. И вот исходя из этого представления AI у людей, в том числе тех, которые инициируют определённое регулирование, есть представление об искусственном интеллекте как о нечто очень опасном, непредсказуемом и так далее. И мне в этом плане всегда хочется обратить внимание, что существует, очень часто существует серьёзный разрыв между тем, что у вас в голове, и тем, что происходит в реальности. И если этот разрыв преодолеть, может показаться, что, может быть, и не надо нам в обязательном порядке заставлять разработчиков искусственного интеллекта выкладывать их код в открытый доступ. Может быть, есть сферы, где это нужно. Отдельные сферы. Я с этим, пожалуй, даже соглашусь. Наверняка найдутся ситуации, когда мы придем к тому, чтобы Ту или иную разработку каким-то образом проверяли на внешний аудит, может быть, даже и публичное общество. Сложно пока сантизировать, где и как это будет. Не исключаю, что это будет. Это один из инструментов регулирования. Но идея повального выкладывания в доступ всей своей кухни... У нас, по-моему, ни в одной индустрии это не действует. Поправьте меня, если это не так. Но почему мы именно в AI к этому идем, я понимаю, но, значит, я должен поддерживать это. 

S04 [00:29:09]  : Андрей, спасибо. Давайте тогда перейдем дальше к теме вашей книги и непосредственно вашей работы, насколько я понимаю. Есть разные, насколько я понимаю, подходы, если я не прав, тоже поправьте меня, в регулировании искусственного интеллекта в разных странах. И верно ли утверждение, что, в общем, западные страны трактуют эту тему немножко по-другому, чем восточные страны? И так ли это или не так, и в чём разница, и в конечном итоге, как вы видите место России вот на этом раскладе? 

S03 [00:29:49]  : У меня на эту тему действительно есть несколько работ. По-моему, 7 у нас коллективных монографий даже есть. Крайняя вышла действительно в этом году, потому что мы отслеживаем остановление и регулирование примерно с 2017 года. Могу сказать, что действительно в «Абсолютном право» подходы существенно разнятся. Это правда. Европейский подход, он очень характерен, он является в каком-то смысле квинтэссенцией работы Совета Европы. Права человека, безопасность, верховенство законное, и демократия – это вот те лозунги, которые являются основным девизом работы Совета Европы. И под этим подходом подразумевается то, что регулирование носит такой достаточно сложный характер для развития многих технологий. Все тщательно там выверяется, проверяется, и лишний раз что-то не пустится на рынок, и какой-то эксперимент не подержит. В этом плане из Европы удивительным образом выделяются отдельные страны, потому что не все склонны к достаточно жесткому регулированию. И вот такой же достаточно консервативный подход порождает очень любопытные регуляторные новеллы, которые потом расходятся по всему миру. Один из примеров таких регуляторных новелл Это уже популярный теперь во многих странах мира режим регуляторных песочниц, который изначально появился в праве Англии достаточно консервативно, как понимаете, страны, в том числе и с точки зрения финансов. как как раз своеобразный способ экспериментов с теми или иными технологиями. И то есть вот здесь такое интересное сочетание, с одной стороны, ну, правда, консервативный подход, но, с другой стороны, в условиях этого консервативного подхода поиск каких-то новых возможностей. Очень любопытно в этом плане посмотреть на судьбу регулирования в сфере данных, потому что GDPR, этот европейский акт, является притчей во языцах, Вот. И я видел несколько исследований европейских, которые оценивали GDPR с точки зрения его влияния на искусственный интеллект. И хотя там немножечко некоторая ангажированность прослеживается, мне кажется, вывод в этих исследованиях был такой, что GDPR практически убил AI в Европе. Повторюсь, я считаю, что немножко там, конечно, есть в некоторых вещах преувеличение. Это независимо какая там консалтинговая компания исследовала, но не слишком известная. И, тем не менее, сам факт, что такие исследования проводятся, значит, наглядно показывает, что не все однозначно позитивно оценивают влияние GDPR на различные те или иные индустрии. И пару лет назад как раз Сами власти ЕС признали, что для большего соответствия этой политике инновациям нужно определённые изменения в GDPR внести, обсуждаются идеи дата посредников и так далее. То есть мы видим, что в Европе очень характерный пример. Сначала достаточно жёсткое регулирование, затем из него начинаются искать какие-то такие своеобразные выходы, характерные для того, чтобы можно было ту или иную технологию развивать. Это, конечно, не характерная история для Китая и США, потому что в США, здесь, я думаю, любой юрист легко на этот вопрос бы ответил и без темы EI, в США англосаксонская система права, кстати говоря, как и в Англии. Но в США там вообще очень ярко выражена сила прецедентов судебных, плюс еще гремучая смесь федерального законодательства и законодательства штатов, и получается очень причудливая схема. Например, по Стэнфордскому индексу февральскому этого года в США лидеры по количеству принятых актов в сфере EI. Но эти акты достаточно специфические, раз. Во-вторых, там в некоторых штатах есть жесткозапретительное регулирование, а в некоторых вообще как бы делайте более-менее что хотите. То есть в США в этом плане очень нехарактерная юрисдикция, по ней вообще сложно сделать выводы. Китай – тоже очень любопытный пример, потому что, во-первых, у них специфическое законодательство, они еще не все его переводят на английский язык, это первое, а второе, там очень серьезное значение имеет администрирование, и третье, там тоже есть законодательство по регионам, И четвертое – законодательство. Там очень важно смотреть с позиции его правоприменения, что совсем сложно. Почему? Вот последнее важное. В Китае приняли закон в том числе о персональных данных. Там был еще закон о кибербезопасности. И вроде как он очень напоминает GDPR. Но при этом я разговаривал с несколькими практиками, отслеживая, как он реализуется. Многие говорят, что на самом деле он совсем не так исполняется, как исполняется европейское законодательство. Кроме того, очень важно вчитываться в текст закона, а это не всегда бывает очевидно. Вы, наверное, слышали, что в Китае были инициированы целый ряд законодательных предложений по дипфейкам. Меня спрашивали несколько раз, и СМИ тоже, как я могу это прокомментировать. Я могу сказать, что мне для того, чтобы понять, что реально там предложили, нужно посмотреть внимательно на текст сакона, значительно внимательно, и посмотреть на его правоприменение. В Китае это очень важно. Это, в принципе, везде важно, но, как никогда, это важно в Китае. И вот эти три центра силы, США, Европа и Китай, каждый идут достаточно, как мне кажется, своеобразным путем. И возникает вопрос, а где здесь находится Россия? Россия, как бы не пошло это звучало, идет каким-то своим особенным путем. Вот хоть убейте, я не могу подобрать к этому другого слова. У нас традиционно очень близко законодательство континентальной германской системе права, то есть мы вроде как заимствуем многие европейские институты. Ну, окей. При этом у нас есть специфика с точки зрения правоприменения, близкая к Китаю. У нас нет такой же ситуации, как в Америке, когда очень сильно региональное законодательство. Но, как и в США, в некоторых случаях большое… Ну, не такое, конечно, большое, но некоторые значения имеют отдельные судебные решения. Я как человек, который 15 лет работал в судах, хорошо знаю, что то-то или иное разъяснение пленума Верховного суда, оно может иметь значение. Позвонили, меня выбило. Слышно меня? 

S04 [00:36:38]  : Да-да-да. 

S03 [00:36:40]  : оно может иметь значительно большее значение, чем даже та статья, которую ты читаешь в законе. И поэтому у нас, конечно, сейчас формируется свой трек с точки зрения регулирования искусственного интеллекта. Этот трек достаточно интересный, потому что у нас правительство первое в мире приняло, имею в виду на уровне правительства, концепцию, как должен регулироваться искусственный интеллект. И вот пока там концепция до 2024 года, мы более-менее последовательно по этой концепции движемся. Но жизнь часто вносит свои коррективы, я думаю, что неизбежно какие-то отходы от этой концепции будут. 

S04 [00:37:18]  : Спасибо. Мы дошли до конца моих вопросов, но вот здесь есть еще вопросы от коллег. Собственно, по правопрорегулированию. Каким образом, по вашему мнению, в настоящее время должна Должна регулироваться эксплуатация недо-AGI, например, TeslaBot или более простых автономных универсальных роботов-помощников в домашнем быту или садово-агородном хозяйстве, которые могут появиться в самое ближайшее время, Виктор Казаринов спрашивает. 

S03 [00:37:50]  : Знаете, есть такая особенность у AI. Он вроде как может использоваться в разной сфере жизни. И знаете, какая история? У нас в каждой сфере нашей с вами жизни вообще-то уже есть сложившееся регулирование. Это на плане мне было очень интересно. идти диалог с европейцами, которые напридумывали, особенно по инициативе Европарламента, множество законодательных новелл, сначала для роботов в 17-м году. Они с 12-го по 17-й год занимались темой роботов, потом ИАЭМ. И они напридумывали кучу новелл, которые отлично попадают в средства массовой информации, но затем начинаются поставлять новеллы со сложившимся регулированием, и во многих случаях отдают назад, говорят, что, ребята, у нас тут как бы и так все, в общем-то, неплохо регулируется. Поэтому я хотел бы здесь к этому вопросу как раз эту ремарку отнести. Вы знаете, я, может быть, не очень хорошо понимаю, как будут работать эти Tesla-боты по помощнике в хозяйстве, но я хорошо понимаю, что в России есть потребительское законодательство и законодательство о безопасности определенного рода продуктов. Оно складывается из множества самых разных актов, но оно есть. Есть ответственность гражданская, административная и уголовная за оказание услуг или продажу товара несоответствующего качества, особенно если это привлечет определенные последствия. И у нас есть суды, которые более-менее даже в отсутствии закона разбираются с кейсами. У нас нет такого раньше, что я сказал, о, закона нет, я закрыл дело. Никогда такого не было, клянусь вам. Поэтому скажу вам так. Я думаю, что если будет какая-то особенная специфика об этих отношениях, то, скорее всего, мы об этом узнаем после каких-то отдельных кейсов. Но если сильно повезет, мы попробуем заранее построить эту специфику и внести какие-то корректировки в закон. в современном цивилизованном обществе, законодательство, которое регламентирует достаточно подробно все сферы практически жизни, оно позволяет нам использовать почти любые достижения, кроме отдельных сфер. Я приведу несколько примеров. Вы не можете использовать искусственный интеллект здравоохранения, пока у вас не появится регулирование, которая объяснит, как проводить клинические, технические испытания, короче, как выдавать регистрационные удостоверения, как проверять их безопасность. Вот нет у вас их и не будет у вас в медицине, потому что в медицине четко сказано, без регистрационного удостоверения в медизделии не используйте. У вас не поедет беспилотник на дороге, если у вас правила дорожного движения, у вас не будет прописано, на каких условиях беспилотник может ехать, у вас не будет прописано, может ли человек отвлекаться от управления машиной. Вот в таких ярко выраженных примерах вам без очевидного регулирования не обойтись. А в применении какого-то бытового сервиса, услуги у вас, в принципе, регулирование, как правило, уже складывается. Именно поэтому ни в одной стране мира, например, я не видел явно выраженного закона о, например, румных колонках. Понимаете? Но потому что, как правило, их действие и так подчиняется законодательству самому разному, в том числе законодательству о конфиденциальности и честной жизни. 

S04 [00:41:01]  : Спасибо. Ну и вот раз уж заговорили о запретах и разрешениях, следующий вопрос от Виктора Казаринова. Следующий вопрос. Вы говорили про то, что опасно выкладывать, например, с карты атомных станций, потому что они могут быть, очевидно, использованы ненадлежащим образом. Вот. И у меня там был следующий вопрос, а не повлечет ли публикация всех открытых кодов доступ к технологиям и я и террористам? Вот. И очевидно, риски такие есть. Ну а здесь Виктор Казаринов спрашивает. Как вы считаете, есть ли вероятность того, что разработки в области HDI могут запретить простым смертным ввиду больших рисков и потенциальной опасности, как это в области оружия, микробиологии и прочего подобного? 

S03 [00:41:52]  : Я думаю, что нет. Вообще слово «простым смертным» меня очень сильно смущает. Мне кажется, что это тоже такое некое упрощение. 

S04 [00:42:00]  : Как оружие. Пистолет можно купить в оружейном магазине, только если у тебя справка от нарколога есть, например. 

S03 [00:42:07]  : Ну, это правда, но отличие искусственный интеллект, как я понимаю, от оружия или от создания ядерной бомбы в том, что вы искусственный интеллект делаете всё-таки в виде кода. Не будем же отходить от того, что искусственный интеллект в традиционном понимании, но это всё-таки определённые алгоритмы, это не совсем по-другому. компьютерная программа или программа для ВМ, как у нас есть такой термин в законодательстве. И вам, мне кажется, будет очень сложно запретить создание в целом искусственного интеллекта. Хотя, если мы говорим про AGI, то, конечно, такие перспективы неоднократно обсуждаются, что будет запрещено пытаться создавать что-то, что мы не можем контролировать и так далее. Но, повторюсь, пока мне это кажется теоретическим разговором достаточно эпистолярного жанра. Гугл может делать, а я у себя дома не могу его делать. Юристы, мы очень практичные люди, нам нужно понять, кто что как делает, мы сразу предложим формы, способы контроля и ответственности. 

S02 [00:43:20]  : Минутку, я извиняюсь, здравствуйте, я автор как раз этого вопроса. Ну вот компьютерные вирусы, это же программы, их можно налево и направо писать, какие угодно, насколько опасные. 

S03 [00:43:33]  : Вы знаете, вы, наверное, можете создавать какие-то компьютерные программы. Дальше будет применяться законодательство конкретной стороны, которая, например, запрещает вам вообще создавать такие программы или, например, запрещает их распространять. Есть же между этим разница? Скажу честно, я давненько не практикую, я не помню, какой сейчас у нас в России действует режим регулирования вирусов. Запрещается ли у нас именно создание программы? По-моему, запрещается. Но, согласитесь, здесь же все-таки важно, что если вы сейчас, Виктор или кто-то, вот Антон, я сяду, сейчас создам какую-то программу, которая окажется, например, вирусом. Такое же может быть. Что я здесь должен сделать? Сам себе по рукам ударить, особенно если этого не знаю. В юриспруденции очень важны в любых составах правонарушений его элементы, одним из которых является наличие умысла или неумысла. Чтобы разбирать такие кейсы, нам очень важно будет понять, с какой юрисдикцией мы работаем и с чего складывается создание AGI. Правда, мне сейчас очень сложно понять, как выглядит процесс создания AGI. Повторюсь, потому что я не дата-сайентист. Отличается ли он от того, что делают ребята у меня на этаже здесь, где я работаю в рабочее время, или не отличается, то есть от того, что вы делаете, коллеги? Когда нам, юристам, объяснят эту особенность, мы, наверное, сможем ответить на этот вопрос более внятно. 

S04 [00:45:08]  : Спасибо. Еще есть два вопроса, которые, мне кажется, сейчас становятся актуальными. Кейс первый – это когда программа создает произведения искусства. А мы знаем из истории про Миджорни, что программы создают произведения искусства, которые занимают первые места. Первый вопрос. Если программу, к примеру, тренировали на произведениях Ван Гога, И она начинает рисовать картины в стиле Ван Гога. Вообще имеют ли право наследники, правоприемники Ван Гога подавать в суд на либо программу, либо того, кто эту программу хостит на своих серверах, провайдерах которые распространяют информацию да то есть если как бы предмет для иска да и кто будет ответчиком это один вопрос а второй вопрос другой что если я да как человек значит начинаю распространять продукцию как это называется интеллектуальную собственность да то есть искусство по предмету искусства это объект интеллектуальной собственности и Если эти объекты интеллектуальной собственности созданы пусть не на основе произведения других художников, пусть эксперт не может определить, что это подделка под Ван Гога или подделка под Рубенса, пусть это уникальное произведение, но кто является, так сказать, собственником этого произведения, за кем закрепляется интеллектуальная собственность? За владельцем программы? за владельцем оборудования за тем кто программу написал или за программой самой вот как вот эти два вопроса вы видите могут быть решены вопрос для 

S03 [00:46:54]  : Потому что когда мы говорим про ИАИ, так абстрактно, про его регулирование, самые частые вопросы, значит, кто понесёт ответственность и кому принадлежат права на объекты, которые ИАИ создаёт. Всемирная организация интеллектуальной собственности, ВАИС, 4 или 5 лет уже ведёт как раз исследование юрисдикции, например, того, какой правовой режим нужно сформировать для таких созданных искусственным интеллектом объектов. Смотрите. Ответ здесь такой. Можно ли подавать иск? Иск можно подавать всегда. Право граждан на судебную защиту более-менее гарантировано в каждой стране мира. Какие будут последствия и кто выиграет? Вопрос, наверное, был в этом все-таки. Вопрос с позиции текущего российского права. Как-то не странно. Четвертая часть кодекса определяет те объекты, которые подлежат защите, и как художественный стиль. А когда вы говорите про Ван Гога, мы говорим про художественный стиль все-таки. Это очень спорное понятие. Если я правильно помню, позиции авторского права, сам стиль не охраняется. И близкий к этому, это моё мнение, повторюсь, близкий к этому – это споры вокруг персонажей произведений. или они охраноспособными, и можно ли использовать этих персонажей или нет. Даже этот вопрос, казалось бы, для многих из нас, наверное, очевидных. Образ Чебурашки, Кота Матроски. Этот очевидный вопрос не всегда был очевидным. Но вот Ван Гог не лучший пример, потому что там охрана авторских прав. У нас авторские права, друзья, не охраняются бесконечно. Возьмите лучше какого-нибудь современного художника или писателя, интеллект изучит все его произведения и создаст такое же. Я гарантирую, что нам предстоит увидеться в ближайшие годы и, может быть, даже в России, потому что с распространением генеративных нейросетей обязательно возникнет вопрос 20 книжек потом взял 21 написала я и без моего согласия да еще и в моем и это спор будет весьма не фактических обстоятельств дела будет соотноситься там везде значит потому что раз защиту хотя мы все связано там более-менее условно бернской конвенции но все равно есть понимание что есть существо у нас даже был такой эксперимент в мире он их они юристов точнее выдавали иски в разные страны мира с требованием признать изобретательство, правоавторство, за искусственным интеллектом и посмотреть, какой будет результат в той или иной стране. И могу вам сказать, что эти споры, безусловно, будут очень… Я понимаю, в какую сторону будут доводы защиты истца и ответчика, но в целом могу сказать так, с позицием… чужого стиля, чужого образа будет индивидуальным. Нужно будет смотреть, насколько там действительно охрана способна. Ну, охрана способна или не охрана способна, не признают ли его пародии. Вы извините, я сейчас говорю как юрист, но я говорю как юридический момент. Что называется, если по-человечески, мы будем оценивать, ну, действительно, насколько здесь сколько здесь называются личные имущественные права автора. Исходя из этого, будем принимать решение, может быть, корректировать регулирование. Но вот вопрос, кому принадлежат права, он, мне кажется, в этом плане значительно проще. Почему? Потому что, когда кто-то использует нейросеть для создания произведения, как правило, он подчиняется тем условиям использования нейросети, которые опубликованы ее создателями. И вот что там указано, прочитайте эти условия и, в общем, там более-менее все будет понятно. Часто задают вопрос, типа, а я же научил нейросеть. Вот это же она по моему запросу создала эту картинку, значит, я автор. Слушайте, это как бы можно бесконечно, конечно, спорить, уходить в нюансы, что является творчеством. Мы, юристы, это можем бесконечно делать, клянусь вам. Кто создал нейросеть, тот определит определенные… будет в большинстве случаев отталкиваться от этого. 

S04 [00:51:58]  : Спасибо. 

S03 [00:52:00]  : У меня прерывается, извините. 

S04 [00:52:07]  : Андрей, спасибо. Последний вопрос от Игоря Пивоварова. В случае AGI. Появляется этика самого AGI, потому что это будет самостоятельный субъект, принимающий решения. Какой будет этот объект этика? Как регулировать создание такого AGI? Как общество обеспечить, чтобы работа по созданию AGI, особенно в крупных корпорациях, велась так, чтобы у него была нормальная человеческая этика? И может ли быть, кстати, такая общечеловеческая этика вообще? Так, Андрей, звук отключился. Простите, я так слышно? Да-да-да, сейчас слышно. 

S03 [00:52:58]  : Так, вопрос. Слышно меня, да? 

S04 [00:53:03]  : Да-да-да, слышно, ага. 

S03 [00:53:08]  : в том, что сейчас всегда за всем и так стоит человек, и этика – это инструмент регулирования отношений между людьми прежде всего. Появится ли своя этика у AGI? Мы начали с того, что зависим в определении AGI, в его создании. Если мы фантазируем и называем разум, который, возможно, никогда не будет изобретен, то мы можем здесь провести своеобразную аналогию с юридическими... юридических лиц. Этика. Наверное, она есть, корпоративная этика. Из чего она складывается? Из отношений людей, из того, что люди заложили в эту этику. Что будет у Эджай? Нам очень часто хочется думать, что Эджай будет обладать некой своей этикой, которую он сам выдумывает. То есть, как правило, Это первый пример, который вам может прийти на ум всем. Нам всем всё значительно сложнее. Честно, мне кажется, всё значительно сложнее. Скажем так, будучи созданным искусственно, любая искусственная сущность, она… заимствует, которая есть у людей. Ну так происходит, понимаете? И я думаю, что какой-то отдельной этики AGI, наверное, пока мы говорить не будем, но с точки зрения поисследовать этот вопрос, где-то перейти на поря фантазии, Юристам нужно. Вот честно, Антон, мы с вами обсуждали, что одним из проектов как раз исследовательских, которые можно было бы сделать, это более глубоко посмотреть на эту тему. Я вижу в этом не фантазёрский риск, я в этом вижу совершенно юридический риск. Сейчас регуляторы конкретных стран мира или, например, международных органов не смотрят на тему ЭДЖАИ как на серьёзную. Ну вот так вот приучили, собственно, сами учёные, что долгое время про ЭДЖАИ было говорить зазором. Ну хоть убей, я сам это видел. я не говорю что про него прямо по-прежнему уже утверждают что он будет завтра но И с юридической точки зрения лично мне очень не хватает как раз этих юридических исследований, последствий действия GI, в какие рамки он будет вписан. Будут ли это законодательные рамки? Будут ли это этические рамки? Будут ли это нормативно-технические рамки? Как их формулировать, особенно учитывая, что это явление не создано, а нам, судя по всему, ну, нужно как-то попытаться сделать? Вот это поди для самостоятельного исследования. И это исследование, мне кажется, должно включать в себя работы не только юристов, но и целого ряда, скажем так, смежных отраслей, чтобы посмотреть на проблему со всех сторон. Человечество, как мне оказывается, еще с таким вызовом, Не то чтобы сталкивалось, но при этом оно само по себе его поставило и само по себе уже надумало определенные риски теории. А вот жизнь может все расставить по своим местам. И вот эти все-все-все факторы нужно каким-то образом нам учесть. Поэтому вот меня часто спрашивают ребята, особенно те юристы, перед которыми я выступаю, молодые. чем вот там позаниматься. Говорю, ребят, поисследуйте большое количество рисков, которые на самом деле существуют и носят отчасти где-то эфемерный характер, а тот, кто сможет их приземлить, объяснить, что они реально практически воплотимы и показать, как практически воплотить регулирование, тот внесет огромный вклад в развитие вообще этого общества. Поэтому вот я бы здесь хотел поставить такое многоточие. 

S04 [00:56:44]  : Хорошо, Андрей. Огромное спасибо вам за уделенное нам время. Ждем, когда будет возможность еще раз к нам появиться. Успехов вам в вашей работе и до новых встреч. 

S03 [00:56:56]  : Спасибо вам за интересные вопросы, за интересную дискуссию. Понимаю, что где-то я отвечал достаточно где-то уж совсем уклончиво, но вы простите, пожалуйста, у юристов, мы вообще редко отвечаем коротко и по делу, как в известных анекдотах. Но я убежден, что дискуссии очень нужны, поэтому я с большим удовольствием принял приглашение выступить, потому что без обсуждений этой темы продвижение новых технологий, вообще прогресса, Мне кажется, в принципе, сейчас невозможно. Это раньше на словах говорили, что давайте все всесторонне обсуждать на дебатах, но при этом как бы все делали все по-своему. В современном информационном обществе голос каждого в реальных, это звучит как политический лозунг, но голос человека имеет значение, потому что каждый из нас вносит вклад в эту этику, даже просто используя определенным образом телефон. И с этой точки зрения, конечно, я с большим удовольствием всегда принимаю участие, мне было бы интересно. Дальше понаблюдать, как будет развиваться дискуссия, услышать самые разные точки зрения. Даже если они радикально отличаются от моей, это замечательно. Спасибо большое. 

S04 [00:57:59]  : Хорошо. Спасибо, Андрей. Мы попробуем еще продолжить дискуссию и потом, может быть, захотите посмотреть, как все закончится. Спасибо. Счастливо. коллеги есть предложение если есть желание значит немножечко еще продолжить дискуссию может быть у кого-то есть либо какие-то краткие комментарии до Либо какие-то вопросы, которые, мне кажется, кто-то, может быть, хотел бы услышать более развернутый ответ или дать свой собственный ответ на эти вопросы со своей стороны, то, пожалуйста, это можно сделать сейчас. Я думаю, давайте попробуем поднятием рук это делать. Да, вот вижу, Борис поднял руку. Борис, пожалуйста, вам слово. 

S06 [00:58:53]  : Добрый вечер. Сначала у меня вопрос, но и есть свой вариант ответа. Регулирование искусственного интеллекта – это регулирование чего? Области деятельности, продуктов или и того, и другого. Потому что искусственный интеллект, есть, ну, общепринятые смыслы, два смысла. Это как область действительности, типа физики. или как продукты с искусственным интеллектом. И вообще искусственный интеллект – это не система, это свойства системы. Ну, например, компьютер с искусственным интеллектом или робот с искусственным интеллектом и так далее. Так вот, дело представляется, что регулирование должно быть и того, и другого, но это надо разделять. И второе, я тут подготовил в основе своей статьи специфики настоящего исторического момента для выживания человечества и регулирования искусственного интеллекта. Экран можно показать? 

S04 [01:00:04]  : Давайте по порядку. Ответ на ваш вопрос. 

S06 [01:00:08]  : Коротко, основная тезис. Целесообразно различать регулирование искусственного интеллекта в связи с локальными или с экзистенциальными угрозами для всего человечества. Значит, локальные угрозы, типа неправильной диагностики автопилота автомобилей и так далее, это, безусловно, надо регулировать, но очень по-разному, в зависимости от предметной области. А вот есть экзистенциальные угрозы, Я её вижу в том, что потенциально системы с искусственным интеллектом и роботы могут создать псевдобиологический вид. Вот так вот в фантастике, что цивилизация роботов на не биологической основе, и этот псевдовид может вступить в видовую конкуренцию с человечеством. И поскольку компьютеры развиваются быстрее, эволюционируют, чем люди, то это может быть опасно. И отсюда нужно Но у псевдобиологического вида должно быть, по крайней мере разумного псевдобиологического вида, должно быть как минимум два свойства. Живость, как у биологического вида, и разумность. И живость оснащает возможность к самовоспроизведению, типа плодитесь и размножайтесь. Ну а разумность – это возможность приспосабливаться к окружающей среде. Так вот, их можно разделить. Разумность искусственного интеллекта нам нужна, и мы ей уже широко пользуемся, поручая ей решать систему с искусственным интеллектом, но со слабым. Мы поручаем решать различные задачи, и нам от этого жить лучше. А вот живость, чтобы системы с сильным искусственным интеллектом, а то и самопроизвольно размножаться и эволюционировать быстрее, чем люди, вот это экзистенциальная угроза. И важно и достаточно использовать разумность, но не допустить живости. То есть воспроизведение систем с сильным искусственным интеллектом, особенно вне виртуальной области, а в реальной области, offline, должно жестко контролироваться людьми и не быть самопроизвольным. Тогда можно будет избежать экзистенциальных угроз. Но в крайнем и самом крайнем случае, как в фантастических фильмах, будет необходимо и достаточно отключить электричество и интернет. если он будет только в виртуальном пространстве, а вот если ему будут подчиняться роботы, которые смогут самовоспроизводиться, то это уже не поможет. Ну и кратко, человеку и человечеству от искусственного интеллекта нужны инструменты, а не конкуренты. Поэтому перспективная область – это слабые искусственные интеллекты и сильный искусственный интеллект, как надстройка над ними, которая помогает эту систему слабых интеллектов использовать эффективно для нужд людей. суперискусственный интеллект, который будет иметь собственные интересы, нам не нужен и опасен. Спасибо. 

S04 [01:04:00]  : Борис, спасибо. Владимир Смолин, по-моему, вы хотите высказаться, да? Владимир? Так, Владимир, мы вас не слышим. Так, Владимир, вы что-то говорите? Так, Владимир, похоже, не выходит на связь. Коллеги, есть еще какие-то точки зрения? Так, Владимир, мы видим ваше лицо, но не слышим вашего голоса. Я, к сожалению, вижу поднятую руку. Да, Виктор, пожалуйста. Виктор Казаринов. 

S02 [01:05:02]  : Еще раз здравствуйте все. Ну вот хотел бы немножко ремарки по прошедшему. докладу и ответом на вопрос. Я немножко извиняюсь, что я вторгся в ответ докладчика. 

S04 [01:05:18]  : Нет, нормально, у нас же диалог. 

S02 [01:05:20]  : Перебил, но там, да, действительно, это был диалог и было как бы к месту. Но тем не менее, значит, мне кажется, что вопросы, которые обозначены были в заголовке доклада, то не полностью были раскрыты, потому что А вот по практике, особенно по российской практике, как-то не шибко там было в конкретике. То есть общие фразы, слишком общие фразы. И так как это представитель Сбербанка, то, наверное, человек, докладчик говорил, что большей частью это его личное мнение. Тем не менее, у меня осталось такое недоумение, довольно большое, о том, что в нашем российском законодательстве все равно большая дыра, несмотря на то, что вроде как собирались, даже про этику, какую-то декларацию или еще что-то, этику искусственного интеллекта в российском сообществе. Потом и регулирование беспилотных автомобилей, регулирование другой техники. уже как бы пододвигается это дело. Ну даже электросамокаты стали регулировать. Поэтому это не искусственный интеллект, это просто средство, которое повышено опасности. Так вот, мои-то вопросы были о чем? О том, что в ближайшем будущем средства автоматизации, которые в домашнем хозяйстве будут, это не электромясорубка, куда палец засунул и палец отрезало. Это более сложные вещи, которые могут быть неожиданно. Их поведение непредсказуемо будет. потому что она будет руководствоваться, такая система, своими внутренними автономными мотивами. И они не очевидны. То есть, допустим, локтем зацепится или повернет не туда, человеку на ногу наедет, там какая-то колесная штучка. Я все о своем, о девичьем, о простых вещах, которые могут наполнить наш быт в ближайшее время. И законодательство, я так чувствую, будет топтаться идти вслед, а не впереди. События, которые могут быть, какие-то травмы наноситься, какие-то недопонимания человека и машины. То есть вот эта человеко-машинная этика взаимодействия, в которой надо будет вливаться, правовые акты, когда придется и в суде разбираться, и еще как-то. Вот эти вопросы, сказал докладчик, что это будет пока регулируется вещами такими, как про бытовые машины, про персональные данные, но этих законов, этого регулирования явно недостаточно, потому что возникнут новые ситуации, которые неприменимы к этим вещам, потому что, говорю, это не какое-нибудь там бритва, электробритва или стиральная машина, которые ведут себя как конечные автоматы в большей части. И даже роботы-пылесосы, которые катаются, он наступил, подскользнулся на него. Они пока безобидные, но когда они будут более высокими, выполнять более сложные действия по команде хозяев, это может быть в ущерб другому человеку, который находится в этой же комнате. Допустим, один ребенок даст команду, а второму человеку может быть нанесен бред с помощью этого прибора внутри одной и той же комнаты. Как это регулировать? Пока я чувствую, что вопрос будет повышен в воздух. Потому что все особенности применения данного аппарата в инструкцию не вложить. То есть, как вот микроволновку. Не засовывать голову в микроволновку, там еще что-то, там толстые помуды пишут про обычный бытовой прибор. Здесь такое еще сложнее написать. И у меня вот такое вот замечание, что Мы столкнемся с очень серьезной проблемой в ближайшее время, если такие приборы начнут появляться. Наши или зарубежные. А кроме как защиты персональных данных, чтобы камеры не наблюдали. Да, вот сейчас вот роботы-пылесосы или другие вещи, они обладают сертификатами. Там есть специальные сертификаты, как я узнал, которые гарантируют, что ваши персональные данные не уплывут в облако куда-то. Ну, хотя сложно в них верить. Вот такая ремарка. Спасибо. 

S04 [01:09:51]  : Спасибо. Была еще рука у Виктора Носко. Виктор, пожалуйста. А потом Владимир Фролов, пожалуйста. 

S00 [01:10:03]  : Да, здравствуйте. Добрый вечер, Виктор. Так как я с Андреем немножко перебрасывался словами на конференцию OpenTalks в этом году, в феврале, я к нему подошел и, собственно, про блокчейн спросил. Я вот сегодня не слышал. Был ли вообще вопрос про блокчейн? Отвечал ли что-то Андрей? 

S04 [01:10:23]  : Я смотрю, я задавал вопрос более простой. У меня был вопрос про блокчейн, но поскольку у докладчика было немного времени и были другие вопросы и на предыдущий вопрос он ответил. Я не стал этот вопрос задавать, я пришел к выводу, что его не имеет смысла задавать. 

S00 [01:10:50]  : Ну да, хорошо. 

S04 [01:10:51]  : В общем, две маленькие... Давайте, может быть, я просто, так сказать, прокомментирую, чтобы, раз уж вы спросили, значит, там, значит, был какой вопрос, сейчас я скажу. Я задавал вопрос по поводу демократизации, да, вот, что... нужно ли демократизировать искусственный интеллект, потому что когда говорят про блокчейн в связи с искусственным интеллектом, обычно это говорится в связи с его демократизацией, что искусственный интеллект, что все должно быть на блокчейне, что все должно быть открыто, а две позиции, которые Андрей обозначил, что, во-первых, требовать открытости просто нельзя, да? Ну, во-первых, потому что почему всё должно быть открыто, да? Почему люди должны делиться своей интеллектуальной собственностью, да? Во-вторых, некоторые вещи нельзя делать открытыми, потому что люди не имеют права ими делить, имеют право... ими не делиться, а потому что просто делиться этими вещами будет опасно, да, потому что эти вещи могут быть на поле использованы во вред всему человечеству или каким-то группам лиц. И в-третьих, и в-третьих, и в-третьих, ну вот да, вот как бы два основных аргумента. Соответственно, Ну, блокчейн – это же просто способ хранения информации, соответственно, какая разница, делаем ли мы, разрешаем или запрещаем распространение искусственного интеллекта не на блокчейне или на блокчейне, какая разница. Мне кажется, ответ был бы такой, поэтому, может быть, вы с этого места прокомментируете. 

S00 [01:12:33]  : Да, тогда у меня три ремарки. Ну, в общем, две. Я бы начал с того, что когда будет внедряться ИИ. Мне кажется, что вот Виктор Казарин до этого говорил про опасности, то, что у нас роботы там будут, мы с ним будем сталкиваться и так далее. Я бы сказал, что эти опасности очень маленькие. По сравнению с теми, и в том числе опасности AGI, что он там захватит мир и так далее, все понятно. Эти опасности в СМИ очень хорошо и много обсуждаются. Я сейчас не об этом. Мне кажется, намного раньше возникнет опасность другого рода, другого класса. Помните, на конференции AGI в 2020 году? Или в прошлом году спросили у президента, было спрошено это голосом синтезированным, может ли АИ заменить президента. Ну в чем как бы смысл, да, то есть это была как шутка подана, но действительно госструктуры могут и должны быть заменены алгоритмами. Потому что многие вот эти споры по поводу правильного устройства экономики, должна ли быть она с невидимой рукой рынка, или она должна быть вот такая регламентированная, как в Китае, когда, то есть, ну не либеральная, условно, да, то есть, когда вы ограничиваете рынок, и есть там различные экономисты, там типа Хазина, я думаю, многие смотрят, они за это ратуют, да, что у нас, мы должны закрыться, закрыть рынки, контролировать наши какие-то ключевые отрасли, развивать ИИ, развивать микроэлектронику и так далее. Соответственно, первыми в социальное волнение внесут вклад проблемы, связанные с заменой людей, чиновников высокого ранга, которые находятся на больших должностях, замены их алгоритмами. Условно, там некоторые части министерства будут заменяться алгоритмами. Эта же ситуация пчелы против меда. то есть люди не захотят уходить, да, они будут говорить, они будут бороться, это будут лудиты, они будут бороться и говорить, что ваши алгоритмы неправильные, нехорошие, да мы тут люди, все лучше понимаем, мы все разберемся и так далее и тому подобное. Значит, первое, у общества должен быть механизм прозрачный, как им голосовать за эти правки, да, то есть что могут люди мы противопоставить тому, что Мы хотели бы неэффективных людей заменить на эффективных. Что мы можем противопоставить? Мы можем противопоставить что-то типа прозрачного голосования. Здесь нам блокчейн уже нужен. Это первый пойнт, потому что у нас есть проблема. Нам же скажут, что голосование накручено. Согласны? То есть, как только общество скажет, что оно хочет заменить алгоритмом какого-то человека, система будет сопротивляться. Это, я думаю, всем понятно, что система всегда ратует за то, чтобы она оставалась целостной и нерушимой. И чтобы сдвинуть систему, ее видоизменить, должна быть некая другая система, которая ее, собственно, обладает силой. Но для того, чтобы обладать обществу силой, нужно обладать прозрачностью. То есть здесь вот такое голосование какое-то, какие-то петиции, вот что-то такое у нас должно быть здесь, блокчейн. Вторая роль блокчейна, она в том заключается, вот то, что написал в комментарии, заключается в том, что не идет роли сейчас, по крайней мере, о том, чтобы код или модель выкладывать в доступ. Я говорил бы о том, что хотя бы бенчмарки выкладывайте, о том, что биосов нет в моделях. То есть я бы хотел понимать, как банки принимают решение о выдаче кредита, хотя бы, знаете, Пусть это будет закрытая группа дата-сайентистов, которые проверяют адекватность модели на непредвзятость по различным признакам. Потому что кто знает, вдруг банки принимают предвзятое решение по региону, где находится этот регион, в восточной, в западной части и так далее, по многим-многим признакам. Национальностей много у нас в России. Откуда мы знаем, что банки действительно честны? Мы не знаем этого. Нам это просто в СМИ говорят, мы не знаем. То есть речь о чем? Должна быть группа специалистов независимых из различных отраслей, которые в закрытом режиме проверят, что эти модели так работают, получат бенчмарки. Эти бенчмарки информация об этих бенчмарках выкладывается на блокчейн, все люди могут смотреть, они не могут смотреть модели, они не могут их получить, скачать и использовать, но они могут смотреть бенчмарки. И, соответственно, поскольку блокчейн обеспечивает невозможность изменения этого задним числом, Мы как общество получаем большую стабильность, потому что мы знаем, что нас не обманывают, а запрос на справедливость в обществе очень большой, поэтому нельзя допускать как бы нестабильности в обществе, то есть любое государство, любое там правительство за стабильность. Как эту стабильность обеспечить? Ну вот так обеспечить, то есть тогда действительно как бы но не первого класса проблема не будет но не возникнет так остро как она могла быть сейчас но не второго вот себя я преодолел мне кажется технические трудности если меня слышно перья вас не слышу там там звук выключен 

S04 [01:18:00]  : Да, сейчас, пожалуйста, Владимир Смолин, потом Владимир Фролов, потом Виктор Казаринов. 

S05 [01:18:06]  : Хорошо. Ну, значит, во-первых, приятно было послушать специалистов из Сбербанка, которые, ну, с другой стороны это смотрят. То есть, конечно, он понимает, что вот в Сбербанке там какая-то серьезная работа ведет, а мы тут любители, ну, мягко говоря, ерундой занимаемся. 

S04 [01:18:19]  : Так, Владимир, давайте, пожалуйста, по существу. Это раз. 

S05 [01:18:25]  : Второе, что важно, что вот, собственно, и сейчас вот Виктор говорил о том, что у нас есть некоторое идеальное общество, в котором… Так, у Владимира проблемы со связью, поэтому Владимир Фролов, пожалуйста. 

S01 [01:18:41]  : Здравствуйте, спасибо, что… Я прерываюсь. 

S04 [01:18:47]  : Да, Владимир, Владимир, Владимир Смолин, извините, у вас какие-то проблемы со звуком, давайте сейчас, у нас уже Владимир Смолин, Фролов начал говорить, потому что у вас перерывы какие-то были. Сейчас давайте Владимир закончит, потом снова к вам вернемся. Владимир, пожалуйста. 

S01 [01:19:03]  : Спасибо. Я извиняюсь, что я прервал человека. Спасибо большое, что даете мне высказаться. Я по поводу вопроса по поводу стиля, если натренирован на каком-то стиле. Я хотел бы, ну, собственно говоря, у меня два замечания, то есть хотел обратить внимание на две вещи. Что обычно это всё-таки тренируется на... попытки предсказать или статистику, или что-то существующее. Когда тренируется человек, он тренируется совсем из других, чаще всего из других соображений. Например, как его работа оценится на выставке, даже не на текущей выставке, а через 5 лет. А с искусственным интеллектом это всё-таки чаще всего повторение. Поэтому тут есть проблема, что творение искусственного интеллекта, если это не инструмент человека, а более-менее автономная вещь, это регрессивные. А то, что создаёт человек, это прогрессивные. То есть у человека больше шансов сделать шаг вперёд. Я очень много сейчас смотрю вот эти генерированные картинки, и сейчас мне уже не становится интересно, как оригинальной идеей, я понимаю, что это практически шум какой-то. В лучшем случае, если какой-то специалист будет из этого шума что-то выбирать толковое. Или человек будет использовать это как инструмент, то есть искусство будет создавать какие-то запросы, и именно в этих запросах будет какая-то интеллектуальность. Это первое замечание. Я обращаю внимание, что творение искусственного интеллекта, к сожалению, регрессивное, но это мое мнение. Второе замечание. Скажем, когда ГО тренировалось играть, это совсем немножко другая постановка вопросов. Во-первых, здесь нет авторства, разве что правила игры в ГО, которые не являются, наверное, авторским правом. Чтобы отделить котлеты от мух, это совсем другая область системы, играющая в ГО. Там есть возможность нагенерировать всякие варианты игры. К этому вопросу совершенно не относят. Это может быть как раз прогрессивным. Такие вот системы, которые играют в гов, шахматы. Это по вопросу я высказался по поводу авторских прав. Теперь мне бы хотелось, вот Борис Новиков говорил про опасность искусственного интеллекта, то есть совсем про другую область сказать. В чём мне видится разрешение вопроса опасности? Совершенно правильно говорится, что нельзя разрешать искусственному интеллекту бесконечно размножаться, но мне кажется, здесь вопрос в другом. Вопрос в подкреплении. Если человек делает подкрепление сам, и это жестко зашивается, не дается искусственному интеллекту менять, то это относительно безопасно. В биологических системах, которые сами распространяются, это подкрепление модифицируется, и в зависимости от того, насколько этот организм распространился, закрепляется та или иная степень правила подкрепления. Вот этого делать нельзя в искусственном интеллекте. Я вижу решение этой проблемы в том, чтобы жестко зашивать систему подкрепления. И тогда безопасности будет как бы на порядок меньше. Все, спасибо. 

S05 [01:22:58]  : Мне еще подождать? 

S04 [01:22:59]  : Да, пожалуйста, Владимир. Давайте, я надеюсь, что у вас не будет рассоединения. 

S05 [01:23:05]  : К сожалению, это не от меня зависит. Да, я тоже надеюсь. Значит, вот все вот эти, как бы, рассуждения, которые были сегодня, в том числе вот нашими авторами, которые, собственно, вот наш уважаемый гость из Сбербанка, я даже слушать не устал, вот, они, собственно, направлены на ловлю глох. То есть, понимаете, вся эта вагонеткология, вопрос того, что, значит, Чьи авторские права, если мы нарисовали картинку, они, собственно, значит, если Google решит выпускать роботов на улице или Elon Musk решит свои машины выпускать на дороге, то они там будут. И у нас нет никаких законов, которые мы могли, как бы мы могли на них воздействовать. То есть основная проблема состоит не в том, что вдруг прилетят инопланетяне, которые мы не знаем какие. и сделают, что захотят, а в том, что есть корпорации, государства, которые могут делать с нами все, что захотят, и мы никак не можем на них повлиять. Вот это является основной проблемой. А уже, значит, как регулировать вот этот искусственный интеллект, оно вторичное. Потому что, собственно, при всём моём хорошем отношении к членам этой группы, вряд ли кто-то в одиночку создаст сильный искусственный интеллект. Включая, я могу говорить, что у меня прекрасная теория, но все про меня скажут, что и я тоже не создал. Один. А если вдруг меня возьмёт Гугл, и Гугл решит, что давайте это всем продавать. Значит, нет никаких законов, которые бы остановили Google. И если там в Китае что-то произведет, нет никаких законов. То есть проблема не в том, что мы не знаем, что делать с искусственным интеллектом. Мы не знаем, что делать с государствами и крупными корпорациями, чтобы остановить их действия, которые и без всякого искусственного интеллекта направлены во вред человечеству. И можно приводить кучу примеров, там про наш басманный суд, который там самый басманный суд в мире. 

S04 [01:24:59]  : Владимир, давайте мы будем без политических выпадок. 

S05 [01:25:06]  : В государствах нашего мира, во всех, не только в России. И в Штатах, и в Англии, где там вроде бы нам нравятся суды, но сами же жители этих стран признают, что суды защищают права богатых. И когда юристы нам рассказывают, что сейчас вот у нас такие технологии, блокчейн, который все выровняет, значит блокчейн точно так же будет определяться. 

S04 [01:25:30]  : Владимир, извини, давайте не будем передергивать, юрист нам не рассказывал про блокчейн, про блокчейн рассказывал Виктор Носков. 

S05 [01:25:38]  : Сейчас нам рассказывали, да, это не юрист. Ну и блокчейн, они тоже достаточно романтично к этому относятся, что вот мы введем блокчейн и все станут равны. Это, мягко говоря, некоторое, скажем так, преувеличение. И проблема состоит не в том, что мы не можем управлять сильным искусственным интеллектом, который, уважаемый юрист сказал, что неизвестно, когда вы его создадите, а в том, что мы сейчас не можем влиять на как внутренние, так и внешние международные организации, страны и прочее. И это является более серьезной темой в юридическом области. Ну, и ей, собственно, никто не хочет заниматься, поскольку, как бы, вот так же... Ну, я, опять-таки, не буду приводить политических примеров. То есть, если вам платят деньги, за что? Что вы плохое про того, кто вам платит деньги, не говорите. Вот, собственно, общая ситуация. 

S04 [01:26:27]  : Хорошо. Спасибо, Владимир. И, Владимир Фролов, вы еще хотите что-то сказать? 

S01 [01:26:35]  : Нет, прошу прощения, я просто... Спасибо. 

S04 [01:26:37]  : Хорошо, тогда Виктор Казаринов, пожалуйста. 

S02 [01:26:43]  : Спасибо, что еще отдали слово. Опять несколько ремарок. Первое — это относительно цитирования или похожести произведения искусственного интеллекта на человеческий. Тут довольно все прозрачно. Люди воруют друг у друга музыки, там стихии и прочее. Уже это все и так ясно. Насколько цитирование похоже на копирование. И тут уже судится там в разных странах свое законодательство выработалось. Поэтому нового тут я не думаю, что что-то нужно. Это первое. Второе. Ремарка Виктора Носко. Мне кажется, что уплывание на блокчейн это немножко Притянутая за уши проблема для искусственного интеллекта. Давайте сначала разберемся с применением блокчейна хотя бы в выборах. Посмотрите, из-за чего происходят огромные потрясения во всем нашем мире. Потому что выборы считают невисправедливыми, неправильными, недоверяют. В любой стране, хоть где-то применили блокчейн к выборам. Вот примените, будет отличный пример для того, чтобы в искусственный интеллект это пошло. Вот где практика. Я не вижу этой практики, нет желания. Обычно блокчейн туда, где денежки, где банки. А теперь о банках. Когда наш советский союз попилили и выделили оттуда банковскую систему, то я считаю, что это плохо. Я, когда в 1991 году чуть ли не под пулями брал в банке деньги обычные для наших деятельностей, нашей предприятия, понял, что вот эту сферу не надо выделять из государства. Это кровеносная система нашей страны. Вот эти фальшивые ВИЗО чеченские и прочие пошли. И сейчас то же самое. Как много банков позакрывали. Поэтому я считаю, что в каждом банке должно быть хотя бы 51% государственный. Так же как железнодороги, Газпром. Те вещи, которые стратегически важны. Поэтому частный капитал в банковской сфере это все-таки зло. По моему личному мнению. И поэтому, когда я говорил о государственной машине, еще давно-давно на одном из семинаров, я как раз подразумевал, что должны исчезнуть налоговые инспекции, пенсионные фонды и так далее. Просто их функции должны выполнять программные агенты, действующие в единой общей среде. Но здесь же должны быть и казначейство, и банковская сфера. Поэтому доверие к такой системе будет формироваться немножко по другим принципам, чем взять, проверить какой-нибудь Сбербанк наполовину частный или какой-то другой или третий банк. Эта проблема может быть сама уйти со временем. Вот такое мое замечание. То есть по мере усиления автоматизации государственной деятельности с помощью искусственного интеллекта А вот как раз к вопросу о президенте. Президент может быть и человек, естественно, в ближайшее обозримое будущее, но помогать более квалифицированно все-таки должны автоматические помощники, потому что человеческий фактор и до сих пор является не очень совершенным, к сожалению. Спасибо. 

S04 [01:30:18]  : Владимир, спасибо. Виктор, извиняюсь. Коллеги, есть еще какие-то вопросы? Или подведем черту под нашей дискуссией? Если честно, у меня тоже возник один вопрос, когда говорилось про собственность того, что надо смотреть сезонное соглашение данной нейросети по поводу того, кто является собственником и, например, если нейросеть как код распространяется под лицензией MIT, к примеру, то собственником может быть кто угодно. Дальше возникает вопрос, а кто будет собственником, кого считать собственником. в результате тренировки этой нейросети на данных. То есть, если я взял какие-то данные и на них долго и упорно, используя свои вычислительные ресурсы, за которые я плачу деньги, как, например, Виктор Носко это делает, тренировал нейросеть. Кстати, вопрос, Виктор, к вам. Может быть, вы прокомментируете. Если вы взяли нейросетку под MIT лицензии или Creative Commons и потратили кучу денег на то, чтобы тренировать ее на свой домен. и начинаете зарабатывать деньги, то очевидно на вас распространяется обычное законодательство, где вы оказываете некоторую услугу, вы создаете некоторый продукт. В данном случае нейросеть вам досталась бесплатно по MIT лицензии. А вы просто создаете некоторый контент, используя эту нейросеть как бесплатный инструмент. Соответственно, в рамках действующего трудозаконодательства никаких вопросов к вам, как к собственникам произведенного контента, не может быть, сколь интеллектуальным бы ваш контент не казался. Правильно я понимаю? 

S00 [01:32:30]  : Это ко мне вопрос, да? Да, да. Я просто не помню содержимое MIT лицензии, но если там явно написано, ну как правило, на гитхабе можно прочитать, что доступно к коммерческому применению. Правильно? Там есть? Тогда да, конечно, никаких вопросов. Есть лицензия с требованием у системы Drupal, могут все загуглить, там требование такое, что у них огромное количество модулей, но все модули, которые расположены на этом сайте Drupal, Конечно, они сделаны независимыми разработчиками, они open-source, но у них требование, если вы берете этот модуль себе и модифицируете его, то вы потом обязаны его выложить тоже, не закрывая код, то есть под этой же самой лицензией. Не помню, как лицензия называется, но вот такая жесткая. условно, коммунистический такой подход к тому, чтобы система оставалась открытой. И там модель монетизации другая. То есть там люди зарабатывают, условно, не на продаже этих модулей, а на том, что они делают конечные продукты, типа сайты, и, в общем, на этом зарабатывают. То есть модель другая. 

S04 [01:33:38]  : Хорошо, спасибо, Виктор. Борис, пожалуйста, у вас есть комментарий? 

S06 [01:33:45]  : Есть комментарии по поводу собственности. 

S04 [01:33:51]  : Так, Борис, ещё раз, пожалуйста, я случайно кнопку нажал, звук вам отключил случайно. Отключился звук. Всё-всё-всё, сейчас слышим, пожалуйста. 

S06 [01:33:58]  : С моей точки зрения, продукт нейросети ничем не отличается от продукта станка-автомата. И нейросеть не отличается от программы для станка-автомата. Если мы на автомате производим, ну скажем, ботинки, то ботинки принадлежат владельцу станка автомата. А, соответственно, картины, лекарства, книги, тексты книг и так далее принадлежат владельцу техническая система, на которую установлена какая-то нейросеть. И более того, проверка. Нужно проверять не саму нейросеть, любую, а результаты, которые она выдает. Нам не важно, лекарство придумал, форму лекарства получена при помощи нейросети или при помощи лаборатории, в которой работают люди. Есть определенные правила проверки лекарств. И оно может пойти в применение вне зависимости от того, откуда получилась его формула. Откуда люди придумали лекарство, мы тоже не можем объяснить. Откуда люди придумали эту формулу. 

S04 [01:35:26]  : Спасибо, Борис. У меня тут есть пара комментариев, наверное, уже в завершение, если никаких больше комментариев не последует. Во-первых, я просто фиксирую, что, хотим мы это или нет, на сегодняшний день так называемое рабовладение, оно имеет место быть. То есть, каким бы интеллектуальным наш продукт ни был, он всегда будет находиться в собственности у человека. И поэтому, если в какой-то момент приблизится ситуация, когда мы вдруг создадим чудесным образом, пока непонятно каким создадим IGI, который начнет приближаться к человеческим возможностям, то тогда могут возникнуть вопросы все-таки, а насколько человек имеет право узурпировать права на произведенный этим самым HCI контент. Это один комментарий. Но можем только надеяться, что эта проблема будет в далеком будущем и нам не придется ее решать. А второй комментарий – это то, что пока Законодатели думают, как и какой искусственный интеллект запретить. Некоторые площадки уже запрещают это без помощи юристов. Например, вот пришла только что новость в ленту, что порно от нейросети отменяется. Kickstarter запретил сбор средств на нейросеть. которая генерировала эротику и контент 18+. Краудфандинговая платформа решила поддержать бастующих против работы и художников и вернула деньги всем вкладчикам, ну и так далее. Так что сообщество, наверное, может даже и без помощи юристов само ограничить распространение неправильного искусственного интеллекта или применение его в неправильных областях. Хорошо, коллеги, всем спасибо. Продолжим обсуждение в группе AGI Ethics. Ну и через неделю, почти в новогоднюю ночь, Борис Новиков нам расскажет свою историю и свою перспективу про построение AGI. Всем спасибо и до новых встреч! 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
