## 27 октября - Импульсные НС для AGI - Михаил Киселев — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/a1F35mNTrKg/hqdefault.jpg)](https://youtu.be/a1F35mNTrKg)

Суммаризация семинара:

ТЕМА
- Семинар посвящен импульсным нейронным сетям (ИНС) и их потенциалу для создания сильного искусственного интеллекта (АГИ).

СУТЬ
- Михаил Киселев, ведущий семинара, работает в трех организациях: Росатоме, лаборатории Касперского и ЧГУ, где занимается теорией и фундаментальными аспектами ИНС.
- Он подчеркивает важность сложности системы и её реализации для достижения сильного интеллекта, приводя примеры из биологии, где мозг лягушки с миллионом нейронов не способен на сильный интеллект.

ДЕТАЛИ
- Михаил Киселев рассматривает три ключевых аспекта ИНС: размер нейронной сети, подход к обучению и энергетическую экономичность.
- Он акцентирует на важности размера нейронной сети, сравнивая мозг человека с миллиардами нейронов и современные нейросети, которые не достигают необходимой структурной сложности.
- Второй аспект – это подход к обучению, Михаил Киселев говорит о локальном подходе к изменению весов с учетом активности и состояния нейрона.
- Третий аспект – энергетическая экономичность, достигаемая за счет event-based computations, где нейронная сеть активируется только в момент возникновения события, что позволяет снизить потребление энергии.

РЕЗУЛЬТАТЫ
- Семинар показывает перспективы ИНС для решения сложных задач, включая когнитивные и структурно сложные проблемы, которые требуют больших вычислительных ресурсов и параллельной обработки.
- Михаил Киселев намекает на потенциал ИНС для создания систем, способных к обучению и адаптации в реальном времени, подобно человеческому мозгу.
- Важно отметить, что семинар не ограничивается только ИНС, но и рассматривает другие подходы, такие как ВТА (взвешенные товарищи аномалий), которые позволяют использовать параллельный brute force для решения задач с большим числом возможных вариантов.






S01 [00:00:01]  : Да, трансляция наконец у нас работает. Ну и сегодня, благодаря настойчивости и счастливому стечению обстоятельств, у нас наконец в гостях Михаил Киселев, который нам расскажет про импульсные нейронные сети и могут ли они использоваться для создания сильного искусственного интеллекта и как. Михаил, пожалуйста. Да, здравствуйте. 

S02 [00:00:24]  : Я должен извинить, что это выступление не совсем обычное, видимо, для этого семинара времени. Просто я вынужден ездить взад-вперед. Работаю в нескольких организациях, не очень меняло просто пояс, поэтому вот такой перенос. Я сегодня расскажу о… и о том, какое они имеют отношение к нашему предмету, к сильному искусственному интеллекту. Какая у меня позиция вообще, где я всем этим занимаюсь? Ну, я работаю, на самом деле, в трех местах. Во-первых, я работаю в Росатоме, точнее, в дочерней структуре, сейчас начиная с Цифрум. Собственно, поэтому презентация, она такая под эгидой Росатома немного. этими вещами, скорее, там занимаемся. Кроме того, я работаю в лаборатории Касперского, но там мы занимаемся более прикладными штуками всякими, меньше отношения к АГИИ. Ну и, наконец, я руковожу лабораторией нейроработанного вычисления в ЧГУ, в Чеховском государственном университете, где мы, собственно, тоже занимаемся теорией и более фундаментальными аспектами. Итак, сегодня у нас речь пойдет про импульсные нейронные сети и, собственно говоря, какое они имеют отношение к созданию сильного искусственного интеллекта. Сразу в качестве преамбулы и вводной мысли, которая сделает понятным смысл дальнейшей презентации. Я расскажу про свой взгляд на создание искусственного интеллекта, сильного. На мой взгляд, он заключается в том, что на данный момент, разумеется, будут разные этапы его создания. Я считаю, что до этого дела еще не скоро дойдет. И на каждом из этапов этой дороги будут важны свои аспекты, свои темы. Вот на том этапе, на котором мы сейчас находимся, по моему мнению, важнейшей темой является база. База вычислительная, база алгоритмическая, база структурная. То есть пока еще мы находимся на уровне кирпичиков. Чтобы создать здание, надо, чтобы кирпичики были прочными, удобными, чтобы в них можно было делать что-то большое. Вот как раз об этих кирпичиках и пойдет речь, об этой базе. Мое выступление будет состоять как бы из двух частей. Из софтварной, так сказать, части и теоретической части, где я буду говорить о том, как мне кажется, что может являться такой базой в смысле теоретической, идейной. А во второй части пойдет больше речь про аппаратную базу. Это будет о том, на чем, собственно, это будет делаться. Видно, да, слайды переключаются, все хорошо. Ну вот, собственно говоря, в историческом контексте всего этого искусственного интеллекта, как он развивался. Четыре фазы, всем они достаточно видны, ясны. Первый фаз первого наивного моделирования нейронов. Созданы только что компьютеры недавно. Давайте мы на них сделаем нечто похожее типа того, как в мозге, и получим нечто хорошее. Соответственно, первые нейроны – это всем известный перцептрон и все классические работы. Здорово, но на биологию, как выяснилось, совершенно не похоже. И тогда даже и развитие этой темы особо не получило, потому что еще машины были слишком слабые, чтобы на них можно было что-то большое делать из нейронной сети. Как-то это разочаровало всех немного. Все сказали, давайте вообще не будем ничего имитировать, а просто будем делать искусственный интеллект как с нуля, как мы мыслим его себе, так и будем делать, не имитируя естественный интеллект. Ну и тогда, поскольку, значит, кажется, что интеллект – это, в первую очередь, предприятие решения, это какие-то полномерные действия и все прочее, то есть кажется, что основой для этого должен быть логический вывод. Ну, по-моему, это не только логический вывод, это индукция, индуктивные методы. Потом добавились еще методы data mining, которые тоже, можно сказать, нечто обобщающее. Но это должны быть некие формальные системы, и на их основе можно весь интеллект выразить, и он нам будет все делать. Это была эра экспертных систем. И они, в общем-то, уже достигли некоторых результатов, т.е. секретно с ним применялись в жизни, в народном хозяйстве, для диагностики медицинской, для прочих вещей. И, в общем-то, был некий успех. Но тоже стало понятно, что все это дело не создать самим, надо, чтобы он развивался сам этот интеллект, его не закодируешь как данность, поэтому экспертная система здесь тоже нам помочь не мог. Ну и тут как раз возник еще один кризис, он был решен тем, что обнаружились способы сделать нейронные сети очень большими, и при этом там не иметь проблему с переобучением и там со всеми другими делами. Вот возникла эта замечательная идея, связанная с свершившимися целями, с глубинным обучением. Ну и, собственно, мы имеем сейчас вот то, что мы имеем. Это прогресс нейросетевого подхода, где нейронные сети буквально во всем и везде. Кажется, что они могут все. И, в общем, у многих есть идеи про то, что, собственно говоря, вот, практически это уже Сильный искусственный интеллект – надо только чуть-чуть поднапрячься, увеличить всё раз в сто, и всё получится. Начнётся и писание стихов, и размышления о смысле жизни, и всё остальное. В чём хотим от сильного интеллекта? На самом деле тут тоже имеется некий кризис. Видно, что движение это замедлилось. Вначале был огромный прогресс. я потом сейчас чуть позже скажу в силу чего она замедлилась, в силу чего мне вот этот подход кажется тоже не соответствующим, неправильным. и тогда вот родилась идея как бы нейронных сетей нового поколения уже совсем реалистически, где был за основу взят бионический принцип. Тут мы все-таки хотим более-менее имитировать то, что в голове, то, что в мозге. Вот у нас мозг прекрасно функционирует, давайте сделаем нечто похожее, и это будет тоже здорово функционировать. Ну вот, собственно, об этом самом Вот чем он объясняется, в чем он выражается в современных нейросетях. Для нас интеллект искусственный, прикладной в том числе, не только большой, это как бы две вещи. Это малый интеллект, это много всяких девайсов, мелких-мелких, автономных, которые всюду и везде вокруг нас и выполняют интеллектуальные функции в телефонах, в камерах, в каких-то, не знаю, интернет-вещей, умная кожа, ну чего угодно. Ну и, с другой стороны, это какие-то огромные системы вычислительные, которые на все больше и больше наращиваются, и все больше и больше ближе к этому самому сильному интеллекту. На самом деле, проблемы и там, и там, как ни странно, примерно одинаковые у текущей парадигмы. И они разбиты на три группы. Это, во-первых, энергоэкономичность, масштабируемость и необходимость взаимодействовать с сложным асинхронно-динамическим миром. Ну, по меркам экономичности тут, наверное, даже особо говорить нечего. Видно, что разрыв просто фантастический. Потому что, с одной стороны, нам физиологи говорят, что мозг в нормальном обычном состоянии, когда мы не решаем шахматные задачи, а просто сидим и смотрим, он где-то потребляет 20-30 ватт энергии. Ну, чуть увеличивается, можно сказать, на русском мыслительном. Ну, в то же время Кристофари, наш флагман отечественного суперкомпьютерстроения, потребляет мегаватты. Причем делает он на самом деле то, что даже трехлетний ребенок сделать… вернее, не может сделать даже то, что трехлетний ребенок делает легко. Поэтому видно, что разрыв на много-много порядков. Он просто вообще катастрофический по энергопотреблению, совершенно катастрофический. И без решения этой проблемы энергоэкономичности мы дальше далеко не уедем, поскольку сейчас уже крупные процессоры, суперкомпьютеры – это нечто вроде как антронный коллайдер. Если много наций напряжется, Google бросит на это многомиллиардные силы, тогда это можно сделать, а больше никак. Но дальше, понятно, что экстенсивный рост – это уже невозможно. ну и это, собственно говоря, взаимосвязано с проблемой масштабируемости. очевидно, что… ну, для меня, по крайней мере, очевидно, что сложные задачи и сложные цирктуальные системы невозможно реализовать на простом каком-то субстрате. Должна быть сложная система, структурно сложная на уровне ее реализации. В мозг лягушки нельзя вместить сильный интеллект, чуть больше миллиона нейронов. Поэтому размер важен нейронной сети. Если мы знаем, сколько в мозге нейронных связей, их 60-70 миллиардов нейронов, у каждого нейрона может быть десяток, десять тысяч Нам, даже вот этими мегапроектам Google и всем остальным, все эти известные модели, GPT-3 уже старая маленькая модель, сейчас гораздо большая модель, но все равно по структурной сложности там не хватает еще двух порядков. Ну и, как я сказал, дальнейшее наращивание вряд ли возможно. Ну и, наконец, сам подход. Понятное дело, что сейчас, поскольку нейронные сети обычные, должны встраиваться в устройства, которые взаимодействуют с окружающей средой. Сейчас этот подход преодолевает его ограниченность. функционирует нейронная сеть. У нее есть два периода – период обучения и период инференса. Вот мы сначала ее учим, у нас есть обучающие некие примеры, то есть некие фреймы, ограниченные фреймы данных, там, скажем, вот у нас фотография кошечки, вот фотография собачки, вот она создает задачу, сперва есть ли у них отличение одного от другого. То есть есть четко отчерченные какие-то фреймы данных, есть обучающие выборки, а потом вот мы ее научили, она у нас занимается инференсом. Разумеется, если мы понимаем АГИ как именно токсинный интеллект, как отнечественно человекообразное, в том смысле по возможностям, близкое, то тут договорится по-другому, так же, как улучшается в человеке. То есть, во-первых, это инкрементальное обучение, которое длится всегда, всю жизнь. И, во-вторых, у нас нет никаких фреймов данных и, более того, нет никаких обучающих и тестовых выборов. У нас есть просто жизнь. просто потоки информации, в которой мы что-то учимся, чего-то забываем, то есть совершенно другой протокол обмена с окружающей средой информационной, вообще не фиксирован. Можно, конечно, это как-то делать и в традиционных нейронных сетях, но это крайне крайне-крайне неадекватно. Это не подходит под изначально заложенную в их идеологию способ обмена информацией с внешним миром. Ну и вот, собственно говоря, импульсные нейронные сети были придуманы отчасти как… ну, были придуманы они поэтому, но, в общем, получилось так, что они, видимо, представляют собой метод решения всех трех этих проблем. За счет чего? Собственно говоря, чем импульсная нейронная сеть отличается от обычной традиционной нейронной сети? Отличия основных два. Во-первых, это способ передачи информации от нейрона к нейрону. Он отличается от обычного традиционного способа передачи не важно. это числа действительно, могут быть как-то ограбленные. в случае импульса нейронных сетей между нейронами передаются спайки. ну, то, что соответствует импульсам, нервным импульсам у нас в голове. это некий все время одна и та же амплитуда, это связано с физиологией, как у нас в голове происходит, и очень маленькая тоже постоянная длительность. то есть сам по себе объект не является информативным, а информативным является фактически к совокупности некий, то есть передается совершенно по-другому представленная информация. И второе – это все нейроны функционируют совершенно независимо друг от друга. Нет никакого синхросигнала, который бы как-то в каком-либо смысле синхронизировал их взаимодействие. Это тоже отличается от подхода традиционных нейронных сетей, потому что они обычно слоистые, даже если они не слоистые, а интервалентные, как LSTM, все равно их выполнение синхронизировано. пока у вас сверточный слой не посчитается, слой пулинга у вас считаться не начнет. это совершенно синхронное оперирование. вот здесь, в университетах, это в принципе не так. нет никаких слоев, никаких синхроний, вообще ничего этого нет. Тем самым мы получаем возможность ответить на все три вопроса, все три проблемы, которые я описал вверху. Энергоэкономичность основана на чем? На том, что это типичный пример, так сказать, event-based computations. Событие произошло, spike получен, он отработал, после чего снова все стопится и энергии не тратится, никаких решений не протекает в формулах. Точно так же, собственно говоря, почему у нас наш мозг такой энергоэкономичный в сущности, если так очень грубо, то на самом деле основным объемом мозга составляет белое вещество, так называемое, а не серое. Серое вещество – это тела клеток. А белое вещество – это связи. Связей гораздо больше по объему, чем тел клеток, но, как я говорил, связи не потребляют энергии практически пока. То есть по ним прошел нервный импульс, это затратная вещь, она энергично затратилась, он прошел и все, и снова ничего не потребляет. Поскольку относительная частота генерации спайков нейронами, она довольно маленькая, как нейрофизиологи говорят. В общем, на самом деле мозг почти совсем стоит, грубо говоря, ничего не потребляет особо. Но здесь тот же самый принцип. Если говорить о математике, с числами надо делать довольно сложные операции. Главное, что делают нейронные сети с числами, это их перенажают в виде матричных операций, электронных операций. Фактически это операция свертки. Это умножение и сложение многочисленное. в случае, если у вас это нервный импульс, с ним там нечего складывать или умножать. там можно какие-то простые аддитивные операции, как мы сейчас увидим, операции сравнения и все. то есть это гораздо более экономично по вычислению, по отношению к оперированию с числами. Ну вот в этом, собственно, и основные подходы к ответу на энергоэкономичность. Масштабируемость. Поскольку у нас нет никакого синхронизирующего механизма, то неважно, сколько у нас нейронов – миллион, или 2 миллиона, или 10 миллионов, в общем-то, ничем принципиально это дело не ограничено. Ну, если у нас есть возможности аппаратные, то теоретически это неограниченно масштабируется. И асинхронность тоже видна, поскольку у нас нет ничего синхронизирующего. У нас эти спайки приходят на нейроны в произвольном порядке, как никому неизвестно. То есть нет никакого четкого протокола взаимодействия. ограничивающего сценарий взаимодействия нейросети с внешним миром. Как нейронные уставы свободно обмениваются спайками, так и они из внешнего мира получают спайки, сами генерируют какие-то команды. Ну, я все-таки небольшой, поскольку многие уже знают, что такое импульсные нейронные сети и как они устроены, но все-таки я, чтобы было понятно, какие-то самые простые модели расскажу. Как устроен самый простой, но существенно имеющий необходимый функционал импульсный нейрон. Это так называемая модель нейронно-пороговый индуктор с утечкой. Он крайне прост. И выражается вот такой вот аналоговой схемой электрической, которая там внизу. То есть у нас имеется конденсатор, он соответствует мембране нейрона. Имеется сопротивление ему параллельное. Это соответствует всяким каналам, которые через мембрану ведут. И пороговый элемент. И функционируется эта система крайне просто. Вот у нас пришел импульс. Импульс добавил какой-то заряд на конденсатор. Если ничего не происходит, то этот заряд стекает через сопротивление. Если у нас пришло много импульсов, то каждый из них поднимает заряд конденсатора на какую-то величину, она достигает некоего порогового значения, срабатывает пороговый элемент, и нейрон сам генерирует импульс. Вот, собственно говоря, крайне простая такая логика. В общем-то, это так и биологических нейронов, если сильно-сильно огрубить. Поэтому, в общем-то, наверное, допустимо сказать, что если элементарные операции в обычных компьютерах являются известной всем логически операцией над битами или нет, то самая элементарная информационная операция в голове, в мозге – это фиксация совпадений. Что делает нейрон? Делает простую вещь. Он регистрирует приход нескольких спайков в течение небольшого временного окна разные synapse. это самая базовая операция, которая лежит в основе всех вычислений, которые делает мозг. я не беру пластичность. на этом графике, собственно говоря, показано то, что я только что сказал. вот у нас первый один пришел спайк, он вызвал небольшое увеличение этого потенциала, но ничего не произошло, он стек и все. два спайка тоже, а вот там много спайков, раз-раз-раз мы пересекли порог, сами сгенерировали спайк, и вот, собственно, произошло это событие. Как это формализуется? Самый простой способ это формализовать – есть так называемые одномерные модели нейронов, где нейрон характеризуется одной фактически величиной – этом мембранным потенциалом, она здесь обозначена как У. Это на самом деле семейство моделей, и они отличаются тем, что происходит после того, как нейрон получил спайк на синапс, и что происходит после того, как нейрон сам сгенерировал спайк. То, что происходит при получении спайка, выражается в функции ε, которую мы видим от времени, прошедшей после получения спайка. А то, что происходит с потенциалом, когда сам нейрон генерирует спайк, выражается в функции ε. Соответственно, поставляя разные функции, мы получаем немного разные модели нейронов. Еще есть условие, что если потенциал достигает порога, то мы сами генерируем спайк. Чуть более сложная модель, но более реалистичная и более функционально богатая – это модель, где у нас спайки меняют не потенциал на этом конденсаторе, а меняют свойства сопротивления, которые параллельны. На самом деле, в физиологии как? Там приходящие спайки действительно меняют свойства разного рода ионных насосов, находящихся в мембране нейронов, они не напрямую действуют на потенциал, а через работу этих насосов. В частности, можно сказать, что там есть насосы, которые увеличивают концентрацию, разность концентраций ионов, калий, натрий. и помогают нейрону сработать. Это возбудающие синапсы. И есть синапсы, которые открывают другие насосы, которые, наоборот, уводят потенциал от порога уровня, тем самым мешая нейрону сработать. Это тормозные синапсы. и у нас имеется две проводимости. одна проводимость в сторону нижнего потенциала, самого минимально возможного, а другая, наоборот, в сторону верхнего потенциала, который лежит над порогом. и, соответственно, от их динамики сработает неровный линия, как здесь показано на этих уравнениях, где CE – это тормозная проводимость, а CE – это возбуждающая проводимость. Ну и, наконец, модель, которая используется очень часто в реальных задачах по моделированию нейросетей. В частности, я ее пользуюсь, и она реализована в нескольких существующих нейропроцессорах. Это так называемая модель порогового интегратора с утечкой и адаптивным порогом. Она похожа на модель, о которой я говорил на двух предыдущих слайдах, самую простую модель. Она отличается от них только тем, что пороговый потенциал, при достижении которого нейрон срабатывает, он не константа, а он сам меняется. Он сам меняется по очень простому закону. Когда нейрон генерирует спайк, он немного повышается, а потом он релаксирует к своему базовому уровню. Эта маленькая добавка дает нам на самом деле две очень существенные фичи такого нейрона. Это, во-первых, свойства гомеостаза, потому что если нейрон становится очень активным, у него повышается порог вследствие этого правила, и это мешает ему делаться еще более активным. То есть активный нейрон сложнее активировать, потому что у него порог высокий. Второе – это то, что нейрон реагирует на начало стимуляции, а не на саму стимуляцию в гораздо большей степени. Это зафиксированные экологические нейроны. То есть если мы начнем этот нейрон стимулировать, то сначала он будет генерировать спайки часто, а потом все медленнее и медленнее, потому что у него порог просто будет выше. вот эти два свойства оказываются очень важными при реальном моделировании, поэтому эта модель очень простая, используется часто на практике. я сейчас просто скажу пару слов о том, что был такой ученый, вернее не был, а есть. просто сейчас он не очень ученый, скорее сейчас бизнесмен. Это Евгений Яжекевич, наш человек, кончивший МГУ, который задался как-то вопросом. Вот мы знаем, что нейрофизиологи фиксируют очень разнообразную динамику реальных нейронных логических. Они не только могут отдельные спайки генерировать, они могут такие пачки спайков генерировать со сложными соотношениями между размерами этих пачек. Примерно около 12 разных, существенно разных мод поведения нейрона нейропсиологи выявили. Он задался целью, а какие уравнения минимально могут воспроизвести все эти 12 типов активности. Он вывел эти уравнения. Они показаны на этой резочке желтой. В результате получилась довольно известная модель так званой нейронной Жюкевича, которая считается сейчас самым простой моделью, которая воспроизводит довольно известную физиологию, более-менее качественно, по крайней мере. Но многие пользуются этой моделью. Ну а вообще, это тоже всем уже вкратце, если совсем углубиться в биологическую реальность, то нейрон описывается сложной моделью. Она называется моделью Ходжкина-Хаксли, за которую современная Нобелевская премия была получена. Это сложная система нелинейных дифференциальных уравнений, оперирующаяся такими вещами, как концентрация разных ионов и так далее. Она хороша, но для нашей цели она малопригодна, потому что просто исследовать там несколько нейронов можно, а когда там этих нейронов становится миллионы, то уже решение всех этих диффуров становится задачей непосильной, да и, в общем-то, не нужной, если тот же нейрон Ижекевич примерно то же самое вполне плохо воспроизводит. И теперь кратко о том, как же, собственно говоря, информация представляется в нейронных сетях. В обычных нейронных сетях ее не надо представлять, она там также представлена в виде чисел, которые после числа передаются, и все. Как я сказал, спайки не информативны, поэтому нужно придумывать какие-то способы кодирования числовых данных, категориальных данных. Для этого имеется несколько разных подходов. Более того, по всей видимости, в биологическом мозге Многие из них имеют место. Разнообразие вполне биологичное. Самый простой способ – это частотный. Мы просто берем и говорим, что вот у нас есть нейрон, если он часто генерирует спайки, значит большая величина. Если редко, то… маленькая лично. это здорово, но это медленно, потому что, чтобы набрать статистику часто или нечасто, нам надо, понятно, долгий интервал времени. если мы хотим закодировать что-то быстроменяющееся, то такой способ не подходит. плохие динамические характеристики. другой вариант – мы берем какое-то количество нейронов, скажем 100, Если какое-то маленькое окно времени, из них много активных, это большой сигнал. Мало активных – маленький сигнал. Это здорово, потому что можно кодировать нечто быстрое, но это малопродуктивно, потому что для этого требуются большие достаточно, чтобы закодироваться на величину. 100 нейронов, грубо говоря, не очень экономично. Есть позиционное кодирование. Это как у нас в улитке уха кодируется частота звука, который мы слышим. Улитка уха – это некий линейный порядок рецепторов. И одни рецепторы на одном конце этого порядка реагируют на низкие звуки, кодируют низкую частоту, а другие – высокие звуки, высокую частоту. у нас есть линейка нейронов, активность нейронов в одной стороне – это слабый сигнал, в другой стороне – сильный сигнал. Все эти методы можно назвать ансейдронными, поскольку они не учитывают точного положения спайка на временную оси. Но есть еще методы, которые привязываются к точной позиции до миллисекунды спайка. Вот, например, допустим, у нас есть 4 нейрона. Если мы видим, что они спайканули в такой-то порядке с такими-то задержками, то у нас, допустим, кодируется одна величина. Ну, это категориальная величина. Если это все в другом порядке с другими задержками – другая величина. Естественно, 4 нейрона могут закодировать сотни категориальных величин. Это очень хороший способ кодирования, потому что он очень экономичен. Более того, видимо, он наблюдается и в Корее главным мозгом. Если у нас есть два пары нейронов, то по запаздыванию генерации спайков одного нейрона с другого, то мы можем этой задержкой кодировать временную числовую величину. Если у нас есть некий глобальный процесс, я говорил, что нет синхронизации. Но одно дело – нет синхронизации явно, другое – это значит, что нет какой-то наблюдающейся коллективной активности, которая может явиться следствием каких-то динамики мозга. Такая активность общая, генерализованная имеется. Это всякий род ритмы. Мы знаем, есть альфа-ритм, бета-ритм, тета-ритм и так далее. Так вот, известно это измерено… Имеются нейроны, которые очень специфично реагируют на положение крысы в виде фазы спайка этих нейронов по отношению к тетеритму. В какой фазе тетеритма они спайкуют, кодируют нечто, в данном случае на положение крысы. Мы видим, что имеются очень разные подходы к кодированию информации. Но самое, конечно, интересное – это обучение. Как мы говорим, все это хорошо, кодирование и все прочее, но, разумеется, главное, что надо решить, какую проблему надо решить, строя интеллект сильный и даже не очень сильный – это обучение. Что мы здесь наблюдаем в сравнении с обычными сетями? Как учится, обычные сети знают все. Поскольку обычная сеть представляет из себя, в сущности говоря, непрерывные функции многих переменных, этой сети и веса связей, то мы можем с помощью градиентного спуска, у нас всегда есть некие критерии, можно построить частные производные от этого критерия по, допустим, весам, ну и посмотреть, как нам надо двигаться в сторону, как надо менять веса, чтобы этот критерий там у нас максимизировался. на чём и основан всем известный алгоритм обратного распространения ошибки, который прямо приводит нас к цели, если мы не свалимся в локальный максимум. В случае с импульсными нейронными сетями всё не так здорово. А именно, как мы видим, импульсная нейронная сеть является по своей сути дискретной системой. она работает все или ничего, то есть спайк он либо есть, либо нет. поэтому дифференцировать там, строго говоря, нечего, просто у нас вообще нет никаких производных ни в каком месте. поэтому прямое применение градиентных методов там невозможно, хотя существуют, безусловно, люди как-то пытаются выйти из этой ситуации, строят там всякие так называемые псевдоградиенты, суррогатные градиенты, но в общем это все такая некая алхимия по большому счету. на самом деле это все должно быть не так. ну и на самом деле еще к тому же Если мы берем бионический принцип, то биологи, видимо, не знают, не видят никакой механики, похожей на backpropagation в нашем мозге. Там нет этих каналов обратного распространения ошибки. Поэтому принципы обучения импульсно-нейронных сетей, они тоже бионические, они взяты в сущности с того, как это наблюдается в биологии. И они отвечают главному принципу, который называется Обучение импульсных нейронных сетей тоже является коррекцией весов, изменением веса синапсов, но это изменение основывается только на показателях активности присинаптического нейрона и посинаптического нейрона. Каждый отдельный нейрон не знает, догнал там мышка-заяц, вернее, кошка-мышку или не догнал. Она знает только про свой нейрон, который до и который после. Все. А про зайцев, мышек она вообще в принципе не знает. Они, вернее, нейрон. Это дает возможность… С одной стороны, это тяжело, потому что в случае backpropagation мы напрямую выходили к решению задачи. Здесь между этим самым пластичностью решения задачи лежит много-много уровней, и надо еще сделать так, чтобы эти уровни были преодолены. Но, с другой стороны, это же оказывается и плюсом, потому что тем самым мы, опять же, не нуждаемся в никакой синхронизации. Как у нас там сеть посчиталась, прошел сигнал вперед, потом она отчитывает назад, корректируя веса. Все строго синхронизируем. Здесь не так. Как-то между собой нейроны общаются, как-то у них модифицируются веса. Абсолютно независимо. Опять же, это дает возможность сделать сколь угодно масштабируемые системы, сколь угодно системы, никак не привязанные каким сценарием взаимодействия с внешним миром. Ну, то есть вот это большой плюс. И более того, первый принцип такого обучения сформулирован был еще давно, там был такой Хебб, нейрофизиолог, который говорил, что те нейроны, которые вместе спайкуют, они должны иметь связь между ними. Потом был кузуальный смысл, то есть если один нейрон часто заставляет срабатывает другой нейрон, то такая связь усиливается, и наоборот. И этот эффект был наблюдаем 25 лет назад. место такая механика. график зависимости изменения веса от чередования импульсов нейрона презинаптического и постинаптического. то есть если сначала на синапс приходит спайк, а потом этот нейрон, которому синапс принадлежит, сам спайкует, то такая связь усиливается. Это, соответственно, у нас правая часть графика. Если все в обратном порядке, то такая связь ослабевает. Это так называемая базовая модель STDP, спайк тайм индепендент пластичности, которая в сообществе легла в основу большинства методов обучения непространенной ровной сети, которые сейчас имеют место. но эта модель пластичности, она хорошая, но она обладает кучей недостатков, а именно она ведет к неустойчивости, потому что, ну, представьте себе, сети, мозга, ну, сейчас выбросные у них могут иметься какие-то занутые контура нейронов. Разумеется, по таким контурам, если пробежит сигнал, то он усилит все веса этого контура, потому что для всех этих нейронов выполнится правило Хебба, они усилятся, и пробегание спайка по такому контуру сделается еще более вероятным. то есть если мы имеем это правило и применяем его к рекуррентной хаотической нейросети, то у нас получается, что эта сеть разобьется на такие очень суперсильные аттракторы, на какие-то контура, по которым все время будет бегать возбуждение, но ничего хорошего из этого не произойдет, то есть это никакого нетривиального процесса нам не даст. Много имеется минусов этого базового механизма, поэтому были придуманы куча его обобщений, которые лишают его неприятной положительной опадной связи. Они тут на этом слайде отчасти перечислены. например, в довольно эффективном модели ограничения общего суммарного веса. Мы говорим, что суммарный вес – это некий ресурс, и он на весь нейрон постоянен. Если у нас какие-то синапсы усиливаются, то они должны ослабляться. Но это позволяет нам не делать суперактивных нейронов, а вносит конкуренцию между синапсами. модулируется, есть эффект усталости, т.е. нейрон, который часто генерирует спайки, становится непластичным, как раз чтобы избежать этого эффекта, который я сказал, и т.д. Существует целый зверинец самых разных обобщений этого моделя с ВДП, но они все поддерживают принцип локальности. Поэтому вопрос об этом еще пока стоит, каким должна быть в точности асинаптическая пластичность, к какому закону подчиняться. Но главное, какой вообще подход нативный к получению импульсных нейронных сетей, который именно присущ им? Потому что в принципе, я сразу оговорюсь, есть еще вариант, когда импульсные нейронные сети просто являются эквивалентом традиционных нейронных сетей. Доказана теорема, что с помощью импульсной нейронной сети достаточного размера можно сколь угодно точно воспроизвести любую традиционную сеть. поэтому, соответственно, как часто на практике пока еще и поступают, если мы хотим использовать преимущества импульсно нейронной сети, а именно ее, допустим, энергоэнергономичность, но не хотим ничего нового придумывать, мы берем обычную традиционную нейронную сеть, обученную уже на что-то там, и делаем ее имплементируем в каком-нибудь нейрочипе, ну и все, и замечательно. Но это не то, что мы хотим. Мы хотим, чтобы это было настоящее импульсное решение и так далее. А тогда нам надо, естественно, имплементировать обучение в самых импульсных терминах. Так вот, как я уже говорил, главная функция, основная функция нейрона импульсного – это нахождение корреляции. Поэтому и обучение, оно, в сущности говоря, тоже в терминах корреляции, что, в общем-то, вполне естественно. Что такое, допустим, антисупервайз леонид? В терминах импульсных сетей это нахождение корреляции в ходном сигнале, потому что антисупервайз леонид – это нахождение некой структуры в ходном сигнале. а если у нас входной сигнал – это просто набор спайков, то там структура может быть только в одном, некая коррелированность их появления в среднем или точной коррелированности, какие-то обобщенные корреляции в приходе спайков по разным входам или во времени, так или иначе поиск корреляции. Supervised learning тоже выглядит так же, но только это поиск корреляций между входным сигналом и неким сигналом, примечающим какие-то целевые переменные. Опять же, это все в термах корреляции делается. Или антикорреляции, что одно и то же, поскольку у нас есть тормозные нейроны, которые в сущности могут выполнять операцию не. Поэтому корреляции или антикорреляции – это одно и то же. Reinforcement learning тоже так же может быть сформулировано, но в более сложных терминах. Тут у нас присутствуют три вещи. У нас тут присутствует входной сигнал, описание мира, есть некие действия сети, какие-то команды. и есть оценка. может быть положительная, может быть отрицательная в виде оценочного сигнала. тут у нас корреляции более хитрые, они включают три этих типа сигналов. но, в общем-то, всегда подход один и тот же. есть некие концепции, как можно, в принципе, любую задачу обучения, чего бы то ни было, сформулировать в термах изменения весов, сформулируя ее как, так сказать, нахождение телеперативной корреляции. Причем это может быть иерархичным, то есть мы можем… эти свойства, коррелированность ее основания может быть многоуровневая, на разных временных скалах, ничто нас в этом не ограничивает. есть, правда, подход, но он к сильному интеллекту не очень подходит, но о нем скажу для полной картины просто. это подход так называемой машины в жутком состоянии, где у вас нейронная сеть вообще не учится никак, она статична, но она очень большая, она очень большая и очень хаотичная, все функции в том, чтобы как-то сложно реагировать на внешнюю стимуляцию. вот у нас есть, мы как-то подаем какие-то меняющиеся сигналы, они как-то очень сложным образом реагируют, только она сама сложная. а вот над этим всем стоит уже некий механизм, как правило, простой, линейный классификатор, который в качестве предикторов использует активности, просто счетчик импульсов отдельных нейронов. Получается гиперразмерное, очень большое описание. Но мы знаем, что большая размерность – это не только проклятие, это еще и благословение, как мы там много раз видели. то есть задача часто бывает, если мы говорим о сдаче классификации, она часто бывает сепарабельна в этих терминах, линейно сепарабельна. и поэтому простой линейный классификатор, используя эту сложную активность, может решать задачу. но это все-таки вне сетевая механика, поэтому я просто упомянул, чтобы соблюсти полноту. Но очевидно, что самый главный плюс, даже если говорить не о сильном интеллекте, а об обычном применяемом интеллекте, практически применимом – это динамика. То есть главным плюсом импульсных сетей – это возможность учитывания динамических активистик. времен и так далее, последовательности. Это еще связано с тем, что, вот еще одна вещь, которую я не сказал, то, что на самом деле выбросы в нейронных сетях используются в качестве нетривиального элемента вычисления задержки распространения импульса. То есть если в обычных сетях об этом речь не идет, то там считается, что от нейронок нейронный импульс передается за один такт, по мере синхронизации слоев. то в импульсных сетях и в голове распространение спайков происходит с некоторым временем. И это время – это не минус, а скорее плюс. Именно оно придает свойство сложной динамической системы нелинейной импульсной нейронной сети. То есть импульсная нейронная сеть, с одной стороны, является универсальным вычислителем, как это показано, а с другой стороны, она является сложной динамической системой. Этим она дает возможность работать с сложными динамическими процессами, именно поэтому. Но для этого, в сущности говоря, чтобы это было так, необходимо, чтобы иблисная сеть обладала памятью. Потому что если мы говорим о основании каких-то пространство-временных образов, этот образ длится во времени, нам надо к его концу помнить, что было его начало, грубо говоря, что мы его распознали в его целостности. Что же у нас может быть памятью в импульсной нейронной сети? Довольно много разных подходов к этому. Например, память может быть какая-то регулярная активность, постоянная каких-то контуров. Вот у нас как ячейка памяти, мы ее активировали, замкнутая нейронная сеть, она активна. Пока мы ее не подавили чем-то, она помнит, что она была активирована. Это некая память. Память на уровне отдельных нейронов. Скажем, у нас модели с меняющимся порогом. Этот порог может меняться медленно. Мы знаем, что нейроны потенциально-мембранные – это быстро меняющаяся величина. Она порядка 10 миллисекунд характерное время. то порог может быть медленно меняющийся величиной, секунды может быть и так далее. Причем это тоже имеет место, каждый может в этом убедиться. Если мы посмотрим глазами на какую-то яркую точку, а потом закроем глаза, у нас получится, что эта точка видна только в ее негативе. То есть яркая точка сделала усталыми нейроны, у них поднялся порог. Теперь фоновый шум сквозь нейроны не проникает, делая нам фактически молчащие нейроны. черная зона такая. то есть это тоже вполне нормальный способ иметь память, имплементировать память. есть еще несколько подходов. вот один подход был предложен он называется полихронные группы. я, по-моему, на следующем слайде. у меня есть отдельный, поэтому я чуть позже рассмотрю. ну и, например, вот еще есть такой подход непрерывный аттрактор, так называемый. это такое? Представьте себе, что у нас есть нейроны, образующие некую тоже линейку, некий порядок, и они выстроены так, что нейроны соседние, они друг друга стимулируют, а отдаленные нейроны друг друга подавляют. То есть у нас имеются длинные подавляющие связи и короткие стимулирующие. Тогда, если мы что-то возбудим, какой-то кусочек этой линейки, то за счет того, что у нас есть локальные возбудящиеся будет стабилизироваться, потому что отдаленные связи тормозящие будут мешать ему распространяться. причем мы можем фактически перемещать это возбуждение по этой линейке. это нам дает возможность запомнить, в сущности говоря, действительное число по положению активного кусочка в этой линейке. Такой аттрактор, у вас получается медостабильное состояние, но образует непрерывное континуум состояния, если у вас много нейронов. Вот как раз про один из возможных механизмов памяти, о котором говорил Ежекевич. Он как раз основан на синхронном кодировании информации и связан с тем, что, допустим, у нас простая такая схемка из семи нейронов, связанных вот такими-то связями. Они показаны с такими-то задержками, они тоже есть. Получается такой эффект, что если мы простимулируем сначала нейрон, потом нейрон n1, n2 в такой последовательности, то это вызовет последовательность спайков, которые пробегут по всем нейронам этой семерки в некоем порядке. если мы подадим другой последовательности N2 и N1, синим показано на секции C, то у нас тоже они все будут активированы, но уже в совершенно другом порядке. если при этом наложить на это эффект, что у нас коротковременные синапсы временно усиливаются. есть кроме долговременной памяти, которая навсегда есть, еще коротковременная пластичность, которая есть какое-то время, а потом наносит синапс, где синапс падает до базового уровня. то тогда, допустим, n1, n2 простимулированы, как было на рисунке b, у нас пробежали возбуждения вот этим путем, и все именно эти красные связи усилены. Тогда за счет того, что у нас в сети всегда имеется некий шум, у нас всегда имеется некая возможность, что с шумовыми импульсами будут именно в таком порядке возбуждены n1, n2, у нас снова проиграется этот же самый сценарий, этот же самый последовательный спайк. Но чем дальше, тем эта вероятность будет меньше и меньше, поскольку у нас релаксируют наши веса к базовому уровню. То есть мы один раз таким образом возбудив нейроны, мы оставим след этого возбуждения, которое спонтанно может воспроизводиться за счет шумовой активности базы. Шум всегда важен. Это отдельная история, зачем нужен шум нейронный. Но на самом деле, как показал Ижекевич, такой способ хранения информации чрезвычайно экономичен. Относительно небольшие нейронные ансамбли могут таким образом хранить огромное количество разных значений. Количество разных значений может превыходить количество нейронных ансамблей, как показано Ижекевичем. Перед тем, как мы займемся железом, просто хотел бы такой небольшой слайд о том, как все это относится с нейрофизиологией. Считаю, что эта наука, поскольку она бионическая, она в очень большой степени использует данные нейрофизиологии. И наоборот, ее взаимообогащает. Вот, допустим, у нас имеется такой человек, Константин Владимирович Анохин, академик, один из наших ведущих нейробиологов. У него очень много моделей, которые он получает из измерений непосредственно нейробиологических. но они в принципе могут быть проверены, они могут дать начало каким-то алгоритмам уже сфиксированным в виде моделей, ну и в то же время эти модели могут дать какие-то новые предсказания, ну и это все может замкнуться, и это может оказаться наоборот очень плодотворным для нейрофизиологов, потому что они могут знать, какие эффекты искать, чтобы подтвердить те или иные свои теории. Поэтому такой цикл очень полезный для обеих наук. И теперь об аппаратной стороне дела. Вот почему такие нейроны являются… реализуются в виде железа, в виде соориентированных микросхем. ну, во-первых, мы видели самый простой нейрон, лиф нейрон. он вообще может быть просто реализован с помощью сопротивления и конденсатора, но еще пороговый элемент. Поэтому есть возможность аналоговой имплементации таких нейросетей. Аналоговая имплементация – это здорово, потому что, понятно, одно дело, мы читаем экспоненты численно, у нас падает число по экспоненте, это трансцендентная функция, сложная целая история, много сложений, умножений. А другое дело, что мы ту же самую экспонентную можем не считать, просто мы ее можем сделать в виде конденсатора из противления. это можно сделать крайне экономично, быстро и так далее. поэтому аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая аналоговая но если мы остаемся в рамках цифровой реализации, то тоже все довольно неплохо, потому что, как я говорил, одно дело – обработать поток чисел, сложных перемножив, сложив, пропустив через функцию активации трансцендентную часто или линейную, а другое дело – обработать переход спайка. поскольку в нем ничего нет, это один вид информации, то там некуда засунуть сложности, все очень просто. поэтому аппаратная реализация может оказаться очень простой. ну и, как выяснилось, даже если бы эти экспоненты, релаксации этих потенциалов вместо экспоненты используем линейную функцию, то ничего плохого сильно от этого не получается. а это вообще элементарно совершенно моделируется. то есть мы видим, что потенциально такие сети крайне дружественны по отношению к что мы и наблюдаем. Если говорить о том, что сейчас происходит, то используются три подхода. Наиболее обычный пока – это использование обычных универсальных компьютеров, только сильно параллельных, типа GPU. Мы, например, работаем с GPU и GPU-кластерами. Есть специализированный, например, нейрокомпьютер, хотя он является обычным компьютером, универсальным. Это огромное количество ARM-процессоров, вполне традиционных. Но это в силу параллельности, в силу других особенностей, можно сказать, что это нейрокомпьютер, это компьютер-спиннакер. Манчестерского университета. 

S00 [00:52:14]  : Проект уже 15 лет развивается. 

S02 [00:52:17]  : Я потом его посмотрю. Покажу. Вот гибридный подход – это цифроаналоговый. Почему это нельзя сделать полностью аналоговым? К сожалению, вот почему. Хотя нейрон можно сделать аналоговым, но связи между нейронами аналоговыми сделать нельзя. Это у нас в голове, там имеются проволочки, соединяющие нейроны. Мы пока в кремнии и никак такие вещи делать не умеем. Мы не умеем выращивать связи произвольным образом, 10-14 степени – это нереально. Поэтому, чтобы воспроизвести обмен информацией между нейронами, даже если сами нейроны являются аналоговыми, нам надо делать какие-то пакеты, маркетизация и все остальное – это уже чисто цифровая вещь, она не может быть сделана аналоговой, к сожалению. Поэтому это гибридные подходы, но все равно они есть. в стэнфорде нейрогрит и в гейдельберге brain scales реально существующие проекты. ну, правда, нейрогрит сейчас как-то подувел. ну и, наконец, самое, на мой взгляд, интересное – это цифровой подход, но специализированный. это создание чипов, которые не умеют ничего, кроме как довольно много. тут перечислено 4, но на самом деле их уже там порядка под десяток уже. это я просто самый известный из них перечислял. в том числе там есть и отечественная разработка, компания Мотив разработала нейрочип Алтай. Ну, сейчас мы посмотрим дальше. Чуть-чуть, как моделировать нейронную сеть на GPU. Это импульсно-нейронная сеть. Это означает моделировать три вещи, которые периодически чередуются. Во-первых, надо обновить состояние каждого нейрона и увидеть те нейроны, которые сгенерировали спайк. Это надо моделировать распространение этих спайков по сети. Ну и с каким-то там, иногда, это нечастый процесс, апдейтить их веса. Вот, собственно говоря, моделирование импульсно-нейронной сети – это вот три этих процесса. Если говорить о ГПУ, то можно ли сделать сильный интеллект на ГПУ? Ну, тут проблема в том, что одна плата GPU, конечно, мощная штука, но ее ограничение налагает следующее. Это, во-первых, память, порядка десятки гигабайт, это некое ограничение структурное, которое сейчас позволяет делать так, что у нас фактически наиболее эффективно, не более 1024 синусы на нейрон эмулируется, просто из-за неких, я не буду сейчас уточнять, программно-аппаратных особенностей этой системы. И самое главное – это высокая латентность обмена по PCI-Express этой картой, что не позволяет делать сильного масштабирования на много-много карт. и вопросы обмена спайкой. то есть, в общем, GPU, как здесь сделано вывод, это где-то до 100 тысяч нейронов 10 тысяч символов на нейрон. то есть, это мозг чего-то, не знаю. не помню эту знаменитую картинку по разных животных и какие у них мозги. ну, в общем, если говорить о кластере, ну, это все равно в реальном времени, ну, миллионы, может быть, десятки миллионов, но никак не более, там начнутся бутылочные горлышки из-за обмена, просто интерфейсные проблемы начнутся. Да, и вот я как раз говорю, что как же, собственно говоря, если у нас нет проволочек, как же эмулируется обмен спайками. С помощью вот таких пакетов формируются пакеты, причем изначально мы берем основку, что в принципе доставка этого пакета не гарантируется, то есть наши алгоритмы все должны быть толерантны к потере информации в том смысле, что у нас будут какие-то проблемы с коммуникацией, так что не все пакеты будут доставлены. Чуть-чуть о том, что сейчас есть. Это спиннакер. Система – это ARM-процессоры, кристаллы из 18 ARM-процессоров и близкорасположенная память. Они объединяются в такие логические, троидальные структуры. Получаются из них такие платы, из плат делаются стойки, из стойки делается компьютер. Сейчас этот компьютер весьма мощен. До 100 миллионов нейронов он мог бы эмулировать. Тут все зависит от того, насколько активный обмен спайками. Тут имеется еще одна размерность. Не только число нейронов, не только количество синапсов, но и средняя активность, средняя числа генерации спайков. Потому что главная проблема возникает даже не с пересчетом состояния нейронов, а с доставкой огромного количества спайков. И это лимитирует в большой степени наращиваемость этих систем. вот это первый чип, который был сделан специально для того, чтобы эмулировать импульсную нейронную сеть. он называется TrueNorth, он был сделан в 2014 году, и это первый фактически не фон Неймановский вычислитель из таких массов применений. процессор, нет единой памяти, нет очереди команд, вообще ничего этого нет. И тем не менее, поскольку на этом процессоре можно закодировать любую архитектуру импульсной нейронной сети, а доказано, что импульсная нейронная сеть в целом, вообще говоря, это универсальный раститель, то это, можно сказать, универсальный раститель не фон Неймановского типа. С ним все замечательно, кроме одного, поскольку это первый блинкон, у него сильно ограничены их характеристики. Хотя на одном ЧП, может быть, до миллиона нейронов сымитировано, но веса этих нейронов являются крайне дискретными. Фактически веса нет, вес тормозной, вес стимулирующий. И все. Без их величин. И они не могут быть изменены. Перезагрузка всей сети. Поэтому нет обучения, очень простые бинарные веса. И тем не менее, даже такая штука вполне применима. Правда, не у нас и в довольно печальной области. Это в основном военные применения. ну вот, собственно говоря, я сейчас немного об этом... я уже, наверное, немножко закруглюсь, чтобы нам оставить время на дискуссию. Что есть сейчас самое крутое в этой сфере? Самое крутое в этой сфере – это процессор Лойхи компонентов. Точнее, сейчас же это Лойхи-2, потому что вышло второе поколение этого процессора. тут все по-серьезному. тут есть и обучение, и веса тут уже, на них там на каждый вес 8 бит, то есть это 256 разных весов, и много-много чего еще. Это выглядит вот так. Это тоже не фон Неймановский вычислитель. Тут, правда, есть универсальные процессоры. На этом чипе имеется три 86-х процессора Lakemont, но они выполняют чистослужебные функции. Они в самой нейронной сети не участвуют в ее перевычислении. Они там выполняют функции мониторинга, загрузки и системные всякие вещи. Сама нейронная сеть эмулируется на нейроядерах, между которыми имеются четырехнаправленные маршрутизаторы, одноугольная маршрутизация, спайк происходит. причем это, в отличие от других подходов, тут доставка спайка гарантирована. если у нас очень много спайков, то все это просто замедляется, уменьшает стартовую частоту, иначе у нас разруливаются все эти очереди, накопившиеся спайков, если их там слишком много. интересно реализовано обучение. обучение формализуется в виде такой формулы, то есть изменение веса в нем определяется такой аддитивной формулой. каждая излагаемая – это произведение разных факторов. число этих факторов могут включаться в наличие спайка, так называемые следы спайков – это экспоненциально сглаженные, как мы видим на графиках, просто эти дельта-функции, дискретные спайки, у них такой экспоненциальный след. Это чисто локальный подход, как мы и говорили. В изменение веса могут входить только величины, характеризующие активность и текущее состояние нейрона до связи и после связи. Но весьма гибко. В такую формулу можно загнать любые существующие модели пластичности синоптической. Вот мы видим примеры разных совершенно моделей, и они все укладываются вот в такую вот аддитивно-мультипликативную формулу. Хотя просто что угодно там задать нельзя, но очень широкие классы, которые включают практически все, что надо, туда можно таким образом записать. Более того, Сейчас на самом деле и некий алгоритмический компонент туда введен, потому что Loic 2 имеет возможность с помощью микрокода эти правила еще модифицировать. Они, конечно, там очень маленькие, эти размеры, эти микрокоманды. Тем не менее, это позволяет еще более увеличить обучение набор правил синтетической пластичности, и при этом все равно это остается непонейменовским устройством, если говорить о всей системе в целом. Более того, там даже заложено то, что пока еще не очень востребовано современными нейрофизиологами, современными людьми, которые занимаются моделированием нейронных сетей, а именно там синапсы образуют не некий такой плоский порядок, а синапсы могут которые составляют древовидные структуры. Допустим, какие-то символы могут быть приписаны к одному узлу этого дерева, к другому. И в каждом узле дерева имеется некая операция, которая как-то комбинирует значения потенциалов, накопленных на каждом предыдущем узле. Например, можно бы устроить какой-то гейтинг. Одни синапсы сами не меняют потенциал, но они, допустим, определяют пропускание или непропускание потенциалов с более низких уровней дальше. Можно устраивать хитрые нелинейные взаимодействия между синапсами, что, наверное, наблюдается в природе. Биологические нейроны пока еще совсем практически не востребованы моделями существующими. То есть это сильный еще запас на будущее. Это очень богатая система, где можно очень много чего имплементировать. Ну и вот это все выражается в таких вот устройствах, которые на этом основе делаются, начиная от маленьких штучек, двух кристальных, как Apoca Bay, которым можно, например, девес-камеру, событийную камеру прицепить, она будет там крайне экономично что-то распознавать. Ну и вот стоечки такие большие. уже, которые все больше и больше будут расширяться, и как раз вот это вот то, что справа, это как раз то, что по направлению, может быть, к сильному интеллекту. И сам Intel, они не скрывают, что их цель – это, в конечном итоге, сильный интеллект. Несколько раз я поскольку сам общался с руководителями этой группы, непосредственно я был там на их воркшопах, то, в общем, в частных беседах их цель – это АГИ. Ну и немного о нашей разработке отечественной. Это нейрорачип Алтай, который я недавно имел удовольствие представлять, поскольку я тоже входил в состав людей, который им занимался и сейчас делает математику для него. На последней конференции, которая была на нейроматематике, мы показывали, как Эта самая LTI распознаёт вращение вентилятора с помощью событийной камеры, направление, скорость и так далее, что это всё живое, это всё уже есть, это можно потрогать, там и кристаллы все наличствуют. Ну это как True North, только чуть-чуть поослабее пока. Но мы сейчас работаем над тем, чтобы сделать его больше похожим на логики, чтобы придать ему функции обучения и так далее, чтобы его функционал был более современным, отвечающим по требностям. Ну и вот, собственно говоря, в этой сфере что можно ожидать? Во-первых, моя позиция такая, что сейчас главное – это железный. Нам надо сделать, чтобы людям, занимающимся АГИ, они ничем себе не отказывали, чтобы у них были безграничные возможности вычислением по всему. мне кажется, это сейчас одно иллюзорно. ну и это будет все делать, потому что сейчас как грибы после дождя растут в этих проектах нейрочипах, там, я знаю, макидо, еще там ряд, я о том здесь не говорил. и этого будет все больше и больше. То есть эти устройства не фонейменовские, которые именно для эмуляции импульсных сетей, они будут появляться все чаще и чаще. Разумеется, да, причем они, поскольку люди все-таки считают деньги, они делают так, чтобы на этих кристаллах можно было эмулировать и обычные сети. Например, там сделано так, что на многих из них, на логике 2, что спайк, он может быть не битым, а может быть числом. то есть мы в принципе можем синхронизировать насильно эту систему, передавать не спайки, а числа, и мы получаем прекрасные платформы для эмуляции обычных нейросистей. и это все делается все более И вот упор делается на неограниченную масштабируемость, то есть упор делается на то, чтобы сделать из этого все более и более крупные системы, на которых будет строиться АГИИ. Ну вот, на самом деле, там имеется еще много подходов, о которых я не сказал, которые тоже могут быть чуть-чуть позже, правда, оказаться до этого решающими, например, мемристоры. Это то, что позволяет, не буду сейчас рассказывать, что это такое, отдельная тема, то, что, в принципе, может позволить сделать аналоговый синапс, чтобы обучение синапса, изменение веса синапса было не со своими проводцами, а аналоговым. Похоже на механизм СТДП, но чисто аналоговая вещь. Люди занимаются созданием кремниево-белковых систем. Я знаю Виктора Казанцева из Нижнего Новгорода. Он выращивает культуру нейронов на микросхемах, на кремниевых, устраивает между ними взаимодействие и показывает, что это может быть некой такой синергии, что это может образовывать некий единый эффективный ансамбль. В общем, это всего лишь такая фантастика, но вот этим люди тоже занимаются. Ну вот, наверное, такое общее резюме. Да, еще вот, если мы говорим, что это бионическое направление, то непонятно пока, где существенное, где несущественное. Тот же Константин Лохин настолько знает про мозг, что просто вообще непонятно, что из этого можно откинуть, а что принципиально важно. у нас в мозгу одних нейромедиаторов разных порядка десятков. причем каждый у них есть своя динамика. зачем столько – пока непонятно. и надо ли столько достаточно двух – тормозных и глутамат и гамма-аминомасляная кислота. Зачем нужно еще – не ясно. В общем, много-много всего, но чего из этого выбирать, чего не выбирать – непонятно. Это будет одним из вопросов развития этого направления, если говорить в сторону нейрофизиологии. то, что касается самоорганизации. у меня очень долго были исследования, касающиеся того, что я пытался в хаотических сетях ставить условия роста каких-то структур когнитивных. чтобы они сами за что-то организовывались, структурировались за счет пластичности. в общем, этот допустим, еще были такие вещи, связанные с генетическим алгоритмом, но не в терминах хромосом, описывающих параметры какой-то задачи или сети, а в терминах самих сетей. я беру две сети, грубо говоря, разрезаю их напополам, две половинки сшиваю, получается эти. То есть это я к тому, что мы пока не знаем не только, какие параметры должны быть университетами, но и какие структуры из них составлены. И может, мы просто таким образом эволюционным сами набредем на какие-то вот, за счет того, что вот тут, как всегда, игра случая, игра этого скрещивания этих мутаций, мы найдем какие-то структуры, которые вдруг оказываются очень ценными, а мы их просто не видели, потому что не досмотрели. Такой проект у меня тоже ведется. не так активен, но в общем я к этому еще вернусь. то есть вот тут можно ожидать самого разного. и вот мы сейчас в том плите, о котором я имею удовольствие относиться, брусатами, причем очень разными. это и железные вещи, и теоретические вещи. они здесь примерно тричисленные. и, как я сказал, мы имеем тоже в голове цель, ну, конечно, поскольку мы практически коммерческие, и весь движение в эту сторону, к сильному интеллекту, безусловно, у нас присутствует и будет, наверное, тоже развиваться. Ну вот еще маленький обзор, где этим еще занимаются. В общем, их не очень много. Детям занимаются, мембристами занимаются люди-поручатники, там и в Лативии, и в Санкт-Петербурге. Вот мотив нейротехнологии. Нервизиология – это Константин Мойхин с его институтом перспективных исследований мозга. Это все, кто этими вещами занимается. Ну, собственно, все. Я оставил где-то порядка получаса для разговоров, дискуссий и всего прочего. И поэтому с удовольствием отвечаю на вопросы и комментирую, и все остальное. 

S01 [01:11:51]  : Михаил, огромное спасибо. Для меня, на самом деле, за последнее время был самый интересный доклад. Я, правда, обнаружил, что многие вещи, которые вы говорили, я слышал раньше, но все равно это как-то очень полезно было. Системная прочистка мозгов на эту тему и плюс дополнительная информация. Вот. У нас, видимо, народу мало. Поэтому я не вижу большого числа вопросов. За исключением как от себя самого. Поэтому давайте так. Я правильно понимаю, что через полчаса вам уже убегать надо на паровоз? Да. 

S02 [01:12:23]  : Совершенно верно. Чуть даже пораньше. 

S01 [01:12:26]  : Хорошо. Поэтому давайте так. Без десяти. Без десяти надо бы уже все. Давайте я тогда начну со своих вопросов. А у кого еще вопросы есть, давайте свободно подключайтесь и в режиме прямого диалога задавайте вопросы и уточняйте. На часть вопросов вы уже ответили, поэтому я пойду с конца. Я правильно понимаю, что в принципе можно было бы строить архитектуру импульсных нейронных сетей. чтобы возможные связи не закладывались в архитектуру, а генерировались динамически. Как это происходит в голове? В голове же нейроны могут прорастать? 

S02 [01:13:10]  : Да, конечно. Причем, более того, тут имеется очень простой смысл этого именно структурного изменения. Вот он в чем. Например, если мы рассмотрим VTA-сети, верно, TXL-сети. Это сети, которые для unsupervised learning применяются. У них восходящие стимулирующие связи и латеральные тормозные связи. Если это достаточно большая сеть, то часто бывает так, что нейроны, которые победители часто бывают, они полностью подавляют остальные нейроны. Остальные нейроны молчат. Молчащие нейроны не пластичные. Так установлен закон пластичности, что он привязывается к моменту генерации спайка нейронов. Они бессмысленны, поэтому что делается? Эти нейроны, если они долго молчат, они уничтожаются и пересоздаются снова с новыми связями, поэтому у них появляется шанс, допустим, уже распознать какие-то меньшие по значимости коллиляции, которые упущены этими победителями. То есть мы видим структурную перестройку для того, чтобы у нас просто более рационально использовались вычислительные возможности в сети. И вообще самая разная необходимость для структурной перспективы, она есть, и она у меня использована. Все это сейчас эмулируется на моем пакете, который называется ArnieX для эмуляции инстаграммированных сетей. Там предусмотрены, возможно, динамические пересылки структуры, и выращивание новых связей, и пересоздание нейронов, и всего остального. Поэтому да, безусловно. 

S01 [01:14:39]  : А скажите, вот эта фича, она есть в каких-то пакетах, в реализациях, которые вы перечисляли, или это вот только экспериментальные эмуляторы это делают? 

S02 [01:14:51]  : Ну вот, если говорить о треновстве, он вообще совершенно жесткий. То есть там даже изменить вес нельзя, надо все перезагружать, поэтому там, конечно, нет. В лоихе, в общем, ну надо очень серьезно, то есть большая процедура, медленная процедура загрузки всего этого, то в тех экспериментах, которые поражены ВПУ, да, там это фактически делается на лету, то есть там нейроны переключаются и мигрируют на лету без каких-то сложных процедур. 

S01 [01:15:28]  : Понятно. То есть в идеале динамическая топология нужна, но в железной реализации такого пока нет. 

S02 [01:15:35]  : В железе пока это не поддержано, эффективно не поддержано. Понятно. 

S01 [01:15:41]  : А к вопросу аккумуляторов. Допустим я хочу попробовать в своих практических разработках категораться с реализацией каких-то алгоритмов на вот этих принципах, о которых вы рассказывали. Я могу взять какой-то пакет типа того, который вы назвали, и попробовать на нем, не имея у себя там ни TrueNorth, ни Loihi железя, или это все недоступно? Ну, конечно же. 

S02 [01:16:06]  : Я хотя имею доступ к Loihi, то есть я физически, поскольку наш университет входит, пока еще входит. в консервационном INRC, Intel Neuromorphic Research Community, то я имею доступ к логике, но, в общем-то, все равно, поскольку логики мало по всему миру, там очень жесткие ограничения по квотам, поэтому реально там ничего посчитать. Я, скорее, использую этот доступ для того, чтобы понять, как он у них устроен, чем реально что-то считать. Реально я считаю на GPU-шках и GPU-пластерах, которые мне предоставляет для этой цели и Росатом, и на Касперском мне возможности, конечно, в рамках их проектов это делать, но тем не менее. 

S01 [01:16:47]  : Ну а для этого какие-то, хорошо, пусть на GPU-шках можно играться, а какие-то для этого пакеты есть доступные или это все частное, в закрытом доступе? 

S02 [01:16:56]  : Пока я пользуюсь собственным пакетом, который я сам разработал, называется ArnieX. В принципе, это коммерческое приложение, но, в общем, на каких-то условиях сотрудничества можно рассмотреть вопрос, как его могут предоставить на каких-то условиях, не денежных, так скажем. Хотя это коммерческие продукты. И плюс к этому, сейчас мы делаем на Касперском библиотека, которая будет открыта. Это будет open-source, которая называется Касперский нейромаркет-платформ, КНП, и она будет открыта. Когда она будет выпущена, пока не берусь говорить, это как бы неизвестно, тайна военная. опенсорсовские пакеты, но они не позволяют... у них проблемы с эффективностью. То есть, это скорее исследовательская вещь, которая позволяет очень хорошо исследовать детально, что там происходит в ансамблях. Там и Станья Ронов, Бриана есть такой пакет известный, но он медленный достаточно. Сети пакета весьма медленные, они позволяют исследовать поведение небольших относительно импульсных сетей, но не решать большие проблемы. 

S01 [01:18:11]  : А тогда с практической точки зрения, если я правильно понял, если хочется поэкспериментировать, то можно связаться с вами напрямую и либо для академических целей получить ваш пакет, либо для коммерческих целей его приобрести, либо подождать, когда выйдет open source от Касперского. 

S00 [01:18:26]  : Да. Понятно. 

S01 [01:18:28]  : Спасибо. Кстати, а вы со Сбером как-то работаете? Нет, со Сбером мы никак не работаем. Понятно. А знаете, Сбер что-то в этой области делает? 

S02 [01:18:39]  : Я знаю, что у них недавно стали заниматься импульсными нейронными частями. Там появился некий человек, который этим занимается. не имели вообще, в принципе. Сейчас они что-то такое там делают, но что они там делают, я пока не могу сказать, я не знаю. 

S01 [01:18:59]  : Да, интересно, потому что жалко, что с вами не встретились года три назад, когда начали писать эту книжку зеленую про сильный искусственный интеллект Сберовскую. Там, по-моему, как раз тема импульсных нейронных сетей не была затронута. Да, я видел ее, по-моему. Нет, там не была. Да, это, видимо, упущение. Хорошо. Следующий вопрос. Лоихи и прочее где-то в промышленности сейчас используются за рамками ресечки? 

S02 [01:19:29]  : То есть есть какие-то… Турновцы используются, а лоихи пока нет. Пока это ресерверская штука, но вот взят курс уже на коммерциализацию, и вот сейчас ведутся… Во-первых, в робототехнике уже создаются какие-то самообучаемые, самоуправляемые дроны, гражданские. То есть турновцы используются в военной индустрии. И вот есть в Мюнхене и Цюрихе группа Интелла, которую возглавляет Юлия Сандомирская. К сожалению, мы ее приглашали на нашу конференцию, но она не смогла приехать. Неожиданно, что она хотела, но буквально за день до конференции сказала, что она не может резко. И вот первые индустриальные приложения буквально, я думаю, в течение следующего года появятся. Airbus очень много денег вкладывает именно в логики для своих применений, так что да, скоро будет. 

S01 [01:20:27]  : Я правильно понимаю, что основное применение – это кейсы классического IML, но с гораздо большей эффективностью вычислительной? 

S02 [01:20:38]  : Да, то есть первое, что делается, первый плод, который легко взять, это в тысячу раз меньше энергопотребления. То есть все то же самое, только в тысячу раз меньше по энергии. 

S01 [01:20:49]  : То есть тоже самое и машинное зрение, но гораздо более эффективное, которое можно затолкать в гораздо меньше железа с гораздо меньшей батарейкой. 

S02 [01:20:57]  : Да, без связи с сервером, то есть все распознавание и обучение распознаванию прямо на кристалле, от маленького аккумулятора без трафика, без посылки чувствительных данных, занимающих много мегабайт, куда-то там вдаль. Все локально. И это, конечно, большой плюс. дроны летают, и все остальное. Следующий этап – это обучение на чипе, чисто импульсные фичи, которые будут имплементированы. 

S01 [01:21:30]  : Но в случае TrueNorth вы сказали, что обучения нет, если я правильно понял. Там обучаться не получится, там только распознавание будет, но все равно более эффективное. 

S02 [01:21:39]  : Но инференс, он тоже занимается энергией. 

S01 [01:21:46]  : По поводу бревовидных синаптических структур, которые, как я понял, только у ЛОИХ есть, здесь два вопроса. Во-первых, занимаетесь ли вы вот этими исследованиями в этой области, как раз бревовидными синаптическими структурами? И второй вопрос, а есть ли понимание, как возможно обучение? в рамках этой структуры. То есть, как делается обучение классической нейронной и импульсной нейронной сети вы рассказали, а с древоидной структурой как обучать? 

S02 [01:22:17]  : Должен сказать, что я не знаю. Нет, то есть я чувствую, что это очень богатая фича, что она приносит некое совершенно новое измерение во всю эту механику. Но я пока не знаю, как ее использовать, не использую. Это некая такая штука, которая потенциально мощная, но непонятно, что с ней делать. 

S01 [01:22:40]  : Перед тем, как Вы это сказали, я как раз набивал вопрос, что есть же действительно данные, в том числе в Nature, публикации последних лет, что оказывается много интересного происходит именно в этих древовидных структурах, а потом же там у нас еще действительно нейромедиаторов, как Вы много сказали. Но, если я правильно понял, в вашем понимании пока нет понимания действительно, что мы можем отбросить, а что важно. И, как я понимаю, разные нейромедиаторы сейчас стопудово никем не используются, только в зачатке к исследованию как раз древовидных структур. 

S02 [01:23:16]  : Спасибо. Допаминовая тема. Допамин тоже как отдельная, с вполне внятными целями применяющаяся. Это один из способов. reinforcement learning основан именно на подкреплении, которое мыслится как констрация допомина. поэтому в моем подходе, кстати, я использую даже не констрацию допомина, а у меня просто reinforcement считается тоже спайкой которые имеют семантику… структура сети такова, что придает им семантику сигналов прощения или наказания через особые свойства синусов. Конечно, все равно о моделировании десятков нейромедиаторов речь не идет. Понятно. 

S01 [01:24:08]  : То есть два. Один основной, а второй подкрепляющий. Спасибо. 

S02 [01:24:13]  : Тормозной. У меня три синапса, грубо говоря. Это внуждающий, тормозной и подкрепляющий. 

S01 [01:24:19]  : Три типа синапсов. Да. Спасибо. Следующий вопрос. А наказывающего нет? 

S02 [01:24:25]  : Это то же самое, что просто там обратный знак законов пластичности. Он просто ведет к подавлению определенных весов, а не к их увеличению. А так механика в сущности та же самая. Кстати, это имплементировано в лоуихе физически. которые имеют три разные роли в Камчатках. 

S01 [01:24:43]  : Спасибо. Следующий практический вопрос, вы на него частично ответили, то есть есть open source, будет точнее возможно, а это реализация CC++, если я правильно понимаю, да? 

S02 [01:24:57]  : Ну, совсем, даже еще более интересно, то есть все написано на C++, но в типовых случаях это не требует программирования, то есть то, что описывается на XML, просто, ну, некие там, описываются какие есть популяции нейронов, как они между собой связаны, это в типовых случаях вообще программирование не требует. Если говорить о моем пакете, это RNEX. В более сложном случае предоставляется C++ API, который позволяет детально 

S01 [01:25:25]  : угодно точно. А какие-то байдинги для Python, для Java? 

S02 [01:25:31]  : Нет, я не делал этого. Я считаю, что у меня есть два крайних, либо совсем декларативное описание, которое для удобства пользования, либо совсем детальное, высокоэффективное описание, то требуется C++ на мой интерфейс. 

S01 [01:25:43]  : А раз уж вы XML считаете удобным декларативным описанием, а не было соображений более дружественное что-то, чем XML, что-нибудь JSON-подобное, к примеру? 

S02 [01:25:54]  : Не, ну можно было бы сделать визуальный редактор этого всего, тогда это было бы вообще уже на уровне картинок и просто операция с мышью. Но это все-таки требует времени, а я одним этим занимаюсь вот так вот по большому счету. 

S01 [01:26:09]  : В общем, требует времени. Спасибо. Спасибо. Еще какой комментарий. У вас ИНС, если я правильно понимаю, в контексте вашей презентации ИНС – это импульсные нейронные сети. Аббревиатура ИНС часто используется как искусственные нейронные сети. 

S02 [01:26:30]  : Я извиняюсь, в нашей дискуссии я всюду стараюсь но поскольку это была презентация, которая сделана на основе другой презентации, и в нашем коллективе я прошу прощения. 

S01 [01:26:44]  : Спасибо. Еще поскольку других вопросов нет, я продолжаю. 

S00 [01:26:53]  : Можно мешаться? Да, давайте. Михаил, я по поводу замечания на тему C++. Чисто как просто комментарий. На самом деле сейчас такая сложилась тенденция, что действительно питоновские биндинги люди хотят видеть, чтобы пользоваться. Прямо очень хотят. Не хотят они в C++ копаться с одной стороны, с другой стороны. Им скучно конфигурировать XML. Я сам на это наткнулся. То есть у меня такая же позиция, как у вас примерно. Она работает плохо. 

S02 [01:27:25]  : Там будут питоновские биндинги. 

S01 [01:27:29]  : Как вы говорите, все будет хорошо. То есть, вы с Касперским на тему питоновских биндингов работаете? Да, я участвую в создании их библиотеки. Прекрасно. Здорово. Спасибо. Дальше следующий вопрос у меня был. Вы про ВТА сказали. Есть такая гипотеза. Я ее даже в группе высказывал. Не знаю, видели ее или нет. В чем плюс систем? Не обязательно это импульсные нейронные сети. Это могут быть другие архитектуры. Но в чем я вижу один из плюсов. Может быть, это не единственный, но их наверняка много. Когда мы решаем сложные когнитивные задачи, там есть проблема размерности и числа возможных вариантов, которые в общем случае решаются только через brute force. Если мы пытаемся решать задачи такой размерности на нейронных сетях, синхронных, то совершенно правильно мы утыкаемся во время вычислительное и вычислительные затраты. А если мы реализуем это на асинхронной системе, с массивной параллельностью, то мы можем, по сути дела, реализовывать параллельный brute force, который по winner takes all прерывается, и мы, соответственно, как только мы получаем приемлемое решение, мы перестараем перебирать все остальные ветки. Я правильно понимаю, что вот эта история – это одна из фишек, собственно, которую дают ИНС? 

S02 [01:28:53]  : Да, да, именно в общем-то так. Та часть – это совсем далекое проведение этой мысли – это квантовое вычисление, где у нас реализуются не явно все схемы, но потом из них одна проецируется. Это основная мысль, что у нас параллельность совсем глубокого уровня. Но в выпускных сетях тоже такая же. У нас много параллельного всего. Первое находится что-то интересное, оно выигрывается, а остальное подавляется. 

S01 [01:29:19]  : Хорошо. А, кстати, вот вы Ежекевича упоминали. Вы с ним сотрудничали как-то или нет? Или просто на конференции? Я просто лично его знаю. 

S02 [01:29:27]  : Я ездил к нему в гости, когда он организовал свою Brain Corporation в Сан-Диего. Я там неделю провел, пытаясь понять, что он делает, как, чего. Но я бы не сказал, что… Сейчас он не занимается практически наукой. При том, что он был один из самых корифеев этого всего и один из самых известных ученых, он потом ушел в бизнес резко, связанный с построением роботов и операционных систем для роботов, но совершенно не импульсного характера обычного. И настолько, видимо, там ему все стало хорошо, что он совсем покинул науку. Он не публикуется ничего там давно уже. 

S01 [01:30:03]  : Понятно. А вы говорили про Алтай и сейчас говорите, что вы делаете для Касперского. Это как-то связано или это разные направления? 

S02 [01:30:13]  : Да, связано. Касперский финансирует Алтай как железо, а вы с ними работаете как софт. Ну да, я вообще ничего не понимаю. То, что я сейчас говорил про все эти процессы, я в этом не разбираюсь абсолютно, это чисто внешнее описание. Я занимаюсь теорией, выработкой спецификации для этих чипов. Некие хотелки, что бы хотелось видеть в функционале этих чипов, но никак не зарегистрировался. 

S01 [01:30:46]  : То есть то, что вы эмулируете на обычных же пулушках, потом предполагается, успешный результат ваших экспериментов предполагается к переносу на Алтай. Да, в общем-то да. Здорово. Ну и, кстати, тогда уж, если я правильно понимаю, Значит, у нас импортозамещение идет с оставанием в этой области, если в Лоихе так все хорошо, а Алтай где между Лоихе и Трунурсом примерно? 

S02 [01:31:15]  : Ну Алтай это все-таки похоже на Трунурс, он даже не между пока Лоихе и Трунурсом, это просто наш российский Трунурс. нет, он чуть получше, там с весами все не так плохо, они тоже не пластичные, но они, по крайней мере, там больше бит для веса, поэтому там чуть получше, но, в общем, пока мы хотим, чтобы это было лоихи, но это пока не лоихи совсем. 

S01 [01:31:39]  : но есть какие-то надежды? 

S02 [01:31:42]  : да, да, мы сейчас работаем над новым поколением 

S01 [01:31:47]  : все здорово так ну дальше значит вопрос а вот значит дальше здесь уже пара вопросов как бы совсем базовых по воду вот смотрите значит вы там значит на втором слайде вы говорили значит что вот про несколько Есть подходов, вы исторически разворачивали. Что там есть у нас биологические модели, которые слишком простые. Потом есть небиологические модели, которые тоже затыкаются. Потом у нас есть искусственные нейронные сети, которые на самом деле искусственные и нисколько не нейроморфные. А потом сразу переходите на импульсные нейронные сети, которые более нейроморфные и при этом имеют структурную сложность. Вопрос. Есть же сейчас подход, который развивается в ряде западных проектов. У нас коллеги во главе с Витяевым этот проект развивают. строим все-таки системы на семантических сетях или на графах, по образу второго пункта вашей презентации, как бы экспертные системы, но при этом все веса на этих графах, они вероятностные, там разные формулы вероятностные используются, то есть мы используем вероятностные сети, но используем их на семантических графах. Что вы думаете по поводу этих подходов безотносительно к тому, что вы делаете? 

S02 [01:33:22]  : Ну, я думаю, что это полезная вещь. Ну, я сам еще некоторое время занимался капитальной лингвистикой. И вот такими глубокими методами, именно связанными с пониманием предложения, то не только парсинг предложения, но и некое семантическое, так сказать, обработка, семантическое некое представление. Как прикладные штуки, это, да, очень классно и хорошо, все-таки у меня пока это отрыв от… где-то потом, когда мы будем уже оперировать многодесяткомиллионными сетями, там, наверное, эти идеи как-то сольются, мы начнем понимать, как вот эти вот сети автоматически проецируются на какие-то нейронные структуры, там уже можно будет… а пока это здорово, но пока вот этот гэп, который имеется, он не дает невозможности 

S01 [01:34:20]  : А вот тогда, наверное, уже последний вопрос. Смотрите, я правильно понимаю ваше последнее высказывание, что если мы строим базовые системы на вот этих нероморных принципах, которые вы рассказали, для реализации ассоциативного, так сказать, мышления, то потом, на базе вот этих ассоциативных нейроморфных методов, мы можем уже строить методы логического, высокоуровневого семантического мышления, как система 1.2 или система 1 у Канемана. Я все время путаю, какая первая, какая вторая. Когда мы мыслим понятиями, логическими связями и концепциями... Да, там что-то такое уже образуется, конечно. А вот вы как-то видите, как может вот эта семантическая, понятийная, символьная логика зиждаться на вот этих спайках, бегающих спайках на уровне фундамента? 

S02 [01:35:22]  : пока до этого слишком далеко, я, конечно, таких вещей не вижу, но, допустим, нечто похожее на это зачаточное, я, допустим, сейчас занимаюсь model-based reinforcement learning, то есть у нас были работы по model-free reinforcement learning, у нас все получилось, но это просто стимул реакции, это все хорошо, все спайковое, все работает, наводилась у меня камера на некий объект, держала его чисто в reinforcement понимании. Сейчас мы делаем model-based. Система должна строить некие внутренние представления правил внешнего мира. Должны быть какие-то понятия, допустим, пинг-понга. Будет понятие, что шарик движется на правильную ракетку. Это примитивное обобщение. Из них должны будут выстраиваться некие схемы, чтобы было элементарное планирование. Все равно уже некая семантика чуть-чуть здесь появляется. Ну и мне сейчас приходится делать некие нейростивые структуры, которые могли бы формализовывать эти вещи, выделять эти понятия, взаимосвязи между ними, их оперативно применять для планирования и так далее. То есть эти задачи мне уже приходится в зачетном понимании решать. Ну, конечно, как там будет в терминах понятия, на это у нас будет как-то отвечено. 

S01 [01:36:43]  : Огромное спасибо. Вот по последнему поводу, чтобы уж подвести итог, если вам потребуются бета-тестировщики вашего open-source проекта с одной стороны, а с другой стороны как-то повзаимодействовать на тему того, как из низкоуровневых структур формируется высокоуровневая, то было бы интересно с вами посотрудничать в обозримом будущем. 

S02 [01:37:05]  : Павел Бенцовский, я понимаю, тут полностью рулят люди из Касперского. Это надо с ними. Я тут один из исполнителей, не более чем. 

S01 [01:37:14]  : Ну, хорошо. Ладно. В любом случае, огромное спасибо вам за доклад. Было очень интересно. Спасибо и счастливо вам съездить. Все хорошо. Был очень рад вам. Коллеги, всем спасибо за участие. До свидания. 

S02 [01:37:29]  : Да. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
