## 11 августа - Этика AGI — насколько это важный вопрос? — Сергей Карелов и Игорь Пивоваров (уточняется) — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/GgUaeTVX4T0/hqdefault.jpg)](https://youtu.be/GgUaeTVX4T0)

Суммаризация семинара:

Семинар был посвящен дискуссии по теме этики искусственного интеллекта. В рамках мероприятия были затронуты вопросы, связанные с разработкой и использованием ИИ, а также темы, касающиеся этических систем в США и Китае и их влияния на мировую науку и политику. Особое внимание было уделено техническим аспектам этики, таким как использование данных без уведомления и другие моменты, которые могут вызвать этические споры.

На семинаре были заданы вопросы о том, как создание сильного искусственного интеллекта (AI) может повлиять на общество и какое влияние может оказать экзистенциальная опасность, связанная с данными технологиями. Участники обсудили, что нужно обсуждать эти вопросы как в научном сообществе, так и в общественных дискуссиях, и как важно продумывать культурные аспекты этики искусственного интеллекта.

Еще одна важная тема, поднятая на семинаре, это вопрос экзистенциального использования технологий. Участники высказались за необходимость обсуждения этики в контексте использования ИИ для решения задач в общественной жизни, а не просто для исследования мозга или создания технологий ради технологий.

В заключение семинара обсуждалась необходимость обучения общества и специалистов этическим принципам работы с искусственным интеллектом. Участники подчеркивали важность развития этического образования и курсов по этике ИИ.

Стоит отметить, что многие идеи, обсуждаемые на семинаре, были навеяны такими мыслителями, как Рэй Курцвилл, чьи работы влияют на современные дискуссии о будущем искусственного интеллекта и его потенциальном влиянии на человечество.







S04 [00:00:00]  : Антон, спасибо. Все сообщения есть, отлично. Ну что, друзья, всем добрый вечер. У нас сегодня зажигательный, ожидается, семинар совершенно. Мы проводим в рамках нашего сообщества IJI Russia и давно назревала необходимость, ну и там разные разговоры по поводу этики, эта тема такая немножко хайповая понемногу кто хочет высказаться сегодня у нас будет такой специальный особенный такой заход в тему и на него навел как раз своим просто одним из последних постов сергей карелов и с которым мы сегодня поразговариваем про этические системы, ну, про отчет, собственно говоря, на Флориде, про этические системы США и Китая, и как это повлияет. А с другой стороны, хотелось бы немножко более про технические вещи, это уже как бы аспекта темы поговорить, и поэтому у нас сегодня Сергей Карелов и Константин Воронцов. Константин Вячеславович один из самых известных, он входит в топ-20 специалистов по машинлейнингу в стране и занимается NLP моделями уже давно. Сергей, добрый вечер. Добрый вечер. Всем добрый вечер. Константин, добрый вечер. Добрый вечер. отлично так но я виноват я хотел сделать свайды к этому семинару но не сделал поэтому я сделаю маленькая коротенькая вводную просто словами для тех кто только входит в тему лика или скажем не некое понимание этика искусственного интеллекта некоторые аксюморон потому как этика это философской дисциплины изучающая морали нравственность человека мораль человека то каким образом он принимает те или иные решения а так как искусственный интеллект пока особенных никаких решений принимает вроде и субъекта там никакого нет вроде как и ну никакой морали там нет и говорить об этике напрямую этого агента мы не можем и сегодня говоря про этику мы в первую очередь говорим про этику разработчик и пользователя в этой связи что касается разработчиков человек который разрабатывает систему и пользователя там есть как бы технические аспекты этической этики ну условно например разработчик там взял не думая там где-то дата сеты или там использовал персональные данные особенно не там не уведомляя никого и такие моменты достаточно ну такие я их называю для себя техническими, а есть моменты такие ну прям моральные, когда разработчик делает ну допустим там систему для распознавания лиц или там систему для управления там автономным летальным оружием и он понимает он что он делает или не понимает он как бы осознает для чего он это делает или не осознает и этика пользователя тоже там есть технические аспекты но нас интересует больше именно моральный аспект когда пользователь то есть тот заказчик там человек или группа людей они используют технологии там для того ли друг для другого и вот собственно там луча на флориде в своем отчете и благодаря сергею карелу мы это сейчас широко как бы знаем в русскоговорящем сообществе высветил проблему про разную совершенно разные этические ценности и разных сообществ, а в первую очередь как бы двух глобальных лидеров США и Китая и вот давайте мы с этого начнем потому что от этой этики и разговор начнем с этого как бы с этики в каком-то смысле разработчики разработчиков пользователей сегодняшних и от них мы потом чуть дальше перейдем и вернемся обратно к этике возможного и джай который будет создан и насколько это повлияет или не повлияет в общем вот такой заход но начну я к сергея корелова из его этого замечательного поста и вообще я большой поклонник сергею ваших постов несмотря то что я их регулярно критикую там в комментариях и это понятно да но это я думаю позволяет только дискуссии быть ярче и разнообразнее и вот этот меня лично давно задевает что в нашем русскоговорящем сообществе но как бы реально острого дискурса мало в особенности по поводу этики вроде как все понимают про этику очень многие сейчас про это говорят и говорят какие-то такие банальные вещи а тема на самом деле очень острая такая живая и яркая и вот этот пост который вы написали он мне кажется высвечивает одну из главных этических вообще дилемм и сегодняшнего сегодняшнего сегодняшнего отрасли искусственного интеллекта расскажите про эту дилемму в чем она для тех кто там не читал и что по этому поводу думать но сначала для тех кто не читал я коротко напомню что почти говорилось о том что по сути дела делается 2 и 

S02 [00:05:10]  : искусственных интеллектов сейчас в мире, грубо говоря, один американский в кавычках, второй китайский. И в первую очередь они отличаются именно тем, что мы сейчас весьма условно называем этикой. для того чтобы понять что такое эта самая этика мне сейчас придется сделать одно еще отступление но прежде я все-таки закончу с этим постом лучана пишет что очень большой объем документов они подняли работ научных и практика в обсуждениях участвовал там порядка 50 человек в общем видно, что Китай и Америка, а под Америкой я понимаю и Европу и весь западный мир, делают разные интеллекты, которые отличаются этическими основами людей, которые отвечают за разработку, за внедрение, легальные аспекты и так далее. У китайцев все ориентировано на пользу всего общества. Если искусственный интеллект так или иначе этически ущемит каких-либо индивидуала в чем-либо, ну так тому и быть, потому как лишь бы была польза обществу. И таким образом все системы связаны с наблюдением, распознаванием, с применением юридических консультантов на базе искусственного интеллекта и так далее и так далее, они все исходят из этой презумпции, что главное, чтобы было о пользу обществу, а уж что там до конкретного человека, пусть, как говорит, сидит, помалкивает, подравнивает под все. западно-протестантская этика, американская, да и не только протестантская, на самом деле и в католицизме, хоть там и есть и у нас в православии некое общинное начало, но человек все равно остается главной ценностью и, собственно говоря, искусственный интеллект, который разрабатывается, он разрабатывается в первую очередь с позиции тех этических подходов, когда ну как человек, правилость и конституциальность, вопросы добра и зла с позиции конкретного индивида. Вот два разных подхода и делаются два искусственных интеллекта. но дело в том что и за этой статьей в флориде и я довольно много с ним обсуждал этот вопрос и уже и до того как он это опубликовал и после понимаете какая вещь с понятием искусственного интеллекта у нас уже сложилась большая путаница. все ее признают, но как-то привыкли уже, смирились. все понимают, что интеллект человеческого уровня это одно, а те системы, которые сейчас разрабатываются, эксплуатируются за другое, некий сверхинтеллект, который когда-то возможно будет, теоретически может и будет в будущем, это третье, но тем не менее, как все смирилища с этим живут. вкладывая в понятие искусственный интеллект, ну, то свое, что прагматично является предметом обсуждения. С этикой получилось еще хуже, с этикой искусственного интеллекта. Потому что, на самом деле, понятие этики искусственного интеллекта, оно существует в двух совершенно не пересекающихся плоскостях и подставе. Первая — это этика искусственного интеллекта будущего, неважно какого будет интеллект человекоподобный или вообще сверхчеловеческий интеллект, но это некий интеллект будущего, который мы пока не знаем как сделать, и обсуждать этику вот того самого интеллекта искусственного будущего бессмысленно, потому что мы не знаем, как его делать, а в зависимости от того, как его будут делать, то тут и вопросы этики будут встраиваться как-то так или иначе. поэтому нужно четко говорить, что мы говорим о этике искусственного интеллекта настоящего, того, который есть сейчас, тот, который разрабатывается сейчас, который появится в ближайшие годы и, наверное, скорее всего, в ближайшие пару десятилетий. Это означает, что мы обсуждаем искусственный интеллект как инструмент и как средство для автоматизации механизм, средство и механизм для принятия решений. Вот, собственно говоря, два круга задач, которые очерчивают абсолютно все применения, ну почти все, 99 процентов применений. случае, если мы говорим о этике искусственного интеллекта как инструмента, то какой можно привести пример? ну есть вот системы, которые позволяют, можно купить даже по интернету, которые позволяют подслушивать. подслушивать, вообще находясь за пределами комнаты, здания, направил соответствующие подслушивать. это инструмент, хороший инструмент, а этика его использования регулируется, во-первых, этическими соображениями, но это как-то так нехорошо, чтобы отслушивать, но на самом-то деле она регулируется законами, регламентами, и если вы вообще по-хорошему такое шпионское оборудование его не каждому должно продаваться, не каждый имеет право его использовать, не каждый имеет право пользоваться результатами этого подслушивания, то есть целая выстраивается большая-большая-большая схема, которая включает в себя с одной стороны некие процедуры, регламенты политики, с другой стороны вполне конкретные законы и практики, таким образом регламенты процедуры политики с другой стороны законы и практики но вот вам как инструмент это дело хитро регулироваться должно и это означает что любое применение искусственного интеллекта инструмента с точки зрения этики применения должно аналогичным образом регулироваться с другой стороны второе применение искусственного интеллекта это для принятия решений как средство автоматизации принятия решений вплоть до вот полной автоматизации в общем-то здесь понятно что это будет распространяться далеко не на всякие решения Решение о том, что достаточно воды в сливной бачок наливать, система автоматизации принимает без человека, и мы как-то не ставим вопрос, насколько этично применяет этот поплавок, останавливает эту воду. Когда же вопрос идёт, скажем, об автономном оружии, яркий пример, всем понятный и актуальный. то встает вопрос, опять же, не об этике уже инструмента этого, нет, вопрос совершенно другой. Здесь встает вопрос об этичности передачи машине вопросов, которые должны решаться людьми. относятся далеко не только к автономному оружию, здесь куча других применений. Начиная от применения, а выдача в конце концов кредита или не выдача кредита? Мы передаем это решение машине или нет? А решение врача? мы передаем это решение, либо все-таки врач определяет человек. Таким образом, еще раз резюмирую и подхожу к концу своего уточнения, которое, в общем-то, ни у меня, ни у Флориды не прописано, а нам очень хотелось бы здесь, благо мы решили копать поглубже, хотелось бы всем понимать. и таким образом мы не говорим об этике искусственного интеллекта будущего по одной простой причине, потому что мы пока не знаем как его делать и в зависимости от того как он будет делать тогда там и так и будет ставится вопрос. мы говорим об искусственном интеллекте настоящего и ближних десятилетий и это значит мы говорим о этике инструментов, которые мы используем, искусственный интеллект как инструмент. и это означает, что здесь должны быть политики, процедуры, регламенты, а также законы и практики. и мы говорим, второе, о этике вопроса передачи машине, этично ли передать машине принятие этого решения. после того, как это будет принято решение, когда будет сказано, да, мы передаем это машине. то есть вопрос, последний пример, самоуправляемому автомобилю вопрос не в том, как сделать искусственный интеллект автомобиля этичным, он не может быть этичным, он у нас используется здесь, инструмент мы решаем вопросы, опять же, разрешаем выпустить такси на улице, это определяется регламентами политики, или не разрешаем, разрешаем пользоваться только на таких трассах, или только на других трассах, на закрытых, на открытых, а вот вопрос полной передачи принятия решения, то есть в системе, когда, например, самоуправляемый авто будет решать, кого задавить, бабушку или дедушку. в этом этично ли вообще это передать? то есть если мы приходим к решению, что это неэтично, то это означает, что все, тут ставится большой крест, это означает, что либо это приложение полной автоматизации не выпускается в практику, либо выпускается в ограниченное применение там, где это разрешено. И из этого следует всего, что я сказал, всего лишь одно уточнение, но, на мой взгляд, важнейшее уточнение к тому, что сказал Игорь. Вы, Игорь, сказали, что этот вопрос сейчас в первую очередь касается вопрос этих искусственного интеллекта, в первую очередь вопрос разработчиков и пользователей. Я бы это дело существенно расширил и в Флориде постоянно об этом говорят. Это в первую очередь вопрос, это вопрос общества, гражданского общества, вопрос власти, государства, вопрос сообществ научных и разработчиков и так далее. для того чтобы решить какой класс задачи этической решает и этика и как инструмента или этика и как принятие автоматизации человеческого решения и дальше подвергнуться создание всех этих регламентов процедур и полисе законов и практик – это задача далеко не только разработчиков и пользователей, а это задача вот такого куда более широкого круга ответственных людей, специалистов и граждан. но без этого совершенно нельзя, потому что в противном случае будет нарушаться наша этика, чисто человеческая этика общества, этика людей. 

S04 [00:17:35]  : именно она сейчас является предметом главным для обсуждения отлично отлично сергей спасибо огромное я хочу заострить первую часть вашего темы и вы привели это государством и говорили что эти как инструмент должна регулироваться регулируется искусственный интеллект как инструмент регулируется там и этическими соображениями и как бы законодательством регулирования законодательство это как раз то что условно это функция государства то что выпускает государство а мы сейчас видим что вот эти два ну как бы не то что два государства 2 ну как бы два лидирующих государства разрабатывающих применяющих и у них как раз это этика сильно разная и например на примере распознавания лиц если в америке идет там бурное обсуждение можно нельзя какие-то штаты запрещают потому что свобода человека важна частная жизнь там не не не этично следить за всеми то в китае вопрос даже не ставится но как бы массовое распознавание лиц на улицах потому что в интересах государства и в этом плане государство как бы когда оно начинает это регулировать и государство это ведь тоже люди со своими там интересами со своей со своей этикой и в каком-то смысле мы действительно видим как вот этот мир и это разработческая пользовательская среда стремительно разваливается на два больших лагеря для одних это этично а для других это не этично вернее так для одних этично одно а для других этично другой я бы так вот так вот так надо правильно сказать 

S02 [00:19:11]  : Игорь, все так, но именно в этом-то и состоит суть темы, которую Лучана подняла. Я здесь пытаюсь проповедовать важность этого. Вы абсолютно правы, так как речь идет о том, как люди поставят рамки какие этические рамки будут поставлены для инструментов, и какие этические рамки будут поставлены в вопрос автоматизации принятия решений. Поскольку это вопрос абсолютно регламентируется только обществом и властью, а вовсе не разработчиками. Разработчики здесь ничего не могут сделать. Они все равно будут подстраиваться под то, как говорится, власть. если в штатах принято законодательно что можно носить оружие можно носить оружие можно взять мастер можно купить автомат и все разрешено это а у нас это не разрешено понимаете вот и будет ровно то же самое поскольку китайский и американский я в кавычках это называю но все уже понимают что это вот условно это два абсолютно разных направления они вот сейчас будут расходиться как инструменты как и как способ принятия автоматизации решений они расходятся ножницы с каждым годом чем более совершений становится разработка тем больше расстояния между этими двумя ножницами но дальше о чем говорит лучана он правильно говорит, что скорее всего не будет двух. в какой-то момент все равно останется один. в какой-то момент все равно останется один. 

S04 [00:21:00]  : почему обязательно должно остаться один? 

S02 [00:21:04]  : ну понимаете, какая штука? это уже долгий разговор на который вряд ли у нас сейчас есть время это надо уходить в сторону дело в том что технологические нет примера в истории двух технологических направлений, когда одна и та же... ведь речь идет об горизонтальном букете технологий, кое является искусственный интеллект. если бы речь шла о конкретной вертикальной технологии, там как бурить нефть, можно бурить так, можно горизонтально, можно с водой, можно без закачки воды, сделать по-разному. когда речь идет об электричестве, то единственное, что может отличаться, это розетки. да, еще где-то может быть 220, а где-то 127. все, понимаете. когда идет о ядерной энергии разговор, да, можно по-разному там сделать реактор ввр-440, а можно сделать еще там другому, но все равно в горизонтальных технологиях неизвестны примеры. когда бы в какой-то момент они могут развиваться параллельно, но все равно потом побеждает одна. это опять же вот не мои аргументы и даже не аргументы лучана, просто этот вопрос прорабатывался. весьма узнавательно, и если это так, то основной посыл Флориде в том, что давайте сделаем так, чтобы победил западный. потому что он, по крайней мере, вот на этой протестантской... потому что он оксфордский философ. да, потому как если победит китайский вариант, то это будет вот тот самый большой брат, сын большого бога, про который я начал писать даже раньше, чем лучада. 

S04 [00:23:07]  : но на эту тему мне понравилось как сказал один мудрый человек кто мы такие чтобы судить что эти 400 миллионов правы а те полтора миллиарда ошибаются да я лично не готов пожалуй судить но мне интересно вот мы когда с кстати начала очень обсуждали значит вот этот семинар то у него было ощущение что он может третье какое-то видение альтернативное сформулировать, прежде чем мы перейдем к вопросу, коллеги, у нас мы должны Сергея там отпустить чуть-чуть раньше, поэтому мы сейчас комментарии от Константина Вячеславовича, если будет ответ, и потом мы возьмем несколько ответов на вопрос. Если у вас есть вопросы, Сергею Карелову пишите в чат. Константин Вячеславович, что за третье видение альтернативное ты хотел предложить? 

S00 [00:23:54]  : Да, коллеги, я к нему не сразу подойду. Я хочу сначала несколько уточняющих таких фраз произнести. Мне кажется, что вот эта дихотомия, вот эти ножницы, о которых Сергей говорил, они возникли отнюдь не в связи с искусственным интеллектом. Это вообще дихотомия восток-запад. Эта дихотомия в первую очередь коллективизм или индивидуализм. И причем мы запутываемся все время в этих терминах, и на Западе очень часто индивидуализму противопоставляют альтруизм. Это неправильно. Альтруист делает для другого, а коллективист делает для нашего коллектива, для нашей группы. И похоже, что на философском уровне Запад недопонимает смысл коллективизма. И он все время скатывается то в противопоставление с эльтруизмом, то ему кажется, что коллективизм – это однозначно тирания и автократия. Это не так. И вот этот третий путь, он именно увести в сторону от искусственного противопоставления индивидуализма и автократии в сторону более правильную, более цивилизованную, Развитие цивилизации невозможно на текущем уровне технологий разрушительных, мощных, если оно не будет коллективным. То есть в этом смысле я за Восток. Как ни странно, я за коллективизм. Я считаю, что и у индивидуализма нет будущего. просто потому, что настолько мощными стали наши технологии, а индивидуализм настолько поощряет всё животное в нас, а животное в нас стремится к власти, а власть всегда и будет использовать эти технологии для того, чтобы доминировать над сородичами, всё животное в нас. И мы так и будем ходить по этому кругу, пока не самоуничтожимся. Это если говорить о как бы философии с точки зрения того, куда идет цивилизация. Цивилизация неизбежно придет к коллективизму, но он совершенно не обязательно связан с тиранией или автократией. Вот это первое замечание. Второе замечание. Я считаю, что применение глаголов активного действия к искусственному интеллекту уже неэтично. Мы привыкли всей... И мы, специалисты, идем на поводу у журналистов, куда это годится, это кошмар. То есть когда говорят, что искусственный интеллект впервые в истории человечества убил человека... Помните, эта ситуация была, да, там? Какое-то там турецкое, кажется, автономное оружие, где-то там, кажется, в Ливии кого-то там убило. Из этого сделали хайп, но мы с вами уже перестали замечать, как говорят, что искусственный интеллект победил чемпиона мира. искусственный интеллект чего-то там сделал, глаголы активного действия, да, да не бывает такого, потому что до сих пор и еще долго активные действия волеизъявления, да, это будет оставаться только человеческим свойством, значит, это пользователи этого искусственного интеллекта сделали, это офицер отдал приказ использовать автономную систему для того, что вот Автономная система прошла все тесты, она способна убивать солдат противника с точностью, превышающей человеческой, и тогда ее можно использовать на поле боя. Но приказ о ее использовании отдает конкретный офицер. Ну, так, наверное, это происходит, да, то есть говорить, что ИИ что-то сам сделал, это уже неэтично. И давайте, коллеги, ну, я понимаю, что это глаз выпивающего в пустыне, это бессмысленные слова, но я бы призвал отказаться от использования и следить за своей речью, особенно когда мы говорим на широкую аудиторию, на студентов. Нельзя так говорить, мы должны понимать, что такое модели, методы, алгоритмы, вычисления, что никакого интеллекта не существует, это вычислительная имитация и так далее, но мы это все понимаем, но мы на хайпе как бы поддаемся этому обману и уже превращаем его в самообман, к сожалению. То есть, говоря, третий тезис, да, третий тезис, говоря про этику не только искусственного интеллекта, а вообще социотехнических систем, и очень широкий вопрос, вопрос, согласен с Игорем, это не вопрос разработчиков искусственного интеллекта, это вопрос, конечно, стандартизации, регламентов, законов и так далее, причем регламентировать надо те ситуации, которые на первый взгляд кажутся абсолютно безобидными, и я привожу примеры, но давайте уйдем от натасканных вот этих набивших оскомину примеров с беспилотным вождением, с кредитным скорингом, давайте я приведу новые примеры, они не новые, а на самом деле очень старые. 

S04 [00:29:15]  : только имею ввиду, у нас очень мало времени, мы с Сергеем должны раньше Да, я закругляюсь, да. Давайте потом чуть-чуть. 

S00 [00:29:23]  : А, давайте так, да. 

S04 [00:29:23]  : Потом ты это по примеру приведешь. Хорошо. Но вот ты как бы третья твоя картина, которую ты хотел как бы дать, это то, что не надо противопоставлять вот эти две системы, что все-таки цивилизация должна, на твой взгляд, найти более коллективное, наверное, путь к какому-то коллективному сосуществованию, и в этом смысле ты скорее за восточный путь, нежели за... 

S00 [00:29:45]  : Да, вот этот третий путь чуть ближе от середины к востоку. Это мое субъективное ощущение. 

S02 [00:29:56]  : Серафим говорит в другой дихотомии на самом деле. Дело в том, что то, о чем пишет Флориде, вовсе не касается индивидуалистов и коллективистского видения общества. Речь идёт о ещё более глубинных вещах, те, которые описывал там и Вячеслав Всеволодович Иванов, а или я отправлю к замечательному там тому Карла Кантера «Двойная спираль эволюции». Противоставление Востока-Запада идёт по ценностному принципу. Что важнее? Ценность выше чего? ценность человека или общества. вот о чем. это не коллективизм и индивидуализм. это исключительно признание ценности, чья ценность выше. с этой точки зрения это никак не пересекается с индивидуализмом, коллективизмом и так далее и так далее. 

S04 [00:31:01]  : так что это просто в качестве короткого комментария если есть какие-то вопросы я готов еще опять да мне честно мне кажется что это что очень близкие вещи может быть там вопрос в терминологии в разнице понятий но я думаю что константин истович именно про это говорил давайте мы сейчас не будем тратить время на состыковку слов иногда это очень долго сложно и не всегда важно давайте мы сейчас действительно пару вопросов вот у нас тут есть на вопросы сергей ответьте пожалуйста насчет но вот тут вопрос от виктора носков что он много видел ваши подкасты фейсбук и прочее но ни разу не встречал ничего пока об использовании блокчейна для решения проблемы этики он нас такой блокчейн энтузиаст вы думаете, что может быть полезен как-то блокчейн для для решения вопросов этики, может быть там для контроля качества ассетов или там для контроля ответственности или там еще блокчейн тоже универсальный инструмент горизонтальный это означает что его можно приспособить чего угодно и к этому его тоже можно приспособить 

S02 [00:32:09]  : И когда мы говорим о использовании искусственного интеллекта именно как инструмента, то в качестве контроля над инструментом, проверки контроля, ведь на самом деле блокчейн хорош именно как средство коллективного контроля. Это означает, что и здесь это может быть применено. Но это общий ответ. Да, можно. Да, можно. Но в блокчейне много чему можно применить. 

S04 [00:32:36]  : у меня тоже ощущение, что в принципе теоретически можно, но как конкретно вот не очень понятно. еще вопрос по поводу кодекса этики, который сейчас тут очень пиарится. Большой идет на эту тему такое движение со стороны государства, что кодекс этики, к нему присоединяются компании. Вы думаете, что на уровне кодекса этики, ну, собственно, вопрос, что вы думаете про кодекс этики, насколько хорошо и безопасно присоединяться, значит, будет ли получать эти компании какое-то преимущество и должен ли быть баланс какой-то между государством и компаниями, прописанный в этом кодексе этики? 

S02 [00:33:17]  : я думаю, что самая большая беда, которую может постичь кодекс этики, если решат, что кодекс этики решает вопрос этики использования искусственного интеллекта. кодекс этики это всего лишь один из множества, там, тысячи ходов, которые должны быть сделаны, чтобы решить вопрос скажем, использование инструментов и как инструмент. помимо просто кодекса, знаете, как вот существует конституция, если кодекс может быть использован как конституция, но под него еще нужно будет и уголовное законодательство кодекса разработать, и гражданский кодекс, и сотни законов, если рассматривать кодекс этики как некую вот конституцию над всем этим, то от него может быть толк. если же решат, что разработать кодекс этики, а дальше все руководствуютесь кодексом этики, ничего из этого не получится, потому что это как конституция, это только, как говорится, основные вехи и границы и рамки ставят. направления, а все равно разрабатывать регламенты, разрабатывать процедуры, законы, не говоря уж о том, что ответ на вопрос о втором слайсе из двух применения не как инструмента, а как автоматизация решения, это уже, извините, в каком кодексе здесь пропишешь. здесь нужно принимать решение абсолютно индивидуально. также как с с ношением огнестрельного оружия. Либо можно, либо нет. Либо можно применять для... Вот Константин говорит, что человек все равно принимает решение. Как бы так и не проблема. Я выступал здесь на большой конференции Красного Креста недавно по вопросам автономного оружия. В том-то и дело, что уже сейчас существуют системы, используются, практически используются, которые принимают решения сами. и чем дальше с каждым годом таких систем будет все больше и больше, но вопрос о том, можно им разрешить самим принимать решение или нет, его как-то вот затаскивают, замыливают и потом, потом. 

S04 [00:35:36]  : Сергей, из примеров, которые вы приводили, там два примера. Один из них, мне кажется, простой, другой как раз сложный. Простой пример, это когда речь, например, про скоринг кредитов. Это вопрос бизнеса. Когда компания, банк, например, решает, что у него теперь компьютер будет осуществлять скоринг и выдавать решение, выдавать кредит или нет, при всем том, что мне категорически не нравится такая позиция банков, но в частности российских, это же бизнес, они имеют полное право, это как бы их внутреннее дело. Если мне не нравится, я могу уйти в другой банк, который этого не делает. Но если бы это была моя компания, я бы тоже принимал решение, как мне это удобно. А вот с точки зрения государства, например, медицина или военное дело, то есть то, что касается всего общества. вот прям действительно большой вопрос который который там нужно как и я знаете здесь хочу вот последний такой вопрос мы с константином причем когда обсуждали семинары там переписывались то одна из позиций было как раз ну константина что мы как сообщество должны видимо сейчас более активную яркую позицию занимать и в ваших словах сергей тоже раз прозвучало то есть вопрос такой вот вы как бы эту боль сформулировали вот такие разные этические системы и а также скажешь это не совсем дело разработчиков а здесь должно быть влечение государства и научное сообщество и вот вопрос а что мы можем сделать видите ли вы какие-нибудь там Вот что мы можем как научное сообщество, которое осознает проблему, которое видит, что это, ну, реальная проблема, она мировоззренческая, она будет скоро приводить, ну, и уже приводит к серьезным столкновениям. И я уверен, что у научного сообщества по обе стороны куда больше общества, куда больше общего, чем, ну, скажем, руководство по обе стороны стран допустим мне почему так кажется все-таки там научному сообществу проще как-то договориться что может быть что можно было бы сделать что вы думали что мы могли бы делать как как научное сообщество чтобы как-то эту ситуацию уменьшать масштаб проблем 

S02 [00:37:42]  : Вы понимаете, для этого нужны специализированные институты, что цивилизация научилась одним способом решать такие вот большие глобальные задачи, это создание специализированных институтов. Что будет в качестве этого института выступать? Наверняка будет не один институт. на западе существует как минимум там я знаю там дюжину институтов которые занимаются вопросами этики искусственного интеллекта существуют соответствующие профильные комиссии даже и при правительство при парламентах То есть на самом деле должен быть комплекс институтов, каждый из которых берет на себя конкретную роль, что вот одни занимаются этикой и как инструментом. И дальше, допустим, инструменты для медицины, инструменты для образования, инструменты для промышленности, инструменты для энтертеймента. Это все должно на самом деле просматриваться и прописываться в регламентах. Далее должны существовать некие общественные организации, которые будут формировать практики, практики на самом деле. Потому что мало разработать инструменты, должны быть еще и практики использования этого инструмента. Поэтому я еще раз говорю, этот вопрос должен быть институциализирован. На западе это плохо или хорошо ли делается? делается очень по-своему, очень по-своему. у нас я слышу только про кодекс этики, который вот сам все решит и сам все устроит. я считаю абсолютно конструктивно. 

S04 [00:39:39]  : это правда. я с вами абсолютно согласен, что главное, чтобы этот кодекс этики... люди считали, что это главное, единственное, что нужно было сделать, а это маленькие кирпичи. да. да да да ладно сергей спасибо огромное я помню что спасибо вам жаль что но мы еще продолжим следующий раз тема активно и живая мы сейчас сейчас кстати нашла также всем вам желаю хорошо пообсудить спасибо спасибо огромное сергей всего хорошего а сергей спасибо до свидания Константин Вячеславович, вот теперь мы вернемся тогда к месту, с которого я тебя прервал. Мы можем вернуться с него или можно каким-то начать там с комментариев к последним ответов, вопросов Сергея, как тебе удобно. 

S00 [00:40:27]  : Там мы, наверное, вернемся к этим комментариям все равно. Я вот какой пример хотел привести. Он у нас у каждого в каждодневной практике. Вот поисковая система. Все мы пользуемся Гуглом, Яндексом и так далее. Это же хорошо. Это великое достижение цивилизации, науки, технологий. И всем понятно, что там сейчас используются технологии искусственного интеллекта. Те самые, о которых мы говорим. Но давайте вспомним историю, почему поисковики получили возможности для развития. Потому что в какой-то момент в гугле придумали, как продавать слова. То есть они придумали, как они будут зарабатывать на рекламе, на продаже слов. И поисковики нарастили постепенно такие бюджеты, которые позволили им иметь огромные дата-центры, обрабатывать 5 байт данных и так далее. Все прекрасно. зарабатывать на рекламе. Вот когда поисковики занялись этим бизнесом, вот хоть один человек там подумал о социальных последствиях? Я уверен, что нет. А что я имею ввиду под социальными последствиями? У кого они отобрали рекламный бюджет? У средств массовой информации. Значит средства массовой информации лишились значительной доли независимых коммерческих источников доходов. Но им же надо существовать и зарабатывать. Что сделали средства массовой информации? Они начали быть рупором пропаганды. Как это повлияло на социум? Социум поляризуется, это ведет к конфликтам. У людей, у масс людей, да, возникают поляризованные картины мира, которые приводят к войнам. Я не хочу сказать, что это однозначная цепочка причинно-следственных связей, ни в коем случае, но это процессы, которые наложились на другие процессы, способствовали. Я не хочу сказать, что поисковики виноваты в том, что наступил конец журнализма. Конец журнализма стал наступать гораздо раньше. что журналистская этика сменилась на этику постправды. Не поисковики в этом виноваты, но они вложили свой вклад в это преобразование и то, что мы теперь живем в состоянии постоянной информационной войны и не способны отличить уже даже мыслящие здравомыслящие люди, критические мыслящие не способны отличить правду от лжи, и мы замыкаемся каждый в нашем своем пузыре информационном, комфортном. Вот это тот мир, в который мы пришли. 

S04 [00:43:12]  : Где здесь этика? 

S00 [00:43:16]  : Последний вопрос, который я хочу задать, у меня еще второй пример есть. как должны были вести себя разработчики и вот этот бизнес поисковый мог ли он вообще предположить некоторые социальные последствия своей ну в общем-то вопрос выживания да как выжить бизнесу научиться зарабатывать большие деньги это хорошо для бизнеса Но как этот бизнес, желая себе благо, на самом деле ведет себя хищническим образом в человеческом социуме – это очень нетривиальный вопрос. Это вопрос взаимодействия, существования сложных социотехнических систем, где искусственный интеллект – это технология, люди, которые используют эти технологии, часто не понимают, потому что они не в состоянии проиграть эту игру. Нас не учили теории игр. для того, чтобы проиграть, а если я сделаю это, мой конкурент сделает это, а я на общество создам такое-то давление, а государство в ответ, а люди в ответ, а мы все еще в состоянии геополитической конкуренции, к чему это приведет в итоге? Вот так думать нас не учили, а это этика. 

S04 [00:44:36]  : Дружище, мы все-таки немножко ушли от темы. Я с тобой не совсем согласен по поводу поисковиков, потому как все-таки сам интернет по себе как огромное средство привнес в общество сильную прозрачность и скорость передачи информации. И человек стал не только благодаря поисковикам и благодаря тем же средствам массовой информации, только в электронной форме, получил доступ к информации, которую раньше никогда не имел в виду, раньше не имел возможности. Мы раньше жили в маленьких городках, и все, что у тебя было, это крупное общение и центральный телевизор, откуда правда была. 

S00 [00:45:15]  : Игорь, прости, я еще к этому тут же добавлю, что произошло принципиально нового. Каждый человек получил возможность быть в СМИ и быть услышанным миллионами 

S04 [00:45:25]  : людей вот вот это вот я думаю важная вещь и в этом смысле именно соцсети они поисковики стали тем инструментом который каждый стал рупором причем чем более этот рупор я извиняюсь там какой-нибудь яркий или там эпатажный или какой-нибудь там все эти вот блогеры я я с трудом то бы иногда смотрю на вот увлечение людей этими там кого не там смотрят и у кого миллионы просмотров но вот их смотрят и они там властители дум так сказать сегодняшних и вот в этом плане действительно но смотри мы все-таки отвлекаемся но мы немножко отвлеклись сейчас возвращаемся в тему потому как все-таки вот это дихотомия которая про который с которым иначе разговаривать из которой я я согласен с тобой вот мне сам кажется что это вот как бы индивидуализм коллективизм они это другие проекции вот того же самого обращение к ценностям про которые говорил сергей то есть это индивидуальные свободы как ценность и коллективная как бы и общество как ценность и И мне кажется, что именно в сегодняшнем обществе, которое пронизано информацией, информации стало много, она быстрая, мы получаем доступ везде, вот эта дихотомия, она как бы вываливается теперь на каждого человека. И вот эти государства, которые раньше существовали спокойно, на гигантском расстоянии друг от друга, со своими правилами, никто не лез в чужой монастырь, вдруг в общем едином пространстве оказалось, что но это вот как бы сильное столкновение именно смыслов вот этих вот как бы повесток мировоззрений и когда мы говорим про разработку и инструментов и аид ну как бы все-таки мы пытаемся так леньче копировать человека хотя я согласен что сейчас это пока это далеко не интеллект это отдельные инструменты но мы пытаемся все-таки какие-то вещи воспроизводить копировать и в частности например вот эти самые языковые модель языковые модели это попытка, скажем, копировать какие-то... то, как человек разговаривает. И мы делаем большие модели, скармливаем огромное количество текстов, и дальше модель что-то отвечает. Мы ей что-то говорим, она что-то отвечает. И мы, например, помним замечательный пример. Помните, что бот майкрософтовский, по-моему, это был, да, которого выпустили в свободное плавание обучаться на пользователях а пользователи очень быстро научили его плохому я уже забыл как его звали но но как он кончил мы все помним что начал стал расистом матершинником ну потому что с кем поведешься от того и наберешься как говорила моя мама и и И так как мы пытаемся имитировать вот это наше человеческое поведение, мы знаем, что человек так устроен. В этом смысле мы постепенно начинаем приходить к тому, что и от инструмента, и от того, как им пользоваться, все-таки то, как мы его проектируем, неизбежно окажет влияние на то, каким он станет. А чем ближе мы будем подходить к тому, что он будет имитировать или воссоздавать какие-то функции там интеллекта тем ближе мы наверное будем сталкиваться и острее вот с этими самыми нюансами связанными с этикой да но вот я хочу вернуться к последним фразам сергея перед тем как он нас покинул 

S00 [00:48:54]  : Что вот кодекс, зачем он? Кодекс – это ведь первый шаг. Я тоже читал очень интересный пост Сергея Карелова, когда вот этот кодекс вышел, он его сравнил с китайским. и показал, что вот российский кодекс, он просто бла-бла-бла про все хорошее, за все хорошее против всего плохого, а китайский он уже пытается вводить какие-то регламенты. Ну, на самом деле, я не вижу здесь противоречия, просто китайцы сделали два шага, а мы только один. За кодексом обязательно пойдут ограничения, регламенты, законы, причем это, с одной стороны, технические стандарты, что можно, что нельзя, как должна взаимодействовать эта система с человеком, как она должна быть атакоустойчивой и так далее, и там инфобес. Огромное количество технических деталей, которые требуется проработать как следующий шаг. А следующий шаг еще один, это юридические законы, что можно и что нельзя. Возможно, мы через некоторое время придем к тому, что мы вообще запретим чат-ботов. Почему неэтично машине общаться с человеком? Я не утверждение делаю. Я говорю о том, что, возможно, после того, как мы сделаем еще несколько шагов от кодекса этики к реальным регламентам, а эти шаги потребуют от нас теоретика игрового умышления, Масса мысленных и немыслимых экспериментов. Когда люди разговаривают с машинами, к чему это приводит? Возможно, сделав эти эксперименты, мы поймем, что чат-боты – это как ядерное оружие. что они настолько способны нас обманывать, что мы должны от них отказаться. Или другая версия событий. Чат-бот или любой робот, он не должен нами восприниматься антропоморфно. как нечто человекоподобное, мы должны понимать, что это машина. Как манекен в торговом центре. Когнитивное искажение о том, что мы приняли манекен за человека, ловим за полсекунды, хихикнули и пошли дальше. То же самое должно происходить при общении человека с роботом. Просто к манекенам мы уже сто лет, как привыкли, А вот к такому общению мы не привыкли. Нам нужно с детства вырабатывать, воспитать, учить детей в школе, как общаться с искусственными системами. Ты человек, ты создатель, твоя человеческая цивилизация, он машина. Он тебе в помощь. Он твой инструмент, как веник, как экскаватор, как космическая ракета, но ты его создатель, ты им полностью повелеваешь. Вот эти этические моменты, как основа человеческой цивилизации, должны вдалбливаться в школу. Мы с вами тут сидим, такие у нас в школах, этих уроков не было, потому что темп научно-технического прогресса оказался выше даже не то, что там развитие нашего мозга и нас как существ биологических, а даже выше прогресса нашего образования. мы тут сидим все необразованные, мы с роботами общаться не умеем, с чат-ботом общаться не умеем. 

S04 [00:52:06]  : Мне кажется очень крутым и интересным вот этот твой заход по поводу того, что может быть действительно даже окажется неэтичным общаться там с какими-то чат-ботами или что это должен быть как манекен в магазине, но вот сейчас я должен отметить, что сейчас дети То есть мы с тобой по одному относимся, как общаться с этими роботами. Они нас раздражают, бесят. А дети к этому относятся крайне спокойно. Совершенно с этими Алисами, со всеми. 

S00 [00:52:37]  : Нет, для них это энтертеймент. Правильно? Да. Главное, чтобы энтертеймент не подменил реальную картину мира. Это будет крайне неэтично. Сейчас мы входим в виртуальные реальности, виртуальные миры, виртуальные вселенные. Это нас ожидает и наших детей, и ожидает причем как опасность. Эта виртуальная вселенная, она гораздо более привлекательна, чем реальный мир. Тут хочется уйти от сложности реального мира. Там можно всем управлять, там можно управлять сложностями, а в реальном мире ты не поуправляешь. И поэтому люди будут охотно заниматься вот этим эскапизмом, а бизнесы будут активно на этом наживаться. Потому что это целый новый рынок, где людям можно продавать что угодно. Можно продавать рисунок, рубашки за 50 тысяч долларов. 

S04 [00:53:29]  : этого не понимаю пока до сети но нас это ожидает не то что там когда-то там курцвейла обещал это нас ожидает через три года но ты тем самым не как бы не заменяешь одну проблему другой вот смотри например допустим я говорю что там, возможно ли или невозможно сделать этого чат-бота там этичным, не этичным. А ты вот говоришь, может быть, вообще не нужно разговаривать с этими чат-ботами, и это будет признано не этичным. Тем самым, ну, как бы, проблема вроде как выдвигается, что вроде как... не нужно его делать сильно этично, тогда мы приходим к другой проблеме, что, допустим, будет признано, что неэтично разговаривать, и мы вопрос этики чат-бота как бы вычеркиваем, но будут бизнесы, которые будут нелегально применять эти чат-боты в тех же метаверсах, и мы придем к этике, но уже с человеческой стороны, там, этично или неэтично применять то, что было признано неэтичным. 

S00 [00:54:26]  : Игорь, а я не знаю, потому что это вопросы будущих, причем, я думаю, что ближайших исследований. Это серьезная исследовательская проблема, такая социокультурная отчасти, а отчасти технологическая. Я не знаю, придется ли нам вводить законы против метаверсов. Но может быть их придется запретить, а может быть придется регламентировать их как ядерное оружие. Есть красные кнопки, но пользоваться ими нельзя, потому что бабах и всем крышка. Есть ядерная электростанция, и это очень здорово, это цивилизационно. И может быть точно так же будет с искусственным интеллектом. Это в принципе экзистенциально опасные технологии. И что-то из этого обязательно надо брать и использовать, а что-то обязательно как-то регламентировать. Но где провести эти красные линии, мы пока не знаем. Это требует исследований. 

S04 [00:55:17]  : Знаешь, пока я вижу, что это не столько исследования, сколько есть большие компании, большие корпорации, у которых есть большие деньги, они просто, ну как бы, выбрасывают и выбрасывают новые такие типа как игрушки. Это общество воспринимает, люди отдельные там. начинают радостно с этим играться а государство или общество в целом те кто мог бы там как-то заняться регулированием как правило там сидят за всем этим наблюдает и с большим запозданием как-то на это иногда реагирует и то тоже далеко не всегда потому что вот эта культура ты говоришь это вопрос для исследований я честно ну в россии вообще уверен что просто бюджет никакого не найти, но я и в штатах знаю там буквально одно или два некоммерческих организации, которые занимаются исследованием такого плана, и в этом плане это как бы вместо того, чтобы делать исследования, совершать какие-то аккуратные шаги, мы в общем на полной скорости несемся в этот дивный новый мир, все примеряя на себе и все пробуя и это вот такой нюанс и поэтому вопросы этики и этичности этого применения мне кажется сну становится все все все все все важнее все важнее ну хорошо слушай но прежде чем вот перейти еще к вопросам у нас там уже прям я чувствую уже пару человек там уже прям на взводе сидит в частности у нас есть один там там крыши там ген директор компании который чат ботов проектирует ему уже прям явно не хочет войти в дискуссию но до этого все-таки вот а правильно я понимаю что когда мы говорим про то что например есть допустим там китайские разработчики которые, допустим, делают большую модель пангу на триллион параметров и скармливают ей китайские тексты. И есть, допустим, американские разработчики с моделью GPT-3, которые, я готов поспорить, ни одного китайского текста и не скормили, они же в иероглифах, они скармливают, соответственно, всю англоязычную литературу. как бы правильно ли я понимаю что в этой ситуации ну как бы подспудно вместе с этими текстами модели как бы воспринимают но те модели модели компьютерные машин ленинга воспринимают те модели человеческого поведения которые там неявно заложены в этих текстах их потом будут ну как бы там условно ретранслировать а еще при этом мы же сейчас видим ясное движение к тому что автономных агентов начинают проектировать на базе разговорной модели так как не знают что поставить в центр тот например я видел там доклад миша бурцева где у нас есть на входе значит распознавание семантическими сетями мы оттуда генерируем там языковые словарные токены подаем их на вход языковой модели gpt-3 она генерирует там какой-то выход мы этот выход подаем на актуаторы и вот такого значит агента в rl среде мы там обучаем чем сам в центре то вот это языковая модель в которой вот эти как раз которые научно на каких-то текстах то есть ты ты ты ты что думаешь это этика ну как бы подсознательная, что ли, она там уже присутствует в этих моделях? Или ее там нет? Или это мои фантазии? 

S00 [00:58:46]  : Да нет, конечно, там никакой этики. Это такой интеллектуальный попугай-попка, который обучен по сверхбольшой коллекции текстов. Вот, например, мы с тобой понимаем, как продолжить фразу «пусть идут неуклюже». Какое следующее слово? 

S04 [00:59:00]  : Ну понятно, пешеходы по лужам. 

S00 [00:59:02]  : Так вот, эта модель это тоже знает. Почему? Потому что песенка есть в датасете. Конечно. Ну и все. Но никакого понимания того, что вообще пешеходы ходят по лужам, у этой модели нет. Потому что там есть песенка. 

S04 [00:59:14]  : верно но если ты спросишь там можно ли там можно ли убивать убить человека она тебе скажет что там убивать нельзя что почему нельзя если ему смертная казнь назначена первая реакция, которая у модели gpt-3 будет, она будет похожа на первую реакцию, которую будет у человека, который ну как бы без контекста просто отвечает на вопрос на статистике. если ему казнь назначена, это сильно менее статистически частый случай, чем ну как бы in general, поэтому я думаю, что она будет отвечать это без слов попугай такой большой стахастический статистический попугай но он и отвечает но как бы в среднем как человеки отвечают вот англоязычные люди пишут тексты исходя из своих как бы представлений этических вот они эти тексты определять те ответы которые там будут разве нет я вообще бы это не называл искусственным интеллектом это попка автоматическая. Хорошо, это попка. Но дальше представьте, что к этой попке вот приделали компьютер-вижн с семантическим распознаванием. Сделали ей какие-нибудь моторы, по которым она говорит поехать туда, направо, налево, вот она там ездит. И это, допустим, стал, я не знаю, какой-нибудь робот, официант или курьер. которые ориентируются как-то в пространстве. Вот он, допустим, вот он и ездит. 

S00 [01:00:46]  : Только, Игорь, понимаешь, это надо называть машиной, которая выполняет рутинную деятельность. Не надо называть это интеллектом, искусственным интеллектом. Вот мы создали себе лингвистическую ловушку, самим термином искусственный интеллект, чтобы ему пусто было. Вот надо как-то по-другому называть, иначе мы все время будем в этот самообман впадать. Это просто машина, выполняющая некую интеллектуальную работу. Это вот, например, ты подходишь к кофе-автомату, а он интеллектуальный. Ты только подошел, он распознал твою фотографию, налил твой любимый кофе. Ты идешь и говоришь другу своему, мне автомат налил кофе. 

S04 [01:01:28]  : Это интеллект? 

S00 [01:01:30]  : Нет, конечно. Это может быть аттракционом, развлечением, чем угодно, но только не интеллектом. Поэтому, к сожалению, язык – наш враг. Просто способ говорения постоянно вводит нас в этот соблазн. 

S04 [01:01:47]  : И это ты прав, безусловно. 

S00 [01:01:49]  : Смотри, где начинается вопрос этики. Вот, допустим, этот триллион параметров модель обученная и начинает рассуждать о реальном мире. А мы вдруг начинаем этой модели верить. Вот в этот момент наступает та самая красная линия, которую нельзя перейти. 

S04 [01:02:06]  : окей значит в китае за нее перешли потому что ты помнишь что там пару недель назад была новость что они обязали теперь во всех судах решение выносит искусственный интеллект у них есть система государственная, а если судья не согласен с решением, то судья пишет письменное обоснование, почему он не согласен, которое потом идет в датасет, в обучение и так далее. Но важно, что первая линия это ИИ. 

S00 [01:02:33]  : Слушай, вот чтобы разобраться в сути этой новости, надо провести расследование. Вот реально, я его не проводил. Поэтому ничего тут не могу судить о том, возможно, они сильно ограничили те дела чисто стандартными, которые можно алгоритмизировать. Тут вообще, может быть, и искусственного интеллекта не надо. Там 70% всей юридической деятельности вообще алгоритмизируется. 

S04 [01:02:58]  : верно верно слушай но хорошо но вот анализировали но неважно дело есть дело это же речь идет о человеке и мы помним там все эти вот прецеденты этого по-моему штата нью-йорк с которых все началась все это катавасия про этику в юриспруденции где этот алгоритм был обучен на на истории там вынесения этих дел и оказалось что для статистически для черных там приговоры были более жесткие чем для белых и потом этому дали объяснение тени но он вот как бы на основе этой статистики выносил там дальше определение но в данной ситуации я вот про что и это большая языковая модель, которая анализирует какие-то, которая на входе получает большое количество текста и в том числе материалы дела, и она по нему делает какой-то вывод. в этом тексте ну как бы в этих текстах заложено ведь в каком-то смысле там не то чтобы правила там не правила вывода заложено там как бы кейсы если хочешь заложен а если эти базируются ну вот на на законе и в конечном итоге ну как бы на этике этой страны то то есть если допустим условно я бы попал на не дай бог на китайский суд и он бы вынес мне приговор там по по их меркам то может быть я вообще посчитал бы его совершенно там каким-то ну там он был бы для меня диким и 

S00 [01:04:25]  : Ну, еще раз, смотри, я вот в этом кейсе головоко не разобрался, поэтому судить китайский искусственный интеллект не могу, но, возможно, они слегка перегнули палку. Почему? Потому что вот в такого рода вещах должны работать поисково-рекомендательные технологии. Как это делается в медицинской диагностике, как это в конце концов в поисковых системах тоже, мы рассматриваем поисковую выдачу. Так и здесь, это фактически независимый искусственный интеллект, он может выступать в роли такого вот автоматизированного эксперта и его выводы можно подшить к материалу дела, это да, но зачем менять человеческие процедуры судебные, когда суд это все-таки вся юриспруденция, это про взаимоотношения между людьми. И если мы назначим электронного судью и будем беспрекословно верить в его решения, это означает, что мы, в общем, совершили некий антицивилизационный акт. Мы разрушаем нашу человеческую цивилизацию, заменяем такой важный элемент нас самих машиной. Вот это неэтично. Я считаю, что я не знаю, это или не это имело место в этом китайском кейсе. 

S04 [01:05:44]  : Но, кстати, Карелов тоже про это говорил, что именно в этом месте возникает вопрос этичности, не этичности, передавать ли вообще рассмотрение таких дел машине или нет. 

S00 [01:05:55]  : Но опять-таки не передавать, а просто использовать машину как некую поисковую систему. Вот найти схожие дела и сделать некую статистику по судебным решениям, по прецедентам, вот она как раз очень классная функция. Это legal tech вот в чистом виде. Но суда присяжности никто не должен отменять после этого. 

S04 [01:06:15]  : Зачем? Хорошо, слушай, прежде чем перейти к вопросам, все-таки я с тобой согласен абсолютно, когда ты говоришь, что сегодня искусственный директор – это такой хайповый термин, а реально есть отдельные алгоритмы, вычислительные модели, которые делают отдельные конкретные задачи, решают. Абсолютно согласен, вопросов нет. сегодня, но мы все-таки находимся на семинаре и джай раша сообщество, которое там ну обсуждает активно как там делать так называемый сильный искусственный интеллект или общий или human level artificial intelligence whatever и И в этом смысле, ну, по крайней мере, в этом сообществе люди, ну, уверены, что мы рано или поздно его там создадим. Ну, мы надеемся, что на горизонте нашей жизни, хотя надеемся, тут тоже, в общем, вопрос экзистенциальный. А нужно ли нам это вообще всем или нет? Я скорее лично думаю, что скорее нет. Для меня это способ просто исследовать мозг и понимать, что что на себя представить но ну допустим вот мы его создают в этой связи если но и ты согласен что человечество сейчас массово огромное количество людей из научного сообщества во всех странах сейчас ну как бы предпринимает серьезные усилия для создания там вот ну все более совершенных алгоритмов машин ленинга что приближает нас и в конечном итоге хотят создать некий этот общий общий вот в этой перспективе как бы создание некоторого human level допустим artificial intelligence он и обладать и должен обладать какой-то этикой или или их будет вот там условно говоря будет китайский американский или он может быть будет какой-то если скормить ему все тексты мира и там обучить его на всех примерах мира может быть он как раз какой-то среди срединный путь найдет что ты вот на эту тему думаешь 

S00 [01:08:25]  : У меня здесь довольно радикальная позиция. Я тоже скорее считаю, что General AI не нужен. Людям нужно решение их задач. И мы все время выпускаем из внимания, мы так увлечены технологиями, нам так хочется уподобиться Богу. что мы все время упускаем первый шаг любого исследования, постановку задачи. Какую задачу ты хочешь решить? Ты хочешь автоматизировать часть своей деятельности, человек, да? Ну, я автоматизирую ее. Вот мой тезис, что компьютеры на сегодняшний день уже намного совершеннее человеческого мозга. Смотри, как они могут безошибочно и долго помнить. Смотри, как они быстро вычисляют. Смотри, как многое чего делают, чего люди не могут. А подводные лодки тоже совершение людей, а экскаваторы, а подъемные краны. Мы много машин сделали, которые совершение нас по огромному количеству показателей. Все они решают наши задачи. Точно так же и то, что мы называем ИИ, оно должно решать наши задачи. Всё, точка. Мы, это наша цивилизация человеческая, не машинная. Теперь вопрос, а что такое General Intelligence? Почему человек интеллектуален, да? И мы хотим сейчас уподобиться Богу. Это единственная цель, которую я здесь на самом деле... Она подспудная, но единственная реальная цель, которую я здесь вижу. чтобы сделать некое подобие человека. А почему мы стали интеллектуальными, а мы выживали? Мы боролись, мы эволюционировали, боролись за выживание, победили всех. всех себе подобных, мамонтов, мастодонтов и так далее, научились коммуницировать друг с другом, разработали язык, орудия труда и так далее, и так далее, и дошли до сегодняшнего дня до технологий. Теперь мы говорим, мы хотим создать машину, которая умела бы, обладала бы всеми теми когнитивными способностями, которые мы обладаем. Значит, речь ориентация в трехмерном пространстве, распознавание образов, слух, нюх и так далее. И мы вот хотим такое. А мы сами-то почему такие? Потому что мы выживали и всех вокруг победили. А тут мы хотим машину с теми же самыми когнитивными возможностями. Зачем? А потому что она будет заведомо мощнее нас. Она будет уметь копать, летать, плавать, стрелять и так далее. Машины уже все это умеют. Теперь мы наделяем все это интеллектом и самоуничтожаемся. 

S04 [01:10:57]  : Точно, Рэй Курцвилл в полном росте. 

S00 [01:11:02]  : Или же надо четко объяснять, что мы имеем в виду под Artificial General Intelligence, что это по каким-то причинам не то, что я сейчас вместе с Рэймондом, мы сейчас вместе с Рэймондом обрисовали. 

S04 [01:11:20]  : точно так очень пока пока такое оптимистическое в кавычках тема но но с другой стороны ну я я то согласен меня это все тревожит то куда все движется но я верю что человечество как-то последнее время утратила мне кажется свои способности предсказания 

S00 [01:11:45]  : Да их никогда не было. Слушай, Игорь, я просто хотел бы для AGI сформулировать какую-то постановку задачи. Сделать эквивалент на человека – это не постановка задачи. Она ведет к этому неблагоприятному будущему. А что вместо этого? Я же не против технологии. Я наоборот, я за. Я хочу прогресса. Но чтобы этот прогресс помогал человеческой цивилизации, во-первых, остаться человеческой. Я ярый противник трансгуманизма. И чтобы мы не множили риски. чтобы мы эти риски, наоборот, уменьшали, чтобы мы повышали комфортность нашей жизни, чтобы это способствовало нашему выживанию как цивилизации, как биологического вида. Вот как надо изменить постановку Artificial General Intelligence, чтобы она перестала быть такой пугающей. 

S04 [01:12:41]  : тезис мне очень нравится на тему постановки задачи надо отдельно поговорить так коллеги давайте мы сейчас еще немножко введем еще людей в дискуссии я здесь видел как минимум болдачев значит комментировал, и я бы хотел, чтобы он расшифровал тезис. Но, в принципе, поднимайте руки. У нас в семинаре принято поднимать руки, но я кого-то уже видел. И ядный Виктор Носков, мне кажется, там уже кипит просто и хочет высказаться. Я, может, ошибаюсь, но, Виктор, если хотите, вы 100 рук поднимите там в чате. александр болдачев был был тезис такой вы что что что если мы говорим именно о текущем и они джай то противоречие между двумя системы восточно-западной не имеет отношения к проблеме этики инструменты этики принятия решений а я вот бы не согласен если вы голосом хотите это дело прокомментировать то хорошо 

S01 [01:13:41]  : Хорошо, добрый день. Ну, это как мелкое такое замечание, но касалось, конечно, в основном первой части. Мне показалось логично совершенно. С одной стороны, Карелов объявил, что мы будем рассказывать и вести разговор только об инструменте, только о машине. забыв про ИИ, именно с большой буквы или там с АГИ, а просто о некой технологии, и потом дальше постоянно нажимал на технологию, что технология должна быть горизонтальной, технология должна быть такой, этой технологией мы применяем так, то и сяк, электричество. Но при этом акцентировал внимание на том, что совершенно правильно, что вся этика в данном случае при таком уровне обсуждения сконцентрирована только на уровне государства. То есть как государство нам разрешит использовать то или иное устройство. А потом почему-то стал, вернее, изначально стал говорить про две системы. Но эти две системы могут использовать один и тот же инструмент, один и тот же пистолет, одно и то же интернет, один и тот же телевизор, одну и ту же бумагопечатание. То есть по-разному. Никакой связи между горизонтальностью и единственной технологией интернета и двумя системами нет. Как интернет и показывает. Интернет нам показывает. У нас единый интернет, но два законодательства, которые не конфликтуют. И то же самое он привозил пример с оружием. То есть в Америке оружие разрешено, то же самое оружие в России и в Китае не разрешено носить. И здесь нет никакого конфликта. 

S04 [01:15:20]  : Вопрос, даже говоря про единый интернет, у меня уже, честно говоря, берут сомнения. Одна технология, одна технология. У нас нет единого интернета в Китае и в Штатах. Ну хорошо, соединение компьютеров и как бы DNS сервера, которые там сообщают адреса и можно ходить по броузерам на разные сайты, это как бы едино. Но если мы говорим про информационное пространство... Это регулируется государством. Я бы сказал, что они категорически разные. 

S01 [01:15:52]  : Это именно подтверждает мой тезис, что государством регулируется и возможно разное регулировать. Совершенно возможно разное регулирование. То есть, по сути, чем мы сейчас занимаемся? Мы занимаемся извечным вопросом, ну, когда появился огнестрельное оружие. О, да, давайте решать, этично или не этично огнестрельным оружием против стрел. Потом появилось, там, скажем, автомобиль даже появился. Вот как обсуждали, этично или не этично, когда он давит их лошади. Появилось радио, телевидение, смартфон. А этично ли? А телефон появился. Вот этично ли знать, когда человек с кем-то разговаривает по телефону, еще подслушивать. То есть, по сути, мы просто решаем один и тот же вопрос, который решало человечество на протяжении всей истории. То есть, как регулировать с этической стороны новые технологии. Но это не имеет отношения, что ни сам телефон, ни сам телевизор, ни сам интернет, ни сам та машина, которую мы называем, те алгоритмы, которые мы называем искусственным интеллектом, не имеют никакого отношения к этике. Они нейтральны относительно этих. 

S04 [01:16:57]  : Ваша мысль понятна, и на 99% я с ней согласен, а на 1% не согласен, говоря про языковые модели. вот языковые модели, то что мы до этого с Константином как раз обсуждали, но мое личное понимание, накормленных разными текстами с разным содержанием, это точно не... если вы по технологии понимаете нейронные сети, они там и там нейронные сети. 

S01 [01:17:23]  : Игорь, можно я отвечу, вы уже поговорили, то есть я услышал мнение, то есть смотрите, Все поменяется только в том случае, если решение большой модели, большой языковой модели будет применено непосредственно без человека. Пока этого невозможно сделать. Поэтому пока человек делает последнее заключение, На основе чего он может это сделать? Понимаете, у него может друг быть китаец, и он ему наплетет неизвестно каких там примеров. У него друг может быть африканец, который тоже, или он сам погружен в африканскую культуру и будет действовать. Поэтому всегда этика касается только принятия решения человек-человек. только человек-человек. Люди же тоже разные есть, мы одного учили на одной литературе, другой на другой литературе, и здесь разницы нет никакой между этими большими моделями. Но разница только в том, что человек научный на одной культуре принимает решение, и мы его судим за это, этически судим, иллюдически судим, а машина не принимает решение, поэтому разговора нет никакого. 

S04 [01:18:34]  : пока она не принимает решение. 

S01 [01:18:37]  : как раз вопрос стоит о том, что пока мячик на стороне государства. разрешит государство принятие решений? то же самое касается и блокчейна. пока государство не разрешило применять блокчейн в юридически значимых решениях, Он остается на уровне технологии, которую можно применять, и она не имеет никакого отношения ни к политике, ни к экономике, ни к юридической значимости, а только технология, чистая технология. 

S04 [01:19:09]  : Окей, ладно. Кстати, Вячеславович, возвращаюсь к вам. 

S00 [01:19:13]  : Вот мы сейчас сказали, что если человек принимает решение на основе того, что ему подсказывает система, то это уже как бы более этично по отношению к тому, что система автоматом сама это делает. Но я хочу сказать, что и эта ситуация тоже может быть неэтичной. Например, рекомендательные системы. В частности, вот я недоволен YouTube. Почему? Я считаю, что он неэтично понижает мой культурный уровень. Потому что стоит мне один раз посмотреть какую-нибудь развлекалочку, и у меня полный список рекомендаций всякой чухни, которую я листаю экранами и ничего не могу для себя выбрать. Делается ли это случайно или нарочно? Делается это потому, что все люди такие? Или все русскоязычные такие? Большинство русскоязычных такое? Или это наш информационный противник в информационной войне нас специально хочет опускать и понижать наш культурный уровень? Я не знаю. Это конспирология уже пошла. Но то, что инструменты поисково-рекомендательных систем могут использоваться в информационной войне против противника геополитического, идеологического, какого угодно бизнес противника, это абсолютно точно, это используется, этим спецслужбы занимаются, этим занимаются там вот продажи контента и объем этого рынка просто гигантский во всем мире. Использование вот всех этих технологий, даже таких безобидных на первый взгляд, как поисковые и рекомендательные системы. 

S01 [01:20:56]  : Но в этом нет никакого отличия от появления книг, газет, телевизоров и всего прочего. 

S00 [01:21:03]  : Есть отличие. Отличие существенное. Отличие в том, что эти технологии стали настолько всепоглощающими, насколько реактивными. 

S04 [01:21:12]  : Оружием массового поражения. Да. 

S01 [01:21:14]  : Но телевидение точно так же говорили. Пока вы не знали интернета, вы говорили про телевизор. То же самое говорили. Такими же самыми словами. Вот абсолютно. Замените YouTube на телевизор лет 30 назад. 

S00 [01:21:26]  : Все правильно. Гитлер пришел к власти благодаря радио. Это мы все знаем. Но просто масштабы этого явления все более и более, так сказать, все быстрее все это происходит. Быстрее охватываются большие сообщества людей. То есть масштабность явления нарастает. 

S01 [01:21:47]  : Эта история движется всегда так. 

S04 [01:21:49]  : Александр, ну подождите, ну математически-то мы с вами согласны. Помните, как в том анекдоте про математику, который там вылил из чайника воду, поставил, сказал, ну я задачу уже решил. Это правильно, да, математически да, но масштаб происходящего действительно на порядок превосходит то, что было до этого, и сила влияния на общество на порядок выше. в этом смысле мы же про это сейчас. 

S01 [01:22:13]  : Это было всегда в истории, всегда текущий момент. 

S04 [01:22:16]  : Не было фейсбука, у которого было 2 миллиарда пользователей, но не было такого никогда в истории. 

S01 [01:22:22]  : Никогда не было ничего то, что есть, это общие слова. Понимаете? 

S04 [01:22:28]  : Хорошо, ладно. Окей, вопрос вот по поводу... Константин, вопрос по поводу аналогии, видимо, с ядерным оружием. Я вот здесь сейчас не вижу прям первого вопроса, есть какую-то часть обсуждения, но, видимо, смысл в том, что ограничение на ядерное оружие это не ограничение создания, ограничение использования. 

S00 [01:22:50]  : Ну да, это был мой ответ, и это скорее про то, что я бы вот ввел такое понятие, как дистанциально опасные технологии. Это те технологии, которые несут действительно большую угрозу для общества, для, может быть, государства, может быть, для мира устройства нашего, которые могут использоваться для конфронтации, для войн, конфликтов и так далее. И я бы уже относил искусственный интеллект к этим технологиям наряду с ядерным, химическим, бактериологическим и так далее оружием, которое очень здорово зарегламентировано во всем мире. И сейчас мы просто в самом начале этих технологий, мы еще не понимаем опасностей, не все их осознали, не все их ощутили. Вот ядерная бомба искусственного интеллекта еще не взорвалась. Нет еще этой Хиросимы, этой Нагасаки. Но оно в какой-то момент, наверное, взорвётся, мы чего-то осознаем и чего-то начнём запрещать, и вот тогда будут выделены миллиарды долларов на исследования. Те, о которых я говорю, исследования о том, какие должны быть законы, что надо запретить и где провести красные линии. 

S04 [01:24:00]  : можно считать это хорошим предсказанием я согласен кстати это это очень похоже на с сатурным оружием конечно действительно до поры до времени тоже не было нет но таких технологий много и технологии ген модификации там генной модификации да вот сейчас 

S00 [01:24:18]  : Все говорят о том, давайте там генно-модифицировать человека, или что-то в человеке, или ткани выращивать. С одной стороны, здорово, медицина, а с другой стороны, ну, мы не знаем, каких уродов мы вырастем из человека на основе человека. Это же страшно. И это тоже представляет экзистенциальную угрозу для цивилизации. И очень много и в биологии, и в химии, и где угодно, наноматериалы, да, вот нас пугали этой самой нанопылью, которая будет, как наномашины, повсюду проникать, всё разрушать. Ну, пугалка, конечно, фантастика, но, может быть, что-то такое тоже будет. Ну, таких технологий, чем дальше, тем больше. О, кстати, да, я вот могу предложить формулу закона сохранения цивилизации. Красиво звучит, да? Закон сохранения цивилизации. Значит, вероятность самоуничтожения цивилизации прямо пропорциональна произведению трех величин. Первая – энергия, которую мы освоили. Значит, чем больше, тем больше риск самоуничтожения. Второе – количество экзистенциально опасных технологий, то, о чём мы сейчас говорим, как множитель в этой форме. И третье – это количество людей, которые ради власти готовы на всё. Это звериное в нас. Вот чтобы уменьшить вероятность самоуничтожения цивилизации, у нас есть три сомножителя. Что из этого мы собираемся уменьшать? Энергию генерировать не хочется, технологии сокращать не хочется. Значит, чего? 

S04 [01:25:50]  : Зверьми переставать быть. Формула, потому что там нет знаменателя, одни числители. Ну, там с точки зрения нормировки. Это приведет явно к бесконечности. 

S00 [01:26:00]  : Нет, я же говорю, вероятность пропорциональна. Это означает, что ее надо как-то отнормировать. Знаменатель в этой формуле также неконструктивен, как в формуле вероятность вообще зарождения цивилизации на планете. Там тоже есть такая формула. Я не помню, сколько в ней множителей. Но она столь же неконструктивна. То есть пропорциональность есть. 

S04 [01:26:20]  : но на что поделить на черт узнать не знаю так тут сергей терехов пишет что что ядерное оружие просто не придумали как запретить создание смогли только ограничить использование сергей садович но в данной ситуации то тоже никто не может запретить создание потому что команды которые делают 

S03 [01:26:41]  : технологии там машин ленинга они намного меньше и ресурсов нужно намного меньше чем в случае атомного оружия ну я согласен да коллеги спасибо большое и константин спасибо что нашли время к нам присоединиться это очень для нас важно и ценно тут понимаете какая ситуация я вижу что ответ может быть только в области культуры Вот я там выше своих вопросов говорил об этом. Даже будучи юным исследователем, но относительно юным исследователем нейронных сетей, в 1998 году у меня был такой термин, назывался нейросинезация. Я его ввел в дискуссии по нейрокомпьютерам. Это наша конференция по нейроинформатике была. Вот там была такая отдельная дискуссия большая. И видел тогда утопический смысл задачи нейронных сетей в сохранении чистоты, то есть ограждении нас от помоек. То есть мы не ходим на помойки, да, почему? Ну потому что вот культурная наша среда такова, там наши родители, наша школа, там мы все вместе и в том числе и здравоохранение, которое говорит, что на помойке нужно мыть руки и так далее. Вот мы не ходим на помойки, каждый день стараемся их избегать, как-то их, так сказать, собираем в кучу, но вынуждены как-то их там ресайклинг какой-то делать, как-то поливать эти улицы от грязи и так далее. Так вот здесь ситуация примерно такая же. То есть только это должно теперь быть уже на уровне информации. И вот одна из задач таких фундаментальных, которую я тогда видел, и сейчас я эту задачу на самом деле вижу и по-прежнему для искусственного интеллекта, как раз это нейросинезация, то есть это защита людей от информационных помоек. Отличие вот этих вот проблем, которые мы сегодня обсуждаем от ранее существовавших технологических проблем, состоит в том, что воздействие сейчас может стать, и оно стало, инфинитизимальным. То есть мы имеем новую ситуацию, когда огромное-огромное количество воздействий на людей, на психику, на их информированность, на всё-всё вместе, но каждое из отдельных воздействий очень маленькое. Оно бесконечно маленькое, его невозможно никого взять за руку и сказать, вот смотри, ты сейчас повоздействовал на психику большого количества людей вот этой вот этой буквой. Нельзя, потому что воздействие очень маленькое, но актов воздействия очень много. И вот они когда на себя друг на друга умножились, как Константин нам напомнил о том, что есть функция умножения, очень важная функция, они когда умножились, то получилась такая ситуация, что воздействие там макроскопическое, а за руку схватить нельзя. В случае ядерного оружия за руку схватить можно. можно посчитать количество этих головок, можно сосчитать количество лабораторий, которые заняты, можно сосчитать количество урана, которое там от чего-то добывается. то есть там есть некая дискретность, с которой можно в принципе работать. но то, что не смогли полностью справиться с этой проблемой, видимо нашли только такое политическое решение ограничить использование. то здесь мы имеем новый феномен. а именно вот эта неухватываемость зарывку конкретного, так сказать, конкретной причины. И тогда мы к нему должны относиться как фактически, как к вирусу, как к грязи, как к помойке, как вот к чему-то. То есть я просто, понимаете, как вот когда мы были молодые, если у вас возникал какой-то вопрос, то где вы искали источник информации? В энциклопедии. Вы шли в библиотеку, вы шли в энциклопедию, подготовленную большим количеством людей, которые сегодня суммируют знания, которые являются общепринятой какой-то истиной, и вы оттуда брали источник информации и с ней работали. Сейчас, если у меня возникает вопрос, я иду куда? Я иду на помойку. ну то есть просто ну так вот русским русским текстом я иду на помойку и то что мне на этой помойке первая попалась любая банка любая какая-то склянка я не защищен от того что она вот именно помоечная но там все красиво там и с комментариями со ссылками еще вокруг таскать какие-то там атрибутика того что вы там чуть ли не в культурном таскать дубле находитесь там в большом большом театре А в действительности вы приходите на эту помойку и вы начинаете с ней взаимодействовать. Люди не защищены от этой ситуации. Игорь, ты вот улыбаешься, но ситуация-то именно такова. 

S04 [01:31:05]  : Я спыл анекдот в тему, но я его здесь не могу рассказать. 

S03 [01:31:08]  : Да, ну прекрасно. К счастью, это тоже часть нашей культуры. У нас у каждого огромное количество анекдотов, которые мы не можем рассказать. И это как бы никакая жопить и не знать. вот и поэтому вот вот это отношение к этому как экологическому бедствию на самом деле то есть по большому счету мы сегодня имеем дело новая новая форма массового экологического эколога информационного такого бедствия и соответственно все все все нужно рассматривать вокруг вот этого по ним понятия отсюда и роль государства отсюда и роль наш нас как ученых что мы что мы должны что мы можем сделать и мы должны, например, не допускать эмпирики, вот этой вот оголтелой, о которой я выше писал. То есть нельзя опираться на чисто эмпирическое знание, две-три итерации индукции, и уже считаем, что нечто там является доказанным. Ведь большинство выводов, которые делаются сегодня вот этими системами, они не сегодня вообще в принципе, а системами основанных на данных, они эти выводы эмпирические. чтобы было ясное понимание, что это эмпирика, да, и на нее можно полагаться, да, можно, в принципе, наверное, можно. Можно ли ее заменять, теоретическое знание, которое, значит, должно обеспечивать устойчивость цивилизации и так далее? Нет, нельзя. И вот, кроме как государственного и культурного уровня, я здесь не вижу способа, как научиться не ходить на помойку. Спасибо, коллеги. 

S04 [01:32:35]  : Сергей Александрович, спасибо. Константин, что-нибудь добавишь к этому? 

S00 [01:32:40]  : Спасибо. Да нет, ссылочку кинул вам на книгу новую. Ашмановой, Касперской. Цифровая гигиена, 2022 год. 

S04 [01:32:48]  : Да. Все знают, наверное, уже. Ну, кстати, цифровая гигиена – это правильный тоже термин. Я уже не раз встречал. Я подписан там на один канал в Телеграме. Прекрасно. Там одна девочка ведет. Там у нее, по-моему, около 10 тысяч подписчиков. Как раз про поиски. Цифровая гигиена в поисках этой новой цифровой этики. там всякие любопытные моменты типа там удобно ли звонить чеку ли надо написать ему там сперва хоть в мессенджер или там и прочее начинают мелких моментов заканчивая вот этими действительно сложными экзистенциальными вопросами да 

S00 [01:33:24]  : Мы же живем в эпоху не просто постправды, а в эпоху геополитических информационных войн, когда наши мозги, вот каждого гражданина, человека, становятся полем битвы, не инструментом, полем битвы. И нас всё время тянут то в один информационный пузырь, то в другой, а мы, как люди критические мыслящие, в один заглянули – о, а тут всё более-менее доказательно, и даже я поверить готов. И тут же ты заглядываешь во второй, а там всё на 180 градусов перевёрнуто и также убедительно, и ты тоже готов эмоциями своими туда, значит, воткнуться. Ты смотришь туда, там миллион подписчиков, здесь миллион подписчиков, здесь миллион раз автора похвалили одни люди, а там другие люди миллион раз похвалили другого автора, который на 180 градусов. Это вот реальность, с которой мы живем, да, постправда и поляризация общества. И вот с этим что делать? А технологии искусственного интеллекта, NLP, о котором мы сегодня говорим, они же там автоматизируют деятельность ни много ни мало разведслужб всего мира. 

S04 [01:34:34]  : в том числе но знаешь в некотором смысле для меня лично является ценностью что можно увидеть диаметрально разные противопозиции их почитать другой вопрос что большинство людей не приучено искать вторую она как бы находит первую и там нравится-не нравится, а еще хуже если находит та, которая нравится и как бы ей следует, а вот нужно по-хорошему найти там эти две разные, а может три разные. 

S00 [01:35:04]  : Игорь, и все бы это было хорошо, пока это просто позиции, пока это не намеренно спланированная информационная акция, твоего противника, который хочет поменять твой социокультурный код для того, чтобы твою страну было легче завоевать. Вспоминаем искусство войны Суньзы. Это сейчас работает, это битком напичкано технологиями искусственного интеллекта. 

S04 [01:35:32]  : оптимистичненько ничего не скажешь но и в эту же оптимистичную линию вот у антона колони на вопрос по поводу летальных автономных систем это собственно главный я так с чем сейчас стюарт рассел пытается бороться я помню этот ролик был блестящий который они сняли Но ты что думаешь, вообще реально как-то сообществу подействовать так, чтобы запретить разработку летальных автономных систем? С другими словами, автономного оружия, которое само принимает решение об убийстве человека. Константин? 

S00 [01:36:12]  : А, я думал, это Антона вопрос. 

S04 [01:36:13]  : Нет-нет, от Антона вопрос тебе. Ну, нам с тобой, да. Ну, тебе. 

S00 [01:36:18]  : Ну, это плюс один к списку дистанциально опасных технологий. И если как с ядерным оружием, мы можем проконтролировать, что-то запретить, где-то выстроить систему противовесов, когда мы понимаем, что красные кнопки есть, но на них не нажмут. Здесь примерно то же самое. И здесь тоже как бы технологическое развитие неизбежно нас к этому приводит, например, с истребителями. То есть истребитель, который управляется автоматической системой, а не пилотом, он уже вот-вот будет, который где-то уже есть, который в бою более эффективен, чем пилот, просто потому что он выполняет большие нагрузки, быстрее ориентируется в боевой обстановке, он быстрее реагирует. Всё, пилоты уже перестают быть конкурентоспособными. Следующий этап развития – это истребители только беспилотники. Следующий этап развития – это только рои. истребителей, потому что один против роя ничего не может сделать. Следующий этап – это автономное вооружение во всех средах, в воздухе, на земле, в воде. И к этому мы неизбежно придем, потому что люди неэффективны против роев. И в какой момент мы сможем это все запретить? Вообще сможем ли? Я не знаю. 

S04 [01:37:50]  : У меня лично, у Антона был вопрос к обоим, у меня лично эту тему есть давно сформированная позиция, которую я в частности в этой своей книжке изложил, которая «Терпи день. Модель простого мира». Я лично считаю, что для меня это этика разработчика. человек который работает над системами автономного оружия но он должен как бы осознавать что он разрушает там просто человеческое общество что он в конечном итоге к этому это это это может под разными как бы это может обосновываться разными соображениями там защита государства и прочее но я лично верю что государство могут защищать и люди там которые присягу приносят которые там проходит там правильное образование а вот автономные системы и в этом смысле мой мой ответ такой я лично считаю что здесь люди нового то есть разработчики должны проявлять свою сознательность и просто не заниматься этим 

S00 [01:38:49]  : Ну, Игорь купит, купит, заплатит в 10 раз больше и купит. 

S04 [01:38:54]  : Никуда не денешься. 

S00 [01:38:59]  : Тысяча разработчиков откажется, а тысяча первые купятся. 

S04 [01:39:03]  : Ну, хотя бы тысяча откажется. 

S03 [01:39:06]  : Манхэттенском проекте это все проходили. Вот после того, как были проведены первые эти разработки и в Америке и так далее, количество книг, которые тогда было издано, и статей, и обсуждений, и этических проблем, и вот эти ученые, которые там это делали, и которые там не делали, и которые там пытались как-то ограничить это путем распространения секретов там по другой стороне, и так далее, и так далее. Очень-очень много было написано на тему того, что может и что не может делать ученый. Это вопрос, к сожалению, он не... Верно. 

S04 [01:39:39]  : Я согласен. Я же про свой этический выбор, друзья мои. Вы можете сколько угодно мне рассказывать, что это правильно или неправильно. Я свой выбор сделал. Игорь, это ты здорово. И считаю, что каждый человек должен сделать свой личный выбор. Миллион может разных точек зрения. Я считаю, что летальное оружие – это плохо. вот я публично это говорю я не принимаю никакого участия и всем людям которых так лень чем этим занимается говорю ребят это не надо этим заниматься это неправильно да над этим может быть куча разных теорий за против тому что неправильно может быть я там не знаю, но для меня важна личная позиция. 

S03 [01:40:17]  : Сергей Александрович, ну согласись. Игорь, я только снимаю шляпу, но я без шляпы, потому что я в помещении нахожусь. И полностью с тобой согласен в отношении самого придерживаясь того же, чем ты говоришь. Но ты не забывай такую вещь. Вот у нас даже был недавно доклад на АГИИ. где специалисты рассказывали, что они приглашают школьников к разработке, так сказать, системы искусственного интеллекта, нейронных сетей и так далее. И вот эти ребята, восьмиклассники, которые прекрасно программируют, они участвуют в разработке этих систем и так далее. Ты не будешь это делать, допустим, и я даже не буду это делать. Но дело в том, что восьмиклассник, которому что-то поручили, ему сказали, слушай, это круто, это почти так же, как в той же игре, в которую ты играешь, только оно вот еще там веселее. Он будет это делать. и он даже не знает, что он делает. Вот понимаешь, я еще раз возвращаюсь. только культурным способом можно воздействовать. Верно. Невозможно. Это должна быть массовая культура неприятия вот этого. 

S04 [01:41:17]  : Точно. И здесь мы знаешь к чему приходим. К любопытному моменту. Если кто-то не смотрел я очень советую посмотреть недавно было интервью Дудя и Юрия Дудь который снимает и он брал интервью у Николая Солодникова. который ведет канал не поздний один интервьюер интервью другого интервьюера их там очень любопытно эта история и у них там есть такой совершенно замечательный такой заход они очень разные люди там длинное интервью на но реально стоящее как раз про то что происходит но они там частности обсуждают что происходит в стране там какие-то вещи мне нравятся, но для Дудя в первую очередь политика, потому что все должны там политические вещи, а Николая Солодников ему говорит, что культура важна. Если бы люди там читали книги, образовывались и понимали, что то вот этот пласт фундаментальный который под этим лежит он бы как бы повлиял на все и он кажется таким многим людям он кажется таким ну подумаешь культура там что-то такое а вот а вот как раз тот фундамент на котором действительно очень многие вещи этические принципы в том числе если бы мы лучше учили там как как-то правильнее учили на детей в школах и дело не в школах, свои родители правильно должны воспитывать, в общем гигантское конечно. 

S03 [01:42:40]  : Причем я добавлю к этим словам это очень важный момент такой, вот математику массово сделать невозможно, то есть через логику, через математические какие-то виды, под математикой я понимаю в широчайшем смысле, она никогда не станет массовой, а вот культуру массово сделать можно, то есть это то фактически на что единственное на что мы можем опираться. Поэтому фактически вот сейчас и мы как ученые отвечаем на вопрос, что мы как ученые должны сделать. Мы должны продумать, как нам вот эту культуру обратить, как помочь, во-первых, этому культурному слою. Вот я, например, маленькие шаги какие-то свои делаю на конференции в Нижнем Новгороде, которая была. Я опубликовал доклад, который называется «Гуманитарный искусственный интеллект». где я просто пытался, значит, какие-то первые шаги о том, что носители гуманитарного знания, преподаватели, люди, которые доносят исторические вещи огромной массе людей, они должны быть осведомлены об этих аспектах. Для этого надо эти аспекты как-то сделать близкие к ним и так далее. Это, конечно, тоже утопическая попытка, но не предпринимать такие попытки мы, как ученые, не можем. 

S04 [01:43:53]  : поэтому вот такая и здесь важно чтобы мы как сообщество знаешь культуру несли и правильную а не то как вот константин правильно абсолютно сказал про этот youtube который меня тоже раздражает вот этим навязывание вот этой как бы поп-культура, если так можно выразиться. Кстати, Солодников с Дудем тоже об этом разговаривают, об этой поп-культуре, которая как бы победила все, к сожалению. Вот это пони... заниженная планка, а надо сказать, что, ну, на долговидную YouTube и многих таких, значит, там... систем, но как раз работают люди из нашего сообщества, которые, в общем, как-то должны осознавать ответственность. 

S00 [01:44:30]  : Коллеги, Игорь, вот знаешь, есть такая вот мысль у меня, как это все в образовательное русло перевести. Вот уже нам сейчас нужен курс по этике искусственного интеллекта. Как бы этот курс должен был бы выглядеть? На мой взгляд, это не то, что модельки какие-то рассказывают, атаки на безопасность и так далее. Это все технические аспекты, это все важно. Но у нас сегодня на протяжении нашего разговора примерно с десяток ситуаций и кейсов прозвучало, когда какое-то технологическое развитие, на первый взгляд, безобидное, вдруг через некоторое время становится связанным с какими-то угрозами или с каким-то неэтичным поведением или неэтичным взаимодействием людей и технологий. И нам нужно на этих кейсах обществу донести понимание того, вот как бывает, вот примеры наглядные. То есть общими рассуждениями философскими тут ничего не добьешься. А если наглядными примерами, то можно. Вот, например, дипфейк. Вот я, например, для себя тоже, Игорь, как ты принял решение, я никогда не буду заниматься технологиями создания дипфейка. Но мне интересно заниматься технологиями распознавания фейков. Я хочу быть по эту сторону силы, по эту сторону от баррикад и ни за что не пойду туда. Но я с коллегами обсуждал этот вопрос. Я подходил и спрашивал, ты почему занимаешься технологиями дипфейка? И получал три стандартных ответа. Первое, если не я, то кто-то другой. Второе. Я, конечно, хочу заниматься распознаванием, а не атаками, но я не смогу распознавать атаки, если не научусь их имитировать. Ну и третье просто. Это чертовски интересно мне как ученому. Вот три мотиватора, которые двигают людей заниматься нехорошими вещами по темную сторону силы. Что ты с этим сделаешь и как выстраивать культуру, чтобы люди понимали, что если он принимает как исследователь, как ученый для себя морально-этический вопрос оставаться до конца. Балансируя между светом и тьмой, то где гарантия, в какую сторону он свалится завтра, послезавтра или через год? Он не может это гарантировать, но он хочет балансировать, он такой крутой, да? Я между добром и злом хочу туда, хочу сюда, а это вот и есть мораль и этика. 

S04 [01:47:03]  : мне кажется что круто слушай что мы вот к этой в результате под конец самого нашего разговора к этой теме подходим и сергей терехов об этом же сейчас упоминал когда говорил что когда было манхэттенский проект когда была вся эта тема с автономным оружием и все это уже как бы было, но мы не можем, все-таки это все слегка как-то ушло, а сейчас с тем, что вот эти технологии становятся опять очень, ну не взрывоопасными, а как бы опасными, экзистенциально опасными, мы снова видим, что вот в научном сообществе вот всплеск вот этого как бы потребности к пониманию к обсуждению это уже как минимум хорошо вот даже даже факт что мы в принципе ну как бы мне очень не хватает этого в нашем российском дискурсе российскоговорящего сообщества и хорошо что у нас хотя бы там на семинаре где-то мы такие темы подниматься чтобы люди в принципе задумывались над этим что вот есть эта сторона та сторона там что вот там генерация этого дипфейка или там как антон там написал там программисты которые пишут интернет-магазины продажи наркотиков вроде как тоже программист это как бы это другая сторона и чекнул он же ее там он ее выбирая он понимает ее но кто-то закрывает для себя глаза но и если вообще все никто про это не будет говорить это одна история но если мы в принципе как сообщество про это будем говорить будем эти курсы делать то есть важно хотя бы вот это движение но мне кажется оно началось такое по всему миру и ну все-таки хочется надеяться что это приведет к тому что значимое количество умных видящих будущее людей ну начнет это обсуждать вырабатывать искать какие-то решения и глядишь но может быть что-нибудь найдется может быть не сегодня сейчас на нашем маленьком семинаре но мне кажется что сегодня было очень интересно мне лично прям было безумно интересно я огромное спасибо и ушедшему сергея кстани выставить тебе это было прям просто замечательно я у меня уже прям голова полна я предлагаю на сегодня закончить мы как раз должны 8 ровно закончить это вежливо к теме еще надо будет вернуться потому что она столько дала интересных ответвлений надо будет потом пересмотреть и в общем еще кстати нам не позволит высказать общие наверное пожелания присоединяйся к нам пожалуйста почаще 

S03 [01:49:38]  : конечно, сильный искусственный интеллект, который ты там напрямую высказываешься, но мы затрагиваем все абсолютно вопросы, и несмотря на то, что вывеска именно такая, но сутевое содержание, оно очень разностороннее, и нам нужно обязательно к Ритмасу. И поэтому, находив время, четверг удобный, в принципе, вечером и так далее, мы тебя приглашаем чаще. Молодец. 

S00 [01:50:05]  : Спасибо за приглашение, постараюсь. 

S04 [01:50:08]  : Ладно, друзья, спасибо всем огромное, было интересно. Константин Александрович, еще раз спасибо. Спасибо всем. Всем счастливо. Хорошего вечера. Спасибо. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
