## 8 сентября - Обзор семинаров INLP-2021/2022 по интерпретируемой обработке естественного языка - Антон Колонин — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/iK1VRF3lqGQ/hqdefault.jpg)](https://youtu.be/iK1VRF3lqGQ)

Суммаризация семинара:

Семинар посвящен теме интерпретируемой обработки естественного языка (NLP). Это направление в искусственном интеллекте, где языковые модели представлены в интерпретируемом виде, что позволяет понять, что содержится в модели, и объяснять методы работы с ней, включая понимание и генерацию текста. Такие модели удовлетворяют требованиям объяснимого и интерпретируемого искусственного интеллекта.

В вводной части семинара упоминается, что, несмотря на популярность глубоких нейросетей и трансформеров, существуют старые технологии, такие как семантические графы и формальные грамматики, которые все еще находят свое применение, особенно в интеграции с современными подходами и в промышленных приложениях.

История темы начинается с 2014 года, когда Лайн Свепс и Бен Йорцель написали статью, описывающую подход к обучению и построению языковых моделей с использованием метода unsupervised learning и получения моделей формализмом грамматики связей или linear grammar. На семинаре обсуждается развитие этого направления, его потенциальные недостатки, а также возможности и улучшения, основанные на критических конструктивных моментах.

Кроме того, был упомянут проект "Young Grammar", поддерживаемый Лайносом Вепстасом, который работает над интерпретируемыми языковыми моделями и предоставляет доступ к коду проекта в открытом доступе. Система еще не готова к промышленному использованию, но предоставляет возможность для исследований и экспериментов.

В рамках семинара также были обсуждены проблемы и результаты работ, связанные с обучением интерпретируемых моделей и их применением в задачах, таких как генерация текста, вычленение отношения и названия сущностей, а также попытки интеграции с нейросетевыми моделями, в частности GPT и BART.

В итоге, семинар охватывал широкий спектр вопросов, связанных с интерпретируемой обработкой естественного языка, включая исторический контекст, текущие разработки, практическое применение и интеграцию с современными технологиями.








S01 [00:00:08]  : Трансляция, надеюсь, пошла. Коллеги, еще раз всем добрый вечер. Сегодня у нас не совсем обычный семинар. Мы в режиме дайджеста будем делать обзор двух семинаров. по тематике интерпретируемой обработки естественного языка. И в качестве вступления я сделаю небольшую историю этой темы и этих семинаров. Интерпретируемая обработка естественного языка – это про НЛП, где мы предполагаем, что языковая модель у нас представлена в интерпретируемом виде. И таким образом, с одной стороны, мы можем понять, что у нас находится в модели, что она из себя представляет. А с другой стороны, те методы работы с этой моделью, как в части понимания текста, так и в части генерации этого текста, они интерпретируемы и объяснимы. То есть у нас интерпретируемая обработка естественного языка, она в полном объеме удовлетворяет требованиям объяснимого искусственного интеллекта, интерпретируемого искусственного интеллекта. Но и с практической точки зрения это, по сути, зиждается на так называемых семантических технологиях, на графах знаний, на семантических графах, на формальных грамматиках, которые в последние годы не в большой популярности, потому что у нас прут глубокие нейросети. И там получаются все более и более удивительные результаты. И, казалось бы, зачем возиться с этими старыми технологиями, когда есть такие прекрасные глубокие нейросети и трансформеры. Как показывает практика, даже те люди, которые глубоко занимаются глубокими нейронными сетями, они все время ищут в конечном итоге способы как-то это проинтегрировать и с семантическими технологиями, графами знаний, ну а при генерации текстов в промышленных масштабах и в промышленных приложениях вообще используются совсем уже дубовые технологии на основе слотов, заполняемых в предопределенных шаблонах, где даже о символьных вычислениях говорить не приходится. просто заполнение шаблонов. При обработке входной информации могут применяться методы entity extraction и relationship mining, основанные на глубоких нейронных сетях с теми же самыми трансформерами. Но когда мы генерируем текст для решения каких-то приходных задач, все достаточно просто потому что вот не глубокие нейронные сети не позволяют в конечном итоге в промышленных масштабах и с промышленным качеством генерировать адекватный контент вот ну и собственно вот поэтому этой темой люди все равно пытаются заниматься и мне кажется это тема актуальная ну и история вот конкретного семинара и конкретного направления этой темы. Она уходит, на самом деле, 20 с лишним лет назад, когда Бен Герцель создал вот этот вот проект WebMind, в рамках которого мы с ним познакомился. После того, как этот проект заглох, спустя лет, наверное, 10, Бен Герцель перезапустил эту историю под брендом OpenCock. Вот, в рамках этого OpenCog, естественно, нужно было устраивать какую-то часть, отвечающую за Natural Language. В качестве этой части была вставлена технология, я там, кстати, не знаю подробности истории, значит, что появилось вначале Lingrammer как часть OpenCog или Linus Vepsos как часть OpenCog, и кто кого притащил, то ли Linus Vepsos притащил Lingrammer, то ли Линграммуру присоединился Linus Vepsos. Но суть в том, что Linus Vepsos в рамках OpenCog начала заниматься этой NLP-частью. Существенной частью этой технологии оказалась как раз технология Lingrammer, про которую мы чуть-чуть будем говорить сегодня. Ну, не чуть-чуть, довольно много. И в 2014 году Лайна Свепса вместе с Беном Йорцелем написали такую статью типа «Манифест», которая есть… Ссылка на эту статью лежит в начале плана сегодняшнего семинара в Телеграме. И там описывается подход, как мы вообще можем учить и строить языковые модели. методом unsupervised learning и получать в конечном итоге эти модели формализме грамматики связей или linear grammar. Сегодня мы как раз будем говорить о развитии этого направления. В частности, также мы поговорим о том, где это направление может быть недостаточно хорошо, какие у него есть недостатки с одной стороны, с другой стороны, какие На его основе можно строить приложения и какие возможны улучшения. Критические конструктивные моменты будут озвучены. Ну и, собственно, поскольку в какой-то момент Бен Герцель мне предложил этим направлением серьезно позаниматься, и пару лет я занимался, и вот один из докладов сегодняшний, который будет сегодня упомянут, как раз связан с той моей работой Бена. Потом, когда были получены результаты Те, которые я тоже покажу, и они оказались не очень хорошими по причинам, которые я тоже расскажу, и, возможно, Николай тоже про них сегодня скажет. В общем, в связи с ситуацией финансовой в мире и в связи с не очень хорошими результатами, это направление было подзаморожено, Singularity нет. Но, поскольку у меня, например, интерес остался, я Бену предложил в прошлом году организовать семинар по этой тематике. И в этом году уже второй семинар мы проводим, и я эту тему веду на свободное от основной работы время. И Николай тоже тут подключился. В общем, направление живет и в каком-то виде. И я теперь прямо перейду, собственно, к обзору. тех докладов, которые представлены были на семинарах НЛП в прошлом и в этом году. Доклад Лайноса Вепсоса, который является историческим идеологом данного направления, начинателем данного направления на данном историческом этапе, я возьму за этот год. Он в большой степени повторяет итоги прошлого года. Вот, и идея заключается в том, этого доклада прежде всего, что вообще мы любые задачи искусственного интеллекта и машинного обучения сможем свести к задачам на графах. И когда мы говорим о графах, это мы говорим о топологии и геометрии на самом деле, а все численные методы, которые используются для этого, это просто методы, которые нам дают возможность с этой топологией и геометрией работать. Natural language processing – это не более чем одно применение. История эта выходит за рамки грамматики связей. Вторая ссылка в плане семинара, которая идет на статью – это другая статья уже Лайнесса, где он, расширяя грамматику связи, поднимая ее на следующий уровень, строит более сложную конструкцию. Называется shift topological approach to big data. Как-то так, по-моему, называется. Там есть тоже интересные идеи с точки зрения того, как можно то направление, о котором мы сегодня будем говорить, применить к решению вообще любых задач. связанных с искусственным интеллектом. Традиционно начинается история с критики глубоких сетей, что глубокие сети ничего не понимают. Они просто связывают что-то с чем-то и получают нечто. Получают какие-то странные конструкции, не имеющие никакого смысла, но напоминающие кружки, или получают уродливых коров. Следующая идея, которая Лайносом озвучивается, что, как я уже сказал, на самом деле все мы можем свести к топологии и геометрии для того, чтобы нам понять природу и суть, и характер какого-то объекта, нам нужно восстановить его геометрическую структуру, а не просто набибор пикселов и связи между этими пикселами, как это делают нейронные сети. Для того, чтобы распознать тор, нейронную сеть нужно учить на всех возможных проекциях этого тора. В то время как описав этот ТОР просто дискретной сеткой или некоторой функцией, которая этот ТОР моделирует, мы сразу получаем ТОР. Нам не нужны огромное бесчетное количество пикселов, описывающего о всех проекциях. Ну и вот, собственно, переходим дальше к развитию теории графов в преломлении следующей дискуссии, что если у нас есть некоторая структура, которая описывается графом, причем граф может быть в многомерном пространстве в общем случае, мы можем выделить такие вещи, как то, что Лайнесс называет jigsaw puzzle. То есть, если у нас есть некоторая вершина, или некоторый набор вершин, которые определенным образом связаны с другими вершинами. И каждая вершина в определенном контексте для формирования устойчивой структуры предполагает связи с другими вершинами. Вспоминаем валентности, например, атомов для развивания различных химических молекул. И в разных ситуациях валентности могут быть разные. Так вот, в этой ситуации у нас как раз получается, что у одной вершины могут быть, грубо говоря, стрелочные валентности типа папа или коннекторы типа папа, а у других вершин могут быть коннекторы типа мама. И если у нас правильные типы коннекторов папа-мама и правильное число, то у нас из соответствующих вершин могут собираться всегда правильные структуры. Вот пример того, как мы можем с помощью графов, порезанных на коннекторы, полусвязи, в теории, которую развивает Лайнесс, они называются коннекторами. У нас есть, к примеру, у бэкграунда есть бэкграунды типа коннектор, исходящие и входящие. Соответственно, мы знаем, что в светофоре красный, желтый и зеленый бывают на черном бэкграунде. А черный бэкграунд, в свою очередь, в контексте светофора, он может включать в себя красный, зеленый и желтый цвета. Ну и то же самое с формой. Точно так же мы коннекторами можем описать отношения между различными знаками светофора и формой. И вот как бы в семантическом графовом пространстве все множество светофоров такого типа, безотносительно на каком расстоянии мы их видим, под каким углом мы видим, оно просто инвариантно описывается вот этим набором вершин и коннекторов. И для того, чтобы у нас получился полноценный светофор, все эти коннекторы должны правильным образом соединиться. Примерно так же, как в работах Евгения Евгеньевича, правильные части цифр должны правильно предсказать друг друга. И взаимообуславливающие предсказания обеспечивают нам наличие этой цифры. С точки зрения Лайнесса здесь у нас речь идет не о предсказании, а о совпадении коннекторов определенного типа. Ну и вот тот слайд, который ходит по всем докладам Лайнесса, меня, а теперь еще и Николая. Это каким образом у нас устроена грамматика связей. То есть, у нас есть некоторые грамматические категории. Например, грамматическая категория – определитель. Грамматическая категория – животное. Грамматическая категория – женщина или человек. И определенные типы коннекторов. которые они предполагают. Например, если у нас есть переходящий глагол, то вот у него налево есть связь на объект, а направо у него связь с субъект. Соответственно, если мы берем и правильные слова соединяем посредством правильных коннекторов, которые как jigsaw puzzle здесь подходят друг к другу, то мы автоматически собираем правильное выражение. Дальше мы им и в принципе, как утверждает Лайнесс, мы можем дальше распространять это вообще на все. То есть, мы можем говорить о словах предложений, а мы можем говорить о семантике, о семантических соотношениях каких-то сущностей. То есть, в этом случае любое семантическое Бинарное семантическое отношение тоже может быть рассмотрено как комбинация коннекторов. И для того, чтобы правильно и сложно построенный смысл собрался воедино и автоматически получилось некоторое понятие высокого уровня, нам нужно просто, чтобы правильно совпали вот эти все коннекторы. Я выбегу быстро, у нас все-таки сегодня обзорные доклады. Точно так же Linus рассказывает, как мы можем задачу акустического распознавания сигналов и речи в том числе решать абсолютно в той же парадигме. Можно посмотреть видео. Он даже предлагает pipeline, как эта задача может решаться. Дальше. Он переходит к экспериментам. Я забегаю вперед. Проблема большая, к сожалению, проблема дальнейшего развития этой истории, что с 2014 года в свободное от основной работы время Лайнос ведет эти эксперименты, но ничего близкого к каким-то пайплайном и state-of-the-art результатом, хотя бы не то что сопоставимым со state-of-the-art, который имеется у глубоких нейросетей, но даже хоть каких-то чисел, которые бы соответствовали какому-то коду и каким-то данным загруженным в open source с точки зрения формирования модели, их нет. Вот сам проект, который обеспечивает поддержку грамматики связей, это полноценный живущий проект. Там сейчас поддерживается около двух десятков языков, по-моему. Причем английский представлен очень хорошо, русский похуже, но остальные языки по экспоненциальному закону с худшей степенью качества. Но это чисто сам парсинг. Если у нас есть грамматика связей, на некотором языке формальная модель записывается в файл некоторой структуры, то мы можем запустить парсер, который понимает эту грамматику, и распарсивать, и получать деревья грамматического разбора на различных текстах. текстов на тех языках, для которых эта грамматика поддерживается. А вот что касается автоматического формирования этой грамматики, никаких результатов, кроме наших результатов, про которые я буду говорить позже, нет. И никаких прикладных приложений, кроме тех приложений, про которые, опять-таки, сегодня буду говорить, к сожалению, нет. И это является одной из проблем данного направления. Тем не менее, вот Лайонес там ставит какие-то эксперименты, он пытается рассчитывать как раз в предположении о том, что мы можем выявить вот эти парные связи, которые связывают слова в графах. грамматических связей в предложении, если мы можем посчитать взаимные вероятности между этими словами, то вот получаются такие очень интересные распределения. Значит, вероятность появления пары и в зависимости от взаимной информации между этими словами. Дальше Лайнас опять-таки поговорит, что мы можем этот подход распространять вообще на любые задачи распознавания, что тут у нас есть последовательский граф грамматического разбора предложения, а тут у нас есть граф каких-то действий. Что вот если я поднимаю локоть и поворачиваю запястье, то у меня где-то вот здесь вот болит. И вот на самом деле this – это рассматривается как просто на самом деле анафора. То есть, человек говорит «it hurts when I do this», при этом человек поднимает руку и поворачивает локоть на приеме у врача, допустим. И вот этот this в лингвистическом пространстве соответствует некоторому действию в физическом пространстве. И, соответственно, если мы с помощью одного и того же графового формализма, описываем текстовую информацию и Визуальную информацию мы в внутреннем представлении можем их соединять через те же самые коннекторы. Дальше здесь много цифр, но все эти цифры, к сожалению, относятся только к анализам частотностей, вот этих взаимной информации между парами слов. английских арпусах и никаких более внятных практических результатов, к сожалению, нету. Но то, что нам удалось в долго рамках этого формализма получить, я расскажу чуть позже. А сейчас мне хотелось бы, если нет, очень коротких вопросов, потому что дискуссия у нас со мной будет позже. Если пара коротких вопросов есть, можно и сейчас. Если нет, то я слово Николаю передам Михайловскому. Итак, коллеги, есть какие-то вопросы короткие, принципиальные? Нет. Хорошо. Николай, тогда, пожалуйста, вам слово. Да. Добрый день. То, что я сейчас рассказал – это краткое summary докладов Лайнесса в прошлом и в этом году. А Николай сегодня как раз на семинаре в этом году делал выступление сразу после Лайнесса. Николай, пожалуйста. 

S02 [00:18:54]  : Да. Пришел ли мой экран? Да, да. Отлично. Собственно, примерно Полгода назад я начал разбираться во всей этой тематике, пытаться разобраться. У нас был в феврале длинный разговор с Антоном, где он меня просвещал в области Lean Grammar. Но мой интерес к этой области, он идет из немножко другой стороны. Он идет от, собственно, вычислительной лингвистики, а не от сильного AI. Хотя сильное AI тоже, конечно, очень интересно. Во-первых, языковые модели сейчас везде. Это не просто модели, которые присваивают вероятность и последовательность тем слов. основа всего, что сейчас делается в вычислительной лингвистике, поскольку большие языковые модели типа BERT или GPT, они у нас находятся в основе всего, что практически делается сейчас в вычислительной лингвистике. Однако, как только мы хотим перейти от коротких текстов к длинным, Мы встречаем проблемы. Мы в своей практической работе встретили проблему, когда мы пробовали с наскоку позаниматься суммаризацией длинных текстов, например, длинных совещаний. Выяснилось в том, что Есть несколько важных особенностей текста, которые не очень общеизвестны. Одно из них это то, что корреляции в естественном языке убывают в соответствии со степенным законом, грубо говоря, как единица на L в степени гамма, где L расстояние между токенами. Это экспериментально подтверждено многими работами и предполагается, что такое степенное убывание корреляции связано с иерархической организацией человеческих текстов, то есть с текстом мы работаем во многих вложенных контекстах, то есть это приведу пример Антона, то есть есть длинный контекст всего творчества Льва Толстого, это его язык, его время, которое он описывает XIX век, это Тот язык, та среда, про которую он пишет, есть тот контекст, в котором находится роман «Война и мир», и этот контекст довольно широкий. Дальше есть контекст, собственно, описания Пьера Безрукова и так далее, вплоть до контекста конкретного параграфа или предложения. Но тот факт, что еще раз корреляции в естественном языке убывают степенным образом, он неким образом удивительный, потому что регулярные грамматики и марковские цепи, они генерируют экспоненциальное убывание корреляций. Контекстно-свободные или контекстозависимые грамматики, они могут генерировать уже такое убывание корреляции степенное. Именно поэтому для того, чтобы обрабатывать длинные тексты эффективно и качественно, необходимы некие инструменты, которые будут тоже на генеративном уровне способны генерировать убывание корреляции по степенному закону. Доказано это для марковских цепей, но судя по всему и есть очень громкая очень свежая работа июльская гугловская нейронные сети иерархия хомского которая показывает что даже те инструменты которые вроде бы должны работать со степенным убыванием корреляции из из подхватывать иерархические структуры, они этого, будучи обучены в глубоких нейронных сетях, не делают. Ну и простая демонстрация всего этого – это то, что генерация текстов с помощью трансформеров требует всяких трюков для того, чтобы получить достаточно хорошую генерацию. Поэтому, ввиду всего вышеописанного, интересно построение неких языковых моделей, которые будут показывать поведение с медленно убывающими корреляциями, например, устроенных иерархическими, как устроены контекстно-свободные грамматики или как устроены линг-граммы. То есть это необходимое, но недостаточное движение. Соответственно, Для задачи состоит возможность построить языковые модели, то есть некие вещи, которые предсказывают вероятности слов и текстов, которые устроены как линграмы. Такие модели строились. И вот с литерой Temperly, которая Собственно, придумали ту линг-граммар, которую использовал потом Линес Вепста с Герцлем и другие. Они в своих новых, в своих следующих работах, это все 90-е годы, самое позднее, это 2000-й год, Паскин, грамматические биграммы, они пробовали построить формализм языковых моделей, который связан с языковыми, с грамматиками. Но для той конкретной грамматики, которую используют коллеги Антон и Линус, этого не было сделано, поэтому первая вещь, которую хочется сделать, это дополнить формализм link grammar такими штуками, как терминальные коннекторы. Если внимательно посмотреть на тот пример, который уже был приведен Антоном, видно, что некоторые коннекторы остаются без второго куска. И теоретически в это место должно бы, могло бы быть вставляться либо начало предложения, либо продолжение предложения, но могут быть туда вставлены то, что я называл терминальные теги, которые завершают такое построения. То есть, когда мы начинаем генерацию текста с помощью Lingammer, мы должны, пусть с некоторой вероятностью, быть способны прицепить к каждому свободному терминатору что-то и продолжать этот процесс, пока он не закончится тегом терминальным. Вот, собственно, исходный формализм, он не имеет терминальных тегов, он имеет тег начала предложения. Но тег начала предложения недостаточен для того, чтобы из Lean Grammar сделать языковую модель. Во-вторых, я стал рассматривать то, как устроена статистика и как на основе фреквентистской статистики может быть построена языковая модель грамматики связи. И самое главное, наверное, здесь вот что. Когда мы берем просто языковую модель, она является аппроксимацией. То есть, когда мы берем n-граммную языковую модель, она является разложением по chain rule. На самом деле, по-русски тяжелее это все рассказывать, чем по-английски. Все время хочется что-нибудь рассказать по-английски. Когда мы берем n-граммную языковую модель, Оказывается, что она является аппроксимацией полной оценки. И в ней есть некая вероятность контекста, которая разумным образом может быть аппроксимирована единицей. В том смысле, что именно с фреквентистской точки зрения Когда мы имеем длинный кусок текста, он в нашем корпусе встречается один раз, и поэтому вот эта вот приближающая часть, в том числе с фреквентистской точки зрения, может быть аппроксимирована единицей. То есть это аппроксимация, когда мы N-граммой, языковой моделью аппроксимируем вероятность текста, она имеет под собой понятный фриквентистский смысл. А вот дело с графовыми моделями, оно обстоит хуже. Когда мы пытаемся построить языковую модель на основе графа, мы должны вроде бы тоже учесть контекстную информацию, когда мы оцениваем вероятность предложения. То есть вероятность некого предложения Толстого в контексте творчества, скажем, даже Гоголя, она очень низка. вероятность предложения Толстого в контексте текста Толстого довольно велика. Но как мы можем строить языковую модель из дерева? Мы должны начать с некого корневого слова, обычно это глагол, и от него Вот так вот, как я показывал на предыдущих картинках, по вероятностям строить предложение, используя некий либо вероятностный способ выбора следующего слова, либо иной. То есть, если мы аккуратно проделаем эту даже не то чтобы аккуратно, а если мы примем предположение, что вероятности разных ветвей, идущих от корня, они независимы, что на самом деле тоже сильное предположение, надо еще разбираться, что из этого предположения следует. вообще они должны быть зависимы. Но если мы даже такую вещь предположим, мы будем говорить, что вероятность предложения зависит от вероятности слова, высчитанных при условии вероятности всех, при условии всей предыдущей части ветви. которая с одной стороны может быть не нулевой, а с другой стороны она и не очень длинная. То есть она не обязательно состоит из двух слов, корня и конца ветви. Она может состоять из трех, из четырех, из пяти слов. Но она не настолько велика, чтобы так как Н-граммах мы могли отбросить все предыдущее и оценивать вероятность последнего слова в этой ветве только на основе предыдущей. Мы обязательно должны оценивать ее при условии всего предыдущего пути. И это очень важно, потому что вот в классической диссертации Юре Майтишной, там вероятность предложения оценивается на основе исключительно пар слов, а не всего пути от корня. То есть считается, что в дереве в синтаксическом дереве, которое покрывает слово, слово зависит только от предыдущего. И это очевидно неверно. А дальше неверным оказывается весь основной посыл. То, что энтропия модели полностью определяется взаимной информацией, которая в закрыта в конкретных парных отношениях. То есть ключевая вещь, которую я хочу сказать в том, что отношения важны не только парные, а более глубокие, по крайней мере до корня, а на самом деле и в другие ветви и возможно в контекст и только тогда мы можем оценить вероятность предложения вот в этой парадигме каких-то грамматик связей. Соответственно, на этой диссертации Юре построено много работ, в том числе работа Линуса Вепстаса, которую Антон упоминал только что. Соответственно, мы видим, что подсчет парных взаимных информаций между словами не помогает не может помочь оценить вот такие вещи. Собственно у меня все. 

S01 [00:34:59]  : Николай, спасибо. У нас есть один вопрос и два моих комментария. Пока не забыл комментарии. Во-первых, терминаторы в LingQ Grammar есть, по крайней мере в реализации в коде. Я вот там кинул пример, как это работает. И там есть такое понятие left-wall. Это начало предложения. Восклицательный знак и просительный знак тоже являются терминаторами, но уже с правой стороны. В кодовой реализации это есть. Второй комментарий. То, что вы последнее сказали, это действительно очень важно. Проблема заключается в том, что когда я в 2017-2018 году в этот проект влез, Я довольно много потратил сил, чтобы попытаться убедить Бена и Лайноса, что по этой причине формализм и граммар недостаточны. А формализм и граммар определяют определяется понятием dependency parsing, где парс определяется связями между терминальными вершинами. И вершин более высокого уровня нет. Это только про связи между словами. Ничего больше, кроме слов и парных связей между словами, в dependency parsing нет. А в парадигме constituency parsing есть как раз вот эта пресловутая иерархия, есть понятие фраз. И дальше можно говорить о том, что этого тоже недостаточно, что еще нужны более высокие уровни, на которых мы можем хранить какую-то частоту информацию. Но хотя бы уровень фраз, где на уровне фраз, объединяющих два или даже три, а может быть даже где-то четыре слова. Мы тоже можем хранить частотную информацию. Это необходимо. И Бен, и Лайнос, меня убедить не удалось. Они стояли на том, что всегда можно преобразовать другое, но одно другое можно преобразовать с точки зрения структуры. А вот с точки зрения частотной информации, у нас как раз на уровне dependency parsing исчезает или не может сохраниться часть информации, которая у нас может быть на уровне constituency parsing. Спасибо, Николай, что этот тезис вы поддержали и есть еще вопрос от захара понимаешь значит для понимания вот вы когда говорите про корреляции до разные формы корреляции вы говорите про авторитет автокорреляционную функцию с лагами или задержками или корреляцию между и беден имбидингами нет это все эти работы это не про которые я цитировал 

S02 [00:38:06]  : То есть они были написаны на слайдах, а я про них не говорил. Все эти работы, они работают с функциями непосредственно поверх, ну скажем, алфавита. То есть это автокорреляции с лагом. между либо энграмами, либо словами какого-то тезауруса, либо еще некими конструкциями, это работы в основном с 10 по 15 год и там еще нету речи ни про какие эмбеденки. Кстати, раз уж зашла речь про имбединги, то пока я слушал Антона, мне начало казаться, что вообще все эти структуры надо просто в гиперболические метрические пространства засовывать. Они обладают структурой графов, но дифференцируемо. Как я уже Антону говорил, я очень нервничаю, когда что-то нельзя продифференцировать. Мне кажется, ежели эти графы начать в гипераболические метрические пространства засовывать, то с одной стороны оно продифференцируется, а с другой стороны оптимизационными методами тогда уже можно решать задачу в духе любимого главе эмбеддинга. 

S01 [00:39:37]  : Хорошо. Николай, спасибо. Коллеги, еще есть какие-то принципиально важные вопросы? Или мы двинемся дальше? Нет? Хорошо. Тогда двигаемся дальше. Николай, спасибо. Я включаю теперь снова свою часть. Собственно, прежде чем перейти к скучной части по поводу того, как учить такие интерпретируемые модели, там действительно все будет несколько печально. за исключением самого конца. Давайте вначале поговорим о том, что можно с этими моделями делать, предполагая в том, что они есть. Самое простое, что можно делать и что делают в общем в продакшене. Кстати, сейчас запущу ссылку. Видно мой экран, да? Вот, пожалуйста, то есть вот еще с времен Дэна Слейтера проект Young Grammar, который сейчас поддерживает Лайнос Вепстас и его небольшая команда. Коммиты идут там практически каждый день. И как минимум каждую неделю обязательно все время что-то новенькое. Это все написано на C. И вот можно посмотреть в Data лежит список того, какие языки здесь поддерживаются. Наиболее активно поддерживается английский. Но вот видно, что другие языки тоже периодически обновляются. Просились некоторые фиксы в прошлом месяце. Наши коллеги из института информатики в Новосибирске активно участвовали в поддержке как минимум казахского и еще ряда других языков. Но это к слову. То есть парсинг, пожалуйста, если мы получаем дерево, мы получаем категории связи, а дальше мы уже можем там делать извлечение relationship extraction, named entity recognition, точно так же, как мы это делаем в нейросетях, но уже не поверх токенов, а поверх уже грамматических структур. Это как бы понятно, это не обсуждается. Нам захотелось попробовать с коллегой более интересные варианты реализовать, связанные, например, с сегментацией. Поскачу по его склайдам. То есть вот мы пытались решить задачу question-answering. Я покажу, что там получилось. Ну и вот задача с имитацией текста. Как выглядит задача? Мы, к примеру, со speech recognition, вроде того, который можно натравить на запись нашего семинара на гугле, Получаем просто набор слов. Ни один из существующих Speech Recognizer не умеет бить предложение на части и правильно расставлять знаки припинания. А вот мы пытаемся это делать. Как мы пытаемся это делать? Мы берем, предполагаем, что текст, который мы получаем со Speech Recognition уже побит на токены, то есть пробелы, по крайней мере, стоят. и мы можем модифицировать последовательность токенов, мы предполагаем, что этот текст формируется в соответствии с той грамматикой, которая описывается известной грамматикой связи. Вроде той, которую для английского можно скачать с сайта, который я только что показал. После этого мы берём и слева направо начинаем просто строить деревья грамматического разбора. Вот тут у нас дерево построилось, тут у нас дерево построилось. В общем, идём до тех пор, пока у нас дерево строится. Как только дерево перестало строиться, мы понимаем, что всё, здесь произошёл разрыв. И, значит, говорим, окей, здесь у нас конец предложения. Начинаем строить дерево с самого начала. Сначала дальше справа налево. Вот здесь оно у нас снова перестало строиться. Мы снова строим разрыв и идем дальше. Вот, ну и, значит, так, давайте сейчас я полный экран переведу. Вот, соответственно, вот примеры, то есть это может быть извлечение информации со speech recognition, это может быть парсинг веб-страниц, когда мы веб-страницу берем, выдираем текст из HTML и CSS, и дальше нам нужно этот текст привести читаемый вид вот на хтм страницах если у нас там формат представлен с помощью css у нас там знака припинания быть не может и все равно мы как-то можем умудриться вот используя эту технологию выявить законченное грамматически цельное предложение Вот, значит, у нас архитектура того решения, которое мы делали. У нас есть файл Cilingrammar для данного языка. Мы его загружаем в некоторый словарь, который у нас лежит в памяти. Вот стандартный пайплайн парсинга. Это вот после того, как у нас файл лежит в памяти, мы загружаем этот словарь в парсер. И когда на вход к нам приходят предложения, мы получаем разобранное предложение. Вот, кстати, слева у нас как раз терминатор, вот этот вот left wall, пресловутый, из которого начинаются все грамматические разборы. А если нам нужно решать задачу сегментации, то у нас pipeline другой. То есть у нас есть просто некоторая последовательность слов, мы ее подаем на сегментатор, который пытается строить фразы ну и вот у сегментатора получается что строится фраза из первых четырех слов из последних четырех слов строится четыре независимые две фразы по четыре слова в полноценные графы и соответственно мы говорим что окей вот тут у нас есть два предложения И эту задачку пытались решать. И вот у нас результаты. Мы пытались решать его на двух маленьких корпусах. К сожалению, недостатком этой работы является то, что недостаточно большие объемы тестирования. Ну вот у нас был некоторый small корпус, на тренировочном корпусе, том самом, который мы использовали для обучения грамматикам, про который я буду говорить дальше. Мы получили достаточно высокую accuracy 0.97 на разбивку границ. Это маленькие лексиконы, маленький корпус, связанный с общениями, с текстами по поводу жизни одной конкретной семьи. Папа-мама, папа любит сосиски, девочки любят конфеты. Вот такой уровень текста из всего нескольких порядка меньше сотни предложений и порядка нескольких десятков слов в лексиконе. На реальном корпусе, Гутенберг Children корпус, на котором будут все другие эксперименты, про которые я буду сегодня говорить, там получилось акурасе похуже. То есть, получилось где-то 0,77 акурасе. Но, тем не менее, результаты какую-то практическую ценность имеют. Причем мы сравнивали с State of the Art на вскидку на наших корпусах. Там результаты получились сопоставимые. Есть статья, можно посмотреть. Следующий пример, который мы решали, это задача Natural Language Generation. Классическим подходом является surface realization, где мы берем набор слов и соединяем этот набор слов, используя Грамматические правила, известные из базы данных Lingram для этих слов, соединяем их в единственное правильное предложение, и вот получается предложение the cat, кот и маус. Ну вот сразу здесь может броситься в глаза, что мы можем собрать два предложения the cat, кот и маус, и можем собрать a mouse, кот, a cat. Правильно? Потому что cat и mouse в данном случае являются эквивалентными грамматическими категориями. относительно переходного глагола код. И здесь как раз возникает проблема, что для того, чтобы нам генерить внятные тексты, нам нужна не просто некоторая структура. Эта структура, надстроенная над нижним уровнем грамматическим, она должна отражать В идеале она должна отражать семантику. И вообще задача встраивания Lean Grammar OpenCock как раз была в том, что этот Lean Grammar будет на нижнем уровне, а на верхнем уровне будет семантическая база данных с информацией о том, что коты едят мышей и ловят мышей, а мыши не едят котов и не ловят котов. В том случае, про который Николай говорит, эта информация может быть частотной и статистической. То есть, у нас может быть много текстов, где коты ловят мышей, и меньше текстов, где мыши ловят котов. В зависимости от того, насколько мы будем разогревать модель, у нас с большей вероятностью все равно будет получаться, что коты чаще будут ловить мышей. Но это в случае малоинтересно. Если мы знаем все слова, из которых нам надо собрать предложение, эта задача достаточно простая. Есть более интересная задача, которую мы пытались решать. Мы пытались решать задачу, когда нам неизвестны не все слова. Допустим, нам нужно что-то сказать про кошку и мышку. что вот ты знаешь про кошку и мышку мы берем слово mouse и мы берем слово cat и мы все равно получаем грамматически правильное выражение потому что мы берем и значит из тех связей которые известны для кота и мыши мы берем и смотрим те связи которые имеют что называется открытые неподсоединенные коннекторы и пытаемся понять а какие еще слова нам нужны. И вот мы выясняем, что для того, чтобы mouse и cat соединились в правильное предложение, во-первых, к ним должны быть артикли приставлены, и мы добавляем к артикли. И еще какое-то отношение между ними должно быть. Какое отношение? Вот это должно быть отношение cat. Код. Давайте я перейду к примеру. Вот примеры, которые мы получили. Опять-таки, на маленьком корпусе слева, который описывает жизнь одной семьи, мы получили достаточно хорошие результаты. Вот, например, из 92 предложений, в 62 предложениях, 62 предложения оказались уникально точными. То есть у нас на каждый набор слов, причем слов могло быть меньше, чем нужно, то есть могло быть вообще одно слово, из которого мы собираем, пытаемся собрать целое предложение. В общем, 62 из 92 оказались точными. А в 30 случаях из 92 оставшихся у нас оказалось много вариантов, но при этом всегда одно предложение из этих вариантов было правильное. Ну и соответственно такого, чтобы было ни одного правильного предложения у нас не было, то есть accuracy у нас в итоге получилось 1. Но это было сделано для случая, когда грамматика была ограничена данным корпусом. То есть, у нас все грамматическое, набор грамматических правил был построен в результате, на основе той технологии, про которую я буду рассказывать дальше. То есть, мы, имея просто набор текстов, построили грамматику, мы обучили модель, а потом с помощью этой модели принялись генерировать тексты. И тут у нас, если у нас текст находится Как бы тестовый корпус совпадает с тренировочным корпусом, неудивительно, что мы получаем на курсе единичка. А вот с правой стороны, тут вот неправильный заголовок над графиком, над табличкой, с правой стороны мы делали следующее. Мы брали тот же самый тестовый корпус. маленький, но грамматику мы брали реального английского линграммера, то есть мы взяли настоящий английский линграммер, который Лайнос с его командой там создавали много лет и на нем тестировали. Ну вот у нас получилось так, что у нас все получилось гораздо хуже, например, то есть accuracy у нас получилось в общем 0.7, Однозначных фраз получилось только 80-92. Множество разных фраз, где одна всегда точная получилась 57 из 92. Ну и 27, чтобы заведомо были ошибки, у нас не получалось, но 27 фраз из 92 нам дало столько много результатов, что мы просто по вычислительным возможностям обрывали случаи, когда у нас получалось больше, чем 25 возможных кандидатов в этом эксперименте. они у нас оказались за рамками рассмотрения. Ну и сопоставляли результаты генерации с какими-то state-of-the-art, тоже получилось сопоставимо. Вот comparison with prior work. Вот результаты достаточно хорошие, но с поправкой на то, что набор очень маленький. Убедительно эта работа, к сожалению, считать нельзя. У меня было достаточно времени и сил, чтобы на больших корпусах ее проверять. Ну и кроме того, это уже к ссылке к коду. И последняя задача, которую мы решали с помощью этой же Language Generation, с помощью этого же Generation Framework, это контекстная генерация, то есть в режиме Question-Answering. Когда мы берем и задаем, к примеру, вопрос, если есть какой-то текст, где что-то написано про жизнь рыб, И мы задаем вопрос, допустим, что такое туна? Вот из туна. Текст, который генерируется, он генерируется в соответствии с грамматикой связи, известной для английского языка. Но контекст определяется теми связями, которые актуальны только в размере данного текущего текста. То есть, текст распарсивается. В этом тексте мы считаем частоты связей. И в зависимости от частотности связей в данном рассматриваемом корпусе текста мы выбираем наиболее частые связи. текст генерируется именно в контексте этого корпуса. Таким образом, у нас есть отдельная тоже публикация была на AGI в прошлом году, где как раз задача language generation, sorry, question answering по контексту, конкретному по тексту тоже решается, но и тоже, к сожалению, выборка и набор тестирования пока были не очень большими. Хотя на небольшой выборке тесты показатели по Акурысе соответствовали вполне с глубоко нейросетевым аналогом. так здесь я наверное сделаю паузу есть какие-то вопросы коллеги вот части того что можно сделать этого до это извиняюсь это вот одна работа да давайте я здесь прервусь какие-то вопросы есть по land segmentation generation equation answering с помощью линграммера вопроса нет но есть предложение просто 

S02 [00:55:59]  : После этой вещи как-нибудь обсудить. Просто, Антон, напомните мне про генерацию текстов из неполного набора слов, неполный surface, realization. 

S01 [00:56:13]  : Генерация. Николай, а вы напишите тогда прямо в чате. Вы в чате. Я сейчас пойду дальше, а вы напишите в чате. Коллеги, просьба, у кого есть вопросы для последующего обсуждения, у нас, я надеюсь, время останется, пишите все вопросы не срочно, а для обсуждения просто в чат. Хорошо. Теперь я перехожу ко второй работе, которую делал другой мой коллега по текущему проекту. Там мы зашли немножко с другой стороны. Сейчас я найду эту презентацию у себя. Задача была связана с анализом сентимента, текстов, настроений и использования данных социальных сетей для предсказания криптовалют. И проблема заключалась в том, что нам нужно было определять сентимент. И у нас было понимание… Поскольку обзор… Перейду к последнему слайду. Мы в какой-то момент стали просто пробовать кучу разных моделей. Мы перепробовали порядка 20 с лишним различных моделей по анализу сентимента, который это можно найти на хагенфейс и где угодно в open source с тем, чтобы попробовать определение сентимента на том корпусе новостном, который мы собираем в твиттере и в реддите в части криптовалют, для предсказания событий, которые с ними связаны. Мы с удивлением обнаружили, что лучше всех Был из них только один, который называется FinBert. И FinBert – это BERT, который зафайн-тюнинг на финансовых данных. И мы обнаружили, что если у Файнберта accuracy была 33%, то моя моделька, интерпретируемая… Не моделька, а алгоритм и соответствующая интерпретируемая модель, которую я в своем проекте несколько лет развивал, она оказалась с таким же показателем. То есть, те же самые 0.33. Но если для дальнейшего улучшения качества файнберта нам бы потребовался дополнительный размеченный набор, которого у нас не было, то поскольку в нашем случае модель интерпретируемая, то в течение двух недель ручного файн-тюнинга этой модели на экспертном уровне нам удалось поднять точность до 0 57. Дальше мы просто остановились, потому что были другие задачи. Это вот то, что касается сентимента анализа. В чем сущность этой модели? Сущность этой модели, во-первых, она основана на N-граммах. Я чуть позже скажу, при чем здесь грамматика связи. Во-первых, она основана на н-граммах или лексемах. Во-вторых, там есть такое понятие priority on order, которое мы взяли из одного из предыдущих проектов. Суть заключается в том, что если у нас есть, к примеру, тетограмма not a bad thing. выявлено в конкретном тексте, то у нас в отношении N-грамм, нот ЭБ, 3-граммы нот ЭБ, в отношении B-граммы Бэт Синг, в отношении Юни-грамм, нот ЭБ, Бэт Синг, в отношении их всех играет, что называется, латеральное торможение или ингибирование. То есть, у нас составные части не рассматриваются, если есть соответствие высокого уровня. Структуры более высокого порядка подавляют структуры более низкого порядка. Это как бы встроенная, что называется, захардкоженная фича. Это одна захардкоженная фича. Соответственно, у нас нет проблемы двусмысленности, связанной с отрицанием. То есть, NoteGood – стандартная система распознавания сентимента на NoteGood, либо могут распознать как более сильно негатив, чем good, либо могут распознать good как более сильно негатив, чем not, и в общем получается конфуз. У нас как раз за счет этой фичи этого нет. Ну и вторая интересная фишка нашего сентиментоанализа, что мы отдельно анализируем позитив, негатив, общий сентимент и контрадиктивность. То есть в нашей модели, к примеру, если в некотором В тексте у нас присутствует и позитивный, и негативный сантимент. Мы отдельно говорим, что вот у нас столько-то позитивного сантимента, столько-то негативного сантимента. Общий сантимент получается нулевой. А стандартная система распознавания – это текст идентифицируют как нейтральный. А у нас появляется дополнительная метрика – contradictiveness. Соответственно, мы каждый текст измеряем не в одномерном пространстве с тремя значениями – позитив, негатив или нейтральность, а мы измеряем в четырехмерном пространстве, где каждый из этих четырех метрик имеет саму свою экспрессию. И аналогично мы решаем задачу по когнитивным искажениям в психотерапевтической литературе. Известно в разных школах психотерапевтических есть разные числа этих когнитивных искажений. Где-то их 12, где-то их 14, где-то их 16, где-то их 18. Мы взяли работу, где их 12, потому что в этой работе в приложении к работе приводятся как раз лексемы, лингвистические паттерны, которые соответствуют этим когнитивным искажениям. И в той работе, которая в этом докладе представлена, как раз и анализируются вот эти когнитивные искажения, и показывается, что эти когнитивные искажения кстати, еще лучше, чем сентимент, они коррелируются с какими-то изменениями на финансовых рынках в части цены, так и в части объема торгуемых активов. К чему это в данной теме? К тому, что если мы будем пытаться решать задачу выявления вот этих вот как они называются, лексем или энграм. То есть, на самом деле, это не энграмы, потому что, когда мы строим грамматическую связь, лексема у нас, на самом деле, она может включать в себя разрывы. То есть, лексема, она описывается энским граммой. Например, неплохо. Может быть, не очень плохо, не совсем плохо, Но как бы абстракт высокоуровня эликсима – это неплохо, а там «совсем» или «очень» – это как бы уже детали, которые мы можем не рассматривать в общем случае. Так вот, если мы хотим пытаться выявлять вот такие вот значимые эликсимы из N-грамм, то мы упираемся, так сказать, в очень большой объем перебора. То есть, у нас есть большая вычислительная сложность. И это просто экспериментально известно. А если мы будем пытаться вот эти н-граммы, точнее лексемы, искать на основе уже построенных деревьев грамматического разбора, у нас существенно снижается вычислительная сложность, и есть большие возможности по автоматическому формированию этих лексем. на основе каких-то корпусов, либо размеченных, либо неразмеченных, но это вот уже тема отдельного разговора и некоторое направление, дальнейшее направление работы, как на размеченных, неразмеченных корпусах, на которых построятся деревья грамматического разбора, как выявлять значимые лексемы, как для задач классификации текстов, так и для задач Named Entity Recognition, так и для задач Relationship with Extraction. По этому докладу есть какие-то вопросы? Оперативные, быстрые? Если нет, тогда я сейчас огляну в план. Значит, и пойду дальше. Так, где у нас план? Дальше у нас по плану идёт, собственно, да. То есть, теперь, что называется, самый главный вопрос. Откуда нам взять эти грамматики? То есть, где мы возьмём эти формальные грамматики? Я уже говорил, что, к сожалению, никаких значимых результатов в части автоматического формирования грамматики, чтобы это можно было бы сопоставить с чем-то Лайносу, получить не удалось. Это удалось получить нам. Какие результаты были, я сейчас расскажу. Мы начали с того же самого подхода. У нас есть некоторая грамматика, которую мы хотим каким-то образом получить на основе неразмеченного корпуса. Вот примерно в каком виде мы ставим задачу. То есть, вот у нас общая задача, как она есть по обработке языка. То есть, есть некоторая социальная среда. Это могут быть либо люди, если это разговорный интеллект, либо это может быть корпус текстов, если это работа с текстовыми источниками. Мы можем подавать эту информацию на вход. И если мы подаем на вход, то мы две вещи можем с ней делать. Во-первых, мы можем учиться этому языку и формировать какую-то грамматику и онтологию, которая основана на этой грамматике. С другой стороны, если у нас эта грамматика энтологии уже построена, мы можем воспринимать этот текст. После того, как мы восприняли текст и хотим как-то на него отреагировать, мы можем, справляясь с этой грамматикой, осуществлять генерацию текста, направляя его в окружающее пространство. Три основных подзадачи. НЛП – это обучение языку, восприятие языка и генерация языка. Все они основаны на грамматике и онтологии. Но мы сейчас ставим задачу в этой работе только То есть, пытаться выучить грамматику и, возможно, какие-то корешки онтологии в полном объеме. Онтологию мы не можем выучить, потому что нам нужно для этого некоторое соприкосновение с реальным миром. А мы работаем только с текстами. Я не буду говорить сейчас про пайплайны. Это можно посмотреть доклад на английском, почитать статьи. Я перейду сразу к результатам. результаты, которые у нас были получены. Во-первых, три корпуса. Маленький корпус – Proof of Concept English. Это маленький корпус, где всего было уникальных слов 55, общее число 388, предложение в среднем каждое слово в среднем встречалось 7 раз, общее число предложений. Папа любит маму, мама любит папу, дочка любит сосиски, молоток. Папа работает молотком. Вот такого типа предложения. Со средней длиной предложения 4. Второй корпус – это мы его сами составляли, просто для задач отладки самого пайплайна. Другой корпус, который мы брали, публичный – это разговоры мам со своими детьми. Тоже достаточно простая структура, но объем уже существенный. И третий – это собрание сочинений для детей и юношества на английском языке из корпуса Гутенберга. Вот пример того, как мы работаем со словами в векторном пространстве. Это картинки, очень похожие на то, что делал товарищ Микола в своей работе World2Vec, что мы можем, имея n-граммы, строить векторные имбединги слов и помещать их в векторное пространство, и описывать с близостями, и выявлять близость между ними. Видно, что здесь значит пирог близок сосиски да а дочка близка к сыну а папа близки к маме вот но вот и справа и слева это слова в векторных пространствах не эмбеддингов основанных на энграмах с левой стороны это слова в векторном пространстве коннекторов которые соединяются с этими словами. То есть, это коннекторы, где у коннектора есть направление. Он смотрит вправо или смотрит влево. Это не просто последовательная связь с каким-то словом или некоторые близости с этим словом. Это именно связь с этим словом в дереве грамматического разбора. То есть, эти векторные пространства, в этом смысле, они являются более точными. А справа все гораздо хуже, потому что справа, точнее, или лучше, справа у нас векторное пространство даже не отдельных коннекторов, а у нас векторное пространство конкретных дизджанктов. То есть, например, если у нас есть слово likes, то эта точка определяется тем, что вот у этого слова слева там есть определенный субъект, а справа есть определенный объект. Допустим, там father likes mother или father likes футбол. В этом смысле эти векторные пространства, которые мы смотрим на коннекторах или на дизджантах, они гораздо более разреженные, с одной стороны, гораздо более точные, чем векторное пространство, которое строится на основе н-граммных имбеддингов. Ну и дальше мы решаем разные задачи на основе слов, построенных в этих векторных пространствах. Во-первых, можем строить деревья классификации и можем строить антологии. То есть, мы видим, что у нас там родители и дети имеются в одном кластере, причем борьба родителей у нас – это один подкластер, а дети с хьюманами – они в другом подкластере. Вот так вот получилось. Вот у нас еда и предметы тоже в своей ветке иерархии, причем вот отдельно есть еда, отдельно есть инструменты. А также у нас отдельно есть такие инструменты, как молоток и пила, которые используются для строительных работ. Отдельно есть телескоп, который используется для разглядывания звездного неба. Получаются вменяемые результаты по построению онтологии. И вот другой пример. Мы правильно выявляем такие группы слов, которые определяют время дня, время суток, месяц, день, отношение к текущему времени. Пайплайны я обсуждать не буду. Но результаты получились не очень хорошими. В каком смысле? Все-таки вынужден сказать про пайплайн. для того, чтобы было понятно, в чем проблема. У нас на входе есть некоторый корпус. И этот корпус, для того, чтобы дальше построить грамматику, мы должны каким-то образом распарсить. То есть мы должны построить на основе просто последовательности токенов, мы должны построить деревья. Вот они, справа. Вот ISO, DSO. Вот видите, красные стрелочки, они показывают те связи, которые мы вывели. И вот после того, как мы каким-то образом эти связи выявили, мы берем и с помощью граммар Леонера или граммар induction формируем грамматику связей, которую уже потом мы можем использовать где угодно в тех примерах, про которые я сегодня говорил. Какие проблемы здесь возникают? Я сразу перечислю все проблемы, чтобы не забыть. Проблема номер один, что вообще Lingramar, существующий формализм, структура файла и Lingramar parser, у них вообще ничего нет про вот эту частотную модель, про которую рассказывал Николай. Практически ничего. Там есть некоторая вещь, которая где-то как-то за уши может быть притянута к частотам. Там есть понятие коста. Кост-связи. То есть, там можно вручную, когда настраиваешь этот словарь связи, можно вручную приписать каждой связи некоторый вес. Вот. И, соответственно, или кост. Нет, значит, наоборот, не вес, а кост. Некоторые кост. Соответственно, когда из набора для последовательности слов подбирается адекватное дело грамматического разбора, то, так сказать, учитываются косты связи. То есть, если мы можем получить граф с меньшим совокупным костом, выбирается как раз наименее дорогой граф. Можно как-то рассуждать, во-первых, о том, как эти косты соответствуют вероятностям. Но, скорее всего, никак. Ну, а самое главное, что механизма автоматического определения этих костов нет. В принципе. Это вот одна проблема. Вторая проблема, про которую мы уже говорили с Николаем, что отсутствует хоть какой-нибудь более высокий уровень, то есть нет возможности получать структуры более высокого порядка, значит, с какой-то натяжкой в существующей формализме линграммер, Есть иерархия частей речи, но она именно иерархия, а не хитерархия. То есть, через механизм так называемых макросов можно разделить слова, допустим, на подгруппы. Допустим, есть глаголы мужского рода, глаголы женского рода, глаголы существительные множественного числа, существительные единственного числа. В принципе, это можно как-то развести. Также можно там учитывать с помощью так называемых суффиксов амонимию. К примеру, допустим, слово «со». Вот из вот этого примера. Сейчас я где-то его найду. То есть, у нас есть «со» в качестве глагола, а есть «со» в качестве существительного. Так вот, в грамматике связей слово «со» может быть записано с разными суффиксами. Допустим, «со.в» – это будет глагол, а «со.н» – это будет noun, то есть, существительное. Но это вот такие костыли, которые на самом деле не позволяют в полном объеме работать с иерархиями. Это вот второй косяк. И третий косяк самый главный. Я говорил про pipeline. Самая главная проблема, с которой мы столкнулись, это то, что для того, чтобы получить вот эти адекватные грамматики, нам нужны адекватные связи. А для того, чтобы получить адекватные связи из исходного корпуса, нам нужно непонятно что. Вот я сейчас, когда перейду к результатам, я это прокомментирую, но суть в том, что непонятно вообще, как получать сколько-нибудь годные связи, сколько-нибудь годные деревья для того, чтобы из них можно было бы получить сколько-нибудь годную грамматику. Собственно, результаты. Что мы здесь сделали? Вот здесь можно выделить три блока. Самые верхние четыре строчки – это эталонный корпус из нескольких порядка сотни предложений. Средний – это разговоры мамы и детей. Это средние объемы и сложности корпуса. Ну и последний – это огромный корпус для детей и юношества на английском языке. Вот это вот блоки. Теперь разберем верхний блок. Первые две строчки – это с помощью двух алгоритмов кластеризации, ИЛЕ и ОЛЕ. Что такое – это не важно. Это просто некоторые способы построить грамматические категории в этом векторном пространстве. Мы берем и пытаемся построить граммартику. Что мы делаем? Во-первых, откуда мы берем парси? Вот здесь парсы мы строим вручную. Первые две строчки – это парсы, построенные эталонным путем, и качество этих парсов 1.0. У нас есть способ, как определить соответствие парсов идеалу. Парсы у нас идеальные. После этого, когда мы строим грамматику и пытаемся проверить, насколько вообще грамматика в принципе разбирает текст, вот получается, что тексты, разобранные с автоматически построенной грамматикой, вообще стопроцентно разбираются. Ну и также считаем f-меру, насколько полученные парсы соответствуют тем парсам, которые мы должны получить. Тоже единичка. Все прекрасно. А теперь идем следующим образом. Мы берем и строим парсы вот с помощью тех методов, которые предлагают товарищ Денис Юрят в работе, упомянутой Николаем, и тем способом, который получает Лайнесс. То есть, когда деревья грамматического разбора строятся на основе Minimum Spanning Tree, основанном на взаимных информациях между словами. И вот мы получаем здесь следующее, что если мы берем на основе этих парсов, построенных на взаимной информации между словами, Pairwise Mutual Information так называемая, мы построим деревья грамматического разбора даже для такого простого корпуса, у нас точность этих парсов получится 71. Эфмера по связям. И дальше, независимо, какой алгоритм кластеризации для построения грамматических категорий мы берем, 100% парсится, но качество примерно такое же, что на входе, то на выходе. Еще не sheet in, sheet out, но уже не 1-0, а 1-0. Переходим к более сложному корпусу. Опять-таки, если мы берем… Здесь уже вручную корпус большой, вручную его не распартишь, поэтому в качестве входных парсов мы тупо брали известный уже правильный английский линграммер, парсилиим. Опять-таки, качество парсов 1.0. Разбираем почти всё. Качество тоже близко к единичке. Всё хорошо. А вот если мы попытаемся деревья грамматического разбора построить на основе взаимной информации, уже раз качество самих парсов 0.68, а качество самой грамматики вообще уже не ниже Плинтуса, но на уровне Плинтуса, 0.45-0.5. Ну и последнее. Если берем настоящий взрослый корпус, парсы опять-таки генерим в одном случае с помощью английской грамматики. Качество паросов идеальное. Разбираем 63%. Качество грамматики уже не очень хорошее, но там есть над чем работать, там даже понятно было, какие проблемы нужно решать. Но если мы берем и строим на вход подаем парсы, которые получены с помощью взаимной информации 0.52, то вот у нас опять-таки разбираем все, но качество оставляет желать лучшего. И, собственно, это был тот самый результат, который заставил глубоко задуматься. И самое интересное, что еще в итоге получилось, оказалось, что для того, чтобы получить примерно такое качество пасов, Можно делать две вещи. Одна вещь – это просто случайным образом соединять слова в предложении. И получится не то, чтобы сильно хуже, но не намного хуже. А можно просто последовательно соединять слова в стиле linked list. И для английского языка качество будет примерно такое же. Зачем нам тогда гренерить какой-то огород, если мы можем, соединяя просто последовательные слова, для английского получать тот же самый результат. Результат все равно плохой. И последний, когда мы как раз обсуждали эту тему очередной раз с Лайносом, как раз на том воркшопе, который мы сегодня обзираем. Извините за выражение. Вот, значит, Linus предположил такую идею, что не нужно вообще, значит, брать какой-то один парс, нужно вообще просто брать, значит, текст, значит, все возможные деревья из каждого конкретного текста, вот, генерировать те, так сказать, строить кластера слов и те кластера слов, которые будут, так сказать, каким-то образом их оценивать, на основе этих кластеров, которые будут получать высокое качество оценки, уточнять деревья грамматического разбора и ходить по кругу в таком цикле. Но детали того, как это делать, пока что остаются за рамками понимания и, наверное, текущей дискуссии. вот значит ну и да и значит сказать теперь чтобы уж сразу закруглить свою часть Я перекину мостик еще к одной проблеме, которая была в этой работе. В этой работе мы считали, что на вход нам подаются уже токенизированные тексты. И для того, чтобы, например, из Gutenberg Children получить последовательность слов с правильно очищенной пунктуацией, пришлось писать достаточно громоздкий рукотворный парсер, который бы правильно разбирал точки, что в каком случае там точка является заключающей точкой или точкой предложения. А в каком случае точка является обозначением в десятичной дроби. То же самое касается запятых. Кавычки – это было что-то вообще страшное. Проблема правильной идентификации одинарных и двойных кавычек отняла очень много сил. Ну и поэтому вот один вывод, один запрос, вывод из этой работы был такой, а вообще, как нам парсить? И если что-нибудь более правильное, чем минимум Spanning Tree Parser и Mutual Information, И вот отчасти Николай попытался дать ответ на этот вопрос. А вторая проблема, которую мы пытались решать, это как все-таки сделать полный пайплайн от Supervised Learning, который бы включал себя в токенизацию. И здесь как раз я перехожу к той работе, которую я делал в этом году. Я ее на этом семинаре докладывал, поэтому я сейчас просто обозначу основные моменты для тех, кто здесь первый раз. Можно будет посмотреть предыдущий семинар, где я подробно эту тему рассказываю. Задача обозначена была таким образом. как обнаружение лексикона и пунктуации без учителя. То есть, как мы ставим свою задачу? У нас есть некоторый неизвестный нам язык, и мы хотим узнать, каков его лексикон и какова пунктуация. Но мы работали с тремя языками. Кто не был на наших семинарах, может попытаться загадать, какой из этих трех языков какой. Каждая строчка соответствует некоторым языку. Один из них русский, один из них английский, один из них китайский. И вот эти облака точек соответствуют некоторым метрикам этих языков в некотором пространстве. Каких метриков, в каком пространстве, я скажу чуть позже. Задача у нас в конечном итоге заключается в том, что описывать языковую модель как некоторый граф, который включает в себя понятия низкого уровня, допустим, синонимы, тулинол или эцетаминофен. frustration или anxiety. Это вот некоторые синонимы, которые объединяются некоторым логической, могут быть объединены в некоторый кластер, определяющий группу синонимов, а логически это у нас дезюнкция. Также у нас могут быть некоторые конъюнктивные Узлы в этом семантическом графе, которые говорят, что что-то должно быть вместе с чем-то. То есть, если экзистенциальная фрустрация или фрустрация экзистенциальная в русском языке, порядок незначим. Главное, что эти слова должны быть вместе. Потому что экзистенциальная фрустрация определяется парой этих слов. Ну, а в английском языке они должны объединяются не просто конъюнцией, они определяются еще последовательной конъюнцией. То есть, по сути, это вот как раз энграма или лексема. Ну, и дальше, так сказать, мы можем переходить на более высокий уровень, где у нас есть абстрактные сущности, там, глаголы, существительные. У нас могут быть абстрактные лексемы или грамматические шаблоны. Или типовые фразы типа субъект, предикат, объект. И, в общем-то, как раз вот такой вот семантический граф. Семантический в том смысле, что каждая вершина имеет свой смысл. Это либо слово, либо смысл, либо конъюнция, либо дизъюнция, либо последовательность слов, смыслов, конъюнции или дизъюнции. Вот. Что вот такой вот Направленный граф – это, значит, не иерархия, это направленный граф, потому что у нас у каждой вершины может быть много и родителей, и детей. Собственно, может описывать языковую модель. Да, ну естественно, что как вершины, так и стрелочки, они все имеют свои частотные характеристики, про которые как раз говорил Николай, и которые отсутствуют, к сожалению, в линграмме. Мы хотим построить такую модель. И при этом линграмор является некоторым частным случаем вот той модели, про которую я сейчас говорю. То есть, линграмор – это у нас, грубо говоря, где-то нижний слой, где есть только два уровня конъюнкции и дизюнкции. Для того, чтобы решить эту задачу, мы пытались для начала решить задачу токенизации, отталкиваясь от двух работ, которые мы здесь приводим в качестве ссылок. Сразу забегая вперед. Чтобы быстренько закруглиться, во всех этих работах качество токенизации без учителя оказывается достаточно низким, включая качество токенизации, основанное на взаимной информации. Относительно высокое качество получается не на основе метрик типа взаимной информации или условных вероятностей, где условные вероятности – это как раз то, про что Николай рассказывал. Взаимная информация – это когда речь идет о парах, а условная вероятность – это когда у нас не пара, а N различных термов может быть использована, может совместно использоваться. Так вот, с условными вероятностями, с взаимной информацией получилось не очень хорошо. А вот метрика Transition Freedom или Freedom of Transition, вот из одной из двух цитируемых работ, она заработала очень хорошо. И я сразу же перепрыгну к табличке с результатами, что нам удалось получить в этой работе для трех языков. на которые я ссылался. Для английского языка качество токенизации на том корпусе, который мы проверяли, оказалось 0,99. Если мы будем тренировать модель на всех последовательностях слов и будем считать переходы… Да, кстати, что такое свобода перехода, я должен пояснить. свобода перехода это вот что такое что если у нас есть к примеру некоторая последовательность символов допустим root это вот как раз старт termination токен про который николай говорил потом идет буковка p и потом буковка Т. Допустим, если у нас идет такая триграмма или триплет последовательность трех символов, то если нам в лексиконе или в корпусе известно, что после таких последовательности идет либо точка, либо S, либо пробел, то вот у нас степень свободы или степень свободы перехода из вот этой вот трешки в одно из трех состояний равняется 3. А если бы мы взяли только одну буквку «Т», то естественности свободы перехода было бы больше. И вот как раз используя, просчитывая свободы переходов, двигаясь по последовательности символа слева направо и справа налево, и анализируя аномальное отклонение этой свободы переходов от каких-то пороговых значений, мы как раз получаем автоматически токенизацию в английском языке на уровне 0.99 по Эфмере, по русскому 1.0, а по китайскому хуже 0.71. При этом, если мы для сравнения будем смотреть, а как бы делалась токенизация на основе лексикона, то есть предположение, что мы знаем лексикон и просто выделяем все слова, как отдельные токены, выделяем все слова, которые есть в лексиконе. то, заранее его зная, то в английском получится то же самое, 0,99, при том, что в нашем эксперименте мы лексикон не знаем. В русском получается на основе лексикона даже хуже, потому что выясняются не все слова в корпусе оказались в лексиконе. Вот. Ну, а в китайском получилось лучше, потому что само качество не очень хорошее. Вот. Ну и вторая задачка, которую мы, метрика, которую мы считали, это мы мерили точность вообще, насколько вообще те слова, которые мы находим, они вообще есть в лексиконе. Вот. И оказалось, что в английском точность 0.99, то есть практически очень хорошо. Вот. В русском вообще в единичку попадаем. В китайском тоже, несмотря на то, что токенизация до китайского была не очень хорошая, все-таки почти все слова, которые мы выделили, были в лексиконе. То есть, в общем, достаточно неплохо. Но теперь, собственно, самая большая проблема, которая остается теперь, если даже не брать китайского. Окей, мы можем так хорошо токенизировать, но ведь нам для того, чтобы токенизацию делать, нужно определить параметры токенизации. То есть, на каких порогах, каковы пороговые значения той модели, которая мы берем вот этих отклонений вот этой свободой степи свободы перехода значит какие пороговое значение мы берем для того чтобы сказать ага вот здесь вот разрыв вот ну например там у нас еще было несколько параметров но вот один из них и вот как определять эти параметры то есть мы для того чтобы решить эту задачу мы вот делали достаточно большой и длинный дорогостоящий поиск в пространстве параметров и находили оптимальное значение, при котором токенизация делается правильно. Наконец, переходим к этим точкам распределения. Мы попытались посмотреть на эталонных корпусах, на эталонных токенизаторах. А вообще, что будет, если мы будем пытаться строить модели с различными гиперпараметрами? и с одной стороны оценивать качество токенизации с помощью этой модели Эфмерой на известном корпусе, с помощью эталонного токенизатора этого Эфмера, это по вертикальной оси, а по горизонтальной оси отложены некоторые метрики, которые позволяют оценивать качество токенизации безотносительно к эталону. Вот, например, внизу написана мера на основе энтропии. Что за мера, тоже сейчас не буду рассказывать, но, грубо говоря, мы берем некоторую нормированную энтропию и ее инвертируем. Соответственно, чем меньше энтропия, тем лучше. Антиэнтропией мы это называем. Здесь это коэффициент сжатия. То есть, если мы смотрим на задачу токенизации, как на задачу сжатия набора данных. CSF1 – это cross split f-мера. Это мы берем, разбиваем корпус на две половинки, строим модели на первой и на второй половинке независимо, а потом первую модель токенизируем Первый корпус токенизируем моделью, полученной на втором корпусе, а второй корпус токенизируем моделью, полученной на первом корпусе. И там и там считаем F1, меряем средние. А вот третья метрика – это мы пытаемся взять средние между этими тремя метриками. Получается, что в данном случае для английского языка мы можем действительно видеть какую-то линейную зависимость между этими универсальными статистическими, энергетическими, энтропийными параметрами. и, собственно, той Фмерой. И можно спекулировать, что если не брать какие-то аутлайеры, которые тут справа, слева болтаются, то вот можно сказать, что чем больше эта универсальная метрика, тем лучше будет нам F1. И тогда, получается, мы можем строить модель токенизации, вообще не зная ничего о языке, исходя чисто из этих энергетических метрик. Дальше смотрим, что у нас на русском получается. На русском получается все хуже. Линейная корреляция усматривается, но есть аутлайеры, они выглядят гораздо серьезнее и непонятно, что с ними делать. Наконец, для китайского тоже. Аутлайеры очень сильно мешают и получаются какие-то облака дополнительных точек. С которыми тоже не очень понятно, что делать. Вот пример того, какой язык какой. Первый был действительно китайский, где всё хуже всего. Второй, где лучше всего это был английский. А третий – это между ними был русский. Всё. Собственно, спасибо. Если есть какие-то короткие вопросы, 

S03 [01:36:16]  : вот этого части то я готов ответить если нет то мы переходим последнему докладу на 15 минут чтобы продолжить обсуждение вопрос был один он связан с тем что вы говорили что нужно прислать вероятности и вот вероятности потом при составлении фраз выбирать по наибольшей вероятности. вот в таком подходе разве вероятность не должна быть условной, то есть у нас есть уже какой-то контекст, и тогда вес присваивается не просто так, потому что он чаще встречался, а потому что он еще как-то коррелирует с контекстом. 

S01 [01:36:57]  : все правильно. да, да. то есть задали вопрос, да? 

S03 [01:37:01]  : да. 

S01 [01:37:02]  : Значит, смотрите, здесь есть как минимум два уровня. Может быть, их на самом деле больше. Почти наверняка их больше, но давайте я пойду от начала. Первая проблема, на которую указывал Николай, что это проблема того, что парных вероятностей между слов для того, чтобы даже говорить о грамматике, недостаточно. То есть, в Lingrammary… Давайте я сейчас свернусь, чтобы было понятно. Смотрите. Во-первых, вот как раз Лайна здесь привозит какие-то эти значения взаимной информации. Но вот обратите внимание, что, например, переходящий... Так, где у нас переходящий? Нет, это не переходящий глагол. Где переходящий глагол? Вот. Честь. Честь. Переходящий глагол – честь. У этого переходящего глагола есть две связи. Так вот, смысл переходящего глагола в том, что эти две связи, они ходят вместе, то есть, они должны быть вместе. И поэтому увеличена вероятность или там кост с точки зрения линграммера, она должна присваиваться вот комбинации этих связей, взятых вместе, по идее. И, в принципе, линграмор, он позволяет присваивать косты не отдельным связям, а парам этих связей. Но когда Лайнос и Денис Юрец, про которого как бы идеолог вот этого... направления формирования парсов на основе взаимной информации строят вот эти вот деревья, точнее в режиме unsupervised, они строят эти деревья на основе только парной статистики, потому что они не знают, какие связи объединяются вот в кластера типа вот этих вот субъекта, связи типа субъекта на объект. И в этом смысле это как раз проблема, что мы пытаемся построить какую-то грамматическую модель, исходя исключительно из парных связей между слов в отрыве от контекста их связи с другими словами. Это первая проблема. Вторая проблема, на которую вы сказали, что даже... Да, и Николай как раз и говорит, что нужно смотреть именно совокупность всех слов, которые связаны с собой в цепочку, и считать вероятности, именно условные вероятности всех слов, которые есть в цепочке. Но в цепочке не с точки зрения n-граммы, как это... считают существующие state-of-the-art нейросетки, а именно с точки зрения вот этого грамматического графа. То есть мы от fighting не идем на british, мы от fighting идем сразу на rule. А вот от рул мы можем назад вернуться на бритиш. Или от бритиш перейти на рул. То есть у нас на самом деле файтинг и бритиш не связаны. У нас файтинг связан здесь рул. И вероятности должны считаться именно в контексте правильного дерева. Это вот первое. Второе – это то, про что вы, видимо, сказали, и о чем, собственно, attention в трансформерах. Что у нас даже на самом деле связи, которые чисто грамматический характер имеют, они находятся в некотором контексте в каком они это рассматриваются. I saw a man… Ну, то есть, я сейчас не соображу, какие на русском языке бы вести примеры. Но, в общем, есть ситуации, когда в зависимости от одни и те же слова в одном же предложении… А, ну, пожалуйста, косой, косил, косой, косой. Для того, чтобы понять, кто косой, является ли косой именем или является косой определением человека. или является косой кличкой, зависит от контекста. Там Михаил Косой, значит, был косарем. Косой, значит, косил косой. Во втором предложении косой – это фамилия, да? А если там Петя Косой, и Петя был косой, и там косой, значит, косил косой, то косой – это уже, так сказать, просто характеристика человека. Может быть, пример не очень удачный, но второй или третий уровень контекста, который тоже отсутствует полностью от существующей линграммар-модели – это как раз уровень контекста более широкого. не грамматического, а именно семантического, если угодно. И это как раз делается в нейросетях с помощью… Как это называется? Промпт инжиниринга достигается. Как раз одна из тем, которая поднималась на семинаре в дискуссии между Николаем и Лайносом, как раз была, что линграммар и подобные решения на его основе. Нужно как раз этот уровень каким-то образом прикорячивать, но как пока никто не знает. Так, еще какие-то есть вопросы принципиальные? Так, значит, сейчас вопрос вероятности. Да, это вот, собственно, Захар. Ну, Захар, может быть, давайте так, давайте мы дадим вам слово, а потом уже перейдем к дискуссии. 

S03 [01:42:59]  : Да, ну вы, в принципе, ответили на этот вопрос сейчас. 

S01 [01:43:01]  : Хорошо, давайте тогда. Вы слайды покажете или мне ваши слайды показать? 

S03 [01:43:06]  : А, я покажу. Там много переделок местами. 

S01 [01:43:12]  : Я коллеги представлю. То есть, на самом деле, то, что Захар будет рассказывать, это как бы немножечко с другой стороны. Это будет, насколько я понимаю, как раз от нейросетей. И в данном случае, я думаю, не будет речи идти о тех вещах, про которые говорили мы с Николаем. Но вот Захар, пожалуйста. Видно экран, да? Да. 

S03 [01:43:39]  : Вот по ней демонстрация пошла. Мы занимаемся в основном тем, что работаем с большими языковыми нейронными сетями, но в основном это GPT, BART, сети, которые обучены на огромных объемах текста, и нам их нужно адаптировать под свои задачи. Возникает такая проблема, что вообще существует такой метод, довольно известный, это так называемый Prompt Engineering, либо FewShot, ZeroShot, но они не всегда подходят, нам хочется добиться более точного средования датасета, потому что в датасете есть какие-то свои распределения, то есть условной вероятности переходов между словами. Сеть их не изучала, когда училась на общем заседе, который включает в себя там всю Википедию, много других еще источников. И вот наш засед еще очень часто является очень маленьким. И вот основная задача – это как бы обучать сеть на маленьком датасете, моделировать его, но при этом применять какие-то выученные правила из того датасета, на котором она училась раньше. И вообще сделать это как-то еще регулируемо. То есть, чтобы сейчас она применила больше, если можно так говорить, знаний из того, что она изучала раньше, а вот сейчас, в следующий раз, чтобы она применила все-таки ближе к датасету была. И вот, чтобы решать эту задачу, мы разработали такой метод, называется Fast Expert Tuning. Идеянно, он очень сильно напоминает адаптеры и, по сути, работает примерно как адаптеры. Идея адаптеров заключается в том, что мы не учим всю сеть, мы вставляем некоторые Ну, здесь вот видно, вставляются некоторые слои, учатся только эти слои. Ну, здесь, например, показан BattleNet адаптер, когда у нас разменяется часть, снижается, потом увеличивается. И вот этот адаптер, он обучается. Преимущество адаптера заключается в том, что мы обновляем очень маленькую часть сети, порядка 1%. Ну, и сами адаптеры, они довольно легковесные. Их легко там передать, распространять. У нас есть одна обычная сеть, например, GPT, которая весит порядка 3 гигабайта или полтора гигабайта. То ее пересылать будет довольно проблематично, адаптер может весить единицы мегабайт или килобайты, которые очень легко передать. Ну, есть, кстати, параллельные адаптеры, которые работают параллельно слоя. Ну, например, здесь стала лора представлена. Наиболее единоблизкий к нашему методу – это адаптер на базе. Мой – это Mixer of Experts. В чем он заключается? У нас есть выход в трансформер от Layer Normalization. есть роутер адаптеров, который, по сути, нам дает вероятность того, какой из адаптеров сейчас использовать. У нас есть ряд адаптеров, называемых экспертами. Ну, здесь это слои прямого распространения полносвязные, он фультинутит. И далее они суммируются с весами, вот это вот, с теми весами, которые дает роутер. У нас идея очень похожа на идеи Moe, но есть некоторые отличия. Также у нас принимается вот здесь, кстати, одно из основных отличий, что принимается в Moe на вход классификатора, принимается выход со слоя нормализации, Layer Normalization слой. А у нас принимаются токены входа. Далее у нас есть классификатор экспертов. Он также присваивает каждому эксперту свой вес. Но мы берем не всех экспертов, а некие топ-к, например, топ-3 экспертов берем. И уже для них пересчитываем вероятности, чтобы сумма по 3 экспертам была равна 1. и смешиваем. Но помимо этого всего у нас есть еще и вероятностная модель, которая параллельно обрабатывает последовательность токенов и говорит, какая будет вероятность перехода между двумя токенами. После этого здесь у нас происходит произведение, взвешенное между адаптером между выходами адаптеров вот здесь вот есть еще слой классификатора он выдает свои вероятности у нас есть вероятность и вероятность на модели и они возводятся в разные степени например если мы возводим в степень вариативности вероятность на модель вероятность на модель возводится в степень единственности на вариативность Вероятность от классификатора разводится в степени вариативности. Таким образом, мы можем выбрать, с какой точностью он будет следовать датасету. Далее мы выбираем токен. Токен выбирается так же, как он выбирается и в обычных языковых моделях, то есть это BeamSearch либо Nuclear Sampling. Основное преимущество нашего подхода заключается в скорости обучения. Даже по сравнению с другими адаптерами у нас обучение происходит быстрее за счет того, что мы адаптеры ставим только последний слой и просто параллельно к нему применяем вероятность на модель, которая учитывает либо просто закон распределения токенов, либо переходы между ними. Ну вот здесь показано, что на DataSet от GPT обучалась на CPU, не на GPU. Ну, то есть не на графическом процессоре, а на обычном центральном процессоре обучалась 85 секунд. Вот. Ну, это функция ошибки. И пример. Если мы берем обычную GPT и говорим ей начать генерацию с нового сталкера. то она говорит, что сбойник рассказывает писать о Сергее Рукьяненко. Потом мы берем небольшое описание игр. В основе здесь про сталкера, есть про другие игры, но в основе про сталкера. Здесь про сталкера, вот тут про сталкера. И обучаем на этом сеть. После этого мы хотим сгенерировать, выставляем вариативность 0.1 и получаем что-то игра с открытым миром, либо различные описания процессов, происходящих в игре. Другой пример – это использование вот этого FastExpertTuning в обучении чат-ботов. Мы взяли модель, которая есть, в принципе, в открытом доступе. это Rudeal GPT-3 Medium Based on GPT-2, то есть от Сбербанка моделька Rudeal GPT-3 Medium Based on GPT-2, и вот ее затюнили под диалоги и выложили, вот авторы, которые это сделали, они выложили ее в открытый доступ и опубликовали статью о том, как она работает. Вот, кстати, вот из их статьи скриншот. Мы взяли эту модель, также использовали FastExpertTuning для того, чтобы ее затюнить. И вот мы здесь можем видеть, что эта модель начинает понимать, как ее зовут. Ну, то есть она уже не путается, потому что если вы возьмете эту модель, скачаете с рейнфейса и будете спрашивать, как ее зовут, она будет постоянно отвечать разные имена. Если ее затюнить, то она начинает отвечать одно имя. на кой была зачинена. Но в основном она ведет диалог в принципе так же, как ведется диалоговая незачиненная модель. То есть логика диалога не ломается, и она как была диалоговой, так и остается диалоговой. Ну, также у нас есть библиотека, которая это делает. Ну, здесь просто пример кода. как это делается, происходит обучение. Другой метод контролируемой генерации, который мы тоже разрабатывали, разрабатывали его на базе БАРД, разрабатывали его на базе GPT, это генерация по каким-то ключевикам. То есть у нас есть ключевики, например, «удачи на», «желаю тебе», «тренировка 6.0». Это слова, на какие ориентироваться, это слова, которые должны обязательно присутствовать. Мы их передаем в BART и получаем нормальный собранный текст. BART в этой задаче показывает себя намного лучше, чем GPT. Есть похожий пример у нас на GPT. Вот, когда мы говорим так же, что мы хотим текст параллельно, мы хотим, чтобы там была игра, чтобы там было слово купить. Начинаем, задаем начало генерации и генерируем уже готовый текст с помощью GPT. Вот. Ну, на этом все. Если есть вопросы, могу ответить. 

S01 [01:54:06]  : Захар, спасибо. Коллеги, есть какие-то вопросы к Захару? Спасибо. Захар, по вопросам. По поводу дискуссии Николай Михайловский предложил обсудить инкомплект Surface Realization для Simulization. Николай, пожалуйста. Так, а Николай с нами. Ага. Да, ну вот Николай отводился. Вот, значит, вопрос, значит, я как бы понял, видимо, в чем вопрос, что, значит, по-моему, он говорил как раз то, что его интерес, значит, к теме как раз интерпретируемых языковых моделей связан с его задачами по сумеризации. Вот. В моем понимании, Суммаризация – это один из тех возможных применений, которые я перечислял. Вот, значит, и это близко, это близко к, в каком-то смысле, Question-Answering. То есть, Contextual Question-Answering. То есть, в каком виде мы решаем задачу Question-Answering, о которой я рассказывал. Это у нас есть некоторый текст. И мы хотим задать вопрос. Есть литературное произведение, к примеру, про Винни-Пуха и Пятачка. У нас есть вопрос такой. Кому на день рождения ходили Пятачок и Винни-Пух? И это будет ослик и ая. Или там кто, за кем охотились Пятачок и Винни-Пух. И это будет слонопотан. То есть, это ответ на вопрос. А дальше мы можем перейти к следующей теме. Мы можем задать вопрос такого рода. А чем занимались Пятачок и Винни-Пух? И вот если мы зададим вопрос, чем занимались Пятачок и Винни-Пух, то, по идее, здесь должно уже пойти некоторый дайджест, что Пятачок и Винни-Пух вместе ходили на день рождения Кослику-Яйя, они вместе ходили в гости к кролику, и они вместе охотились на слонопотама. И это уже будет то, что можно называть context-specific summarization. А если мы говорим просто про Summarization не контекст-специфик, то есть Summarization, не зависящая от контекста, то в этом случае, по идее, мы должны рассказать то, про некоторый текст. Вопрос получается открытым. Что ты думаешь про этот текст? И мы должны дать некоторый текст, который описывает некоторые ключевые моменты, связанные с этим текстом, отличающие его от других произведений. Например, если мы говорим, задаем вопрос, значит, хотим суммаризировать книжку «Пятачок» и «Винни-Пух», то мы должны как бы сопоставить эту книжку с корпусом других произведений литературы, выявить, обнаружить, что отличным от всех других произведений литературы является дружба между медведем и свиньей. И рассказать, что книжка «Винни-Пух и Пятачок» описывает похождения медведя и его друга поросенка. Вот это будет как раз, по сути, суммаризация, не связанная с каким-либо контекстом. Но в данном случае контекст, он как бы, то есть в случае суммаризации без контекста, в моем понимании, контекст задает весь весь корпус той литературы, в контексте которого делается суммаризация. И эта суммаризация как раз должна дать отличие контекста данного произведения от включающего контекста корпуса мировой или технической литературы. То есть, если, к примеру, мы берем не детскую литературу, а мы берем, к примеру, доклад Захара, то, наверное, суммаризации будет не использование трансформеров, потому что нельзя суммаризировать доклад Захара, что Речь идет, что это доклад о трансформерах. Нет, доклад не о трансформерах. Доклад идет о файн-тюнинге трансформеров. И вот как раз файн-тюнинг трансформеров является отличием данного доклада от других докладов на тему трансформера. Вот это то, что я могу сказать про Incomplete Surface Realization Desamorization. Захар спрашивает, есть ли код по Fast Export Tuning в Open Source. Захар отвечает, что на данный момент код не публиковали. Коллеги, есть ли ещё вопросы какие-то или комментарии? Захар, у меня к вам есть вопрос технический. Там вот в какой-то момент вы даёте промпт про сталкера, значит, и в конце промпта у вас кавычка идёт. То есть, начинается «сталкер» идёт, а там потом кавычка. Это вот что-то означает «кавычка» после слова «сталкер»? 

S03 [02:00:08]  : А нет, это то, что мы начиная с слова сталкер закавычили, что с него начинаем. 

S01 [02:00:13]  : Но смотрите, перед не было, то есть кавычка была в конце слова, а в начале кавычки не было, то есть это вот что-то, это какая-то причина? 

S03 [02:00:22]  : Нет, там писали просто сталкер и оно до тюнинга, либо после тюнинга, если поставить вариативность близкую к единице, оно продолжает, что Это сборник рассказов писателя Сергея Лукьяненко. А, ну да, она саму поставила кавычки, это уже GPT поставила. 

S01 [02:00:40]  : А, то есть кавычка уже поставила GPT, то есть всё. 

S03 [02:00:44]  : Да-да, а в другой раз, когда написали Stalker, она не поставила кавычки, а поставила PRA и написала «Игра со скрытым миром». Это уже после тюнинга. Да, ну и, кстати, если ставить вариативность равную единице, то после тюнинга она также будет писать про фильм, романы, вот про рассказы Сергея Лукьяненко также будет писать. Здесь сайт «Близко к нулю» будет писать именно про игры и там описывать при некоторых вариативностях сюжетные какие-то моменты в играх. То есть такое тоже бывает. То есть вот как раз таки, чем меньше вариативности, тем ближе она будет идти к тому распределению, которое было в ретросети, ну и При нулевой бывает такое, что просто копируют часть датасета, ну 0.1 когда ставят, ставят она именно из той же, ну близкой к датасету генерируют, но не копируют часть датасета, но если там увеличить это до 0.5, то начать отдаляться от него, если там до единицы, то она практически такая же, как была до обучения. Вот тоже делали тесты, смотрели, что у нас эта матрица, которая получается при тюнинге, она практически не изменяет ту матрицу лесов, которая была до тюнинга. То есть она ее немного изменяет, но в основном весь вес перекладывается в большей части на вероятность на модели. Поэтому степень при вероятности, она играет такую большую роль. 

S01 [02:02:17]  : Понятно, спасибо. Коллеги, еще есть какие-то вопросы или замечания? 

S00 [02:02:27]  : У меня такой вопрос. А вот интерпретируемая обработка естественного языка, это некоторое такое направление. А вот в ваших конкретных работах, это принципиальные положения, или нет? И зачем, собственно, вы в своих работах в точности придерживались интерпретированной обработки естественного языка? Предполагает ли это, что вы не используете принципиально нейронные сети, например? 

S01 [02:02:58]  : Ну, смотрите, Евгений Евгеньевич, значит, вот как бы подход Лайноса и подход меня заключается в том, что мы рассматриваем символьную обработку как просто способ... Опять-таки, я не буду говорить за Лайнаса, извините, а буду говорить за себя. В моем понимании это... Во-первых, это просто исследовательский элемент, то есть понять, можем ли мы, в принципе, не прибегая к технологии черного ящика, решать задачи вот в той парадигме графовых языковых моделей, в которых мы это пытаемся делать. Это вот один аспект, да, как бы чисто исследовательский. А второй аспект – это аспект экономический. Потому что для того, чтобы решать задачи с помощью нейронных сетей, нам нужны очень большие вычислительные ресурсы, связанные с высокой связанностью нейросетевых моделей. В то время как модели символьные, если они уже построены, они обладают существенно меньшей связностью. Есть гипотеза о том, что они обладают существенно меньшей связностью. И, соответственно, они могут выполняться на гораздо более скромных вычислительных ресурсах. И в этом смысле мы можем использовать, обсчитывать задачу не на тех вычислительных средствах, которые... Одни и те же задачи мы, возможно, можем решать не на тех вычислительных средствах, которые сейчас становятся под санкциями, а на более дешевых вычислительных средствах и доступных, которые есть. Например. То есть, это чисто экономические моменты. С одной стороны. С другой стороны, там же задача многоуровневая. Например, если мы решаем какую-то прикладную задачу, например, распознавание речи, то распознавание речи может делаться одним способом, а разбиение этого речи на предложение может делаться другим способом. то есть нейросимульную интеграцию никто не отменял и очень можно быть, что возможно комбинация разных методов при построении уже полноценных систем. 

S00 [02:05:40]  : на самом деле сейчас сложность обучения нейронных сетей она сейчас нивелирована таким образом, что например BERT, который действительно, чтобы его обучить, чтобы получить действительно большие затраты, но его же применяют как? Что эта система используется для того, чтобы решать различные задачи, просто дополняя BERT специальной дополнительной настройкой, и эта дополнительная настройка делается несложно. Специально настройки того же мира для решения уже некоторых конкретных задач. Например, для финансовых задач или для того, чтобы оценивать значимость тех или иных слов. эмоциональную значимость. То есть в этом случае не нужно действительно большие вычисленные ресурсы, нужно просто взять Берн и более точно его настроить на некоторую конкретную задачу, на одну из тех, которые вы решаете. То есть этот вопрос решается. 

S01 [02:06:46]  : Ну, Евгений Евгеньевич, насчет Берта я как раз пример приводил. Давайте я его сейчас снова покажу. Конкретно как раз по поводу Берта. Вот это тот самый пример, что мы взяли для решения совершенно конкретной предлогной задачи. Взяли из коробки Берт. уже предостроенный на финансовую модель. И взяли мою модельку для определения сентимента и получили один и тот же 0.33 Accuracy. Но для того, чтобы сделать дальнейшую настройку Берта, нам нужно было бы разметить дорогостоящий корпус. То есть, это нелегко. Для того, чтобы донастроить BERT нужен корпус. Нужно создавать тот корпус. Даже то, что Захар показывал. Эксперт-тюнинг. Все равно эти корпуса надо создавать. И донастройка этого корпуса все равно не дает гарантию. Результаты будут непредсказуемы в конечном итоге. Нужно будет мало того, что надо настраивать делаете этот корпус для fine-tuning, а потом еще нужно подбирать вот эту температуру, чтобы с большей вероятностью выскакивали правильные слова. А мы берем просто в ручном режиме, понимая какие лексемы у нас отсутствуют, а какие лексемы лишние, мы редактируем, вручную редактируем модель, и мы за две недели повысили качество больше, чем в полтора раза. 

S00 [02:08:30]  : Но чтобы использовать сентименты, вам же тоже пришлось обучаться? 

S01 [02:08:36]  : но смотрите значит в данном случае мы идем от исходя из из коробки да то есть вот файнберд у нас из коробки и здесь значит наша модель она тоже и айдженс она тоже из коробки вот то есть мы сейчас идем говорим про проду настройку да вот для фонтю да можно было бы просто взять фейнберд который не использует сантименты и на и достроить его используя ваши 

S00 [02:09:02]  : И в этом случае получилось бы лучше? 

S01 [02:09:05]  : Не знаю. Не знаю. Не знаю. А вы попробуйте. Может быть. Вопрос в том, для того, чтобы донастроить, потребовался все равно какой-то корпус. Да, конечно. Не уверен, Евгений Евгеньевич. Но, опять-таки, это надо пробовать, да. Коллеги, еще какие-то вопросы есть? 

S03 [02:09:44]  : Хорошо. Технический вопрос. Где можно было бы попробовать вашу систему, которую вы говорили? Или она сейчас закрытая? Какую систему? Ну, о которой вы рассказывали. Меня особо интересовала генерация 

S01 [02:10:02]  : текста по какому-то множеству слов то что мы рассказывали смотрите значит там к сожалению это все релиза то есть код он доступен в открытом доступе сейчас я дам ссылку так сейчас но так сейчас я прямо найду ссылочку на проект гитхабе так сейчас так сейчас так вот он Но там всё, к сожалению, в виде далёком от продакшена. То есть, это всё на уровне МВП и для промышленного применения непригодно. Это делалось для написания статьи, а не для каких-то практических экспериментов. на сегодняшний день. Есть попытка эту работу продвинуть, даже один из участников нашего сообщества помогал нам продвинуться недавно, в конце прошлого года, но пока это еще далеко. Спасибо. Практического применения. То есть, посмотреть можете. Там даже примеры какие-то, по идее, должны запускаться. 

S03 [02:11:33]  : Спасибо. 

S01 [02:11:38]  : Хорошо. Коллеги, еще вопросы? Тогда я всех благодарю за участие, за интерес и за вопросы и за комментарии. Всем спасибо и до свидания. Да, значит, в следующий раз в четверг у нас, я так понимаю, планирует Юрий Бабуров. Он еще не подтвердил, поэтому следите за анонсами, будет у нас доклад или нет. И доклад, если я правильно помню, он будет про переход от игрового Потом 22-го числа у нас будет перерыв, поскольку будет Байка-конференция, поэтому все желающие могут поучаствовать в конференции Biologically Inspired Cognitive Architectures на английском языке. Как туда регистрироваться и участвовать онлайн, это можно узнать на сайте Байка-2022. А 29 числа уже Сергей Шумский, я надеюсь, нам расскажет, он предварительно подтвердил состояние их работ с Игорем Пивоваровым, развитие проекта «Адам», который Игорь рассказывал у нас на семинаре. какое-то время назад и которые они представляли на конференции AGI 2022. Так что следите за анонсами и всем счастливо. До свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
