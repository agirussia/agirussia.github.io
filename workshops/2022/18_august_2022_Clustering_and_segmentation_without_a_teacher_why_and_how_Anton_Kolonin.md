## 18 августа - Кластеризация и сегментация без учителя - зачем и как? - Антон Колонин — Семинар AGI
[![Watch the video](https://img.youtube.com/vi/oTBxdQL_lnU/hqdefault.jpg)](https://youtu.be/oTBxdQL_lnU)

Суммаризация семинара:

На семинаре было рассмотрено две темы, связанные с кластеризацией и сегментацией данных. Первая часть семинара была посвящена техническим аспектам этих задач, в частности, вопросу об автоматизации процесса кластеризации без вмешательства учителя. Это стало предметом интереса после того, как автор семинара столкнулся с проблемой отсутствия точных ответов на свой вопрос в сообществе и был вынужден самостоятельно исследовать вопрос.

Диалог начался с вступления докладчика, который поделился с аудиторией своими размышлениями о текущих проектах, в которых он применяет методы кластеризации и сегментации. В частности, упоминались финтеховский проект и проект в области обработки естественного языка, где применялись методы кластеризации для выявления закономерностей и структуры в данных.

Ключевым моментом стала демонстрация процесса кластеризации и сегментации, где были показаны методы определения кластеров и символов для описания состояний рынка. Особое внимание было уделено задаче квантификации или символизации состояния рынка, где данные метрик описывают состояние рынка в многомерном пространстве, и необходимо определить символы для описания этих состояний.

Вторая часть семинара была связана с более сложными аспектами кластеризации и сегментации, и была планирована для отдельного обсуждения в рамках конференции AGI на английском языке, а также в рамках семинара по обработке естественного языка на русском языке.

В ходе семинара были заданы вопросы и обсуждены различные аспекты кластеризации и сегментации, включая вопросы о философских основаниях кластеризации, задачах здравоохранения, и о применении методов кластеризации для решения практических задач, таких как выявление паттернов манипуляции финансовым рынком и распознавание закономерностей в естественных языках.

Семинар также затронул тему взаимодействия с различными группами населения и воздействия на них в контексте распространения информации о мерах защиты от вирусных напастей. Были обсуждены различные подходы к сегментации и воздействию на разные группы населения в контексте здравоохранения и распространения информации.

В целом, семинар представлял собой глубокое погружение в технические, философские и практические аспекты кластеризации и сегментации данных, и был направлен на обмен знаниями и опытом между участниками в рамках их профессиональной деятельности.







S01 [00:00:06]  : Коллеги, всем добрый вечер. Сегодня у нас не совсем обычный семинар, в том плане, что я буду говорить о достаточно технических вещах, которые, скажем так, будут две части. Эти две части являются кусочками относительно большой работы, которая, с одной стороны, имеет отношение, с моей точки зрения, к AGI, с практической точки зрения она имеет отношение к целым двум проектам. Один проект, который я веду в Singularity, нет, Singularity, да, в части Fintech. Другой проект, который я веду для собственного удовольствия в рамках Natural Language Processing. Supervised Interpretable Natural Language Learning. И эти две части, они такие, что первая часть, это на самом деле, с моей точки зрения, просто курсовая работа. И я бы не стал это публиковать, если бы не вопрос, который у меня возник в какой-то момент в ходе вот этого большого, так сказать, многоуровневого проекта. У меня возник достаточно простой, как мне казалось, вопрос. Я решил не мучиться и задать этот вопрос в нашем сообществе в Телеграме. И к своему большому удивлению обнаружил, что несмотря на то, что многие люди пытаются на него давать ответы, ни один из этих ответов, он, в общем, близко не стоял к тому, что на самом деле хотелось бы получить. И вот, к сожалению, там не было уважаемого Сергея Терехова в этой группе в Телеграме. Потому что он, наверное, сразу же дал бы ссылку на большую красную толстую книжку, которую он показал в прошлый раз. И он бы, наверное, указал мне на номер страницы, где написаны все ответы на мои вопросы. Но поскольку этого не произошло, я стал копать сам. В общем, я выкопал все, что мне было нужно, и это, собственно, я сегодня расскажу. И я не буду удивлен, если в конце первой части семинара Сергей наконец пришлет мне ссылку на правильную страницу в этой большой красной книжке, где будет написано все, что я сейчас буду рассказывать в результате собственных раскопок. Или, может быть, он пришлет мне ссылку на другую страничку, где будет написано, что все совсем не так, а все совсем по-другому. А поскольку никто на этот вопрос мне ответить не смог, то вот, мне кажется, то, что я расскажу, будет иметь некоторый практический интерес для нашего сообщества. А вторая часть, если на нее останется время, это будет уже чуть более сложное. И если вдруг мы вторую часть рассмотреть не успеем, Я про неё буду говорить ровно через сутки на семинаре про Interpretable Natural Language Processing в рамках конференции AGI. И на английском языке это будет происходить через сутки. А также эту тему я планировал ещё рассказать на одном из своих семинаров уже на русском в нашем сообществе в рамках темы Interpretable Natural Language Processing. итак значит какие это две темы давайте я начну показать что-нибудь показывать так это не то да вот так и мне нужно сделать демонстрацию экрана да и так какую задачу Хотелось решить, на самом деле эта задача связана с несколькими проектами, но вот наиболее остро она возникла в нашем финтеховском проекте, где стала задача осуществить квантификацию или символизацию, можно это по-разному сказать, в состоянии рынка. То есть, предположим, что у нас есть некоторый рынок, который в режиме реального времени описывается некоторыми метриками в н-мерном пространстве, где каждая метрика, каждые метрики соответствуют некоторой измерении в этом пространстве. И мы хотим описать эти состояния рынка какими-то символами для того, чтобы описать это состояние рынка какими-то символами, то есть какими-то выраженными точками в этом многомерном пространстве, мы должны по каждой оси определить, есть этот символ или это характеристика этого символа или нет. Например, если мы посмотрим на ряд метрик, которые мы можем посчитать на основе динамика рынка. Поясню, что вот то, что здесь показано на кривой справа, это только одна из метрик. В данном случае это цена. А на основе цены и на основе других составляющих, объемы продаж, объемы покупок, количество продаж, количество покупок, соотношение между объемом продаж и объемом покупок. Сейчас мы считаем порядка 111 метрик на основе всех рыночных данных, которые забираем. И есть еще на примете примерно полусотни метрик, которые мы еще можем посчитать. Из этих метрик мы пытаемся выделить, прежде всего, некоторые метрики, которые характеризуются устойчивыми состояниями, бинарными. Например, отсюда видно, что первая метрика характеризуется нормальным распределением. Соответственно, все значения где-то около нуля. И нельзя сказать, что когда что-то больше нуля, оно существенно отличается относительно того, что что-то меньше нуля, потому что основной максимум находится в районе нуля. А вот некоторые метрики, они характеризуются бимодальным разделением, когда у нас либо все в одной куче в минусах, либо это все в другой куче в плюсах. И, соответственно, если мы здесь увидели метрики типа А и Б, или, например, метрику С, которая тримодальная, то по тем метрикам, которые у нас, что называется, бимодальные или мультимодальные, мы каждую такую моду значение этой метрики можем характеризовать как некоторым символ. Допустим, если у нас в данной метрике D находится в куче D1, мы говорим, что у нас есть некоторая характеристика D1, или фича, или штрих, или что угодно, или фича, причем это символьная фича, то есть она либо D1, либо D2. Другого состояния мы считаем нет. И соответственно, когда мы вот эти совокупности символьных фич типа A1, B1, C2, D2, E2, F2 собираем, то мы можем сказать, что вот такая характеристика фич, значит, она соответствует некоторому символу. Например, мы считаем, что, допустим, две палки вертикально, а посередине палка горизонтальная – это символ N. Или когда одна палка вертикальная, а сверху одна палка горизонтальная – это символ T. То есть вот каждый набор таких символьных фичей соответствует некоторому высокоуровневому символу. И дальше мы можем решать задачу выявления каких-то псевдостимульных закономерностей, про которые Евгений Евгеньевич Витяев как раз в своих работах, связанных с финтехом, рассказывает. Этот слайд уже был на нескольких презентациях. Мы на основе таких символов можем каким-то образом пытаться структурировать состояние рынка, выделять там какие-то характерные состояния, характерные ситуации и делать все то, что не является предметом нашей дискуссии. Так вот вопрос, который у меня возник и который поначалу мне показался очень простым, а как мне вообще говоря вот отсортировать для начала из тех 111 метрик, которые мы считаем формально, те метрики, которые пригодны для такой символизации, которые нет. То есть, мне требовалась простая функция, которая просто говорит, что A, B, D, E и E – это бимодальные метрики, C и F – это тримодальные метрики, а там, соответственно, вот это вот, вот это вот и вот это вот, которые тут даже не перенумерованы, это там унемодальные метрики или нормальные метрики. Ну, в общем, они нам не подходят для вот такой вот процедуры синхронизации. И, в общем, все ответы, к моему удивлению, которые уважаемые участники сообщества давали группе в Телеграме, они сводились в разными формах, там были разные алгоритмы, разные подходы, разными словами это было, но что нужно просто взять, продифференцировать распределение и посчитать максимумы. Грубо говоря, сколько максимумов, сколько экстремумов на распределении тем или иным способом посчитали, вот столько у нас мод распределения есть. Я поясняю, почему это теперь не работает. Потому что количество экстремумов определяется, извините, количеством интервалов в распределении. Если мы возьмем распределение с одним интервалом, в данном случае, у нас получится, и начнем считать экстремум, у нас будет один экстремум, потому что всего один интервал. Если возьмем 2, их получится 2. А если возьмем столько, сколько сейчас, получится 1, 2, 3, 4, 5, 6, 7, 8, ну и так далее. Тут уже порядка 10-20 мод получится. Хорошо, теперь предлагаю, товарищи, это надо сгладить. Но извините, если мы возьмем, смотря какое окно сглаживания возьмем, если возьмем окно сглаживания 1000, у нас опять-таки получится одно распределение. Если возьмем окно сглаживания 0,001, у нас получится то же самое. Это уже возникает ситуация, когда мы просто играем в подбор параметров, что в данной ситуации не устраивает, потому что меня как пользователя вот этой гипотетической функции устраивает только то, что я забрасываю туда данные, поток данных, который я получаю с датчиков или откуда-то. И она мне говорит, сколько у меня распределений. А окно сглаживания и количество бинов в распределении я задавать не собираюсь, потому что я не хочу этим манипулировать. Я хочу получить просто количество распределений. Вот. Ну и, соответственно, мне пришлось сначала пойти поискать самому. Все, что я нашел на эту тему, это некоторые разные способы определения того, насколько у меня данное распределение нормальное или ненормальное, или насколько одно распределение соответствует другому распределению, и, таким образом, создав эталонное бимодальное распределение, я могу смотреть, насколько этому эталонному бимодальному распределению соответствуют другие распределения. Ну, опять-таки, здесь все начинаются какие-то игры в эталоны и сравнения с этими эталонами. Перескакивая к тому, что в итоге получилось, я скажу, что получилось просто банальная кластеризация, причем с Методом кластеризации я не заморачивался, я взял то, что лежало сверху, стандартный алгоритм K-means. Я с большой вероятностью, имея некоторый опыт в использовании различных алгоритмов кластеризации, думаю, что с другими алгоритмами получилось бы примерно то же самое. Потому что вопрос здесь не в том, какой алгоритм мы берем и чем мы его параметризуем. То ли мы параметризуем его ожидаемым числом k, то ли мы параметризуем его какими-то порогами, в каких случаях точки соединять, в каких разъединять. Важно не то, как мы параметризуем алгоритм кластеризации, а важно то, каким образом мы считаем, оцениваем количество кластеров. Вот, например, вот на этой картинке слева очевидно, что, ну, скажем так, человеку очевидно, кстати, про очевидность человеку будет, собственно, следующая часть, поэтому я забегаю чуть-чуть вперед. Вот здесь вот человеческому глазу очевидно, что кластера три. вот а вот здесь вот это менее очевидно да то есть здесь вот можно поспорить здесь кто-то скажет что кластера 2 левый правый а кто-то скажет что их все равно 3 вот но как посетить хотя по секрету значит опять таки сказать что и левая и правая картинка они сгенерированы одной той же функции которая берет 3 нормальных распределения и просто накладывают их на друг друга просто количество точек у нас разные вот Ну и вот, собственно, все, что нам нужно, это нужно каким-то образом определить количество кластеров. Ну и поскольку, опять-таки, мы работаем с функциями, а не с пространствами, поскольку оценить нам надо распределение функций, то кластеризация у нас очень простая, кластеризация у нас одномерная. То есть мы кластеризуем вообще в одномерном пространстве. Ну и что получилось, что из тех метрик кластеризации, которые я прочитал не в большой красной книжке, которую показывал Сергей, а в других источниках, метрика силуэта, которую мы использовали в нескольких проектах раньше, потом я для сравнения попробовал еще пару метрик, которые были навеяны, с одной стороны, своими собственными соображениями из того проекта, про который посвящен определенной сегментации последовательностей. Я про это, по-моему, чуть-чуть говорил. А нет, не говорил, это я буду еще говорить, если время останется. сейчас или позже вот в общем метрика которая определяет коэффициент сжатия то есть мы пытаемся посмотреть на проблему кластеризации как на проблему сжатия да потому что ну к примеру если у нас есть некоторое облако точек и мы можем сказать что вот у нас к примеру левое облако это в общем все одно и то же а правое облако это все примерно другое и И некоторым образом посчитать, а все отличия рассчитать просто расхождениями между этими реперными точками справа и лево, как-то предъявив центроиды этих мастеров. и все остальные точки описать расхождениями от координат центроида, то мы на самом деле сожмем информацию. Нам не нужно описывать каждую точку отдельности. Мы описываем два центроида и отклонение всех остальных точек от центроида. За счет этого мы экономим информацию, которая нужна для хранения исходных данных. Это подход идеи минимальной длины описания, Можно его некоторым образом притянуть за уши к принципу минимизации энергии, потому что мы хотим меньше тратить энергии на работу с одним и том же объемом информации, поэтому пытаемся все упростить. структуризовать но собственно это вообще задача скластеризация задача упрощения то что на самом деле у нас там не много точек а на самом деле всего два крайних случая датской то что больше единицы то что меньше единицы как было на исходной картинке Вот, в общем, вот, значит, соответственно, вот я попытался, собственно, написать некоторый алгоритм оценки качества сжатия за счет кластеризации, да, когда мы просто выстраиваем некоторую иерархию точек и расстояний между ними и считаем, что чем меньше у нас битов информацию ведет на запись всех этих точек вместе за счет вот группировки их и описания их координат относительно центроидов, тем лучше наша кластеризация. И поскольку этот алгоритм не очень хорошо работал, я на следующем слайде покажу, я сделал еще дополнительное дополнительное усложнение этого алгоритма, где в дополнение вот к этому нормалайза центроид-дистанц, да, кстати, центроид-дистанц, значит, вот, собственно, вот эта метрика, которую, значит, я сочинил, называется, значит, была названа центроид-дистанц, то есть расстояние центроидов нормалайзит, это, значит, мы вот общее расстояние от всех точек от центроидов плюс расстояние самих центроидов от центроидов от общего начала координат нормализуем относительно исходного расстояния всех точек от начала координата, то есть у нас как бы базовое не сжатое представление мы просто считаем координаты от центра каждой точки по отдельности, а сжимаем мы за счет того, что переносим пересчитываем на расстояние от центроидов. Да, Сергей, вы руку подняли. 

S02 [00:17:40]  : Антон Григорьевич, можно вопрос уточняющий просто задать. Я хочу понять, все-таки вы хотите отличать между собой некоторые распределения, которые являются свойствами отрезков ряда? Или вы хотите отличать фрагменты ряда как таковые, конкретное траекторное реализация между собой? 

S01 [00:17:59]  : Вот сейчас, вот тут вот так курсовая работа, которую я сейчас рассказываю, я хочу просто отсортировать. временные последовательности, допустим, цена, объем, звук, цвет, который измеряется во времени, на то, каким количеством модальности он описывается. Является распределение одномодальным? Является ли оно двумодальным? Является ли оно тримодальным? 

S02 [00:18:26]  : Распределение чего? Я беру какой-то отрезок этого ряда. Извините, что я вопрос задаю во время рассказа. я беру отрезок этого ряда, например там 200 отсчетов, и вот эти 200 отсчетов из них я строю распределение. вот это я делаю грубо говоря. теперь я хочу описать вот этот 200 отсчетный кусок ряда, понять, какой символ ему назначить. На что похоже его одночастичное распределение, правильно я понимаю? 

S01 [00:18:56]  : Да, хочу понять, можно ли все значения на данном отрезке ряда свести к какому-то ограниченному числу случаев. Ну, грубо говоря, я хочу понять, можно ли считать, что я встречаю в своей жизни много людей. Я хочу понять, на сколько категории мне их нужно делить. Могу ли я их, например, просто тупо поделить на мужчин и женщин или нет? Или мне нужно их делить на обоих? 

S02 [00:19:25]  : Нет, это понятно, Антон. Я понимаю. Вопрос вот у меня какой. Вот эти 200 значений. Если я их переставлю местами, просто как бы случайно… Ничего не меняется. Ничего не меняется. То есть, вы интересуетесь таким описанием, когда в пределах окна не меняется ничего. 

S01 [00:19:40]  : Порядок значения внутри окна не важен. То есть, про порядок мы сейчас и говорим. То есть, про порядок – это дальше. 

S02 [00:19:46]  : Все понятно. Все услышал. Спасибо. 

S01 [00:19:51]  : Итак, из следующих картинок это будет еще более понятно. В общем, поскольку с помощью вот этой штуковины, я сейчас просто ввожу некоторые термины, а результаты будут дальше. Поскольку нормализованное расстояние от центроида работало не очень хорошо, Станет видно, в чем не очень хорошо. Я ввел еще дополнительную, усложненную метрику. То есть, я стал еще это расстояние множить на количество центроидов. И это, оказалось, работает лучше. То есть, грубо говоря, задачей этой метрики являлась минимизация вот этого расстояния. То есть, чем меньше будет это нормализованное расстояние, тем лучше мы сжали. Оказалось, что на большом числе распределений эта метрика на самом деле монотонно убывает с увеличением числа кластеров. И поэтому, чтобы иметь возможность получить некоторый минимум этой метрики, я стал ее еще умножать на количество центроидов, и тогда у нее стал появляться некоторый минимум, который во многих случаях оказывался правильным. Вот. Ну и, наконец, поскольку по отдельности ни одна из этих метрик... А, значит, еще, так сказать, важно здесь что, что вот эти вот метрика 2 и 3, они хороши чем? Тем, что они работают на любом числе кластеров, начиная от единицы и кончая бесконечностью. А вот у CL-силуэта есть такая неприятная особенность. Значит, метрика силуэт... Да, ну, что такое метрика силуэт, для тех, кто не знает. Вот, то есть тут есть, конечно, странички в Википедии, но, грубо говоря, это следующее. Что, предположим... Мы там все наши точки, вот я прям вернусь, куда-то я не туда поскакал. Предположим, мы на втором распределении посчитали два кластера. Кластер А1 и кластер А2. После чего мы делаем следующее. Мы по всем точкам в каждом кластере считаем, насколько близки они к центроиду. и для них же считаем, насколько далеки они от точек соседних центроидов. Получается, мы, с одной стороны, поощряем группирование точек в кучу, а с другой стороны, мы поощряем группирование точек в кучу в пределах своего собственного кластера и поощряем заудаленность от всех точек в других кластерах. И проблема в том, что определение этой метрики таково, что для одного кластера мы получаем неопределенность. То есть эта метрика очень хорошо отличает два кластера от трех, три от семи, восемь от 164. Но для сказать, насколько хорош с точки зрения этой метрики один кластер, невозможно, потому что для одного кластера метрика не считается. Нужно как минимум два. поэтому была введена такая штуковина такая эвристика которая называется силуэт плюс силуэт коэффициент плюс которая работает следующим образом на тех данных на которые я генерил что если у нас метрика силуэт коэффициент она максимизируется то есть если центроид дистанции они должны быть минимизированы, то силуэт коэффициент максимизируется. То есть идеальная кластеризация – это когда k равняется 1. А плохая кластеризация, к своему студу я сейчас не помню, либо это 0, либо минус 1. В общем, выяснилось, что с точки зрения некоторого стороннего наблюдателя, если у нас метрика силуэта больше чем 0.65, то это совпадает с человеческой интуицией. А если метрика ниже чем 0.65, то человек не может принять решение. Человек просто смотрит на такое распределение. Вот пример того, где метрика силуэта на грани 0.65 на правом распределении. Вот тут она близка к единице. для числа 3, а ну вот, кстати, здесь вот внизу видно, мы уже забегаем вперед, вот смотрите сюда, вот внизу, вот у нас синеньким показана метрика силуэт, вот видим, где она 0,90 с чем-то, вот как раз на числе кластеров 3 метрика силуэт, она очень высока, а вот здесь вот на правой картинке метрика силуэт, она говорит, что у нас 2 кластера, но значение вот как раз где-то ну нет здесь она все-таки повыше где-то 0.75 вот то есть все-таки этот силуэт более-менее уверенно говорит что нам что нам что тут два кластера в общем, метрика, это уже не метрика, а теоретика, потому что мы соединяем две метрики была устроена так, что если у нас выше чем 0.65, то мы верим силуэту, а если меньше чем 0.65, то мы применяем вот эту третью метрику нормалайзинг центроид с дистанцией умноженной на число центроидов и принимаем решение по ней и в этом случае у нас может получиться как раз один кластер Соответственно, формулы все я не привожу, потому что формулы можно посмотреть вот здесь, в ноутбуке. Или полезть туда, если будут интересно обсудить в ходе дискуссии. Начинаем смотреть, как у нас эти метрики работают. Для очевидного случая с левой стороны. Для очевидного в кавычках. Потому что кто-то может начать спорить, что слева у нас два кластера наблюдаются. А кто-то еще начнет спорить, что слева их на самом деле не два, а три. А кто-то еще найдет два кластера вот здесь. Что мы здесь получаем? Здесь получаем, что на такой красивенькой картинке все три метрики дают число 3. То есть силуэт максимизируется на числе трех. значит вот это вот нормалайзер центроид дистанс оранжевая она на то тоже на трех минимизируется а дальше монотонно слабо возрастает вот а вот этот вот нормалайзер центроид дистанс наказанная то бишь умноженная на количество кластеров она опять таки дает четкий минимум на трех А вот в неочевидной картинке все неочевидно. То есть у нас вот эта, которая нормализирует Centroid Distance, она уходит в сторону бесконечности с каким-то невразумительным минимумом на восьмерки почему-то. Видимо, как-то она тут насчитала, что их восемь. Силуэт и продвинутая метрика, наказанная за число центроидов, между собой не соглашаются. Один считает 2, другая считает 3. Теперь давайте посмотрим, как эти метрики работали на разных распределениях. Я много разных распределений попробовал. И как это соотносится с тем, что думают по этому поводу люди. Я устроил голосование. в Google формах и попросил участников у себя на работе и в нашей группе поделать оценки. Потом там будут еще оценки, насколько оценки разных участников и разных алгоритмов бьют друг с другом. Интересные циферки получаются. Смотрим, что здесь мы видим. Мы видим, что большинство людей в случае А и Б проголосовали за одно распределение вопроса, причем звучали так. Либо мы видим вопрос, сколько вы видите распределений на каждой из картинки, либо один. кластер или вообще нету кластеров. Это вот один крайний случай. Либо два кластера, либо три кластера, либо четыре и больше. Было четыре варианта голосования. Соответственно, мы видим, что здесь люди сказали 1-1 и 2-2. Я полагаю, что большинству здесь участвующих это достаточно очевидно. Это большинство. Бары показывают, насколько это большинство убедительное. И вот мы смотрим, что человеческие оценки, они разошлись с обоими метриками, с продвинутым силуэтом. И вот это normalized distance умноженное на число центроидов. Все оказались не согласны. Идем дальше. наложение трех нормальных распределений с различными параметрами. Три распределения одни и те же, но число точек разное, поэтому эти разбросы достаточно большие. И здесь мы видим, что вот у нас силуэт и человек, они сошлись. У людей мы опять-таки видим либо единодушное большинство, либо нормальное большинство. Силуэт совпал. Ошибся чуть-чуть. Идем дальше. Те же самые три распределения, сдвинутые по-разному, наложенные друг на друга, но с большей плотностью. Опять-таки видим то, что люди… И, кстати, видим, что здесь люди гораздо более единодушны. То есть, за исключением одного случая вообще единогласное решение людьми понимается. Опять-таки силуэт совпал с людьми NCDC. NCDC здесь тоже все… У него здесь, у NCDC в таком случае тоже проблем не было. Здесь вообще все договорились. Вот две такие странные метрики. Константа и синусоиды с разными периодами. Ну вот видим, что человек опять-таки совпал с силуэтами. У людей тоже везде большинство больше 50%. Вот NCDC опять ошибся. Дальше вот еще несколько вариантов уродования синусоиды. Опять-таки у людей везде, за исключением одного случая, где 50% только набралось для решающего голоса, а в остальных случаях большинство. Люди совпали с силуэтом. NCDC врет. Ну и последнее, это я уже сгенерировал потом, я решил уже после того, как это голосование было окончено, а итоги голосования я сейчас более подробно рассмотрю. Я решил еще посмотреть, а что у нас будет с экспоненциальными и логнормальными распределениями разных форм. К сожалению, здесь у меня нет коллективной интуиции, как было на предыдущих слайдах. Здесь я применил собственную интуицию, но и моя собственная интуиция здесь мне подсказала следующее. то, как к этому относиться, что вот с правой стороны, в правом случае у нас просто имеет место одномодальное распределение, которое там ненормальное, не совсем нормальное, где-то аналог нормальный, но в общем один максимум. А вот остальные три распределения, они могут быть сегментированы по принципу основной максимум и хвост. Вот где-то здесь у нас кончается основной максимум, а правее у нас хвост. Где-то тут кончается основной максимум, а все остальное – хвост. Где-то тут кончается основной максимум, а все остальное – хвост. То есть мы таким образом можем квантифицировать эти функции. ну и вот получается что вот тут вот внизу моя интуиция опять таки силуэт дал те же самые результаты вот но ncdc как обычно разошелся Ну и теперь, подводя итог всей этой истории, чтобы понять, насколько вообще все это убедительно, я взял и посмотрел, насколько соотносятся формально между собой, с одной стороны, оценки людей, а с другой стороны оценки алгоритмов друг с другом, а с третьей стороны оценки людей с оценками алгоритмов. Для оценки как бы непротиворетивости голосование различных участников. Есть две стандартных. Они, наверное, из социологии пошли. Тут вот на них есть ссылочки. Одна называется Флейскапа, другая называется Крипендорфс Альфа. И, соответственно, они дают очень похожие результаты. Я не заглядывал внутрь, как они считаются. Это, слава богу, были питоновские функции. Вот, ну, в общем, вот что получилось. Значит, если мы посмотрим, как друг с другом соотносятся NCDC и Silhouette+, ну, получается, что, в общем, агримент есть, но средненький, да, то есть moderate agreement. В общем, коррелирует, но так себе. Соответственно, люди, друг с другом, получилось, ну, в общем, чуть-чуть лучше, да, но, в общем, тоже люди между собой плохо договариваются. Вот, NCDC, значит, супротив людей договаривается тоже, в общем, средненько, но хуже, чем вот предыдущие два случая. А вот если мы возьмем осредненную человеческую оценку и силуэта, то мы обнаружим то, что называется OS-POFIC Agreement. То есть, получается, что вот этот вот силуэт плюс, он с подавляющим большинством Нашего небольшого населения он находится практически в идеальном соответствии. Ну вот, собственно, сухой остаток такой, что в своей дальнейшей работе я использую метрику силуэт плюс для того, чтобы, во-первых, отбирать те распределения, которые годны для этой символизации или квантификации. Ну и, соответственно, число кластеров, которые в них есть, определяю по этой метрике. И, соответственно, поскольку в моей ситуации мне нужны два кластера, нужны распределения, которые сводятся к двум кластерам, я беру только те, у которых К получает, максимализируется при значении силуэт плюса 2. Единственное, там есть некоторое дополнение, это я еще дополнительно Смотрю сбалансированность кластеров. В связи со спецификой своих задач я внес некоторый дополнительный фильтр, который говорит о том, что количество точек в этих кластерах должны быть близкие. Например, если мы возьмем эти два распределения, то в левом кластере точек примерно на 10-15% меньше, чем в правом. И если бы я задал порог сбалансированности, метрики сбалансированности, которая вел выше, чем 0,55, то вот это распределение было бы отбраковано. А вот это бы прошло, потому что тут три кластера и в каждом примерно одно и то же число точек. Оно бы с числом кластеров с точки зрения метрики сбалансированности бы прошло. Вот, соответственно, значит, если есть вопросы или замечания, можно их обсудить. И принимаю ссылку на красную книжку от Сергея Терехова. Вот, если вопросов не будет, можно двинуться дальше к более сложным и интересным вещам. Да, Сергей? Так, кстати... Буквально секундочку, я сделаю стоп-ше и открою чат, чтобы было видно, что там понаписалось. Да, Сергей, я слушаю. 

S02 [00:36:07]  : Я опять немножко по постановке вопроса еще хотел уточнить. Вот когда вы предъявляли людям вот эти картинки, вы каждый вариант, то есть каждый слайд показывали сразу четыре картинки и задавали четыре одновременных вопроса. Сколько вы видите на каждой из четырех картинок кластеров? Или же вы каждому отдельному человеку случайно выбирали одну из этих картинок. Кому третьему, кому второму, кому первому. 

S01 [00:36:35]  : Он видит только одну картину. я понял вопрос смотрите там была такая там вот у меня вот сколько у меня слайдов было да столько было грубо говоря вопросов анкете то есть там за раз показывалось 4 диаграммы то есть люди сравнивали между собой 4 картинки и отвечали одновременно на 4 конкурирующих вопрос да понятно это это очень существенная вещь 

S02 [00:37:00]  : поскольку на самом деле вы интересуетесь позицией восприятия человека в отношении какого-то одного распределения, а задавали сравнительный вопрос. Естественно, люди, как всегда, как и машины, они находят шоткат решения, упрощенное решение. Они начинают смотреть, если я на эти вопросы отвечу так, а на эти так, то будет ли различительная особенность между картинками. То есть они концентрируются не на отдельном вопросе, 

S01 [00:37:29]  : Я согласен. Я со своей стороны, исходя, поскольку я в эту тему погружался сильно, я не думаю, что принципиально результаты были бы другими. 

S02 [00:37:41]  : это известно, известно. 

S01 [00:37:44]  : я же не спорю, я же не спорю. 

S02 [00:37:46]  : есть так называемый такой феномен, я вот своим студентам это рассказал следующим образом. вот я им рисовал на доске такую последовательность, сначала цифра 3, потом она все потихонечку модифицируется, модифицируется, в конце концов она превращается в цифру 5. дальше я говорю вот смотрите давайте я закрою правую половину всех картинок ответьте мне на вопрос средняя картинка это 3 или 5 если они видели все картинки слева и это были тройки они отвечали на этот вопрос естественно одним образом если я закрывал левую половину оставлял только пятерки и вот эту вот кривую как бы пятерку они отвечали на другие вопросы другим образом то есть люди переносят на самом деле вот этот вот значит все время целостно анализируют всю информацию и хотя как как бы они отвечают на вопрос о конкретном распределении в действительности они всегда отвечают на целостный вопрос. 

S01 [00:38:45]  : Вот это вот... Сергей, я же с вами не спорю, я с вами согласен, но просто исходя из моего опыта, мне кажется, мне кажется, я не говорю, что я не подтверждаю каким-то образом количество свое кажется, что результаты были бы не принципиально отличные. И, кроме того, поскольку я решал прикладную задачу, я же потом эти результаты верифицировал еще на своих 111 распределениях. Ну и, в общем, я понял, что я свою задачу прикладную решил, то есть я избавился от ручного труда. по ручной сортировке распределения, то есть алгоритм решает ровно ту же задачу, которую я хотел бы решить, но единственное, мне пришлось ввести вот эту метрику дополнительную, дополнительный фильтр на сбалансированность, то есть еще часть отсеивать, которая... то есть мне отсеять те распределения, где были две группы кластеров, ну одна из них существенно больше была. 

S02 [00:39:43]  : но это это вопрос по постановке эксперимента да просто он с точки зрения вот такой именно постановочный а есть еще и довольно обширный комментарий по поводу сетевого подхода к понятию кластера. может быть давайте я тогда его попозже задам? 

S01 [00:40:01]  : давайте мы пройдем по вопросу. Игорь Пиларов пишет камин с неоптимален. Игорь, то что камин с неоптимален это я знаю. алгоритмов кластеризации огромное число. DB scan в ноутбуке тоже есть. вот, но меня интересовал не алгоритм кластеризации, а алгоритм оценки числа кластеров. 

S00 [00:40:26]  : из моего понимания дбскан как раз как алгоритм кластера он как бы камин у тебя еще так он берет у тебя точку случайным образом потом другую там максимально неудалекую и соответственно каким образом строить центра кластеров как бы случайным образом а дб скан берет у тебя точку и ищет все близкие к ней тем самым как бы пытается проявить как раз вот эту структуру кластера вокруг и например по моим ощущениям если возьмешь множество некой даш каминзу например там задачу выявить 10 кластеров он тебе всегда найдет 10 кластеров а дбскан там 3 тебе найдет 

S01 [00:41:07]  : КАМИНС гораздо хуже, потому что на сложных наборах данных КАМИНС может находить разные числа кластеров. Например, в предыдущем проекте мы на КАМИНС очень сильно обиделись, потому что, применяя вот такую технологию, мы там использовали только силуэт, и там распределение было в страшномерном пространстве. То, что я на OpenTalks, по-моему, постерный доклад у меня был несколько годов три назад про как раз Encycloperoid Language Learning. В общем, там с каминзом мы не смогли стабилизировать именно процесс оптимизации числа кластеров. Вот потому что там какую температуру у него, какой рандом стейт в него забросишь, то он и найдет. И мы там как раз использовали агломеративную, не ДБСК, мы там использовали несколько вариантов агломеративной кластеризации, которая была несколько более устойчивая, вот как раз по той причине, на которой ты говоришь. 

S00 [00:42:06]  : Мне, знаешь, Антон, глобально непонятно, я руку не поднимал, мне глобально не очень понятно зачем вообще это делать вот но в каком-то смысле то что там то как мы с шумским там ну просто везет например та же история то есть берется некоторые векторное пространство и в нем значит там выделяется кластера и кластера эти преобразуются в символы я вот к чему представь себе что допустим у тебя ну условно ты пытаешься выявить там много кластеров а их там физически нет но их там есть сильно меньше у тебя в результате получается на выходе кластера в котором ну вот твоими термином там в одном у тебя условно будет там 200 точек а в другом там два там абсолютно как это сказать, в оригинальным образом, случайно создан. Ну и что, у тебя тем самым, у тебя потом в символной обработке, у тебя просто какие-то символы будут встречаться часто, а каких-то вообще не будет. то есть я не вижу проблемы в том чтобы зачем обязательно именно два кластера выделить или там сколько то сколько тебе интуитивно кажется правильным когда ну это потом на этапе следующей обработки в каком-то смысле это уйдет 

S01 [00:43:28]  : Смотри, значит, вот в тех последовательности, в тех распределениях, ради которых это все затевалось, опять-таки, здесь тоже можно говорить про разные проекты, давай по порядку. То есть, если взять финтеховский проект, там все как раз очень просто. Там вот все распределения, которые есть, они либо нормальные, либо бимодальные, либо не поймешь какие. Соответственно, у меня задача выкинуть все распределения, которые бимодальные, выкинуть все нормальные и не поймешь какие, оставить только бимодальные, которые либо цена вверх, либо цена вниз. Либо мы больше покупаем, либо мы больше продаем. Либо рынок медвежий, либо бычий. Все, это вот все очень просто. Это вот один случай. Второй случай, значит, из другой истории. Значит, вот, предположим, мы хотим применить метод вероятностных формальных понятий Евгения Генча и Тяева к примеру обработки, к примеру, к сегментации целевой аудитории. К примеру, да. И у нас есть некоторые атрибуты. У субъектов этой целевой аудитории, допустим, МЖУ, юноша, ребенок или студент, работающий пенсионер. Уже три значения. Дальше, например, уровень дохода высокий, низкий, средний. Дальше, вес. А вот насколько вес делить, к примеру, тут уже возникают вопросы. Расстояние до работы тоже. Как делить расстояние до работы? А для того, чтобы, например, вариантно-формальное понятие применить, мы должны осуществить дискретизацию то есть собственно квантификация то есть вот просто можно посмотреть квантификация да то есть задача квантификации задача квантификации некоторые функции как раз сводится к тому что мы эту функцию разбиваем на некоторое число нормальных конечных состояний. То есть, мы можем сказать, что мы разбиваем вес, к примеру, на 5 категорий, а можем сказать, что на 3. Ожирение – нормальный вес или недостаточный вес. А можем на 5. Рейтинг у нас может быть 3-стар рейтинг, а может быть 5-стар рейтинг. 

S00 [00:46:03]  : то есть понимаешь да верно но у тебя же дальше идет пост обратно я имею ввиду вот ну хорошо разбил ты вес на 3 там опять ты же дальше потом с этими символами оперируешь делаешь из этого какие-то выводы и видишь что там вот эти символы имеют смысл а эти просто ну там не попадаются или они как бы не имеют смысла вот тебе как бы проявление реального реального как бы содержания этого множества они то есть то что у тебя бим то есть хорошо у тебя бимодальное распределение визуально это не обозначает что у тебя ну в общем мой тезис в том что визуально как бы распределение то которое ну там очевидным образом видно глазом и которые можно сделать расстояние точно не обязательно будет иметь какую-то в дальнейшем смысловую нагрузку. У тебя она может оказаться тоньше. 

S01 [00:47:03]  : Я согласен, я согласен, но дело в том, что если ты этого не сделаешь, то ты не сможешь сделать следующий шаг. То есть, грубо говоря, в систему вероятностных формальных понятий ты не можешь ввести данные, если ты их не квантифицируешь. И теперь надо как-то квантифицировать. Соответственно, мы решаем сейчас проблему, что нам надо как-то квантифицировать эти данные, и мы хотим их квантифицировать не некоторым произвольным образом. То есть мы хотим выбрать число 5 или 3 не от балды, а исходя из некоторой логики. И вот как раз эта логика предполагается, она следует значению коэффициента силуэта, в данном случае наиболее оптимальной. 

S02 [00:47:48]  : Антон, я не могу удержаться. Если Игорь закончил, ну давай по вопросам пройдем. 

S01 [00:47:54]  : Давайте по вопросам пройдем. Посмотрим, посмотрим. Так, значит, Сергей. Один и тот же вопрос. А, ну в общем все. Там дальше только комментарии, кто как бы распределение побил. Все, можно обсуждать дальше. 

S02 [00:48:16]  : Антон, я просто хотел... Да, Сергей, пожалуйста. Поскольку очень большую тему ты поднял, понимаешь, что получается, что коротко у меня не получится. А с другой стороны, ты хотел вторую часть рассказать. Поэтому я заранее извиняюсь, если получится длинная. 

S01 [00:48:31]  : Сергей, может быть, сделать так? Вторая часть, она на самом деле не очень длинная. Вот поэтому я, может быть, расскажу. Да, я и подумал. 

S02 [00:48:38]  : Давайте сначала, чтобы… Да, да, у нас время еще есть. 

S01 [00:48:43]  : Хорошо, значит, вторая часть. Давайте я тогда перехожу к второй части. Она в этой же презентации. И я, значит, напомню, откуда взялась вторая часть. Вот это я рассказывал, не помню, то ли в конце весны, то ли в начале лета, что мы пытались найти некоторый способ, как побить некоторые последовательности на кусочки. При этом, значит, с точки зрения финтеховского проекта это могут быть некоторые последовательности на финансовом рынке. типа сначала вздули цену, а потом ее обрушили. Вздули цену, а потом обрушили. Каким-то образом выделили такие паттерны манипуляции финансовым рынком. Есть много примеров тому. У Бена Герцеля был аналогичный проект, где они пытались неисправности предсказывать тоже выявлением некоторых псевдосимвольных последовательностей, выявляя некоторые циклы. это скрыто в работе оборудования. Ну а с точки зрения Natural Language, про который рассказывал в прошлый раз, мы пытаемся просто выявить некоторые С одной стороны, на неразмеченных символьных последовательностях выделить некоторые точки прерывания каких-то закономерных последовательностей, а с другой стороны, вот эти закономерные последовательности, которые разделяются точками прерывания, выявить как некоторые слова. И я рассказывал, что если мы можем использовать, подходить к этому, использовать взаимные вероятности, условные вероятности, а можем использовать пресловутую вероятность свободы перехода и производные метрики от них, ну и показывал, если мы можем, значит, вот некоторым образом оценивать качество... из аргументации в том пространстве гиперпараметров, в котором мы оцениваем вот эти вот отклонения той или иной метрики от некоторого порогового значения. То есть у нас один параметр это порог, на котором превышение метрики говорит о том, что у нас граница между символами происходит. Например, если мы говорим, что у нас метрика выше чем 0,4, то мы ставим разрыв. Или выше чем 0,6 мы ставим разрыв. И между томом и set возникает пробел. Вот. И вот было показано, что мы можем достаточно точно строить вот эти вот токенизации, определить последовательности символов с точностью с f-мерой до 0.94, вот здесь f-мера 0.96, это на английском, там на русском там получалось, где у нас русский. на русский русский. 

S00 [00:51:55]  : Антон, извини, ты вероятно слайды, наверное, показываешь, но забыл экран. 

S01 [00:52:00]  : Да, да, да, да, извиняюсь, да, да, извиняюсь, я сейчас вернусь. 

S02 [00:52:04]  : Вот ты очень хорошо рассказываешь, мы понимаем, о чем ты говоришь. 

S01 [00:52:08]  : Я понимаю, не, ну как бы, так сказать, я же про это рассказывал, да, что вот У нас мы берем некоторый текст том, сет, джинс, райт, энн, энн, туэнн, и мы видим, что с помощью вот этой вот самой transition freedom мы очень четко выявляем границы слов. Вопрос дальше заключается только в том, какое число n у нас используется при разбиении последовательности на униграммы, биграммы, триграммы и так далее, на каком пороге значения метрики мы говорим, что все здесь у нас граница, и с какими параметрами мы собственно, когда мы тренируем модель, на каком пороге мы отсекаем статистически малозначимые переходы между N-граммами. То есть в пространстве трех метрик мы можем подобрать такие параметры, где вот с такими вот параметрами мы можем обеспечить точность текст segmentation или tokenization 0.94% или 0.96% для английского 0.6% для китайского Почему-то здесь нет русского. Не знаю, почему нет русского. Ладно. А, это сокращённое, потому что презентация, кажется. Вот. И задача-то теперь возникала такая. Ну, хорошо. У нас для того, чтобы подобрать вот эти самые оптимальные параметры в пространстве гиперпараметров, мы можем определять метрику точности, исходя из некоторого эталона. Но в реальной-то жизни у нас эталона нет. То есть если мы пытаемся каким-то образом сегментировать язык марсиан... то мы не можем подобрать параметры для автоматической сегментации языка Marcian, потому что у нас нет референсного набора. А разные языки, они все сегментируются с разными точностями. Например, для английского нужно, к примеру, 0,4 порог оптимальный. Вот здесь 0,4 везде. С 0,4 и 0,5 где-то получается оптимальное разбиение. А вот для китайского там совсем другой параметр. Ну вот, кстати, для китайского, допустим, здесь получается 0,2 оптимальная. То есть, для каждого языка, и для русского там по-другому. То есть, для каждого языка нужно находить свое значение гиперпараметров. Дальше возник вопрос, а можем ли мы каким-то образом Я сейчас просто повторяю то, что уже было. Вопрос возник – можем ли мы каким-то образом автоматически подобрать вот эти гиперпараметры, вообще не зная ничего о языке? И выводы, которые я делал в конце прошлого доклада, что надо… Где тут пишется? В общем, где-то я говорил, что как раз задача заключается в том, чтобы автоматически подобрать вот эти наборы гиперпараметров, не зная ничего о языке вообще. Ну и прежде чем двигаться дальше, очередное голосование, которое я проводил, сейчас будет небольшая интрига, которая будет раскрыта в конце. Прежде чем двинуться дальше, я сейчас предлагаю посмотреть на эти три картинки, точнее, на самом деле четыре ряда картинок. Каждый ряд картинок описывает некоторый язык, В некотором пространстве, я сейчас чуть позже скажу, какое это пространство, но это пространство даже так, это Ф-мера. в зависимости от некоторой метрики. То есть, эфмера оценивается по референсному набору. Эфмера считается, исходя из того, что у нас есть эталонный набор того, как на самом деле данный язык сегментируется. А по горизонтали разные метрики, которые претендуют на такую универсальность. Если мы эту метрику знаем, то мы можем по этой метрике найти оптимальную комбинацию гиперпараметров и тогда мы тем самым найдем меру, которая соответствует идеальному F1. например, вот для языка 2 мы видим, что если мы максимизировали вот эту вот метрику, то мы автоматически оптимизировали F1. 

S02 [00:56:55]  : одна точка на графике чему соответствует? Но примененному к какому фрагменту текста? 

S01 [00:57:06]  : Ко всему корпусу. Ко всему корпусу? Ко всему корпусу. К огромному. То есть к корпусу из много большого числа гигабайтов. По параметры корпуса я могу показать. ну давайте покажу значит что было что был понятен масштаб так параметр корпусов вот значит у нас соответственно китайский корпус это да еще нет про гигабайты это загнул вот китайский корпус это они почему вот 8 гигабайтов значит английский корпус это значит 140 мегабайтов максимальный но они объединялись в русском там почти гигабайт вот вот на таких масштабах корпус то есть вот попытки некоторым образом оценить сегментацию корпуса с некоторым набором параметров в контекста значит ground truth оно описывается одной точкой Где она? Сюда. Вот одной точкой. Соответственно... Надо убрать подсказку. Пока не увидели. Случайно. Итак. Давайте посмотрим на эти три картинки и попытаемся загадать, какой язык какой. Даю время, чтобы, если кто хочет, записали на бумажке. Да, объясню. Из этих трех языков соответственно один из них английский, другой это китайский, третий русский. Какой-то из этих языков какой. И в конце будет доклад, ответ. Все загадали? Можно двигаться дальше? Будем считать, что загадали. Итак, вот что у нас получается для английского. Какие метрики мы считаем? Так, а где я говорил про метрики? Так, где-то я говорил про метрики. А про метрики у меня нет отдельного слайда. Ай-яй-яй. Тогда давайте я расскажу, что это за метрики брались. Первая метрика, которая называется S, это антиэнтропия. Что такое антиэнтропия? У нас есть понятие шенноновской энтропии, которое оценивает предсказуемость или непредсказуемость некоторой последовательности, которая может быть обобщена, она стандартно считается на бинарный сигнал 0, 1, 1, 0, 1, 0, 0, 1, 1, 0 Можем посчитать шеннонскую энтропию. А у нас последовательность может быть бимодальная, а может быть энмодальная. Мы как раз плавно соединяемся в точке модальности. Допустим, если у нас английский алфавит, у нас там 20 с чем-то значений символа. В русском алфавите там побольше. В китайском их вообще дофига. То есть каждый символ – это одно универсальное значение, и шенновская энтропия может считаться на любом числе. И если нормализовать по значению этой энтропии на 5-ти размер словаря или на размер алфавита, то мы можем это все дело свести к единице. Ну а, соответственно, антиэнтропия это единица минус энтропия. Соответственно, вот если энтропия равняется нулю, то антиэнтропия равняется единице. Вот, значит, это вот метрика S, анти-S. Вторая метрика, вот она, C%, это коэффициент сжатия. То есть это вот на самом деле тот самый коэффициент сжатия, который для символной последовательности более адекватен, чем минимальная длина описания. Как считается коэффициент сжатия? Но, к примеру, в несжатом виде у нас корпус описывается как число символов в этом корпусе. То есть, грубо говоря, объем байтов или UTF-символов в некотором корпусе. А если мы попытаемся его сжать, то мы можем выявить некоторые закономерности и их закодировать. Допустим, если у нас, к примеру, изначально корпус состоит из четырех букв А, а потом четырех букв В, то мы говорим что у нас а и б значит 4а и 4б вот то есть мы его уже сжали там восходка получается в четыре раза да у нас всего две позиции 4а и 4б то есть у нас вместо восьми циферок получилось 4, да, в 2 раза сжали, извиняюсь. И, соответственно, мы рассматриваем проблему токенизации как проблему сжатия. То есть, если хорошо побили на символы, то хорошо сжимается. Если плохо побили на символы, то плохо сжимается. Соответственно, через С мы оцениваем коэффициент сжатия. Дальше, CSF1. это cross split f-мера что такое cross split f-мера это мы берем 8 или 10 мегабайтный корпус или гигабайтный корпус и делим его на две половинки так разделили корпус на две половинки после чего мы на каждом корпусе тренируем модельку а потом моделькой одного корпуса проверяем модельку другого корпуса и наоборот, и считаем эфмеру того и другого и средние между ними. То есть мы считаем, насколько, грубо говоря, модельно тренированная на одной половинке корпуса оценивает качество модели, посчитанной на другой половинке корпуса. Вот, ну и, соответственно, вот в чистом виде эти метрики, значит, вот они, значит, энтропия Фмера, значит, и коэффициент cross split F1, Fмера, вот, вот коэффициент сжатия, вот, а между ними там всякие производные метрики, то есть я экспериментировал, значит, пытался брать либо, так сказать, среднее между этими метриками, среднее считается как плюс, и перемножение между ними через звездочку. Ну и вот мы видим, что на английском языке наиболее хорошо работающие метрики по всем трем языкам они показаны внизу. То есть вот на английском энтропия хорошо работает, вот коэффициент сжатия ну вроде как неплохо но вот здесь вот вся какая-то ерунда вылезает вот ну и вся cross split cross split тоже кстати хорошо работает даже на самом деле лучше чем терапия потому что вот вот здесь вот как-то по моему вот здесь вот мы уходим на 0 9 а тут мы где-то выше чем 0.9 оказываемся на единичке но производная метрика в данном случае вообще работает лучше всего то есть мы по максимуму вверх влезаем и никаких тут сайд эффектов внизу у нас нет значит если возьмем русский язык На русском языке получается всё несколько хуже. Опять-таки, энтропия работает неплохо, но вот здесь вылезает какое-то безобразное брюхо, вываливается. Производная метрика такая же, как здесь. Она тоже работает здесь, но тут рассыпью какие-то точки вываливаются. Ну и для китайского всё совсем печально. вот для китайского как бы вот можно здесь вот что разглядеть некоторую опять-таки прямую но вот здесь вот вот кстати на коэффициент до коэффициент сжатия по-китайски хорошо работают вот а остальные метрики они эти развалива картинка разваливается вылезает вот какое-то такое брюхо безобразно можно еще раз попросить сколько было в каждом из корпусе значений для языков вот покажите пожалуйста запомнить просто чтобы мы в каком было больше всего? больше всего было в китайском. 

S02 [01:05:27]  : после этого следующий идет у нас русский, правильно понимаю? да. а самый маленький был английский? да. да, все понятно. спасибо. 

S01 [01:05:42]  : намек понял. вот и так дальше дальше дальше дальше да и значит да теперь значит собственно мы уже мы уже приблизились к концу вот Да. Ну и вот опять-таки, значит, конец интриги. То есть, вот что показало интернет-голосование. Интернет-голосование, что, не зная ничего о размерах корпусов, на которые проницательно указал Сергей Терехов, вот, и не имея перед глазами этой презентации, референтная аудитория безошибочно разгадала, где у нас русский, где английский, где китайский. Вот конец истории. Дальше вопрос следующий, который возникает. В пространстве гипер-гипер-параметров, где выбор метрики является гипер-гипер-параметром, находить правильные метрики, оптимизующие гиперпараметры для последующего определения языка. И можно ли или как избавиться от этих выбросов в правых нижних углах в случае русского и китайского. вот но опять-таки значит сергей значит от заранее предохищая вас ваш вопрос значит это надо будет кстати попробовать до игоря я вижу твою руку значит я наверное надо будет не забыть попробовать попробовать урезать корпуса для одних и тех же размеров и посмотреть что изменится вот и кстати сергей спасибо это наверное 

S02 [01:07:35]  : А лучше, если это будет какая-нибудь одна книга или собрание сочинений одной тоже, на трех языках. Какой-нибудь возьмите там, я не знаю, кого перевели. Достоевского перевели, скорее всего. Вот собрание Достоевского на русском, собрание Достоевского на английском, собрание Достоевского на китайском. 

S01 [01:07:51]  : Кстати, сейчас вот мы посмотрим. Сейчас я понял. Давайте сейчас мы еще посмотрим структуру этих корпусов. Секундочку. структуру этих корпусов так где структура этих корпусов то есть в китайском это были новости китайский корпус был новостной вот английский корпус английский корпус был смешанный то есть там было примерно вот да то есть здесь вот это вот литература это браун вот а тут вот кусок социал-мидия вот а в русском русском русском там по-моему книжки в основном были ну если статистически если как бы эксперимент пытаться ставить статистически то конечно надо выровнять все что можно да да согласен согласен да согласен так переходим в чат Сергей Игорь, Игорь, пардон. 

S00 [01:08:59]  : Игорь, ты руку поднимал. Да, да, Антон, я руку поднимал, мне почему-то показалось, что там ты сказал, что проголосовали правильно, а у меня почему-то показалось, что неправильно. Вот слайд предыдущий, который ты показывал. Сейчас, сейчас, давай посмотрим. Где все люди правильно, безошибочно... 

S01 [01:09:18]  : Один чиниз. Гляди, вот лендвич один. Чиниз. Один чиниз. 

S00 [01:09:25]  : Лендвич два. А где это один чиниз? Вот с правой стороны. А, вижу, да. Да-да-да. А, да, вижу. Ага. Потому что я так же определил их. И мне показалось, что вот напротив, сверху напротив чайниз написано Russian, Russian, Russian. Внизу написано чайниз, чайниз, чайниз, напротив как раз русского. То есть у тебя цифры обозначают, а не позиции. Потому что когда смотришь, напротив картинки ее голосовать. 

S01 [01:10:00]  : Да, здесь просто, здесь говорю, ну с телеграмма. В телеграмме было голосование с множеством опций и с тремя блоками. Вот. Ну, я по-другому не смог голосование в телеграмме организовать. Чтобы это было одно голосование. 

S00 [01:10:16]  : Да, понятно. Вот, ну вот Сергей Терехов всё испортил. 

S01 [01:10:18]  : Или наоборот обнадёжил. 

S02 [01:10:22]  : Будем пробовать. Да нет, я просто как ученый рассуждаю, вот и все. 

S01 [01:10:26]  : Я ничего не спорю. Я очень благодарен. Вот завтра, кстати, посмотрим англоязычная аудитория, как отреагирует. Хорошо. Других вопросов нет? Ну, если других вопросов нет, то тогда... А у Игоря рука-то поднята. А, Игорь, Игорь, рука, да. Игорь, вижу руку. Или это старая рука? 

S00 [01:11:00]  : Ну, да, это еще старая рука, я, видимо, ее не... То есть, ее надо было как-то опустить, да. 

S01 [01:11:11]  : Хорошо. Ну, коллеги, если вопросов нет, то тогда... Я желаю всем приятного вечера. Подожди, Сергей Терехов хотел высказаться. А, да-да-да, Сергей, да, я забыл, Сергей, извини, да, я расслабился. Подожди, я жду. Да-да-да, нет, на самом деле у меня есть проблема, я не тороплюсь завершить, потому что мне нужно сейчас перевести свой режим на режим времени Сиэтла, потому что завтра мне придется ложиться где-то примерно через 5 часов. 

S02 [01:11:43]  : поэтому хотя бы еще два часа продержаться готов ну хорошо тогда ну антон я хочу какие комментарии дать по поводу кластеризации ну во первых давай начнем вспомним о том откуда она взялась учет кластеризация что такое там зачем она там взялась как известно, фундаментальная задача – это задача так называемых классических эквивалентностей. если мы начинаем смотреть на самые-самые азы понятия, даже когда еще не введено понятие множества, нет никаких метрик, ничего, там есть просто, как говорит Калмагоров, запас каких-то элементов. на этом старом первичном прото таком объеме возникает вопрос о классах эквивалентности. то есть какие элементы, если мы разобьем каким-то образом вот этот запас элементов на какие-то группы, а внутри группы будем считать элементы эквивалентными, неразличимыми, то соответственно, что такое вот эти классы эквивалентности? на какие классы можно разбить? и уже там, уже на этом уровне, еще даже до топологии, до метрики, там уже возникает главный очень философский вопрос. Дело в том, что любое абсолютно разбиение на группы, на подможество, оно в каком-то смысле относительно оптимально относительно какого-то критерия. То есть, другими словами, всегда можно, за исключением каких-то вычурных случаев, но даже и вычурные случаи, они просто придут к более сложным критериям, но в любом варианте можно произвольно разбить на любые таксоны, на любые классы множество объектов, после этого апостериоре придумать критерий, в отношении которого это развеяние будет либо вообще строго оптимальным, либо будет принадлежать классу оптимальных. ну вот, например, в той задаче по распределению, которую ты показывал, представь себе, что человек, который смотрит на эти распределения, это вот не человек с улицы, который на них смотрит как на картинки художественные, а человек, который погружен в исследования вероятностные каких-то там распределений и так далее. Тогда в его восприятии колокольчатую часть каждого распределения он автоматически зрительно вычитает. Она для него не информативна и незначима, потому что он понимает, что для всего того многообразия, с которым он имеет дело, вот эти какие-то ядрышки, которые там есть, они не информативны, и он прямо глазом их автоматически вычитает из этой картинки и изучает исключительно то, что осталось. И тогда, с его точки зрения, количество кластеров на этих картинках, оно может быть совершенно разным. То есть вот там у тебя были картинки, где после... Это если вычитать вот эти участки, условно говоря, БЛШ. Вот берем из каждой из картинки, максимально аппроксимируем все, что можно, набором гауссианов и вычитаем. Получаем остаток. И после этого к этому остатку человек мысленно видит уже, как музыкант профессиональный, который по нотам слышит музыку, так и здесь человек научный вот так действует со распределениями, сразу видит другие группы, но они абсолютно не похожи на те визуальные группы, которые видят все остальные люди, которые смотрят на это дело непрофессиональным глазом. Другой человек может иметь такое же представление, но относительно интервалов постоянств. То есть он автоматически изучает вопрос не с точки зрения, какая там картинка, какую шейп имеет, а с точки зрения того, что если я предположу на этом участке постоянство значения, то какие риски отклонения от него у меня возникают. И тогда естественной кластеризацией для него будет такая сегментация, на которой вот эти участки постоянства наилучшим образом выделяются. И он может видеть тоже какой-то шейп-кластер. И так далее. То есть количество различных этих самых вариантов и их предпочтительность друг относительно другого – это вопрос, который находится вообще вне научного построения. То есть это вещь, которая становится корректной только после того, когда задан какой-то определенный критерий. Примеров масса абсолютно в прикладных областях. Например, у нас есть люди, пациенты, которые разбиваются на самые разные группы в зависимости от того, с какой нозологией, с каким заболеванием или с каким воздействием мы будем иметь дело. В одном случае мы их начинаем бить по возрастам, в другом случае по регионам, потому что регионы разнообразные, в третьем по полноте тела, в четвертом по лимфоцитам в крови и в общем как угодно. Все кластеризации правильные, но они все для разных целей. В маркетинге масса таких вариантов. Очень важный еще вариант вот этой сегментации, вот этой кластеризации, это когда человек исходит из того, на какие группы я могу влиять. То есть я разбиваю какую-то множество, допустим, если это люди или какие-то эти самые, разбиваю не с точки зрения там похоже-непохоже, а с точки зрения того, что я могу имеющимися у меня методами управления сегментировать какую-то группу и на нее прицельно влиять. тогда для меня понятие молодежь уже слишком грубое понятие потому что вот на эту категорию молодежи могу влиять там условно говоря каким-то фильмом на эту группу какими-то джинсами а это на эту группу мне надо сказать какой-то там я не знаю музыкой или какой-то там хайпом там не знаю чего а вот уже чуть постарше влиять каким-то какими-то ресторанами то есть задача сегментации чего-то на что-то на какие-то группы на однородной будучи поставлена без исходных целей, без хотя бы либо способов, либо возможности использования, либо без критериев каких-то использований и так далее, она, к сожалению, ну не хочу всяких эпитетов употреблять, потому что, ну что же мы с вами обсуждаем, но она неинтерпретируема научным методом. То есть невозможно вот взять две сегментации и научно обоснованно, абстрагируясь от любого способа, от любых критерий, указать, что вот эта сегментация лучше, а вот эта хуже. Потому что всегда найдется критерий, когда ровно наоборот эта штука есть. Дальше возникает вопрос, как это используется во всяких прикладных исследованиях, которые касаются анализа данных и так далее. Тут надо вспомнить понятие оценивания. как вы помните, базовая фундаментальная задача, которая решает сама статистики, это задача оценивания параметров, которая отличается от задачи прогнозирования. прогнозирование – это надо зафитить данные и угадать, что будет в следующий раз, а оценивание – это построение таких содержательных моделей явления, которые пытаются охватить внутреннюю суть этих явлений, а не, так сказать, угадать, как будет держок вести каким-то следующим образом. Вот на эту тему очень хорошая статья Эфронта недавно вышла. Эфронт, автор Bootstrap и вообще одна из живых легенд, 84 года человека, она такая потрясающая статья, он как раз говорит о разнице между оцениванием, прогнозированием и атрибуцией так называемой, то есть отнесением признаков к решению, но это просто вскользь. Так вот кластеры и вот эти таксономии, они используются для задачи оценивания, и в этой ситуации у них простая естественная цель. Они стремятся выделить статистически однородные группы. То есть те группы, к которым можно применить процедуру оценивания. То есть можно считать, что индивиды или объекты или буквы, в зависимости от того, какая задача решается, могут быть в пределах таксона получены независимым сэмплингом. Тогда то разбиение, которое гарантирует наилучшее выполнение гипотезы, для задачи оценивания пригодно. Идеальным, конечно, было бы распределение, когда в одном таксоне один человек, в одном кластере один объект, потому что вот он, есть один. Но эта штука для оценения не пригодна, потому что индивидуальная изменчивость, индивидуальная вариантность, гетерогенность этого объекта такова, что выводы относительно таких маленьких кластеров сделать невозможно. Если мы возьмем очень крупные группы, то тогда выводы будут более устойчивыми, но из-за слишком большого смешивания факторов трудно будет видеть печально-существенную картину. Поэтому смотрят, пытаются построить какие-то промежуточные варианты, но выбор между разными промежуточными вариантами без цели исследования такую задачу даже не ставят. То есть вот нет такого, что вы мне сейчас разбейте, а я потом применю метод и скажу, относительно чего я хотел бы это дело разбиения использовать. Вот так просто нету на статистике такого. Это вне вот этого построения. в этом смысле я просто хотел, Антон, сказать такую вещь, что вот в отношении задач, которые ты рассказываешь, в отношении, например, отдельной задачи сегментации, скажем, временных рядов, если их сегментировать в отношении рисков, то есть в отношении того, что ряд пойдет не так, как мы его прогнозируем, то тогда сегменты, это сегменты, условно говоря, равномерного риска, или там какие-то сегменты, на которых я не могу испытывать риск. тогда существенным является задача о расстройках, так называемые. то есть когда идет ряд, а дальше идет ряд. на эту тему красную книжку я еще раз покажу. тех, кто не видел в прошлый раз, я вот еще раз покажу. вот эта книжка это просто компендиум всякого рода металлопластеризации. а вот в отношении вот этих вот расстроек есть такая интересная книжечка небольшая, которую написал Ширяев. это кафедра теории вероятности московского университета но он и написал для яндекса то есть это было основано это издательство яндекса если видно здесь прямо из дательства Яндекса. Он написал это на основе своих лекций, которые он читал в Яндексе. Эта книжечка вся посвящена как раз расстройкам во временных рядах. То есть она рассматривает ситуации. Я не буду на вскидку какую-нибудь картинку из нее покажу. Например, вот такая картинка. Вот, например, сколько здесь различных диапазонов, на которые надо было бы разбить этот ряд, и в каком смысле, то есть что меняется, как меняются какие-то статистики выборочные, где проводить вот эти самые бины между рядами и так далее. 

S01 [01:21:53]  : Метод Калмогорова-Смирнова там же? 

S02 [01:21:57]  : Здесь общий подход связан с так называемым... Это же научная книжка. Как пишет Ширяев, я, к сожалению, не могу написать в таком объеме полноценную научную книжку. Но максимум, что я могу сказать, я старался написать хорошо. Он основан на проблеме продоподобия. делается гипотеза, что если провести вот здесь границу и посмотреть выборку слева и справа, то не отвергается гипотеза о том, что они разные. тогда нужно пройтись просканировать весь ряд везде предположить что может находиться какая-то граница и посмотреть где мы не отвергаем гипотезу о том что в этом месте эта граница может быть тогда оставшиеся точки это есть вот эти точки сегментации это вот как бы общий общий принцип но он предполагаем 

S01 [01:22:49]  : Я просто здесь, извиняюсь, что проиграл, я здесь хочу как раз ставить, что как раз вот на основе как раз вот этих вот вероятностных, как это называется сейчас, conditional, условных вероятностей, множественных условных вероятностей... Для языка получается не очень хорошо. 

S02 [01:23:13]  : это понятно почему потому что буквы не следует статистическим критериям извлечения из мешка они они логически между собой связаны поэтому для них гипотеза независимости даже в каждой выборке она просто не выполнена возможно она выполнена каких-то категорий из букв или каких-то конгломератов каких-то корреляций но это искусственная конструкция скорее всего человеческий язык не по такому принципу делался поэтому, скорее всего, для языковых таких последностей эта штука не очень будет работать. для финансового она, возможно, будет работать лучше, потому что если считать, что, условно говоря, все, что у тебя известно на рынке, уже отражено в цене, тогда каждый элемент близок к случайному. можно говорить о том, что вот на этом участке действовали одни факторы, они определяли там динамику, а на другом другие факторы тоже определяли динамику, и тогда можно с какой-то степенью притянутости предположений о том, что все участники рынка одинаково информированы или, по крайней мере, основные участники одинаково информированы. и что эта информация уже отражена в сегодняшней цене, тогда можно предположить, что это как бы выборки случайных. Но тоже с точностью до вот этих вот пританцеваний, прихлопываний и при приговаривании можно на эту тему говорить. Теперь в отношении того, как на практике действовать. как сегментировать какие-то отдельные перемены. вот у тебя есть куча признаков, допустим 50 признаков, 20 признаков, ты хочешь их отсегментировать. но как мы уже договорились, у тебя должен быть целевой критерий, для чего ты это хочешь сделать. и вот в отношении этого целевого критерия самый хороший и простой и ясный такой способ это дерево решений. то есть ты просто применяешь дерево решений, которое прогнозируют на выходе распределение классов, которым ты отразил свою цель. то есть постановочная часть задачи сводится к тому, что ты хочешь, чтобы на выходе классы сильно различались. тогда дерево, даже жадное дерево, которое перебирает все переменные и пытается по каждой из них найти пороги, естественным образом сегментирует эти ряды переменных так, чтобы наилучшим образом отобразить твою входную информацию на выходы. Это тоже будет не очень устойчиво, потому что зависит от выборок и так далее. Но, по крайней мере, это вещь, которая может тебя натолкнуть. Во-первых, она тебе скажет, какую переменную она хочет поделить часто, а какую не очень часто. Во-вторых, она тебе скажет, какие переменные вообще не имеет смысла делить в отношении этого критерия. И там вот эти буквы, которые ты придумаешь, они, как эти самые артикли или вставочные слова, ничего не значат. Для языка они нужны, чтобы пелось. а для смысла они для какого не нужны. и тогда по этим осям не будет никаких разделений. и в частности, если ты такого типа анализ будешь проводить, то ты обнаружишь, что в зависимости от той цели, которую ты преследуешь, даже в языке, ты получишь разные результаты. например, если ты хочешь сегментировать для перевода, технически точного перевода, у тебя будет одна какая-то сегментация. Если ты хочешь для выяснения эмоциональной окраски текста и вообще эмоционал, сентимент, анализ, у тебя будет другая сегментация. Если ты хочешь для саморизации, для выделения основных элементов текста, то у тебя будет третья сегментация. Если ты хочешь какой-то генератор стихов на основе примерно того, что написано в этом тексте, для того чтобы легко было рифмы сопрягать, у тебя будет еще какая-то четвертая сегментация. И каждая из них будет оптимальна в отношении только того, что ты хочешь с этим сделать. В этом смысле я хочу просто обратить внимание, что, несмотря на то, что задача кластеризации как бы без учителя, в том смысле, что не говорится по поводу каждого объекта, какому кластеру он принадлежит и какому объекту он принадлежит. но сам принцип вообще как бы ну вот этот вот ценностный то есть это целевая функция ее избежать невозможно то есть она она все равно должна присутствовать она она как бы она не явно там или как-то присутствует я еще хочу обратить внимание вот завершая на очень важный класс задач который почти никогда не рассматривается но традиционно в соответствии с как бы такой западной идеологии индивидуалистической Все время рассматриваются задачи отнесения конкретного объекта к какому-то классу или какого-то объекта, вот этого конкретного объекта к какому-то классу. Вот такие задачи ставятся. С учителем, без учителя, с подкреплением. Но это все про один объект. Но есть классы задач, которые очень важны. Например, здравоохранение, как мы сейчас рассматриваем в популяционной генетике, где существенным является не отнесение данного объекта к какой-то группе, А формирование группы из таких объектов, где вся группа как целостная, выявляет какие-то свойства. То есть объекты сегментируются таким образом, чтобы полученные группы максимально там, скажем, не противоречили по ресурсам. Или там, наоборот, максимально там обменивались чем-то какой-то. Или там автоматически разделились по ареалам или что-то еще. Но это свойства ни одного объекта. а это свойства группы объектов, то есть волк этот прибивается к этой стае, именно к этой стае, потому что в сочетании с другими волками этой стаи он образовывает такой конгломерат, который устойчив и более эволюционно предпочитен. То есть это я называю так называемым популяционным этим самым обучением. Так вот таксономия, то есть кластеризация вот по этому популяционному принципу, это другая тоже новая задача, которую может быть тоже очень интересно оценивать. то есть еще раз отличие не по свойствам отдельного объекта выясняется при международном классе или нет, а по объединению объектов группы выясняется вся группа как целая хорошая или плохая, или отличается от другой группы. вот это такой вот наш этот коллективистский так сказать в пику западному такой коллективистская такая постановка задачи то есть какую группу кто в деревне вместе живет хорошо что вся деревня процветала вместе вот это вот вот такая задача на другой класс задач он редко рассматривается к сожалению а он очень важен для многих приложений поэтому я не хочу тут сейчас говорить о каких-то конкретных методах кластеризации, это вторично. и действительно, dubscan выявляет кластеры сложной формы, density, связанной по плотности, а какой-то еще алгоритм выявляет какие-то компактные кластеры, какой-то алгоритм автоматически отсеивает незначимые признаки, например, я хочу обратить внимание на теорию адаптивного резонанса Стефана Гросберга и Гео Карпента. она как раз ориентирована на следующее, что несмотря на то, что у тебя очень много признаков объекта, лишь немногие из них существенны для отнесения к классу. и нужно в процессе вот этого построения найти те, которые существенны. Вот в отличие от построений, скажем, Евгения Евгеньевича, где все атрибуты являются имманентно присущими объекту, то есть считается, что есть объект, и вот этот список атрибутов, все эти атрибуты, они имманентно и одинаково ценно приписаны к этому объекту, являются его неотъемлемой частью. Это большое, очень серьезное предположение, которое на самом деле, ну тут нет Евгения Евгеньевича, можно было бы на эту тему поговорить, которые ограничиваются, с моей точки зрения, применением логических методов, потому что на практике это не выполняется. Атрибуты могут быть произвольными, не информативными, временными. То есть сейчас вроде бы этот атрибут не важен, но на самом деле он потом не важен. И более того, в разных кластерах могут быть разные атрибуты. Вот это еще очень важно. Еще один из классов алгоритмов, который я использую в своей работе, Это так называемое растущее нейронное дерево. Оно отличается тем, что кластеры каждый раз делятся на две части, но метрики в разных кластерах могут быть разными. То есть этот кластер объединяется по одному подможеству перемен, а этот кластер по другому подможеству перемен. И это отвечает физике дело, потому что эти объекты вот так надо объединять действительно, а эти объекты по-другому. То есть нет вообще единой метрики. Она вообще не едина по всей совокупности. И вот такие подходы для некоторых задач, они оказываются тоже очень интересны. То есть тема очень большая, Антон. И спасибо тебе, что ты эту тему озвучил. Я прошу прощения за сумбурность такого выступления, я его не готовил, это просто существенные вещи, которые у меня возникают в комментариях, поэтому я здесь их излагаю. Но ключевое место это то, что несмотря на то, что задача без учителя, целевое направление той задачи, его надо вложить. нет без него вопрос помешает воздух нету понятия абстрактно правильной кластеризации хотя кажется каждый раз и человеку кажется он каждый раз до вносит туда кто попроще кто посложнее какие пример кто-то сразу спектр выделяет автомат глазом выделяет частотный спектр прям частотном спектр кластеризует есть люди которые-то умеют делать если они всю жизнь работали сигналами они сразу глядя на колебания видят спектр и классифицируют частоты спектра. Всем остальным кажется, что просто какая-то каша, а для них это совершенно ясная картина, разделена на несколько классов. Поэтому нет универсального смысла. Спасибо большое. Извините за такое длинное выступление. 

S01 [01:33:00]  : Сергей, спасибо. У меня встречный вопрос. Когда вы нам расскажете о своей работе? 

S02 [01:33:05]  : Какой из тридцати? 

S01 [01:33:08]  : Та, которая имеет большее отношение к AGI. 

S02 [01:33:12]  : Нет, сейчас прямо напрямую вопросами, связанными с искусственным интеллектом, я занимаюсь только факультативно, хотя 30 лет до этого я занимался вопросами, связанными с нейронными сетями и разными способами их применения. 

S01 [01:33:26]  : Вот то, что вы сейчас говорили про кластеризацию. 

S02 [01:33:29]  : Но это мат. статистика. Это вопросы, связанные с сегментацией, связанные с задачами здравоохранения. Но я не могу дальше распространяться глубже, чем я сейчас сказал. Возможно, наступит время, когда я смогу это сделать. Ну, условно говоря, это касается вот того, что я упоминал. Допустим, вы хотите принять какое-то решение. Ну, я не знаю, вы хотите сделать так, чтобы люди носили маски, чтобы люди защищались от вот этой нашей напасти, которая сейчас плавает волнами, но никуда не делась. 

S01 [01:34:05]  : В Новосибирске сейчас что-то волна пошла, куда все болеют. 

S02 [01:34:08]  : Вот я, кстати, вопрос. воздействовать на разные группы населения, как их убедить, каким способом. Где-то надо клоуна на улицу выпустить, чтобы он в маске бегал, где-то надо строго черно-белое объявление повесить, где-то надо чуть ли не каким-то полувирусом в телеграмме распространить сообщение о том, что вот оказывается, что британские ученые показали, что без маски и так далее. Как воздействовать на разные группы населения, чтобы их убедить защищаться от напасти. Как их сегментировать и целевым образом на них воздействовать. Вот такого типа задачи они стоят. И там действительно тоже стоит задача сегментации. Вы представляете себе, какая у нее функция ценности в этой задаче? она не похожа ни на какие расстояния между признаками даже и близко и вот вот такого типа задач поэтому это проблемная тема и неспроста у калмогоров имена функциональный анализ это прямо там уже на пятой на седьмой странице класс эквивалентности самое фундаментальное базовое понятие для множества элементов с нее начинается вообще вся наука весь фундаментально все функциональные анализы начинается с нее и это это просто фундаментальные задачи просто ничего нет на базе не уже все остальное и метрики и топологии потом и статистики ну все вот а это начинается с самого начала поэтому эта задача еще огромнейшая. Ее можно рассматривать просто какую-то прикладную задачу, подобрать метод, подобрать признаки, глядишь там какой-то критерий и уже сошлось. Вот для практики. На самом деле можно в ней видеть и абсолютно фундаментальный класс. Ровно из-за того, что у нее такая сложная судьба, трудная формализованность и так далее, очень мало по-настоящему математических статей в этой области. потому что математику нечего сказать, вообще-то говоря. математик понимает, что без ценностного смысла разделения объектов на нанотаксоны, на какие-то классы эквивалентности, без внесения ценностного смысла, задача повисает в воздухе. математик не может продвигаться дальше. для него нет понятия, вот это разделение более оптимально, чем вот это. без относительно критерий. поэтому и статьи от серьезных математиков очень мало. ну вот Ширяев вынужден, потому что он там работает, его приглашают и так далее, и все-таки у него теория вероятности, это уже раздел, который там дальше функционально зайдет, но все равно он вносит туда сам ценностные какие-то вещи и в отношении этих ценностных вещей дальше излагает теорию. Без этого невозможно. Еще раз спасибо. 

S01 [01:36:43]  : Спасибо, Сергей. Все очень ценно. Несколько комментариев. Во-первых, насколько я понимаю, у Евгения Генча в его вероятностных формальных понятиях совершенно не обязательно, и даже почти наверняка это не обязательно, все атрибуты имеют равную ценность. У нас какие-то атрибуты могут быть вообще незначимыми. 

S02 [01:37:08]  : да, но просто у него выделяется то подможество, то есть у него как бы строится вот эта вероятностная мера, исходя из того, какие индуктивные функции выделяют какие группы объектов. то есть он выделяет, он строит меру через группы объектов. то есть если вот на эту группу объектов влияет, то мера или, грубо говоря, там вероятность вот этого, так сказать, правила вот этой формы на равном мере тех объектов на которые она как бы влияет. я условно утрирую. но вот это вот привязывание от атрибута к объекту. он же математик, это формальная теория. у него туда не вкладывается никакой смысл, что это веревка тоньше, а это тоньше. логики этим не занимаются. это понятно. Ну да. 

S01 [01:37:57]  : Вот отсюда как раз и возникает задача, что мы толщину веревки не можем померить. Либо она есть, либо нет. И, собственно, задача квантификации как раз в данном случае является необходимой для впихивания на множество исходных данных вот этот формальный математический метод. Вот. Это один момент. Второй момент, значит, я думаю все-таки, когда мы говорим о сегментации, я думаю, сегментация естественного языка, она инвалидна. той задачи, в общем случае, которую мы хотим решать. То есть, хотим ли мы выявлять там когнитивные дискожения, там, определять смысл текста или его тональность, так сказать, разбиение на слова, оно не будет меняться. Вот. Ну, а в остальном, в общем, да, конечно, я согласен. И, кстати, вот как раз эта привязка как классификации, так и вообще любого решения к некоторому контексту – это как раз вот то, что Дмитрий Иванович и Евгений Генчин продвигают в качестве задачного подхода. То есть, какую задачу мы решаем? В контексте этой задачи мы должны делать соответствующую классификацию. Как говорится, контекст все определяет. А вот да, Сергей, спасибо, что вы дали ссылку. Можете еще тоже на первую большую красную книжку ту самую дать? 

S02 [01:39:24]  : Я ее ввел только что. 

S01 [01:39:26]  : Ага, замечательно. Хорошо, вот. Да, и я считаю, что очень ценный комментарий. Жалко, что вы не можете поделиться больше. 

S02 [01:39:35]  : но просто надо же делиться тем что сообщество будет полезно а у меня очень такая специфика что она просто это будет как рассказ какой-то интересный там художественно это наверное да а вот с точки зрения что из этого использовать это уже вопрос такой но но я хочу сказать что если вы будете искусственный интеллект обучать вот именно таким образом дать ему цель и то он под эту цель сегментирует такие объекты, чтобы с них снять как можно больше реварда. И вот как он их сегментирует, это вопрос к этому агенту. И он может сегментировать их так, что это будет не похоже ни на какую естественную человеческую сегментацию, но будет давать больше реварда. Надо быть к этому готовым. Антон, удачи вам завтра хорошо выступить. Будем готовиться. И, коллеги, берегите себя. К сожалению, вот эта зараза никуда не делась. Она модифицировалась и затаилась, но она не исчезла. 

S01 [01:40:34]  : Хорошо. Будем беречься и готовиться. Всем спасибо за участие, Игорь. 

S02 [01:40:40]  : Пока-пока. 

S01 [01:40:41]  : Спасибо. Всем спасибо и до свидания. Счастливо. Спасибо. До свидания. 










https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
